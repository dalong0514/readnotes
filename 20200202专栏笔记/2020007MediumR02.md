# 2020007MediumR02

## 记忆时间

## 20200212SQL-and-Pandas.md

### 1. 逻辑脉络

1、read\_csv() 里，‘index\_col’ 参数设置可以把第一列作为行名，也适用于 read\_excel() 函数；2、pandas.head() 方法， df.head(10) 表示取前 10 行；3、pandas.info() 和 pandas.describe() 都是看 dataframe 的相关信息的。

### 2. 摘录及评论

Both of these tools are really useful. I recommend learning both. The combination will give you the ability to do a broad range of data analysis and manipulation efficiently. Soon, you won’t have to deal with Excel crashing on you anymore. Note: If you are working with really large data sets, you can use Dask which is built on Pandas specifically for big data. I may do a write-up on Dask basics in the future.

2『Dask 是 pandas 内置的一个工具，有机会研究一下。』

As I mentioned in my previous post, my technical experience has almost exclusively been in SQL. While SQL is awesome and can do some really cool things, it has its limitations — these limitations are in large part why I decided to acquire Data Science superpowers at Lambda School. In my previous data roles, I’ve needed to analyze data files from external sources and the tools I had access to either limited the amount of data it could process or took an exorbitant amount of time, making the task so mundane as to be almost impossible to complete thoroughly.

In all the positions I’ve held there has been a data pipeline that goes a little something like this: We received data from an outside source which needed to be analyzed for quality and understanding of ETL requirements. We used excel to do this, but anyone who has tried to use excel for large files knows what a nightmare it is. So we’d write excel macros, but since each file is so different they weren’t always very helpful. We could have spent money on tools built for data analysis, but those cost money, and are a hard sell when those paying the bill don’t directly feel or understand the pain of the data life.

Enter: Python’s Pandas library. My mind was blown. How could I not have known about this incredibly useful, FREE tool? It would have made my life so much easier! How, you ask? Well, let me tell you.

Unlike SQL, Pandas has built-in functions that help when you don’t even know what the data looks like. This is especially useful when the data is already in a file format (.csv, .txt, .tsv, etc). Pandas also allows you to work on data sets without impacting database resources. I’ll explain and show some examples of a few of the functions that I really like:

pandas.read\_csv(). First you need to pull the data into a dataframe. Once you’ve set it to a variable name (‘df’ below), you can use the other functions to analyze and manipulate the data. I used the ‘index\_col’ parameter when loading the data into a dataframe. This parameter is setting the first column (index = 0) as the row labels for the dataframe. You can find other helpful parameters here. Sometimes you have play around with parameters before it’s in the correct format. This function won’t return an output if it is set to a variable, but once set you can use the next function to view the data.

1『read\_csv() 里，‘index\_col’ 参数设置可以把第一列作为行名。经验证，也适用于 read\_excel() 函数。』

pandas.head(). The head function is really helpful in just previewing what the dataframe looks like after you have loaded it up. The default is to show the first 5 rows, but you can adjust that by typing .head(10). This is a great place to start. We can see that there is a combination of strings, ints, floats, and that some columns have NaN values.

    df.head(10)

pandas.info(). The info function will give a breakdown of the dataframe columns and how many non-null entries each have. It also tells you what the data type is for each column and how many total entries are in the dataframe.

    df.info()

pandas.describe(). The describe function is really useful to see the distribution of your data, particularly numerical fields like ints and floats. As you can see below, it returns a dataframe with the mean, min, max, standard deviation, etc for each column. In order to see all columns, not just numeric, you’ll have to use the include parameter shown below. Notice that ‘unique’, ‘top’, and ‘freq’ have been added. These are only shown for non-numeric data types and NaN for numberic. The other breakdowns from above are NaN for these new columns.

```
df.describe()
df.describe(include='all')
```

The isna function on it’s own isn’t particularly useful since it will return the whole dataframe with either False if the field is populated or True if it is a NaN or NULL value. If you include.sum() with isna(), then you’ll get an output like the one below with a count of NaN or NULL fields for each column.

    df.isna().sum()

pandas.plot(). Pandas plot function is really useful to just get a quick visualization of your data. This function uses matplotlib for visualizations, so if you are familiar with that library, this will be easy to understand. You can find all the different parameters you can use for this function here.

When to use SQL vs. Pandas. Which tool to use depends on where your data is, what you want to do with it, and what your own strengths are. If your data is already in a file format, there is no real need to use SQL for anything. If your data is coming from a database, then you should go through the following questions to understand how much you should use SQL.

How much access do you have to the DB? If you only have access to write a query and someone else runs it for you, you won’t be able to really look at your data. This is a time where you should just pull all the data you think you might be needing and export into a csv to use pandas. Another consideration: if a query you will need to run for your data is going to take up a lot of database resources and you know that your database admin wouldn’t allow it or like it, then just pull the data and do the work outside of the database with pandas. Avoid SELECT * in your queries, especially when you aren’t sure how much data could be in a table.

How are you wanting to transform/join your data? If you already know some of the things you want to do with the data like filter out certain values, join to another table, combine certain fields in a calculation or concatenation, etc, it’s going to be easier to run SQL to pull the data as you want it and then export into a csv for any data analysis or data science work.

What are your strengths? The biggest question is where your strengths are. If you feel more comfortable in one or the other, then stick with that language to do your data manipulation.

## 20200212An-Overview-of-Pythons-Datatable-package.md

### 1. 逻辑脉络

数据量很大时，用 datatable 包是个不错的选择，其对应于 R 语言里的 data.table 包。

### 2. 摘录及评论

The datatable module definitely speeds up the execution as compared to the default pandas and this definitely is a boon when working on large datasets. However, datatable lags behind pandas in terms of the functionalities. But since datatable is still undergoing active development, we might see some major additions to the library in the future.

If you are an R user, chances are that you have already been using the data.table package. Data.table is an extension of the data.frame package in R. It’s also the go-to package for R users when it comes to the fast aggregation of large data (including 100GB in RAM).

The R’s data.table package is a very versatile and a high-performance package due to its ease of use, convenience and programming speed. It is a fairly famous package in the R community with over 400k downloads per month and almost 650 CRAN and Bioconductor packages using it(source).

So, what is in it for the Python users? Well, the good news is that there also exists a Python counterpart to thedata.table package called datatable which has a clear focus on big data support, high performance, both in-memory and out-of-memory datasets, and multi-threaded algorithms. In a way, it can be called as data.table’s younger sibling.

Datatable. Modern machine learning applications need to process a humongous amount of data and generate multiple features. This is necessary in order to build models with greater accuracy. Python’s datatable module was created to address this issue. It is a toolkit for performing big data (up to 100GB) operations on a single-node machine, at the maximum possible speed. The development of datatable is sponsored by H2O.ai and the first user of datatable was Driverless.ai.

This toolkit resembles pandas very closely but is more focussed on speed and big data support. Python’s datatable also strives to achieve good user experience, helpful error messages, and a powerful API. In this article, we shall see how we can use datatable and how it scores over pandas when it comes to large datasets.

Installation. On MacOS, datatable can be easily installed with pip:

    pip install datatable

3『

作者的 GitHub：[parulnith (Parul Pandey)](https://github.com/parulnith)

[parulnith/10-Simple-hacks-to-speed-up-your-Data-Analysis-in-Python: Some useful Tips and Tricks to speed up the data analysis process in Python.](https://github.com/parulnith/10-Simple-hacks-to-speed-up-your-Data-Analysis-in-Python)

』

Reading the Data. The dataset being used has been taken from Kaggle and belongs to the Lending Club Loan Data Dataset. The dataset consists of complete loan data for all loans issued through the 2007–2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file consists of 2.26 Million rows and 145 columns. The data size is ideal to demonstrate the capabilities of the datatable library.

```
# Importing necessary Libraries

import numpy as np
import pandas as pd
import datatable as dt
```

Let’s load in the data into the Frame object. The fundamental unit of analysis in datatable is a Frame. It is the same notion as a pandas DataFrame or SQL table: data arranged in a two-dimensional array with rows and columns.

With datatable. The fread() function above is both powerful and extremely fast. It can automatically detect and parse parameters for the majority of text files, load data from .zip archives or URLs, read Excel files, and much more. Additionally, the datatable parser : 1) Can automatically detect separators, headers, column types, quoting rules, etc. 2) Can read data from multiple sources including file, URL, shell, raw text, archives and glob. 3) Provides multi-threaded file reading for maximum speed. 4) Includes a progress indicator when reading large files. 5) Can read both RFC4180-compliant and non-compliant files.

With pandas. Now, let us calculate the time taken by pandas to read the same file. The results show that datatable clearly outperforms pandas when reading large datasets. Whereas pandas take more than a minute, datatable only takes seconds for the same.

Frame Conversion. The existing Frame can also be converted into a numpy or pandas dataframe as follows:

```
numpy_df = datatable_df.to_numpy()
pandas_df = datatable_df.to_pandas()
```

Let’s convert our existing frame into a pandas dataframe object and compare the time taken. It appears that reading a file as a datatable frame and then converting it to pandas dataframe takes less time than reading through pandas dataframe. Thus, it might be a good idea to import a large data file through datatable and then convert it to pandas dataframe.

1『不错的办法，处理大数据时先用 datatable 导入，接着转成 pandas 的 dataframe，这样比直接从 pandas 导入数据还要快。』

Basic Frame Properties. Let’s look at some of the basic properties of a datatable frame which are similar to the pandas’ properties:

We can also use the head command to output the top ‘n’ rows.

    datatable_df.head(10)

The colour signifies the datatype where red denotes string, green denotes int and blue stands for float.

Summary Statistics. Calculating the summary stats in pandas is a memory consuming process but not anymore with datatable. We can compute the following per-column summary stats using datatable. Let’s calculate the mean of the columns using both datatable and pandas to measure the time difference. The above command cannot be completed in pandas as it starts throwing memory error.

Data Manipulation. Data Tables like dataframes are columnar data structures. In datatable, the primary vehicle for all these operations is the square-bracket notation inspired by traditional matrix indexing but with more functionalities. The same DT[i, j] notation is used in mathematics when indexing matrices, in C/C++, in R, in pandas, in numpy, etc. Let’s see how we can perform common data manipulation activities using datatable:

Selecting Subsets of Rows/Columns. The following code selects all rows and the funded_amnt column from the dataset.

    datatable_df[:,'funded_amnt']

Here is how we can select the first 5 rows and 3 columns

    datatable_df[:5,:3]

Sorting the Frame. Sorting the frame by a particular column can be accomplished by datatable as follows. Notice the substantial time difference between datable and pandas.

Deleting Rows/Columns. Here is how we can delete the column named member_id:

    del datatable_df[:, 'member_id']

GroupBy. Just like in pandas, datatable also has the groupby functionalities. Let’s see how we can get the mean of funded\_amount column grouped by the grade column. What does .f stand for? f stands for frame proxy, and provides a simple way to refer to the Frame that we are currently operating upon. In the case of our example, dt.f simply stands for dt\_df.

Filtering Rows. The syntax for filtering rows is pretty similar to that of GroupBy. Let us filter those rows of loan\_amntfor which the values of loan\_amnt are greater than funded\_amnt.

    datatable_df[dt.f.loan_amnt>dt.f.funded_amnt,"loan_amnt"]

Saving the Frame. It is also possible to write the Frame’s content into a csv file so that it can be used in future.

    datatable_df.to_csv('output.csv')

For more data manipulation functions, refer to the documentation page.

## 20200217A-Data-Scientists-Guide-to-Python-Modules-and-Packages.md

### 1. 逻辑脉络

Python 数据分析时要有做自定义包的习惯。

### 2. 摘录及评论

How to create, import and work with your own python packages.

Data science code is often very linear. Extract some data from a source, apply a series of transformations and then perform some analysis, calculations or train a model. However, for readability, efficiency and repeatability it can be useful to modularise and package your code for reuse and collaboration.

When I first started learning to program for data science I found it very difficult to locate simple explanations and tutorials for creating modules and packages, particularly for data science projects. In this post, I will give a very simple tutorial on how to create and use your own packages and modules for data science and machine learning projects.

Throughout this article, I will be using the adults dataset which can be downloaded from the UCI machine learning repository. This is a dataset commonly used to build a classification machine learning model with the goal being to predict if a given adult will earn over \$50k per year or not.

3『[UCI Machine Learning Repository: Adult Data Set](https://archive.ics.uci.edu/ml/datasets/Adult)』

### 01. A data science use case for modules

A python module is simply a set of python operations, often functions, placed in a single file with a .py extension. This file can then imported into a Jupyter notebook, IPython shell or into another module for use in your project. In the below code I have read in the CSV file I will be working with using pandas.

```
import pandas as pd
data = pd.read_csv('adults_data.csv')
data.head()
```

This dataset contains a lot of categorical features. If we were planning to use this to train a machine learning model we would first need to perform some pre-processing. Having analysed this data I have determined that I will take the following steps to preprocess the data before training a model. 1) One-hot encode the following columns: workclass, marital-status, relationship, race and gender. 2) Take the most commonly occurring values, group remaining values as ‘others’ and one-hot encode the resulting feature. This will need to be performed for the following columns as they have a large number of unique values: education, occupation, native-country. 3) Scale the remaining numerical values.

The code that we will need to write to perform these tasks will be quite large. Additionally, these are all tasks that we may want to perform more than once. To make our code more readable and to easily be able to re-use it we can write a series of functions into a separate file that can be imported for use in our notebook — a module.

### 02. Writing a module

To create a module you will need to first make a new blank text file and save it with the .py extension. I will name it preprocessing.py. Let’s write our first preprocessing function in this file and test importing and using it in a Jupyter Notebook. I have written the following code at the top of the preprocessing.py file. It is good practice to annotate the code to make it more readable. I have added some notes to the function in the code below.

```
def one_hot_encoder(df, column_list):
    """Takes in a dataframe and a list of columns
    for pre-processing via one hot encoding"""
    df_to_encode = df[column_list]
    df = pd.get_dummies(df_to_encode)
    return df
```

To import this module into a Jupyter Notebook we simply write the following.

    import preprocessing as pr

IPython has a handy magic extension known as autoreload. If you add the following code before the import then if you make any changes to the module file they will automatically be reflected in the notebook.

```
%load_ext autoreload
%autoreload 2

import preprocessing as pr
```

Let’s test using it to preprocess some data.

```
cols = ['workclass', 'marital-status', 'relationship', 'race', 'gender']
one_hot_df = pr.one_hot_encoder(data, cols)
```

Now we will add the remaining preprocessing functions to our preprocessing.py file.

```
def one_hot_encoder(df, column_list):
    """Takes in a dataframe and a list of columns
    for pre-processing via one hot encoding returns
    a dataframe of one hot encoded values"""
    df_to_encode = df[column_list]
    df = pd.get_dummies(df_to_encode)
    return df

def reduce_uniques(df, column_threshold_dict):
    """Takes in a dataframe and a dictionary consisting
    of column name : value count threshold returns the original
    dataframe"""
    for key, value in column_threshold_dict.items():
            counts = df[key].value_counts()
            others = set(counts[counts < value].index)
            df[key] = df[key].replace(list(others), 'Others')
            return df

def scale_data(df, column_list):
    """Takes in a dataframe and a list of column names to transform
     returns a dataframe of scaled values"""
    df_to_scale = df[column_list]
    x = df_to_scale.values
    min_max_scaler = preprocessing.MinMaxScaler()
    x_scaled = min_max_scaler.fit_transform(x)
    df_to_scale = pd.DataFrame(x_scaled, columns=df_to_scale.columns)
    return df_to_scale
```

If we go back to the notebook we can use all these functions to transform the data.

```
import pandas as pd
from sklearn import preprocessing

%load_ext autoreload
%autoreload 2

import preprocessing as pr

one_hot_list = ['workclass', 'marital-status', 'relationship', 'race', 'gender']
reduce_uniques_dict = {'education' : 1000,'occupation' : 3000, 'native-country' : 100}
scale_data_list = data.select_dtypes(include=['int64', 'float64']).columns

one_hot_enc_df = pr.one_hot_encoder(data, one_hot_list)
reduce_uniques_df = pr.reduce_uniques(data, reduce_uniques_dict)
reduce_uniques_df = pr.one_hot_encoder(data, reduce_uniques_dict.keys())
scale_data_df = pr.scale_data(data, scale_data_list)

final_data = pd.concat([one_hot_enc_df, reduce_uniques_df, scale_data_df], axis=1)
final_data.dtypes
```

We now have an entirely numerical dataset which is suitable for training a machine learning model.

### 03. Packages

When working on a machine learning project it can often be desirable or sometimes necessary to create several related modules and package them so that they can be installed and used together. For example, in my work, I am currently using a Google Cloud deployment solution for machine learning models called AI Platform. This tool requires that you package up preprocessing, training and prediction steps in the machine learning model to upload and install on the platform to deploy the final model.

A python package is a directory containing modules, files and subdirectories. The directory needs to contain a file called \__init__.py. This file indicates that the directory it is contained within should be treated as a package and specifies the modules and functions that should be imported. 

1『解释了包的本质，不过还没太弄明白。』

We are going to create a package for all the steps in our preprocessing pipeline. The contents of the \__init__.py file are as follows.

```
from .preprocessing import one_hot_encoder
from .preprocessing import reduce_uniques
from .preprocessing import scale_data
from .makedata import preprocess_data
```

Modules within the same package can be imported for use within another module. We are going to add another module to our directory called makedata.py that uses the preprocessing.py module to execute the data transformations and then export the final dataset as a CSV file for later use.

```
import preprocessing as pr
import pandas as pd

def preprocess_data(df, one_hot_list, reduce_uniques_dict, scale_data_list, output_filename):
    one_hot_enc_df = pr.one_hot_encoder(data, one_hot_list)
    reduce_uniques_df = pr.reduce_uniques(data, reduce_uniques_dict)
    reduce_uniques_df = pr.one_hot_encoder(data, reduce_uniques_dict.keys())
    scale_data_df = pr.scale_data(data, scale_data_list)
    final_data = pd.concat([one_hot_enc_df, reduce_uniques_df, scale_data_df], axis=1)
    final_data.to_csv(output_filename)
```

The new directory now looks like this.

```
processing
    _init_.py
    makedata.py
    preprocessing.py
```

Now we can go back to the Jupyter Notebook and use this package to execute all the preprocessing. Our code is now very simple and clean.

```
import pandas as pd
%load_ext autoreload
%autoreload 2
import preprocessing as pr

data = pd.read_csv('adults_data.csv')
one_hot_list = ['workclass', 'marital-status', 'relationship', 'race', 'gender']
reduce_uniques_dict = {'education' : 1000, 'occupation' : 3000, 'native-country' : 100}
scale_data_list = data.select_dtypes(include=['int64', 'float64']).columns
pr.preprocess_data(data, one_hot_list, reduce_uniques_dict,scale_data_list, 'final_data.csv')
```

In our current working directory, there will now be a new CSV file called final_data.csv which contains the preprocessed dataset. Let’s read this back in and inspect a few rows to ensure that our package has performed as expected.

```
data_ = pd.read_csv('final_data.csv')
data.head()
```

In this article I have illustrated how using modules and packages for data science and machine learning projects can make your code more readable and reproducible. In my quest to find simple explanations of these processes I found this blog post very useful, and a nice walk through in the python like you mean it project.

## 20200218Soup-of-the-Day.md

Though there are many thousands of lovely clean datasets available out there for a data scientist’s delectation (mostly on Kaggle), you’re always going to have those pesky hypotheses that stay out of their scope. Creating the dataset you do need from scratch is a potentially daunting prospect — even if you are able to see the data on a webpage, actually getting this into a format ready for analysis could involve a lot of manual work.

Note — this is the first in a series of posts. Here, we will cover the mechanics of webscraping Metacritic with the Beautiful Soup library, step by step. Subsequent blogs will dive into the analysis we conduct off the back of it.

### Step 1. Understanding Just Enough About HTML

HyperText Markup Language (HTML) is the code that tells a web browser what information to display on the page. Importantly, it does not say much about how that information should be displayed (websites typically combine two sets of code, HTML and Cascading Style Sheets (CSS) to render a page, with CSS responsible for its look and feel).

This is important, because it’s the webpage’s information that we are interested in, and we know that this information is going to be stored somewhere in the HTML code.Helpfully, most browsers provide an easy way to see which bit of the HTML code refers to specific elements on the page.

The HTML code itself might look intimidating, but the structure is simpler than it seems. Content is typically contained within ‘tags’, tags being the things inside the \<> brackets. For example, when declaring a paragraph of text, the HTML code might look like this:

HTML has different types of tags that do different things — the most common ones (as you can see in the screenshots above) tend to be:

Headers: \<h1> \</h1> (note, we can have h1, h2, … , h6, depending on the desired hierarchy of headers).

Ordered Lists: \<li> \</li>

Unordered Lists: \<ul> \</ul> (i.e. bullet point style lists)

Hyperlinks: \<a href= “EXAMPLE_URL”> </a>

Span: \<span> </span> (used to identify substrings within paragraphs — this is useful if you want your companion CSS code to format only certain words in a sentence)

Semantic Elements: \<div> \</div>

Note — Semantic Elements are a bit of a catch-all category, and ‘div’ is a term you’ll see a lot in many blocks of HTML. A typical use of these ‘div’ tags is to create ‘Sub-Elements’, which can contain lists, other headers, and further sub-elements.

It’s like the folder tree structure in your computer’s file explorer — the root ‘My Documents’ folder might contain documents and files, but it might also contain other folders. These folders may contain further folders in turn.

So if we want to isolate the ‘release date’ for a particular album on this Metacritic page, we can see that this is contained within several nested sub-elements. Ultimately, we can find the release date in the code itself.

As we can see, many tags also have other attributes to further differentiate themselves. These are typically labelled as the element’s ‘id’ or ‘class’. These will be crucial later, when we come to extract data from the HTML code.

### Step 2. Extracting HTML Into a Jupyter Notebook

We can see the HTML in the browser — now we need to get it into a Jupyter Notebook (or equivalent), so that we can analyse it. To do this, we use the python ‘Requests’ library. The syntax for Requests is quite straight forward. Let’s say we want to get the HTML from Metacritic that we looked at above. We can use the .get( ) method:

```
import requests
url = https://www.metacritic.com/browse/albums/artist/a?num_items=100
page = requests.get(url)
```

### Step 3. Reading the HTML Soup

Once we have the ‘request’ object (‘page’), we can use the html.parser feature in the Beautiful Soup library to make sense of its contents.

```
from bs4 import BeautifulSoup
soup = BeautifulSoup(page.content, ‘html.parser’)
```

If we actually call the ‘soup’ variable, however, we see that things are still a bit messy (certainly far from beautiful).

This is where the browser’s inspector comes in handy. Since we probably have a good idea as to what information we want to get from the page, we are able to find the corresponding HTML in the inspector.
So if I want to create a list of all the artist names on this page, I can ‘inspect element’ on the first such name, and see how the information is stored in the HTML.

We can see that the artist name ‘A Camp’, is stored in an ‘ordered list’ (denoted by \<li> tags), with the class “stat product_artist”. Given the structure of the site, we can guess that all the artist names are going to be stored in the same way (though we can of course check this by inspecting elements, as we did with ‘A Camp’). We use Beautiful Soup’s .findAll( ) method to find all instances of ordered lists, with the class “stat product_artist”, specifying these two characteristics as separate arguments.

    artistHTML = soup.findAll(‘li’,class_=”stat product_artist”)

That this gives us an object that is of type ‘bs4.element.ResultSet’. Looking at this object more closely, we can see that this looks a little bit like a python list. In particular, we can index this ResultSet object to isolate the different artist names (we can use the ‘len’ function to check how many we have, in this case 99).

Note that these elements in the ‘bs4.element.ResultSet’ object, are themselves objects with type ‘bs4.element.tag’. This means that they come with a some new methods, which can help us extract the information that we need.

We should note at this point — this was not the only way to get at the artist name information. We could have also gone a layer deeper in the HTML, and tried soup.findAll(‘span’, class_= “data”). However, this method adds a layer of complexity — it turns out that there are three different types of information about each album stored using such tags; artist name, release date, and metascore. This is manageable, but the approach described will probably turn out to be more straight forward.

### Step 4. Extracting Data From The HTML Tags

Getting at information stored in the tags (and ensuring that it’s returned to us in the right format) is not always the most straight forward task, and can require a bit of trial and error. There are a few different methods I tend to go to first.

.get_text( ) tends to be the most reliable. Sometimes we will need to make edits to the strings that it outputs to get them into the desired form. We should therefore try the method over different elements to ensure that we can make the same edit across all of them. We can see here that we need to get rid of the substrings either side of the artist name. We could use a regex method for this, however, we can be lazy and chain a couple of .replace( ) methods instead:

    artistHTML[0].get_text().replace(‘\n’, ‘’).replace(‘Artist:’, ‘’)

We can now iterate through the different elements in artistHTML, extract the artist names, and put them into a list. We can either do this using a loop:

```
artists = []
for element in artistHTML:
    artist = element.get_text()
    artist_final = artist.replace(‘\n’,’’).replace(‘Artist:’,’’)
    artists.append(artist_final)
```

… or a list comprehension, which I find to be much tidier:

```
artists = [element.get_text().replace(‘\n’,’’).replace(‘Artist:’,’’) for element in artistHTML]
```

These lists can then be stored as they are, or one can, for example, put them into a Pandas dataframe. As mentioned previously, there are many, many different methods for extracting data from the soup. As with most things ‘coding’, there is rarely one right way to do something — the Beautiful Soup documentation is a good place to see the different element methods in action.

### Step 5. Getting All The Data…

Once we have extracted all the data we want from a webpage. The next task, depending on the structure of the site, is to extract data from a new page.

If we wanted all of the artist names from Metacritic, for example, we would need to go to page 2, then page 3, and so on (and that’s just for the artists beginning with ‘A’!). Obviously, we would want such a process to be automated with code. There are a couple of ways of going about this.

#### 1. Second Guessing the URL

The URL we just scraped was:

https://www.metacritic.com/browse/albums/artist/a?num_items=100&page=0

The if we follow the link to page 2, then to page 3, we see the URLs are:

https://www.metacritic.com/browse/albums/artist/a?num_items=100&page=1

https://www.metacritic.com/browse/albums/artist/a?num_items=100&page=2

Clearly, we have a pattern — the number at the end of the URL is the page number minus one. This means that we can quite simply loop through the URLs, scraping each in turn.

First, we need to establish how many pages there are for a given letter (there are 11 pages for artists beginning with ‘A’, but clearly there’ll be more or less for artists beginning with other letters).

Given that this number is displayed on the page itself, we can find it in the HTML. We can use the process described in steps 2, 3, and 4 to isolate this number, and assign it to a variable in python (let’s call it ‘pages’).

```
for page_number in range(pages):
    url = f'https://www.metacritic.com/browse/albums/artist/a?num_items=100&page={page_number}'
    page = requests.get(url)
# We then include the scraping code inside the loop, ensuring that data from each new URL is appended to data collected from the previous pages - either in lists, or Pandas dataframes
```

#### 2. Finding the next URL in the HTML itself

Second guessing the URL is sufficient on sites that are nicely organised (like Metacritic), however for other less tidy sites this approach may not work. However, given that we have a ‘next’ page button on the page, we can find it, and its corresponding hyperlink, in the HTML.

Given we’re after a hyperlink, we should be on the lookout for \<a> tags. We can use the soup.findAll( ) method as in step 3. Note, even if there is only one such element, the findAll method is still a list-like object, which we need to index. Given that the URL is in the tag itself, it is unlikely that our trusty .get_text( ) method is going to work here. Instead, we can look at at the element’s attributes, using the ‘.attrs’ property.

```
Nexturl[0].attrs

Nexturl[0].attrs['href']

```

Note how the attrs object looks a lot like a dictionary. We can hence look up the desired value by using the ‘href’ key. We can then use an f string to output the full URL.

This URL can be passed into requests (see step 1), and the resulting page can be scraped. By including these lines of code as part of a loop, we can get our scraping code to go automatically through each page in turn.

Webscraping is an incredibly powerful technique, and a great string to any data scientist’s bow. The above guide is written to be a starting point, though there are many other complimentary techniques (and, indeed, other scraping libraries other than Beautiful Soup) out there.

## 20200218Elevate-Your-Webscraping-With-Splinter.md

In a previous blog, I explored the basics of webscraping using a combination of two packages; Requests, which fetched a website’s HTML and BeautifulSoup4, which makes sense of that HTML. 

These packages are a great introduction to webscraping, but Requests has limitations, especially when the site you want to scrape requires a lot of user interaction. As a reminder — Requests is a Python package that takes a URL as an argument, and returns the HTML that is immediately available when that URL is first followed. Thus, if your target website only loads content after certain interactions (such as scrolling to the bottom of the page or clicking a button) then Requests isn’t going to be a suitable solution.

For the FPL project that I’m documenting in my ‘On Target’ series, I needed to scrape match data from the Premier League website’s results pages. These presented some challenges to Requests. For example, I wanted data from the ‘Line-ups’ and ‘Stats’ tabs on each match page, however, these do not load new webpages. Rather, they trigger JavaScript events that load new HTML within the same page.

I also wanted to scrape the match commentary, which can be accessed by scrolling down the page…

… but is only fully loaded when a user continues to scroll down (similar to ‘infinite scrolling’ sites such as Facebook and Reddit).

### 01. Scraping With Splinter

Instead of scraping with Requests, we can use a Python package called Splinter. Splinter is an abstraction layer on top of other browser automation tools such as Selenium, which keeps it nice and user friendly. Moreover, once we scrape the HTML with Splinter, BeautifulSoup4 can extract our data from it in exactly the same way that it would if we were using Requests.

To get started, we need to download the appropriate browser ‘driver’ for the browser we want to use. For Firefox, this means using Mozilla’s geckodriver (note — Splinter uses Firefox by default). If you’re using Chrome, then you need chromedriver. You will also need to pip install selenium using your machine’s terminal (full details on why are included in the Splinter documentation):

    $ [sudo] pip install selenium

3『[Chrome WebDriver — Splinter 0.13.0 documentation](https://splinter.readthedocs.io/en/latest/drivers/chrome.html)』

It’s also worth noting that the driver file itself (i.e. geckodriver or chromedriver) needs to be included in the root of your repo (this is not an obvious requirement in either the Splinter or Firefox documentation!)

Splinter works by instantiating a ‘browser’ object (it literally launches a new browser window on your desktop if you want it to). We can then interact with that browser by running methods on it in, say, Jupyter Notebook.

```
from splinter import Browser

#State the location of your driver
executable_path = {"executable_path": "/Users/Callum/Downloads/geckodriver"}
#Instantiate a browser object as follows...
#Pass 'headless=False' to make Firefox launch a visible window
browser = Browser("firefox", **executable_path, headless=False)
```

This launches an empty browser window. Note the orange striped address bar, which tells us that it’s being controlled by our Python file. We can now control this browser with some Python commands. Firstly, let’s make it visit a webpage…

```
match_url = 'https://www.premierleague.com/match/46862'
browser.visit(match_url)
```

Looking at the browser window on our desktop, we can see that this has worked!

### 02. Interacting With Elements

Now we have the website loaded, let’s solve the two issues that Requests couldn’t handle. First, we wanted to click on the ‘Line-ups’ and ‘Stats’ tabs. To do this, we first need to see how these elements are referred to in the HTML. Right-click on the button, and select ‘Inspect Element’. We can find the appropriate HTML in the inspector.

So the button is an ordered list element \<li> with class “matchCentreSquadLabelContainer”. Splinter can find this element with the .find_by_tag() method. We can then make it ‘click’ on this button with the .click() method.

```
target = ‘li[class="matchCentreSquadLabelContainer"]’
browser.find_by_tag(target).click()
```

Note — there are six different things that Splinter can use to find an element. These are fully documented here, but include the option to find by element ID, by CSS, or by value.

3『[Finding elements — Splinter 0.13.0 documentation](https://splinter.readthedocs.io/en/latest/finding.html)』

Now that the browser has ‘clicked’ on the tab that we wanted, we can use BeautifulSoup4 to get and store the HTML.

```
from bs4 import BeautifulSoup
html = BeautifulSoup(browser.html, 'html.parser')
```

We can then extract the text information that we want using the same techniques documented in my previous webscraping blog. Of course, if we also wanted to click on the ‘Stats’ tab, we do the same process — check how the tab is called using the Inspect Element tool, then pass this in a .find_by_tag() method, before using BeautifulSoup to extract the HTML.

### 03. Combating the Infinite Scroll Problem

Although Splinter’s Browser class doesn’t come with an in-built ‘scroll’ method, it has an arguably more powerful feature that lets us do this — namely, we can use the .execute_script() method to run JavaScript.

To make the browser scroll to the bottom of the currently loaded page, we use the JavaScript scrollTo() method. This takes x and y positions as its arguments, so to get to the end of the page we pass ‘document.body.scrollHeight’ as the y position (we don’t need to change the x position since we’re only scrolling in a vertical direction). Thus, we run:

```
browser.execute_script("window.scrollTo(0, document.body.scrollHeight);")
```

It may be the case that we need to keep scrolling before everything we need has loaded. And since different matches will have different numbers of events, we will need to do different amounts of scrolling on each page. We thus need some kind of condition that tells our code to stop scrolling. Happily, commentary on the Premier League website always starts with the phrase “Lineups are announced and players are warming up”.

Therefore, if the HTML that we scrape includes this phrase, then we know that we’ve got all the commentary we need and we can stop scrolling. This is a task that’s ripe for a ‘while loop’.

```
#Declare the JavaScript that scrolls to the end of the page...
scrollJS = "window.scrollTo(0, document.body.scrollHeight);"

#...and a variable that signifies the loop's end-condition
condition = "Lineups are announced and players are warming up."

#Instantiate a 'first_content' variable (an empty string for now)
first_content = ""

#Create a while loop that runs when 'first content'
#is not equal to our break condition
while first_content != condition:

    #Scroll to the bottom of the page
    browser.execute_script(scrollJS)
    
    #Use BS4 to get the HTML
    soup = BeautifulSoup(browser.html, 'html.parser')
    
    #Store the first line of commentary displayed on the page as-is
    first_content = soup.findAll('div',class_="innerContent")[-1].get_text()
    
    #Scroll down again, and run the loop again if we
    #haven't reached the line "Lineups are announced..."
    browser.execute_script(scrollJS)
    
#Store the soup that, thanks to the while loop, will
#definitely contain all of the commentary
HTML = soup
```

We can then refactor all of the above to create a function that will take in the URL for a match, and return all the HTML from the match commentary, as well as the data from the Line-up and Stats tabs. Thus, we can automate the scraping of all the match pages from the season so far. Of course, how we organise, store, and manipulate this data is another task entirely (and, indeed, the subject of another upcoming blog).

It’s also worth mentioning that this is very much the tip of the iceberg as far as Splinter functionality goes. Other interesting use cases include:

1. Entering text fields([Interacting with elements in the page — Splinter 0.13.0 documentation](https://splinter.readthedocs.io/en/latest/elements-in-the-page.html#interacting-with-forms))

2. Manipulating cookies([Cookies manipulation — Splinter 0.13.0 documentation](https://splinter.readthedocs.io/en/latest/cookies.html))

3. Taking screenshots([Take screenshot — Splinter 0.13.0 documentation](https://splinter.readthedocs.io/en/latest/screenshot.html))

4. Dragging and dropping elements([Mouse interactions — Splinter 0.13.0 documentation](https://splinter.readthedocs.io/en/latest/mouse-interaction.html#drag-and-drop))

5. And, of course, anything that you can do with standard JavaScript.

At any rate, Splinter is a great little Python package that will help you take your webscraping to the next level! Have a play around with it, and see what you think.

This is the latest post in my blog series ‘On Target’, in which I’ll be attempting to build out a model for ‘Moneyballing’ Fantasy Premier League. I’d love to hear any comments about the blog, or any of the concepts that the piece touches on. Feel free to leave a message below, or reach out to me through [(10) Callum Ballard | LinkedIn](https://www.linkedin.com/in/callum-ballard/).

2『LinkedIn 上已关注作者。作者 medium 上的博客地址：[Callum Ballard – Medium](https://medium.com/@callumballard)，尝试爬取里面的文章。』