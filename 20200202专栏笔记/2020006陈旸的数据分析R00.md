# 2020006陈旸的数据分析R00

## 记忆时间

## 卡片

### 0101. 主题卡——思维和实战经验

如果说工作是公司的事情，那么思维和实战经验的积累则是自己的事情。在思考和实战这条道路上，投入越多，收获就会越多。

这个连接的过程，也是我们从「思维」到「工具」再到「实践」的一个突破过程。如果说重要性，一定是「思维」最重要，因为思维是底层逻辑和框架，可以让我们一通百通，举一反三，但是思维修炼也是最难的。所以，我强调把学习重心放在工具和实践上，即学即用，不断积累成就感，思维也就慢慢养成了。说到底，学习数据分析的核心就是培养数据思维，掌握挖掘工具，熟练实践并积累经验。

### 0201. 术语卡——MAS 学习法

一个高效的学习方法，我把它称为 MAS 方法：Multi-Dimension：想要掌握一个事物，就要从多个角度去认识它；Ask：不懂就问，程序员大多都很羞涩，突破这一点，不懂就问最重要；Sharing：最好的学习就是分享。用自己的语言讲出来，是对知识的进一步梳理。怎么和数据分析建立多维度连接呢？我特意把内容分成了三个大类：第一类是基础概念；第二类是工具。这个部分可以很好地锻炼你的实操能力；第三类是题库。题库的作用是帮你查漏补缺，在这个过程中，你会情不自禁地进行思考。

### 0202. 术语卡——

### 0203. 术语卡——

### 0301. 人名卡——

根据这些证据和案例，找出源头和提出术语的人是谁——产生一张人名卡，并且分析他为什么牛，有哪些作品，生平经历是什么。

### 0401. 金句卡——

最后根据他写的非常震撼的话语——产生一张金句卡。

### 0501. 任意卡——深度学习的本质只是一种机器学习

深度学习固然很火，但本质上，仍然是机器学习的一种，都是帮我们创建分类器。你可以把传统的机器学习理解是专家级的方式，我们观察数据符合哪些特征，然后用各种分类、聚类算法处理这些数据。算法都是我们事先指定的。同样，深度学习更像一个黑盒子，我们无须事先指定提取特征的方法，而是通过深度学习这个大脑让机器自我训练，完成特征的提取。

当你开始思考这些方法之间的共同和差异点的时候，你将会收获更多。首先，你无须为新工具的出现产生无畏的焦虑感，因为每个工具都有适用的条件，深度学习虽然普适性强，但是对数据量的依赖性大，计算量大，收敛时间慢。在数据量不大的情况下，采用传统的机器学习可以更快收敛，同时还能得到不错的结果。

## 发刊词

清华有一门课，叫数据挖掘，正是通过这门课，我学会了如何从海量的数据中找到关联关系，以及如何进行价值挖掘。那时候感觉自己掌握了一门利器，就特别想找到一个钉子，来试试自己手里的这把锤子。当时恰好赶上 2009 年微博的热潮。我用 3 个月的时间就积累了 4 万粉丝，一年的时间积累了上百万粉丝。这是怎么做到的呢？通过数据采集，我收集了每天的微博热点，然后对热点进行抓取、去广告，再让机器定时自动进行发布。同时我让账号每天都去关注明星的粉丝列表，这样可以获得 15% 的回粉概率。久而久之，就会有源源不断的粉丝。

你看，其实就是数据分析帮我做到了微博的自动化运营。这还只是一个小例子，数据分析的影响已经渗透到了我们工作生活的方方面面。通过数据分析，我们可以更好地了解用户画像，为企业做留存率、流失率等指标分析，进而精细化产品运营。如果你关注比特币，数据分析可以帮助你预测比特币的走势。面对生活中遇到的种种麻烦，数据分析也可以提供解决方案，比如信用卡反欺诈，自动屏蔽垃圾邮件等。可以说，我们生活在数据驱动一切的时代，数据挖掘和数据分析就是这个时代的「淘金」，从国家、企业、组织到个人，都一定会关注各种数据，从这些数据中得到价值。

也正是这个原因，数据分析人才成了香饽饽，不管是数据分析师，数据分析工程师，还是数据产品经理，有数据思维的运营人员，都变得越来越抢手。你是不是也已经摩拳擦掌，做好了了解这一领域的准备呢？我想在接下来的 15 周时间里，把自己在清华学习数据挖掘的体会和工作实践中对数据分析的理解，重新梳理整合呈现给你，和你一起在数据分析这个领域来一场急行军。

我也知道数据分析能力很重要，但是数据分析是不是很难？到底该怎么学呢？其实这里有一些误区，数据分析并非遥不可及，它不难，掌握高效的学习方法很重要；但是它也不简单，需要你耐下性子，跟我一起来慢慢掌握数据分析的核心知识点和工具操作。

一个高效的学习方法，我把它称为 MAS 方法：Multi-Dimension：想要掌握一个事物，就要从多个角度去认识它；Ask：不懂就问，程序员大多都很羞涩，突破这一点，不懂就问最重要；Sharing：最好的学习就是分享。用自己的语言讲出来，是对知识的进一步梳理。

怎么和数据分析建立多维度连接呢？我特意把内容分成了三个大类：第一类是基础概念；第二类是工具。这个部分可以很好地锻炼你的实操能力；第三类是题库。题库的作用是帮你查漏补缺，在这个过程中，你会情不自禁地进行思考。

这个连接的过程，也是我们从「思维」到「工具」再到「实践」的一个突破过程。如果说重要性，一定是「思维」最重要，因为思维是底层逻辑和框架，可以让我们一通百通，举一反三，但是思维修炼也是最难的。所以，我强调把学习重心放在工具和实践上，即学即用，不断积累成就感，思维也就慢慢养成了。说到底，学习数据分析的核心就是培养数据思维，掌握挖掘工具，熟练实践并积累经验。

为了能带给你更好的学习效果，我在专栏里设计了五大模块。1）预习篇。我会给你介绍数据分析的全景图，和你进一步探讨最佳的学习路径。我还专门准备了 3 篇 Python 入门内容；2）基础篇。我会带你修炼数据思维，从数据分析的基础概念，到数据采集、数据处理以及数据可视化；3）算法篇。算法是数据挖掘的精华所在，我精选了 10 大算法，包括分类、聚类和预测三大类型。每个算法我们都从原理和案例两个维度来理解，达到即学即用的目的；4）实战篇。项目实战是我们学习的一个重要关卡。我准备了 5 个项目带你真实体验。比如在金融行业中，如何使用数据分析算法对信用卡违约率进行分析？现在的互联网产品都进入到千人千面的人工智能阶段，如何针对一个视频网站搭建视频推荐算法；5）工作篇。我选择了几个大家最关心的职场问题，比如面试时注意什么，职位晋升路径是怎样的等等。

1『数据分析技能培养方法论，一是数据分析思维，二是数据分析的工具，三是数据分析的实战。思维的培养，可以通过在学习工具、实战过程中提炼升华，也可以通过读书学习大牛的思维，还可以学习其他学科相关的知识连接来完善；分析工具里包括基本概念和算法，基本概念比如数据采集、数据处理和数据可视化。算法比如分类、聚类和预测模型；实战要求多做具体的项目。从「思维」到「工具」再到「实践」，再回到「思维」形成闭环。』

通过这个专栏你将收获：数据和算法思维。这不仅是在技术上的思维模式，更是我们平时看待问题解决问题的思维方式。如果你将数据视为财富，将数据分析视为获得财富的工具，那么在大数据时代，你将获得更宽广的视野；工具。用好工具，你将拥有收集数据、处理数据、得到结果的能力，它会让你在工作中游刃有余；更好的工作机会和价值。无论是当前火爆的人工智能，还是数据算法工程师的市场，都很看重数据分析和数据处理的能力。从「思维」到「工具」再到「实践」，沿着这个路径拓展自己的能力边界，拥有更强的竞争力。

### 黑板墙

业务洞察是分析数据的前提，分析数据是理解数据的前提，理解数据是数据挖掘的前提。从业务到数据再到挖掘，每一步环环相扣，相辅相成。业务千变万化，规律亘古不变。

作者回复：有关数据分析思维的书籍，《思维简史：从丛林到宇宙》；有关数据处理的书籍，《数据挖掘：概念与技术》、《Pentaho Kettle 解决方案》、《精益数据分析》、《Small Data》、《利用 Python 进行数据分析》。

2『已下载书籍「2018146思维简史」、「2020037数据挖掘概念与技术3E | 2020037Data-Mining-Concepts-and-Techniques3Ed」、「2020035Pentaho-Kettle解决方案 | 2020035Pentaho-Kettle-Solutions」、「2019064精益数据分析」、「2019065利用Python进行数据分析2Ed」、「2020034Small-Data」、「2020036Data-Mining-for-Business-Analytics-Python」。』

1）首先从爬虫开始是不错的，这样你能感受到成长的过程。2）数据挖掘算法，如果你想了解十大算法的话，理论部分你需要花一些功夫。当然这些在 Python 中都有类库可以使用。做练习的话，你也可以把这些算法都用一遍，然后看下哪个算法模型的结果更好。3）网上这方面的资源确实比较多，他们大多讲的是理论原理。我认为你更注重的在于实战，因为做项目不仅更有成就感，还能更好的让你理解这些算法、爬虫的原理。我会在专栏里给你做个「专属题库」，对应爬虫、数据挖掘这些的题目，你可以做个评测，不明白的地方，我也会给你做讲解。4）资料比较多，但其实不用每个都看一遍。尤其是理论的部分，看一遍就可以了，关键是把它抽出来做个思维导图。

对数据挖掘 / 机器学习很感兴趣，自学有段时间了，也接触了不少工具，但遇到具体问题还是很盲目，有下面几个方面的困惑：1）如何做好「特征工程」，没有思路，也没有思考方向，看了不少博客，所谓的技巧也都知道了，但遇到问题还是用不好；2）对于样本类别不均衡的问题，不会处理，尝试过下采样或过采样，但似乎改变了样本原本的分布，效果不太好；3）对各种机器模型输出的结果没有把控能力，搞不懂为何有时效果好，有时效果却很差。因为没有人带，自学感觉很迷茫，希望能跟随这门课程提升自己应用能力。另外，想请教一下老师，为了能更好地掌握经典的机器学习算法，有没有必要自己实现一遍？

作者回复：我认为非常有必要自己使用这些机器学习算法来解决实际问题。当然原理可以采用伪代码的方式，把流程画出来即可。项目中，很多时候都是直接使用类库，所以你更应该关注的机器学习的效率和结果。很多时候，我们在选择模型的时候，都要试，一次会用多种模型，然后看训练结果的好坏，再决定采用哪个模型。特征工程，以及调试的过程其实就是经验积累的过程，很多时候调参数的时间，比你写程序的时间还要长。但是这个积累过程还是挺重要的，当你有了更多经验之后，这个「试」的效率就会提升！

1『反复提到的一个思路，用多个模型尝试，看结果反馈来判断模型的好坏。』

数据就是这个时代的石油，确实是这样子的，在读研的时候深有体会，实验室的很多科研，项目都需要用到数据分析的思维和能力，工作之后也在为现在的公司处理数据帮助运营人员进行精准营销，无论是传统行业还是互联网行业，这都是一门重要的能力。作者回复：同意你说的，传统行业和互联网行业，不论是运营岗，还是营销岗，都需要数据分析能力和思维。

我是一个想转商业数据分析与挖掘的生物学（生物信息方向）硕士研究生，很需要有一门课大概能告诉我一个算法或者数学模型适用于哪些商业或者运营的情景，这是我现在急需的，也是对课程的期望，哪些东西可以解决哪些问题，也希望作者能推荐一些类似的书。作者回复：我上大学的时候，也了解一些生物信息学的情况，非常能理解你的心情和想转到商业数据分析的决心。我觉得需要从两个方面来下手：1）工具角度：课程里讲的算法，你可以帮他当做是个工具。他的诞生是从数学原理开始，形成的理论模型。这些模型都有自己的特点和适用范围。但总的来说，还是工具。2）商业角度：工作或应用中，首先都是从商业角度出发的，尤其是哪些是高频使用的，或者离「钱」更近的地方，也就是决策价值更大的地方。当然从工具使用到商业价值的转换，还需要你有自己的思维和建模能力。商业相关书籍推荐：《洛克菲勒留给儿子的 38 封信》、《商业冒险：华尔街的 12 个经典故事》、《从 0 到 1：开启商业与未来的秘密》、《商业的本质》。数据分析相关书籍：参考之前的信息。

## 0001结束语.md

我很庆幸能与你们共同度过这段学习时间。从我最开始准备专栏的时候，我就在想，我希望通过专栏交付什么。我希望你通过专栏，不仅仅可以获得知识，掌握工具的使用，更希望你能得到思维和实战经验的提升。

理论到处有，实战最重要。当大家都在讲知识和工具的时候，我更希望你重视思维和实战。因为知识和工具是别人的，思维和实战才是自己的。以我自己的经历来说，我从 10 岁开始编程，从最开始的 Basic 语言学起，再到 Pascal、C++、Java、PHP 和 Python。每种语言的学习切换基本上 2 天就可以完成，因为它们有很多共同的地方，不同地方仅仅在于第三方库和使用的框架。如果我们只关注知识和工具的话，你会发现这些东西更新迭代的速度非常快。可能刚掌握了一个工具，新的工具又出来了。所以我们更应该关注那些不变的东西。

从数据分析的角度上来说，当我们刚从书本上学完了数据挖掘算法之后，你会发现又出现了深度学习。这两年深度学习框架，比如 TensorFlow、Keras、Caffe 大行其道，于是无数讲人工智能的课诞生了，而这些课基本上都是在给你讲解语法和工具的使用。

当大家都在贩卖知识和焦虑的时候，我更希望你能有独立思考的能力。深度学习固然很火，但本质上，仍然是机器学习的一种，都是帮我们创建分类器。你可以把传统的机器学习理解是专家级的方式，我们观察数据符合哪些特征，然后用各种分类、聚类算法处理这些数据。算法都是我们事先指定的。同样，深度学习更像一个黑盒子，我们无须事先指定提取特征的方法，而是通过深度学习这个大脑让机器自我训练，完成特征的提取。

2『深度学习的本质只是一种机器学习。做一张任意卡片。』——已完成

当你开始思考这些方法之间的共同和差异点的时候，你将会收获更多。首先，你无须为新工具的出现产生无畏的焦虑感，因为每个工具都有适用的条件，深度学习虽然普适性强，但是对数据量的依赖性大，计算量大，收敛时间慢。在数据量不大的情况下，采用传统的机器学习可以更快收敛，同时还能得到不错的结果。

1『目前的应用场景数据量不大，机器学习完全可以驾驭。』

当你对这些知识和工具建立不同思考的时候，你更有可能理解和掌握它们。因为这些知识和工具的相同点，就会更容易完成知识和工具的迁移。它们的差异性可以让你更加了解每个知识和工具的特点。所以当知识和工具爆炸出现的时候，我希望你不要随波逐流，可以独立思考，与这些东西建立多维度的连接。同时，实战也是重要的成果体现。这就好比学习开车一样，学会开车和自驾旅行是完全不同的体验。只有通过实战，你才能解决一个特定的问题，领略到路途中的风采，为你的项目简历增加光彩的一笔。

方法比努力更重要。知识和工具是别人的，而思维和实战是自己的。那么在提升思维和实战经验的过程中，我希望你能掌握适合的方法。我见过很多人，选了很多课，自己也很努力，但是收获的效果一般。实际上，好的学习方法比努力更重要。我在专栏的开篇和结束的时候都提到了提问和分享，在开篇中也讲到了 MAS 学习法。实际上我们不是单向地被动接受知识和工具，更要建立思考和连接。建立多维度的连接，一个最好的方式就是学会提问以及学会分享。分享就好比是在测试集上做验证。分享的过程就是重新梳理知识的过程，还能得到别人的反馈，既受益别人，也获益自己。

投入越多，收获越多。当我们建立了正确的学习方法之后，你会发现投入越多，收获越多。在专栏更新的过程里，我很高兴地看到，有些同学自己开始用工具分析王者荣耀的英雄属性，分析他们之间的关联；也看到有同学在用 ARMA 模型对股市指数进行预测，使用爬虫抓取数据……

如果说工作是公司的事情，那么思维和实战经验的积累则是自己的事情。在思考和实战这条道路上，投入越多，收获就会越多。我看到在不少文章的评论区，都有同学们自己总结的笔记，还有人把实战的代码放到了 GitHub 上。这都是在为自己的体验负责。专栏本身只是一个开始，虽然专栏文章已经更新完毕，但大家的笔记分享不会结束。不论你以后是否会从事一份和数据分析相关的工作，我都希望你可以把思考作为一种学习的领悟，把实战当做是一次项目的旅行。在思维和实战经验上，有些许的提升。

### 黑板墙

目前为止报过的网课有：selenium 相关课程和陈老师的数据分析。真的很幸运，遇到的老师不只是讲语法，讲工具，更多的是告诉你多思考。比如，selenium 的老师常说，web/app 自动化测试你掌握了测试思维，不管什么测试工具，稍加练习都能运用自如。而在数据分析的这些课程里，除了语法、工具、技能外，有很多有温度的文字，比如开篇的 MAS 学习方法，贯穿始终的数据思维。在练习中我发现基本上每篇实战的文章里面，都有一段字：我们先把整个项目的流程梳理下，然后就是一个图片写着准备阶段和分类阶段要完成的内容。所以，现在闭着眼，我都可以写出整个流程，多谢老师。其实敲到 41 课感觉有点进行不下去了（不知为啥，敲的有点迷茫），继而看到 44，45 两节的内容，内心重新振作一番，回顾了下之前课程的笔记内容，好像又有点感觉了。

2『去下载有关 selenium 的书籍看，因为因为做动态爬虫的时候要用到它；去力扣上刷题。回复：看过单元测试的相关知识后才知道，web 的测试工具主要是用 selenium。（2020-10-02）』

之前看过吴恩达老师的课，然后开始学 python，但是人工智能还是不懂，以一种的方式算是入门了人工智能，很开心。

最高效的方法应该去「力扣」刷题。很后悔浪费时间。还有一个懊悔的事情。自认为对数据分析掌握通透，但经过一个师兄的预面试，发现最基本的数据分析分类算法有几种，这类问题也不清晰，更何况分辨随机森林和 svm 算法的区别。启示是：要想清楚方法，再去行动，要不然就是做无效的努力。

滢：老师能谈下数据分析未来的职业规划吗？看到宣传的目录上有一讲内容是讲这个，但是学完了没发现是有关于职业规划的章节。我之前从事编程，大学专业原因有统计学基础，因为兴趣爱好，刚转行过来 9 个月的时间，如果想往数据总监上走的话，或者成为一名资深数据分析师该朝着哪些地方用功？老师可以解答下我的疑惑吗？作者回复：慢慢来，我觉得你可以结合业务场景来看数据分析工具的使用。如果你对统计学感兴趣，也可以考虑如何应用进去，比如有个统计学的女生在 Google 实习的时候，提出了 Deep&Cross 算法，后来就发表出来了，成为了 Google 经典的 CTR 预估算法。

## 0116数据分析基础篇答疑.md

### 01. NumPy 相关

答疑 1：如何理解 NumPy 中 axis 的使用？

这里我引用文稿中的一段代码；同学们最容易混淆的是 axis=0 和 axis=1 的顺序。你可以记住：axis=0 代表跨行（实际上就是按列），axis=1 代表跨列（实际上就是按行）。如果排序的时候，没有指定 axis，默认 axis=-1，代表就是按照数组最后一个轴来排序。如果 axis=None，代表以扁平化的方式作为一个向量进行排序。所以上面的运行结果为......；我解释下 axis=0 的排序结果，axis=0 代表的是跨行（跨行就是按照列），所以实际上是对 `[4, 2] [3, 4] [2, 1]` 来进行排序，排序结果是 `[2, 4] [3, 4] [1, 2]`，对应的是每一列的排序结果。还原到矩阵中也就是 `[[2 3 1], [4, 4, 2]]`。

1『就牢记跨行、跨列的概念。排序的时候，0 跨行，1 跨列。』

```py
a = np.array([[4,3,2],[2,4,1]])
print np.sort(a)
print np.sort(a, axis=None)
print np.sort(a, axis=0)  
print np.sort(a, axis=1)  

[[2 3 4]
 [1 2 4]]
[1 2 2 3 4 4]
[[2 3 1]
 [4 4 2]]
[[2 3 4]
 [1 2 4]]
```

答疑 2：定义结构数组中的 S32 代表什么意思？

我文稿中定义了一个结构数组 persontype。这里实际上用的是 numpy 中的字符编码来表示数据类型的定义，比如 i 代表整数，f 代表单精度浮点数，S 代表字符串，S32 代表的是 32 个字符的字符串。如果数据中使用了中文，可以把类型设置为 U32，比如：

```py
import numpy as np
persontype = np.dtype({
    'names':['name', 'age', 'chinese', 'math', 'english'],
    'formats':['S32','i', 'i', 'i', 'f']})

persontype = np.dtype({
    'names':['name', 'age', 'chinese', 'math', 'english'],
    'formats':['U32','i', 'i', 'i', 'f']})
peoples = np.array([("张飞",32,75,100, 90),("关羽",24,85,96,88.5), ("赵云",28,85,92,96.5),("黄忠",29,65,85,100)], dtype=persontype)    
```

答疑 3：PyCharm 中无法 import numpy 的问题。

有些同学已经安装好了 numpy，但在 PyCharm 中依然无法使用 numpy。遇到这个问题的主要原因是 PyCharm 会给每一个新建的项目都是一个全新的虚拟环境。在这个环境下，默认的包只有 pip、setuptools 和 wheel 这三个工具，你可以在 `File->Settings` 里面找到这个界面。这说明 numpy 并没有配置到你创建的这个 Project 下的环境中，需要手动点击右侧的 + 号，对 numpy 进行添加。添加之后，你就可以正常运行程序，显示出结果了。

答疑 4：我不明白为什么打印出来的 name 会带一个 b？

这位同学的代码是这样的：

```py
student = np.dtype([('name','S20'), ('age', 'i1'), ('marks', 'f4')])
 a = np.array([('abc', 21, 50),('xyz', 18, 75)], dtype = student)
 print(a)
 print(a['name'])
 结果：
 [(b'abc', 21, 50.) (b'xyz', 18, 75.)]
 [b'abc' b'xyz']
```

我来解释一下。Python3 默认 str 是 Unicode 类型，所以要转成 bytestring，会在原来的 str 前加上 b。如果你用 py2.7 就不会有这个问题，py3 的 b 只是告诉你这里它转化成了 bytestring 进行输出。

答疑 5：np.ceil 代表什么意思？

ceil 是 numpy 中的一个函数，代表向上取整。比如 `np.ceil(2.4)=3`。

### 02. 数据分析思维培养及练习相关

答疑 1：Online Judge 的比赛题目，数学不好怎么办？

Vol1-Vol32 的难度是逐渐增加吗的，怎么可以选出有易到难的题目？Online Judge 有一些简单的题目可以选择，选择使用人数多，且 accepted 比例高的题目。另外它面向的是一些参加比赛的人员，比如高中的 NOI 比赛，或者大学的 ACM 比赛。你也可以选择 leetcode 或者 pythontip 进行训练。难度不一定是增加的，而是出题的先后顺序。难易程度，你可以看下提交的人数和 Accepted 的比例。提交人数和 Accepted 比例越高，说明越简单。

答疑 2：加餐中小区宽带使用多台手机等设备，不会被检测到吗？

小区宽带和手机飞行是两种解决方案。用手机飞行不需要用到小区宽带。用小区宽带需要使用到交换机，这里可以自己来控制交换机，每次自动切换 IP。

答疑 3：加餐中提到的一万个手机号。。。那怎么更换呢？也要一万台设备吗？

1  万个手机号，主要用于账号注册，通常采用的是「卡池」这个设备。简单来说，卡池可以帮你做收发短信。一个卡池设备 512 张卡，并发 32 路。有了卡池，还需要算法。你不能让这 512 张卡每次操作都是有规律可循的，比如都是同步执行某项操作，否则微信、Facebook 会直接把它们干掉。学过数据挖掘的人应该会知道，这 512 张卡如果是协同操作，可以直接被算法识别出来。在微信、Facebook 看来，这 512 张卡实际上是同一个人，也就是「机器人」。所以卡池可以帮你做短信验证码，以便账号登录用。MIFI+SIM 帮你做手机流量上网用。这是两套不同的设备。

答疑 4：听说企业里用 SQL 和 Excel 进行数据分析的很多，这块该如何选择？

SQL 和 Excel 做统计的工作多一些，涉及到编程的很少。如果你想在这个行业进一步提升，或者做一名算法工程师，那么你都要和 Python 打交道。专栏里数据挖掘算法的部分，是用 Python 交付的。Excel 和 SQL 很难做数据挖掘。如果想对数据概况有个了解，做一些基础分析，用 Excel 和 SQL 是 OK 的。但是想进一步挖掘数据的价值，掌握 Python 还是挺有必要的。另外，如果你做的是数据可视化工作，在企业里会用到 tableau 或者 powerBI 这些工具。数据采集你也可以选择第三方工具，或者自己用 Python 来编写。

答疑 5：学一些算法的时候比如 SVM，是不是掌握它们的理论内容即可。不需要自己去实现，用的时候调用库即可？

是的，这些算法都有封装，直接使用即可。在 python 的 sklearn 中就是一行语句的事。

答疑 6：老师，我现在等于从零开始学数据挖掘，所谓的数学基础指的是把高数学到哪种境界啊？是像考研那样不管极限导数积分每种题型都要会解，还是只需要了解这些必备的高数基础的概念？

不需要求解每一道数学题，只需要具备高数基础概念即可！概率论与数理统计、线性代数、最优化方法和图论这些，我在算法中涉及的地方都会讲到，你暂时不用提前学习这些数学知识。我觉得最好的方式就是在案例中灵活运用，这样可以加深你对这些数学知识的理解。对于大部分从 0 开始学数据挖掘的人来说，可以淡化公式，重点理解使用场景和概念。

### 03. 爬虫相关问题

答疑 1：关于 Python 爬虫工具的推荐。

我除了在专栏里讲到了 Requests、XPath 解析，以及 Selenium、PhantomJS。还有一些工具是值得推荐的。Scrapy 是一个 Python 的爬虫框架，它依赖的工具比较多，所以在 pip install 的时候，会安装多个工具包。scrapy 本身包括了爬取、处理、存储等工具。在 scrapy 中，有一些组件是提供给你的，需要你针对具体任务进行编写。比如在 item.py 对抓取的内容进行定义，在 spider.py 中编写爬虫，在 pipeline.py 中对抓取的内容进行存储，可以保存为 csv 等格式。这里不具体讲解 scrapy 的使用。

另外，Puppeteer 是个很好的选择，可以控制 Headless Chrome，这样就不用 Selenium 和 PhantomJS。与 Selenium 相比，Puppeteer 直接调用 Chrome 的 API 接口，不需要打开浏览器，直接在 V8 引擎中处理，同时这个组件是由 Google 的 Chrome 团队维护的，所以兼容性会很好。

2『去学习 Puppeteer 的知识。』

答疑 2：`driver = webdriver.Chrome()`，为什么输入这个代码就会报错了呢？

报错的原因是没有下载或正确配置 ChromeDriver 路径，正确的方法如下：

1、下载 ChromeDriver，并放到 Chrome 浏览器目录中；下载地址：[ChromeDriver Mirror](http://npm.taobao.org/mirrors/chromedriver/72.0.3626.7/)。

2、将 Chrome 浏览器目录添加到系统的环境变量 Path 中，然后再运行下试试。另外你也可以在代码中设置 ChromeDriver 的路径，方法如下：

```
chrome_driver = "C:\Users\cheny\AppData\Local\Google\Chrome\Application\chromedriver.exe"
driver = webdriver.Chrome(executable_path=chrome_driver)
```

答疑 3：如果是需要用户登陆后才能爬取的数据该怎么用 python 来实现呢？

你可以使用 Python+Selenium 的方式完成账户的自动登录，因为 Selenium 是个自动化测试的框架，使用 Selenium 的 webdriver 就可以模拟浏览器的行为。找到输入用户名密码的地方，输入相应的值，然后模拟点击即可完成登录（没有验证码的情况下）。另外你也可以使用 cookie 来登录网站，方法是你登录网站时，先保存网站的 cookie，然后在下次访问的时候，加载之前保存的 cookie，放到 request headers 中，这样就不需要再登录网站了。

1『 scrapy 框架也可以解决登录的问题。回复：scrapy 应该是利用了 cookie。（2020-10-02）』

答疑 4：为什么我在豆瓣网查询图片的网址与你不一样？https://www.douban.com/search?cat=1025&q=王祖贤 &source=suggest。

咱们访问豆瓣查询图片的网址应该是一样的。只是我给出的是 json 的链接。方法是这样的：用 Chrome 浏览器的开发者工具，可以监测出来网页中是否有 json 数据的传输，所以我给出的链接是 json 数据传输的链接： [https://www.douban.com/j/search_photo?q=王祖贤&limit=20&start=0](https://www.douban.com/j/search_photo?q=%E7%8E%8B%E7%A5%96%E8%B4%A4&limit=20&start=0)

答疑 5：XHR 数据这个是如何查出来的，我使用 chrome 的开发者工具查看 XHR 数据，但是查不到这部分，麻烦老师帮忙解答。

你需要使用浏览器的插件查看 XHR 数据，比如在 Chrome 的开发者工具。在豆瓣搜索中，我们对「王祖贤」进行了模拟，发现 XHR 数据中有一个请求是这样的：https://www.douban.com/j/search_photo?q=王祖贤 &limit=20&start=0

### 04. 数据变换相关

答疑 1：数据规范化、归一化、标准化是同一个概念么？

1）数据规范化是更大的概念，它指的是将不同渠道的数据，都按照同一种尺度来进行度量，这样做有两个好处，一是让数据之间具有可比较性；另一个好处就是方便后续运算，因为数据在同一个数量级上规整了，在机器学习迭代的时候，也会加快收敛效率。2）数据归一化和数据标准化都是数据规范化的方式。不同点在于数据归一化会让数据在一个 [0,1] 或者 [-1,1] 的区间范围内。而数据标准化会让规范化的数据呈现正态分布的情况，所以你可以这么记：归一化的「一」，是让数据在 [0,1] 的范围内。而标准化，目标是让数据呈现标准的正态分布。

答疑 2：什么时候会用到数据规范化（Min-max、Z-Score 和小数定标）？

刚才提到了，进行数据规范化有两个作用：一是让数据之间具有可比较性，二是加快后续算法的迭代收敛速度。实际上你能看到 Min-max、Z-Score 和小数定标规范化都是一种线性映射的关系，将原来的数值投射到新的空间中。这样变换的好处就是可以看到在特定空间内的数值分布情况，比如通过 Min-max 可以看到数据在 [0,1] 之间的分布情况，Z-Score 可以看到数值的正态分布情况等。不论是采用哪种数据规范化方法，规范化后的数值都会在同一个数量的级别上，这样方便后续进行运算。

那么回过头来看，在数据挖掘算法中，是否都需要进行数据规范化呢？一般情况下是需要的，尤其是针对距离相关的运算，比如在 K-Means、KNN 以及聚类算法中，我们需要有对距离的定义，所以在做这些算法前，需要对数据进行规范化。另外还有一些算法用到了梯度下降作为优化器，这是为了提高迭代收敛的效率，也就是提升找到目标函数最优解的效率。我们也需要进行数据规范化，比如逻辑回归、SVM 和神经网络算法。在这些算法中都有目标函数，需要对目标函数进行求解。梯度下降的目标是寻找到目标函数的最优解，而梯度的方法则指明了最优解的方向，如下图所示。

当然不是所有的算法都需要进行数据规范化。在构造决策树的时候，可以不用提前做数据规范化，因为我们不需要关心特征值的大小维度，也没有使用到梯度下降来做优化，所以数据规范化对决策树的构造结果和构造效率影响不大。除此之外，还是建议你在做数据挖掘算法前进行数据规范化。

答疑 3：如何使用 Z-Score 规范化，将分数变成正态分布？

我在专栏文稿中举了一个 Z-Score 分数规范化的例子，假设 A 与 B 的考试成绩都为 80 分，A 的考卷满分是 100 分（及格 60 分），B 的考卷满分是 500 分（及格 300 分）。这里假设 A 和 B 的考试成绩都是成正态分布，可以直接采用 Z-Score 的线性化规范化方法。

在专栏的讨论区中，有个同学提出了「Z-Score」的非线性计算方式，大家可以一起了解下：1）先按公式计算出百分等级。百分等级（年级）=100-(100x 年级名次 -50)/ 有效参加考试人数。这里百分等级是每个学生在该批学生中的相对位置，其中百分等级是按照正态分布图的所占面积比例求得的；2）按照百分等级数去标准正态分布表中查询得出 Z-Score 值，这样最终得出的 Z 分便是标准的正态分布，能够将偏态转化成标准正态。

因为在很多情况下，数值如果不是正态分布，而是偏态分布，直接使用 Z-Score 的线性计算方式无法将分数转化成正态分布。采用以上的方法可以解决这一个问题，大家可以了解下。这里偏态分布指的是非对称分布的偏斜状态，包括了负偏态，也就是左偏态分布，以及正偏态，也就是右偏态分布。

### 黑板墙

第一道题：假设矩阵 `a = np.array ([[4,3,2],[2,4,1]])`，请你编写代码将矩阵中的每一列按照从小到大的方式进行排序。

第二道题：你都用过哪些 Python 爬虫工具，抓取过哪些数据，觉得哪个工具好用？

关于 numpy 中的 axis，可以理解成旋转轴或者映射，尤其是高维数组，不应该死记硬背。0 是第一维度，也就是行，在行上的映射也就是每一列。以此类推。

关于爬虫：selenium+chrome/chromeless/phatomJS，可以处理页面加载后，需要运行 javaScript，元素才会显示的情况；Scrapy 爬虫框架，针对数据量大，层级嵌套较多的网页，框架中用到 yield 生成器，是关键；解析，lxml，bs4 包，正则表达式等。

爬虫用的是 requests，分析用的比较多的是 xpath，有时会用 re，re 有些优势是 xpath 不能替代的。beautiful soup 会用，但不用，因为抓取速度是比 xpath 和 re 慢。用过 senlenium 和 headless chromedriver 抓取过一些只用 javascript 生成数据的网页，xhr 都抓不出什么数据的，加密的太严格了。senlenium 的确可以无脑抓取网页，但很容易崩溃，不稳定，Puppeteer 没有用过，之后会尝试去替代 senlenium 来抓取。

## 0220数据分析算法篇答疑.md

### 17-19 篇：决策树

答疑 1：在探索数据的代码中，`print (boston.feature_names)` 有什么作用？

boston 是 sklearn 自带的数据集，里面有 5 个 keys，分别是 data、target、`feature_names`、DESCR 和 filename。其中 data 代表特征矩阵，target 代表目标结果，`feature_names` 代表 data 对应的特征名称，DESCR 是对数据集的描述，filename 对应的是 boston 这个数据在本地的存放文件路径。针对 sklearn 中自带的数据集，你可以查看下加载之后，都有哪些字段。调用方法如下：

```py
boston=load_boston()
print(boston.keys())
```

通过 `boston.keys()` 你可以看到，boston 数据集的字段包括了 [‘data’, ‘target’, ‘feature_names’, ‘DESCR’, ‘filename’]。

答疑 2：决策树的剪枝在 sklearn 中是如何实现的？

实际上决策树分类器，以及决策树回归器（对应 DecisionTreeRegressor 类）都没有集成剪枝步骤。一般对决策树进行缩减，常用的方法是在构造 DecisionTreeClassifier 类时，对参数进行设置，比如 `max_depth` 表示树的最大深度，`max_leaf_nodes` 表示最大的叶子节点数。通过调整这两个参数，就能对决策树进行剪枝。当然也可以自己编写剪枝程序完成剪枝。

答疑 3：对泰坦尼克号的乘客做生存预测的时候，Carbin 字段缺失率分别为 77% 和 78%，Age 和 Fare 字段有缺失值，是如何判断出来的？

首先我们需要对数据进行探索，一般是将数据存储到 DataFrame 中，使用 `df.info()` 可以看到表格的一些具体信息，代码如下：

```py
# 数据加载
train_data = pd.read_csv('./Titanic_Data/train.csv')
test_data = pd.read_csv('./Titanic_Data/test.csv')
print(train_data.info())
print(test_data.info())
```

你可以关注下运行结果中 Carbin 的部分，你能看到在训练集中一共 891 行数据，Carbin 有数值的只有 204 个，那么缺失率为 1-204/891=77%，同样在测试集中一共有 418 行数据，Carbin 有数值的只有 91 个，那么缺失率为 1-91/418=78%。同理你也能看到在训练集中，Age 字段有缺失值。在测试集中，Age 字段和 Fare 字段有缺失值。

答疑 4：在用 `pd.read_csv` 时报错「UnicodeDecodeError utf-8 codec can’t decode byte 0xcf in position 15: invalid continuation byte」是什么问题？

一般在 Python 中遇到编码问题，尤其是中文编码出错，是比较常见的。有几个常用的解决办法，你可以都试一下：1）将 `read_csv` 中的编码改为 gb18030，代码为：`data = pd.read_csv (filename, encoding = ‘gb18030’)`。2）代码前添加 `# -- coding: utf-8 --`。

我说一下 gb18030 和 utf-8 的区别。utf-8 是国际通用字符编码，gb18030 是新出的国家标准，不仅包括了简体和繁体，也包括了一些不常见的中文，相比于 utf-8 更全，容错率更高。为了让编辑器对中文更加支持，你也可以在代码最开始添加 `# -- coding: utf-8 -- ` 的说明，再结合其他方法解决编码出错的问题。

### 第 20-21 篇：朴素贝叶斯

答疑 1：在朴素贝叶斯中，我们要统计的是属性的条件概率，也就是假设取出来的是白色的棋子，那么它属于盒子 A 的概率是 2/3。这个我算的是 3/5，跟老师的不一样，老师可以给一下详细步骤吗？

这里我们需要运用贝叶斯公式（我在文章中也给出了），即；假设 A 代表白棋子，B1 代表 A 盒，B2 代表 B 盒。带入贝叶斯公式，我们可以得到；其中 P (B1) 代表 A 盒的概率，7 个棋子，A 盒有 4 个，所以 P (B1)=4/7。P(B2) 代表 B 盒的概率，7 个棋子，B 盒有 3 个，所以 P (B2 )=3/7。最终求取出来的是白色的棋子，那么它属于 A 盒的概率 P (B1|A)=2/3。

### 22-23 篇：SVM 算法

答疑 1：SVM 多分类器是集成算法么？

SVM 算法最初是为二分类问题设计的，如果我们想要把 SVM 分类器用于多分类问题，常用的有一对一方法和一对多方法（我在文章中有介绍到）。集成学习的概念你这样理解：通过构造和使用多个分类器完成分类任务，也就是我们所说的博取众长。以上是 SVM 多分类器和集成算法的概念，关于 SVM 多分类器是否属于集成算法，我认为你需要这样理解。

在 SVM 的多分类问题中，不论是采用一对一，还是一对多的方法，都会构造多个分类器，从这个角度来看确实在用集成学习的思想，通过这些分类器完成最后的学习任务。不过我们一般所说的集成学习，需要有两个基本条件：1）每个分类器的准确率要比随机分类的好，即准确率大于 50%；2）每个分类器应该尽量相互独立，这样才能博采众长，否则多个分类器一起工作，和单个分类器工作相差不大。所以你能看出，在集成学习中，虽然每个弱分类器性能不强，但都可以独立工作，完成整个分类任务。而在 SVM 多分类问题中，不论是一对一，还是一对多的方法，每次都在做一个二分类问题，并不能直接给出多分类的结果。

此外，当我们谈集成学习的时候，通常会基于单个分类器之间是否存在依赖关系，进而分成 Boosting 或者 Bagging 方法。如果单个分类器存在较强的依赖关系，需要串行使用，也就是我们所说的 Boosting 方法。如果单个分类器之间不存在强依赖关系，可以并行工作，就是我们所说的 Bagging 或者随机森林方法（Bagging 的升级版）。所以，一个二分类器构造成多分类器是采用了集成学习的思路，不过在我们谈论集成学习的时候，通常指的是 Boosing 或者 Bagging 方法，因为需要每个分类器（弱分类器）都有分类的能力。

### 26-27 篇：K-Means

答疑 1：我在给 20 支亚洲球队做聚类模拟的时候，使用 K-Means 算法需要重新计算这三个类的中心点，最简单的方式就是取平均值，然后根据新的中心点按照距离远近重新分配球队的分类。对中心点的重新计算不太理解。

实际上是对属于这个类别的点的特征值求平均，即为新的中心点的特征值。比如都属于同一个类别里面有 10 个点，那么新的中心点就是这 10 个点的中心点，一种简单的方式就是取平均值。比如文章中的足球队一共有 3 个指标，每个球队都有这三个指标的特征值，那么新的中心点，就是取这个类别中的这些点的这三个指标特征值的平均值。

### 28-29 篇：EM 聚类

答疑 1：关于 EM 聚类初始参数设置的问题，初始参数随机设置会影响聚类的效果吗。会不会初始参数不对，聚类就出错了呢？

实际上只是增加了迭代次数而已。EM 算法的强大在于它的鲁棒性，或者说它的机制允许初始化参数存在误差。举个例子，EM 的核心是通过参数估计来完成聚类。如果你想要把菜平均分到两个盘子中，一开始 A 盘的菜很少，B 盘的菜很多，我们只要通过 EM 不断迭代，就会让两个盘子的菜量一样多，只是迭代的次数多一些而已。

另外多说一句，我们学的这些数据挖掘的算法，不论是 EM、Adaboost 还是 K-Means，最大的价值都是它们的思想。我们在使用工具的时候都会设置初始化参数，比如在 K-Means 中要选择中心点，即使一开始只是随机选择，最后通过迭代都会得到不错的效果。所以说学习这些算法，就是学习它们的思想。

### 30-31 篇：关联规则挖掘

答疑 1：看不懂构造 FP 树的过程，面包和啤酒为什么会拆分呢？

FP-Growth 中有一个概念叫条件模式基。它在创建 FP 树的时候还用不上，我们主要通过扫描整个数据和项头表来构造 FP 树。条件模式基用于挖掘频繁项。通过找到每个项（item）的条件模式基，递归挖掘频繁项集。

答疑 2：不怎么会找元素的 XPath 路径。

XPath 的作用大家应该都能理解，具体的使用其实就是经验和技巧的问题。我的方法就是不断尝试，而且 XPath 有自己的规则，绝大部分的情况下都是以 // 开头，因为想要匹配所有的元素。我们也可以找一些关键的特征来进行匹配，比如 `class='item-root’` 的节点，或者 `id='root’` 都是很好的特征。通过观察 id 或 class，也可以自己编写 XPath，这样写的 XPath 会更短。总之，都是要不断尝试，才能找到自己想要找的内容，寻找 XPath 的过程就是一个找规律的过程。

答疑 3：最小支持度可以设置小一些，如果最小支持度小，那么置信度就要设置得相对大一点，不然即使提升度高，也有可能是巧合。这个参数跟数据量以及项的数量有关。理解对吗？一般来说最小置信度都会大一些，比如 1.0，0.9 或者 0.8。最小支持度和数据集大小和特点有关，可以尝试一些数值来观察结果，比如 0.1，0.5。

### 34-35 篇：AdaBoost 算法

答疑 1：关于 Zk 和 yi 的含义。

第 k+1 轮的样本权重，是根据该样本在第 k 轮的权重以及第 k 个分类器的准确率而定，具体的公式为；其中 Zk 和 yi 代表什么呢？Zk 代表规范化因子，我们知道第 K+1 轮样本的权重为：

为了让样本权重之和为 1，我们需要除以规范化因子 Zk，所以；yi 代表的是目标的结果，我在 AdaBoost 工作原理之后，列了一个 10 个训练样本的例子：

你能看到通常我们把 X 作为特征值，y 作为目标结果。在算法篇下的实战练习中，我们一般会把训练集分成 `train_X` 和 `train_y`，其中 `train_X` 代表特征矩阵，`train_y` 代表目标结果。

### 黑板墙

我发现大家对工具的使用和场景比较感兴趣，所以最后留两道思考题。

第一道题是，在数据挖掘的工具里，我们大部分情况下使用的是 sklearn，它自带了一些数据集，你能列举下 sklearn 自带的数据集都有哪些么？我在第 18 篇使用 `print (boston.feature_names)` 来查看 boston 数据集的特征名称（数据集特征矩阵的 index 名称），你能查看下其他数据集的特征名称都是什么吗？列举 1-2 个 sklearn 数据集即可。

第二个问题是，对于数据挖掘算法来说，基础就是数据集。Kaggle 网站之所以受到数据科学从业人员的青睐就是因为有众多比赛的数据集，以及社区间的讨论交流。你是否有使用过 Kaggle 网站的经历，如果有的话，可以分享下你的使用经验吗？如果你是个数据分析的新人，当看到 Kaggle 网站时，能否找到适合初学者的 kernels 么（其他人在 Kaggle 上成功运行的代码分享）？

编辑回复：首先十个经典算法代表了十种数据挖掘思想，基于他们都有不少算法的变种和改进，对数据挖掘的影响是非常深远的。另外这十大经典算法，解决的问题也不同，按照解决问题来划分的话：1）分类算法：C4.5，朴素贝叶斯（Naive Bayes），SVM，KNN，Adaboost，CART。2）聚类算法：K-Means，EM。3）关联分析：Apriori。3）连接分析：PageRank。

所以这十大算法要解决的问题也不同，比如分类是一种有监督的学习方式，事先知道样本的类别，通过数据挖掘可以将不同类别的样本进行区别，从而对未知的物体进行分类。而聚类是一种无监督的学习方式，事先不知道样本的类别，而是通过相关属性分析，将具有类似属性的物体聚成一类。所以对十大算法的理解，想要知道他们解决的是哪类问题。然后针对同一类问题，比如分类问题，也有不同种解法，比如 C4.5，朴素贝叶斯，SVM，KNN 等。

不同的算法实际上都有自己对这个问题分析的方式，很难说哪种算法更优，哪个算法不好。实际上这和我们的样本有很大关系，不同的样本属性，样本分布，特征值等，采用不同的算法结果都会有差别，最好的方式就是都做一遍，然后选择针对这个训练集 / 测试集最优的算法。所以你能看到，在后面的练习中，我们往往都在采用多种算法。

另外我想说的是，关于算法的研究，这十大算法是根基，很多人都会在这些算法基础上提出自己的模型，就类似于研究生期间发表论文，都是在这些算法（会有这个算法相应的参考文献）的基础上进行的改进。同时，也会给出自己所采用的的数据集，然后针对这个数据集，采用传统方法和改进方法进行对比，得出结论。所以：算法是可以改进的，采用哪个适合和数据集也有关系，很多时候都会做一遍然后选择适合的。

sklearn 自带的小数据集（packageddataset）：`sklearn.datasets.load_<name>`。

几个数据集：1）鸢尾花数据集 `load_iris()`：用于分类任务的数据集。2）手写数字数据集 `load_digits()`：用于分类任务或者降维任务的数据集。3）乳腺癌数据集 `load_breast_cancer()`：简单经典的用于二分类任务的数据集。4）糖尿病数据集 `load_diabetes()`：经典的用于回归认为的数据集，值得注意的是，这 10 个特征中的每个特征都已经被处理成 0 均值，方差归一化的特征值。5）波士顿房价数据集 `load_boston()`：经典的用于回归任务的数据集。6）体能训练数据集 `load_linnerud()`：经典的用于多变量回归任务的数据集。

体能训练数据集中的特征名称：`linnerud.feature_names` 为 `['Chins', 'Situps', 'Jumps']`。鸢尾花数据集的特征名称：`iris.feature_names` 为 `['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']`。

说来惭愧，到现在为止，都还没有注意到 Kaggle 的重要性。刚去看看了入门，发现这篇文章介绍的不错：[关于 Kaggle 入门，看这一篇就够了](http://www.360doc.com/content/18/0106/16/44422250_719580875.shtml#)。

一些摘要：Kaggle 成立于 2010 年，是一个进行数据发掘和预测竞赛的在线平台。从公司的角度来讲，可以提供一些数据，进而提出一个实际需要解决的问题；从参赛者的角度来讲，他们将组队参与项目，针对其中一个问题提出解决方案，最终由公司选出的最佳方案可以获得 5K-10K 美金的奖金。除此之外，Kaggle 官方每年还会举办一次大规模的竞赛，奖金高达一百万美金，吸引了广大的数据科学爱好者参与其中。从某种角度来讲，大家可以把它理解为一个众包平台，类似国内的猪八戒。但是不同于传统的低层次劳动力需求，Kaggle 一直致力于解决业界难题，因此也创造了一种全新的劳动力市场 —— 不再以学历和工作经验作为唯一的人才评判标准，而是着眼于个人技能，为顶尖人才和公司之间搭建了一座桥梁。作者回复: kaggle 给数据分析师提供了非常好的数据集。

为什么三个相关性大的特征只选一个呢？原理是什么？编辑回复：首先特征选择是数据挖掘（机器学习）中的重要问题之一，一般来说对于数据特征空间大的数据集来说，我们需简要对特征进行选择，也就是选取有代表性的特征，来降低特征空间的冗余度，提升算法的效率。特征选择的过程，你可以理解是从 m 个特征中选择 n 个特征的过程，文章中从三个相关性大的特征只选择一个，目的是在于降低冗余信息，缩减特征维数。

## 04数据分析工作篇

1、辅助培养数据分析思维的工具：生命线视角、提问和分享。任何一段经历都可以当作生命线，横坐标为时间轴，纵坐标为重要事件（有正有负），影响力越大其绝对值越大；提问本身就是一种维度的观察。有一个好的问题，才会有好的答案。问题可以帮助我们关注事物的不同方面，而且通常是一些重要的维度，对我们全面客观地分析一件事是非常有好处的。提问题还可以让我们变被动为主动；学会分享是最快的成长。分享可以锻炼我们的逻辑性，分享过程也是对知识重新梳理的过程。另一方面也可以让我们获得别人的反馈，更容易得到正反馈的愉悦。就像我们在做机器学习训练的时候，如果训练没有结果反馈，我们就无法客观地了解对知识的掌握程度。如果能得到别人的反馈，就更容易有收获，训练的收敛速度也会越快。

2、简历是最好的工作梳理。HR 看相关项目简历其背后的逻辑是根据历史信息预估一个人相关的工作能力。实战项目过程中需要做的：1）了解每个实战项目的目标；2）理解每个算法的原理；3）跑一遍项目代码，将运行结果放到 GitHub 上；4）做项目的心得总结。简历重点体现，当时的项目目标、采用的解决方案、实现的代码以及项目过程的总结体会拿给 HR 看。

## 0401如何培养你的数据分析思维.md

今天我们做了一个有关生命线的游戏，你能了解到我们每个人、每个公司、每件事，只要有历史数据，都有可能从中发现规律，从而指导未来。所以说数据分析这件事，就好比是生命线一样闪耀着价值。而培养自己的数据化思维虽然不是一天能练就的，却是重要的事情。很多时候，我们容易被紧急的事情牵着走，毕竟紧急事情的优先级会更高。但人生差距不是在于处理多少紧急的事，而是在于做过多少重要的事。从人性的角度来看，重要不紧急的事是容易被拖延的。不过我有两个工具教你摆脱惰性，一个就是学会提问，它从提问的角度训练我们的数据化思维，让我们对事物看得更清楚，另一个就是学会分享，它从反馈的角度让我们的训练过程更加收敛，效率得到提升，也更容易获得成就感。

数据分析可以是一个职业，一份工作，也可以是一种思维方式。在专栏里，我们更多的是讲解了数据分析工具的使用。从 Python 爬虫到 Python 可视化，再到数据清洗、数据挖掘算法等，而在日常工作中，我们除了需要熟练掌握这些工具的使用外，更主要的是培养自己的数据分析思维。培养数据分析思维不仅对找一份和数据分析相关的工作有帮助，在日常生活中同样会有帮助。

1）我们做一个有关生命线的游戏。你可以把生命线看作是数据可视化，能从中发现什么规律呢？2）当你想知道事情的答案，但不知道从何处下手的时候，要怎么办呢？要学会提问。好的问题就是好的开始。遇到茫然的情况，不妨从提问开始。3）「我平时也有一些关于数据分析的思考，但是效率不高，有什么方法可以提升效率么？」分享是最快的成长，通过反向传播可以让我们更快得到收敛。4）「我也知道数据分析思维的训练很重要，但是平时工作很忙该怎么办？」

一个关于生命线的游戏。举个例子，如果你想知道自己是如何挣钱的，你可以分析自己以往挣钱的经历，也可以是赔钱的经历，把它们写在一个时间轴上，纵坐标是发生的事件，这个事件对你的影响越大，纵坐标的绝对值就越大。通过生命线的分析，我们先把这些事件按照时间的顺序记录下来，然后记录它们的影响力。实际上这些事件，影响力 y 和时间 x 就是你的生命线历史数据，画出生命线之前，你不必思考它们之间的规律是什么。画出来之后，你有 30 分钟的时间，仔细思考和分析它们之间有什么关联。

其实你能看出来，画生命线之前，我们首先需要有客观的记录数据，生命线就相当于数据可视化，更容易让我们找到规律。你可以对这些事件打上不同的标签，比如 12 岁的时候给报社投稿挣到了 180 元，26 岁做自媒体，每个月有 2 万收入等等，那么两件事都可以打上「写作」这个标签。我们之前讲过打标签是一种抽象能力。当你对这些事件逐一分析打标签的时候，就有可能从更高的维度上观察到这些事件的规律。

上面这个是关于挣钱方向的生命线游戏，有空的话你可以做一下，分析分析适合自己的挣钱模式是什么。此外还有一个生命线的游戏，你肯定不陌生，那就是简历。在面试之前，你最重要的信息就是简历。HR 会通过简历筛选符合要求的人，一般来说会根据简历来看职业经历是否具有连续性，比如说这个人做过行政，又做过销售，现在面试数据分析的工作，那么对于 HR 来说，他就没有找到职业方向。所以有些人在投递某个职位前，会特地对简历做有针对性的修改，比如重点呈现和数据分析相关的经历，其他关系不大的经历都 一一 删除，哪怕经历再丰富。不相关的经历其实就是干扰数据，这些并不是 HR 想要看到的！

除了分析挣钱、找工作以外，通过生命线做数据分析还能帮我们做什么呢？它可以分析你的感情经历、是否有偏财运等等。数据是非常重要的宝藏，只是你需要知道如何观察它，使用它。通过历史才能看到未来，如果我们不去分析这些历史，就没有办法找到未来的规律。大到国家，小到个人，都是如此。这也是为什么很多成功人士经常读书的原因之一吧。通过总结别人的成功或者失败的经验，可以启迪自己的人生道路。

提问是最好的老师。当了解数据分析的价值之后，你可能会问，学会提问和数据分析思维有什么联系？实际上提问本身就是一种维度的观察。很多人在做数据分析的时候，首先遇到的问题是没有数据怎么办？数据从哪里来？其实在找数据之前，我们应该先问自己一个问题，我要解决什么问题？要分析什么规律？比如说，你想观察自己挣钱模式的规律，或者想解决个人的情感问题，再或者，想找到一份适合自己的工作等。我们首先需要定义一个目标。

然后围绕这个目标再问自己，这些数据可能会在哪里？是通过分析自己过去的经历找，还是从网上找相关的信息？都有哪些渠道可以收集到这些信息？有一个好的问题，才会有好的答案。问题可以帮助我们关注事物的不同方面，而且通常是一些重要的维度，对我们全面客观地分析一件事是非常有好处的。

从科技进步来看，很多时候都是先有一个问题，再有无数的人前赴后继去解决它。比如世界三大数学猜想，费马猜想、四色猜想和哥德巴赫猜想。比如费马大定理是费马在 1637 年提出的，此后的 300 年间有无数数学家试图去验证它。学会提问不仅可以帮助我们对事物有更全面的认识，还可以让我们变被动为主动。要知道在职场上，大部分人的工作状态都属于被动性，比如等着领导下任务、数据分析结果没出来就怪数据不完整，质量不够好等。被动的状态往往能量很低，或者说创造性很低。只有当你主动思考，寻找答案的时候，才更可能会有有创造力的发现。

以我的学习经历为例，很多人在上学期间，基本上都是老师在课上讲，自己只是听，很少提问，信息仅仅限于单向传递。而我经常会把不懂的问题整理下来，下课的时候主动向老师提问，这样做的好处是，勤于思考，可以让知识尽量没有盲点，另外通过提问和思考的方式 ，也可以让我对这个知识掌握得更牢固。我成绩通常不错，后来保送到了清华计算机系，很多人认为我平时学习是不是很晚，其实并没有，我只是善于找学习的规律，提问思考就是最好的学习方式。它更容易让我们对一件事物建立多维度的认知。

学会分享是最快的成长。如果说培养数据思维从提问开始，那么把总结分享作为结束则是最适合不过的。把学到的知识分享给身边的朋友，可以锻炼我们的逻辑性，分享的过程也是对知识重新梳理的过程。另一方面也可以让我们获得别人的反馈，更容易得到正反馈的愉悦。就像我们在做机器学习训练的时候，如果训练没有结果反馈，我们就无法客观地了解对知识的掌握程度。如果能得到别人的反馈，就更容易有收获，训练的收敛速度也会越快。所以在某种程度上，你可以把分享的过程，理解是在测试集上做验证的过程。它会让你收获更多，成长更快。

培养数据分析思维是重要不紧急的事。你可能会说：「道理我都懂，可就是做的时候想不起来。」那是怎么回事呢？实际上，培养数据分析思维是重要不紧急的事。在工作中，我们经常会被紧急的事情占据带宽。这些紧急的事情对当下很重要，但是放长远来看重要性就很弱了。而拉开我们人生差距的，恰恰是那些重要不紧急的事情上，而不是在于我们每天处理了多少紧急的事。这点很容易理解，毕竟人都有惰性，紧急的事情来了一般都会优先处理。不过你要换一种思考方式，既然我们人生的差距不是在于做过多少紧急的事，而是在于做过多少重要的事，那么从工作的第一天开始，我就应该着重积累重要的事，即使它目前并不紧急。

这样你会发现，当你做过的重要事情越来越多的时候，紧急的事情也就越来越少了。比如你想着如何找到一份更高薪酬更适合自己工作的时候，就不用着急每个月还贷款的事情了。

## 0402求职简历中没有相关项目经验怎么办.md

在专栏的讲解过程中，很多同学都反馈过他们正在找工作，但项目经历这块是自己的软肋。我们关键要弄明白 HR 招人背后的逻辑，把相关的训练经验总结下来写在简历中，最后拆解专栏的实战项目。在这个过程中你需要：1）了解每个实战项目的目标；2）理解每个算法的原理；3）跑一遍项目代码，将运行结果放到 GitHub 上；4）做项目的心得总结。当你自己把这些内容整理出来的时候，你发现自己会更有信心。简历的完善只是表象，实际上最重要的是自己的能力也得到了提升。我在专栏里讲解了理论知识、工具方法和实战项目，希望你把专栏作为一个工具，带你走入数据科学的大门。掌握了这个工具之后，平时遇到问题的时候，你就可以用数据的视角来分析它，使用工具来做模拟，总结结果，进一步完善你的简历。另外，简历是最好的工作梳理。

上节课我讲到了如何培养数据分析思维，它是一个重要但不紧急的事。在工作求职中，你可能会遇到各种又重要又紧急的事，比如填写求职简历中的项目经验。它的重要性在于，HR 一般都会依据简历中的项目经验初步筛选候选人是否符合面试要求，紧急性在于求职找工作往往就是眼前的事，但简历中的项目经验又很难临时抱佛脚。项目经验一般没有弹性，一是一，二是二，一方面要保证真实性，是自己做过的项目，另一方面又很难在短时间内积攒这些经验。

如果没有项目经验，很多人就会感觉无从下手，这时候该怎么办呢？我自己面试过的技术人员少说也有上百人，我想以自己的经验做一些分享，在经验积累上和你分享以下三个需要注意的地方：1）我们求职找工作的时候，要理解 HR 看项目经验的逻辑是什么？2）明确要完善项目经验这个目标后，我们该如何快速定位要积累的内容，并通过实战和训练快速进行提升经验值？3）如何在项目经验中融入自己的心得体会，让你的经验显得与众不同？

HR 看相关项目简历，背后的逻辑是什么。HR 之所以要看相关的项目经验，是因为这些历史信息可以帮助他预估一个人相关的工作能力。知识不等于项目经验，即使你对知识都了解了，在实际项目过程中，还是会遇到各种问题。比如工具包安装不上、中文编码错误、画图显示不出来、算法运行过慢、数据拟合结果不好等各种问题。项目经历相当于一种训练，当你得到了更好的训练之后，数据分析的模型能力也就会越强，然后在「新公司」这个测试集中，就越有可能发挥好的效果。

做过训练和没有训练的人是完全不同的。如果你没有相关的经验，那么你现在找的这份工作就好比是训练集一样，没有一个公司会把他们的项目当做是你练手的数据集。大家都期望你是已经训练好的模型，可以马上开展新的工作，并且产生价值。所以在经验积累上，你要证明给 HR，我做过这样的项目，具备这样的能力。

1『新工作隐喻为「训练集」，贴切。』

你可能想问，项目从哪里来呢？第一个肯定是以往类似的工作经历，第二个就是自己做过类似的项目。但是在简历中呈现数据分析的项目也是需要技巧的，简历不是流水账，你需要重点把当时的项目目标、采用的解决方案、实现的代码以及项目过程的总结体会拿给 HR 看。这样，即使你没有相关的工作经历，如果你能通过专栏实战积累上面的 4 点，对 HR 来说也是有说服力的，这样总比一张白纸要强得多。要知道 HR 背后的逻辑是要通过简历证明你是已经被训练过的模型，可以上手工作了，而不是把新公司当成训练集。

1『项目的目标、采用的解决方案、实现的代码和项目总结。』

如何完善简历里的项目经历。现在我们需要简历中有更多的项目经验。如果你跟着专栏从头到尾完整学习了，在爬虫、数据可视化、数据清洗和集成、数据挖掘算法、图像识别等多个维度进行了实战训练，那么恭喜你，实际上你已经具有数据分析相关的工作经验了。这方面我来简单帮你总结下，梳理出一个项目简历的模板。但最根本的是，你需要自己跑一遍项目代码，完整了解项目目标和解决方案。只有这样，放到简历中的时候才会比较充实。

1、乳腺癌检测：采用 SVM 方法，对美国威斯康星州的乳腺癌诊断数据集进行分类，最终实现一个针对乳腺癌检测的分类器：[cystanford/breast_cancer_data: 乳腺癌检测分类数据](https://github.com/cystanford/breast_cancer_data)。

2、内容抓取：通过 Python 爬虫对豆瓣电影中的电影数据和海报等信息进行抓取：[cystanford/pachong: Python爬虫实例](https://github.com/cystanford/pachong)。

3、邮件数据分析：通过 PageRank 算法分析邮件中的人物关系图谱，并针对邮件数量较大的情况筛选出重要的人物，进行绘制：[cystanford/PageRank: PageRank算法实例-希拉里邮件PR分析](https://github.com/cystanford/PageRank)。

4、微博文档分类：采用朴素贝叶斯的方法，对微博的内容进行分类，最终实现一个简单的文档分类器：[cystanford/text_classification: 中文文档分类数据集](https://github.com/cystanford/text_classification)。

5、电影数据集关联规则挖掘：采用 Apriori 算法，分析电影数据集中的导演和演员信息，从而发现导演和演员之间的频繁项集及关联规则：[cystanford/Apriori: Apriori算法实例-挖掘电影导演的关联规则](https://github.com/cystanford/Apriori)。

6、歌词词云可视化：动态抓取指定明星的歌曲列表，保存歌词文件，去除歌词中的常用词，并对歌词进行词云展示，分析歌曲的作词风格：[cystanford/word_cloud: 词云生成](https://github.com/cystanford/word_cloud)。

7、信用卡违约率分析：针对台湾某银行信用卡的数据，构建一个分析信用卡违约率的分类器。采用 Random Forest 算法，信用卡违约率识别率在 80% 左右：[cystanford/credit_default: 信用卡违约率分析](https://github.com/cystanford/credit_default)。

8、信用卡欺诈分析：针对欧洲某银行信用卡交易数据，构建一个信用卡交易欺诈识别器。采用逻辑回归算法，通过数据可视化方式对混淆矩阵进行展示，统计模型的精确率，召回率和 F1 值，F1 值为 0.712，并绘制了精确率和召回率的曲线关系：[cystanford/credit_fraud: 信用卡欺诈分析](https://github.com/cystanford/credit_fraud)。

9、比特币走势分析：分析 2012 年 1 月 1 日到 2018 年 10 月 31 日的比特币价格数据，并采用时间序列方法，构建自回归滑动平均模型（ARMA 模型），预测未来 8 个月比特币的价格走势。预测结果表明比特币将在 8 个月内降低到 4000 美金左右，与实际比特币价格趋势吻合（实际最低降到 4000 美金以下）：[cystanford/bitcoin: 比特币走势分析](https://github.com/cystanford/bitcoin)。

不一样的项目经历和体会。上面我整理了 9 个项目简历的示例，如果认真学习专栏，并且坚持练习的话，那么不用愁相关的项目经验。如果你希望有不一样的项目经历，那么能融入自己的项目体会和总结的话，就会更好。比如分析比特币走势这一篇文章中，我还提供了沪市指数的历史数据（从 1990 年 12 月 19 日到 2019 年 2 月 28 日），你完全可以采用 ARMA 模型自己跑一遍，然后整理出相关的经历。再或者，我们对毛不易歌词进行词云分析的时候，你也可以分析其他的歌手，或者某个歌手的某张专辑的词云。模型方法是相同的，但不同的数据集出来的结果是不同的。

另外你也可以在项目实战中，融入自己的心得体会。比如在预测比特币走势这个项目中，我们对原始数据进行了降维，按月为粒度进行了统计，实际预测结果与按天进行统计的结果相差并不大，但是数据量降到了 1/30，大大提升了效率。在这个过程中，你应该能体会到数据降维的作用。在信用卡欺诈分析这个项目中，我们观察到数据集的分类样本是不平衡的，针对这种情况，我们到底该采用哪个评价标准呢？为什么采用准确率作为评价标准会有问题？有关这方面的经验总结你也可以简单做个说明，这样不光可以证明你具备这种项目的经验，也能证明针对这类的问题，你都找到了哪些规律。总之自己的心得体会和总结能给项目经验加分不少。