# 2019065利用Python进行数据分析R00

## 记忆时间

## 卡片

### 0101. 主题卡——

这本书的主题核心，就是最大的反常识卡，并且注意时间脉络。

### 0201. 术语卡——广播

不同大小的数组之间的运算叫做广播（broadcasting），将在附录 A 中对其进行详细讨论。

### 0202. 术语卡——结构化数据

当书中出现「数据」时，究竟指的是什么呢？主要指的是结构化数据（structured data），这个故意含糊其辞的术语代指了所有通用格式的数据，例如：表格型数据，其中各列可能是不同的类型（字符串、数值、日期等）。比如保存在关系型数据库中或以制表符 / 逗号为分隔符的文本文件中的那些数据；多维数组（矩阵）；通过关键列（对于 SQL 用户而言，就是主键和外键）相互联系的多个表；间隔平均或不平均的时间序列。

这绝不是一个完整的列表。大部分数据集都能被转化为更加适合分析和建模的结构化形式，虽然有时这并不是很明显。如果不行的话，也可以将数据集的特征提取为某种结构化形式。例如，一组新闻文章可以被处理为一张词频表，而这张词频表就可以用于情感分析。

### 0203. 术语卡——

### 0301. 人名卡——

根据这些证据和案例，找出源头和提出术语的人是谁——产生一张人名卡，并且分析他为什么牛，有哪些作品，生平经历是什么。

维基百科链接：有的话。

#### 01. 出生日期

用一句话描述你对这个大牛的印象。

#### 02. 贡献及经历

#### 03. 论文及书籍

#### 04. 演讲汇总

找一个他的 TED 演讲，有的话。

### 0401. 金句卡——

最后根据他写的非常震撼的话语——产生一张金句卡。

## 序

时过境迁，从本书英文版第 1 版 2012 年出版至今，已经过去了 6 年。这 6 年中，Python 的主流版本从 2.7 升级到了 3.6。无论是否情愿，大部分 Pythoner 都不得不学会适应新版本；而 pandas 则从 0.1.0 版本送代到如今的 0.22.0 版本。版本号的持续増加意味着新技术、新特性的不断丰富。

本书英文版的副书名是「Data Wrangling with Pandas, Numpy, and Ipython」，其中 Wrangling 是一个很难直译的词汇，它的原意是争执、争论，但在书中它描述的是将数据进行规整、处理的意思。希望读者读完本书后，可以使用好 pandas、Numpy 和 python 这些工具，更好地完成数据处理、分析的学习和工作。

## 01. 准备工作

### 1. 逻辑脉络

结构化数据的概念。大部分数据集都能被转化为更加适合分析和建模的结构化形式，虽然有时这并不是很明显。如果不行的话，也可以将数据集的特征提取为某种结构化形式。数据分析的步骤：获取数据；准备数据；转换数据；建模和计算；展示数据。

### 2. 摘录及评论

本书讲的是利用 Python 进行数据控制、处理、整理、分析等方面的具体细节和基本要点。我的目标是介绍 Python 编程和用于数据处理的库和工具环境，掌握这些，可以让你成为一个数据分析专家。虽然本书的标题是「数据分析」，重点确实 Python 编程、库，以及用于数据分析的工具。这就是数据分析要用到的 Python 编程。

当书中出现「数据」时，究竟指的是什么呢？主要指的是结构化数据（structured data），这个故意含糊其辞的术语代指了所有通用格式的数据，例如：表格型数据，其中各列可能是不同的类型（字符串、数值、日期等）。比如保存在关系型数据库中或以制表符 / 逗号为分隔符的文本文件中的那些数据；多维数组（矩阵）；通过关键列（对于 SQL 用户而言，就是主键和外键）相互联系的多个表；间隔平均或不平均的时间序列。

这绝不是一个完整的列表。大部分数据集都能被转化为更加适合分析和建模的结构化形式，虽然有时这并不是很明显。如果不行的话，也可以将数据集的特征提取为某种结构化形式。例如，一组新闻文章可以被处理为一张词频表，而这张词频表就可以用于情感分析。

2『大部分数据集都能被转化为更加适合分析和建模的结构化形式，做一张信息卡片。』

为什么要使用 Python 进行数据分析。许许多多的人（包括我自己）都很容易爱上 Python 这门语言。自从 1991 年诞生以来，Python 现在已经成为最受欢迎的动态编程语言之一，其他还有 Perl、Ruby 等。由于拥有大量的 Web 框架（比如 Rails（Ruby）和 Django（Python）），自从 2005 年，非常流行使用 Python 和 Ruby 进行网站建设工作。这些语言常被称作脚本（scripting）语言，因为它们可以用于编写简短而粗糙的小程序（也就是脚本）。我个人并不喜欢「脚本语言」这个术语，因为它好像在说这些语言无法用于构建严谨的软件。在众多解释型语言中，由于各种历史和文化的原因，Python 发展出了一个巨大而活跃的科学计算（scientific computing）社区。在过去的 10 年，Python 从一个边缘或「自担风险」的科学计算语言，成为了数据科学、机器学习、学界和工业界软件开发最重要的语言之一。

在数据分析、交互式计算以及数据可视化方面，Python 将不可避免地与其他开源和商业的领域特定编程语言 / 工具进行对比，如 R、MATLAB、SAS、Stata 等。近年来，由于 Python 的库（例如 pandas 和 scikit-learn）不断改良，使其成为数据分析任务的一个优选方案。结合其在通用编程方面的强大实力，我们完全可以只使用 Python 这一种语言构建以数据为中心的应用。

Python 作为胶水语言。Python 能变为成功的科学计算工具的部分原因是，它能够轻松地集成 C、C++ 以及 Fortran 代码。大部分现代计算环境都利用了一些 Fortran 和 C 库来实现线性代数、优选、积分、快速傅里叶变换以及其他诸如此类的算法。许多企业和国家实验室也利用 Python 来「粘合」那些已经用了多年的遗留软件系统。大多数软件都是由两部分代码组成的：少量需要占用大部分执行时间的代码，以及大量不经常执行的「胶水代码」。大部分情况下，胶水代码的执行时间是微不足道的。开发人员的精力几乎都是花在优化计算瓶颈上面，有时更是直接转用更低级的语言（比如 C）。

解决「两种语言」问题。很多组织通常都会用一种类似于领域特定的计算语言（如 SAS 和 R）对新的想法进行研究、原型构建和测试，然后再将这些想法移植到某个更大的生产系统中去（可能是用 Java、C# 或 C++ 编写的）。人们逐渐意识到，Python 不仅适用于研究和原型构建，同时也适用于构建生产系统。为什么一种语言就够了，却要使用两个语言的开发环境呢？我相信越来越多的企业也会这样看，因为研究人员和工程技术人员使用同一种编程工具将会给企业带来非常显著的组织效益。

为什么不选 Python。虽然 Python 非常适合构建分析应用以及通用系统，但它对不少应用场景适用性较差。由于 Python 是一种解释型编程语言，因此大部分 Python 代码都要比用编译型语言（比如 Java 和 C++）编写的代码运行慢得多。由于程序员的时间通常都比 CPU 时间值钱，因此许多人也愿意在这里做一些权衡。但是，在那些要求延迟非常小或高资源利用率的应用中（例如高频交易系统），耗费时间使用诸如 C++ 这样更低级、更低生产率的语言进行编程也是值得的。

对于高并发、多线程的应用程序而言（尤其是拥有许多计算密集型线程的应用程序），Python 并不是一种理想的编程语言。这是因为 Python 有一个叫做全局解释器锁（Global Interpreter Lock，GIL）的组件，这是一种防止解释器同时执行多条 Python 字节码指令的机制。有关「为什么会存在 GIL」的技术性原因超出了本书的范围。虽然很多大数据处理应用程序为了能在较短的时间内完成数据集的处理工作都需要运行在计算机集群上，但是仍然有一些情况需要用单进程多线程系统来解决。这并不是说 Python 不能执行真正的多线程并行代码。例如，Python 的 C 插件使用原生的 C 或 C++ 的多线程，可以并行运行而不被 GIL 影响，只要它们不频繁地与 Python 对象交互。

考虑到那些还不太了解 Python 科学计算生态系统和库的读者，下面我先对各个库做一个简单的介绍。

NumPy（Numerical Python 的简称）是 Python 科学计算的基础包。本书大部分内容都基于 NumPy 以及构建于其上的库。它提供了以下功能（不限于此）：快速高效的多维数组对象 ndarray；用于对数组执行元素级计算以及直接对数组执行数学运算的函数；用于读写硬盘上基于数组的数据集的工具；线性代数运算、傅里叶变换，以及随机数生成；成熟的 C API，用于 Python 插件和原生 C、C++、Fortran 代码访问 NumPy 的数据结构和计算工具。

除了为 Python 提供快速的数组处理能力，NumPy 在数据分析方面还有另外一个主要作用，即作为在算法和库之间传递数据的容器。对于数值型数据，NumPy 数组在存储和处理数据时要比内置的 Python 数据结构高效得多。此外，由低级语言（比如 C 和 Fortran）编写的库可以直接操作 NumPy 数组中的数据，无需进行任何数据复制工作。因此，许多 Python 的数值计算工具要么使用 NumPy 数组作为主要的数据结构，要么可以与 NumPy 进行无缝交互操作。

pandas 提供了快速便捷处理结构化数据的大量数据结构和函数。自从 2010 年出现以来，它助使 Python 成为强大而高效的数据分析环境。本书用得最多的 pandas 对象是 DataFrame，它是一个面向列（column-oriented）的二维表结构，另一个是 Series，一个一维的标签化数组对象。pandas 兼具 NumPy 高性能的数组计算功能以及电子表格和关系型数据库（如 SQL）灵活的数据处理功能。它提供了复杂精细的索引功能，以便更为便捷地完成重塑、切片和切块、聚合以及选取数据子集等操作。因为数据操作、准备、清洗是数据分析最重要的技能，pandas 是本书的重点。

作为一点背景，我是在 2008 年初开始开发 pandas 的，那时我任职于 AQR Capital Management，一家量化投资管理公司，我有许多工作需求都不能用任何单一的工具解决：有标签轴的数据结构，支持自动或清晰的数据对齐。这可以防止由于数据不对齐，和处理来源不同的索引不同的数据，造成的错误；集成时间序列功能；相同的数据结构用于处理时间序列数据和非时间序列数据；保存元数据的算术运算和压缩；灵活处理缺失数据；合并和其它流行数据库（例如基于 SQL 的数据库）的关系操作。

我想只用一种工具就实现所有功能，并使用通用软件开发语言。Python 是一个不错的候选语言，但是此时没有集成的数据结构和工具来实现。我一开始就是想把 pandas 设计为一款适用于金融和商业分析的工具，pandas 专注于深度时间序列功能和工具，适用于时间索引化的数据。

对于使用 R 语言进行统计计算的用户，肯定不会对 DataFrame 这个名字感到陌生，因为它源自于 R 的 data.frame 对象。但与 Python 不同，data frames 是构建于 R 和它的标准库。因此，pandas 的许多功能不属于 R 或它的扩展包。pandas 这个名字源于 panel data（面板数据，这是多维结构化数据集在计量经济学中的术语）以及 Python data analysis（Python 数据分析）。

matplotlib 是最流行的用于绘制图表和其它二维数据可视化的 Python 库。它最初由 John D.Hunter（JDH）创建，目前由一个庞大的开发人员团队维护。它非常适合创建出版物上用的图表。虽然还有其它的 Python 可视化库，matplotlib 却是使用最广泛的，并且它和其它生态工具配合也非常完美。我认为，可以使用它作为默认的可视化工具。

IPython 项目起初是 Fernando Pérez 在 2001 年的一个用以加强和 Python 交互的子项目。在随后的 16 年中，它成为了 Python 数据栈最重要的工具之一。虽然 IPython 本身没有提供计算和数据分析的工具，它却可以大大提高交互式计算和软件开发的生产率。IPython 鼓励「执行 - 探索」的工作流，区别于其它编程软件的「编辑 - 编译 - 运行」的工作流。它还可以方便地访问系统的 shell 和文件系统。因为大部分的数据分析代码包括探索、试错和重复，IPython 可以使工作更快。

2014 年，Fernando 和 IPython 团队宣布了 Jupyter 项目，一个更宽泛的多语言交互计算工具的计划。IPython web notebook 变成了 Jupyter notebook，现在支持 40 种编程语言。IPython 现在可以作为 Jupyter 使用 Python 的内核（一种编程语言模式）。

IPython 变成了 Jupyter 庞大开源项目（一个交互和探索式计算的高效环境）中的一个组件。它最老也是最简单的模式，现在是一个用于编写、测试、调试 Python 代码的强化 shell。你还可以使用通过 Jupyter Notebook，一个支持多种语言的交互式网络代码「笔记本」，来使用 IPython。IPython shell 和 Jupyter notebooks 特别适合进行数据探索和可视化。

Jupyter notebooks 还可以编写 Markdown 和 HTML 内容，提供了一种创建代码和文本的富文本方法。其它编程语言也在 Jupyter 中植入了内核，好让在 Jupyter 中可以使用 Python 另外的语言。对我个人而言，我的大部分 Python 都要用到 IPython，包括运行、调试和测试代码。在本书的 GitHub 页面，你可以找到包含各章节所有代码实例的 Jupyter notebooks。

SciPy 是一组专门解决科学计算中各种标准问题域的包的集合，主要包括下面这些包：scipy.integrate，数值积分例程和微分方程求解器；scipy.linalg，扩展了由 numpy.linalg 提供的线性代数例程和矩阵分解功能；scipy.optimize，函数优化器（最小化器）以及根查找算法；scipy.signal，信号处理工具；scipy.sparse，稀疏矩阵和稀疏线性系统求解器；scipy.special，SPECFUN（这是一个实现了许多常用数学函数（如伽玛函数）的 Fortran 库）的包装器；scipy.stats，标准连续和离散概率分布（如密度函数、采样器、连续分布函数等）、各种统计检验方法，以及更好的描述统计法。NumPy 和 SciPy 结合使用，便形成了一个相当完备和成熟的计算平台，可以处理多种传统的科学计算问题。

2010 年诞生以来，scikit-learn 成为了 Python 的通用机器学习工具包。仅仅七年，就汇聚了全世界超过 1500 名贡献者。它的子模块包括：分类，SVM、近邻、随机森林、逻辑回归等等；回归，Lasso、岭回归等等；聚类，k - 均值、谱聚类等等；降维，PCA、特征选择、矩阵分解等等；选型，网格搜索、交叉验证、度量；预处理，特征提取、标准化。与 pandas、statsmodels 和 IPython 一起，scikit-learn 对于 Python 成为高效数据科学编程语言起到了关键作用。虽然本书不会详细讲解 scikit-learn，我会简要介绍它的一些模型，以及用其它工具如何使用这些模型。

statsmodels 是一个统计分析包，起源于斯坦福大学统计学教授 Jonathan Taylor，他设计了多种流行于 R 语言的回归分析模型。Skipper Seabold 和 Josef Perktold 在 2010 年正式创建了 statsmodels 项目，随后汇聚了大量的使用者和贡献者。受到 R 的公式系统的启发，Nathaniel Smith 发展出了 Patsy 项目，它提供了 statsmodels 的公式或模型的规范框架。与 scikit-learn 比较，statsmodels 包含经典统计学和经济计量学的算法。包括如下子模块：回归模型，线性回归，广义线性模型，健壮线性模型，线性混合效应模型等等；方差分析（ANOVA）；时间序列分析，AR，ARMA，ARIMA，VAR 和其它模型；非参数方法，核密度估计，核回归；统计模型结果可视化。statsmodels 更关注与统计推断，提供不确定估计和参数 p - 值。相反的，scikit-learn 注重预测。同 scikit-learn 一样，我也只是简要介绍 statsmodels，以及如何用 NumPy 和 pandas 使用它。

注意：当你使用 conda 和 pip 二者安装包时，千万不要用 pip 升级 conda 的包，这样会导致环境发生问题。当使用 Anaconda 或 Miniconda 时，最好首先使用 conda 进行升级。

1『conda list 后，发现里面的包全跟 pip list 里的包混了，看不出来哪些事用 conda 安装的。想到的折中的办法，用 conda search 去筛选。』

除了在网上搜索，各式各样的科学和数据相关的 Python 邮件列表是非常有帮助的，很容易获得回答。包括：pydata，一个 Google 群组列表，用以回答 Python 数据分析和 pandas 的问题；pystatsmodels，statsmodels 或 pandas 相关的问题；scikit-learn 和 Python 机器学习邮件列表，scikit-learn@python.org；numpy-discussion，和 NumPy 相关的问题；scipy-user，SciPy 和科学计算的问题。因为这些邮件列表的 URLs 可以很容易搜索到，但因为可能发生变化，所以没有给出。

每年，世界各地会举办许多 Python 开发者大会。如果你想结识其他有相同兴趣的人，如果可能的话，我建议你去参加一个。许多会议会对无力支付入场费和差旅费的人提供财力帮助。下面是一些会议：PyCon 和 EuroPython，北美和欧洲的两大 Python 会议；SciPy 和 EuroSciPy，北美和欧洲两大面向科学计算的会议；PyData，世界范围内，一些列的地区性会议，专注数据科学和数据分析；国际和地区的 PyCon 会议（http://pycon.org 有完整列表） 。

接下来，简单地介绍了 NumPy 的关键特性，附录 A 中是更高级的 NumPy 功能。然后，我介绍了 pandas，本书剩余的内容全部是使用 pandas、NumPy 和 matplotlib 处理数据分析的问题。我已经尽量让全书的结构循序渐进，但偶尔会有章节之间的交叉，有时用到的概念还没有介绍过。

尽管读者各自的工作任务不同，大体可以分为几类：与外部世界交互，阅读编写多种文件格式和数据商店；数据准备，清洗、修改、结合、标准化、重塑、切片、切割、转换数据，以进行分析；转换数据，对旧的数据集进行数学和统计操作，生成新的数据集（例如，通过各组变量聚类成大的表）；建模和计算，将数据绑定统计模型、机器学习算法、或其他计算工具；展示，创建交互式和静态的图表可视化和文本总结。

1『数据分析的步骤：获取数据；准备数据；转换数据；建模和计算；展示数据。』

Python 社区已经广泛采取了一些常用模块的命名惯例：

```
import numpy as np
import matplotlib.pyplot as plt 
import pandas as pd 
import seaborn as sns
import statsmodels as sm
```

也就是说，当你看到 np.arange 时，就应该想到它引用的是 NumPy 中的 arange 函数。这样做的原因是：在 Python 软件开发过程中，不建议直接引入类似 NumPy 这种大型库的全部内容（from numpy import *）。

一些有关编程和数据科学方面的常用术语：数据规整（Munge/Munging/Wrangling），指的是将非结构化和（或）散乱数据处理为结构化或整洁形式的整个过程。这几个词已经悄悄成为当今数据黑客们的行话了。Munge 这个词跟 Lunge 押韵；伪码（Pseudocode），算法或过程的「代码式」描述，而这些代码本身并不是实际有效的源代码；语法糖（Syntactic sugar），这是一种编程语法，它并不会带来新的特性，但却能使代码更易读、更易写。

## 03. 内建数据结构、函数及文件

### 1. 逻辑脉络

清楚 python 的内建数据结构有哪些（list/dict/tuple/set）。虽然扩展库如 pandas 和 Numpy 使处理大数据集很方便，但它们是和 Python 的内置数据处理工具一同使用的。

### 2. 摘录及评论

本章将讨论贯穿本书所要使用的 Python 语言内建功能。由于像 pandas 和 Numpy 这类附加库提供了在大数据集上的高级计算功能，所以它们被设计为与 Python 内建数据操作工具协同使用。我们将开始介绍 Python 的常用数据结构：元组、列表、字典和集合。然后我们会讨论如何创建可复用的 Python 函数。我们将介绍 Python 文件对象的机制以及如何与你的本地硬盘进行交互。

元组是一个固定长度，不可改变的 Python 序列对象。创建元组的最简单方式，是用逗号分隔一列值；当用复杂的表达式定义元组，最好将值放到圆括号内；用 tuple 可以将任意序列或迭代器转换成元组；可以用方括号访问元组中的元素。和 C、C++、JAVA 等语言一样，序列是从 0 开始的；元组中存储的对象可能是可变对象。一旦创建了元组，元组中的对象就不能修改了；如果元组中的某个对象是可变的，比如列表，可以在原位进行修改；可以用加号运算符将元组串联起来。元组乘以一个整数，像列表一样，会将几个元组的复制串联起来。对象本身并没有被复制，只是引用了它。

如果你想将元组赋值给类似元组的变量，Python 会试图拆分等号右边的值。即使含有元组的元组也会被拆分。使用这个功能，你可以很容易地替换变量的名字，其它语言可能是这样，但是在 Python 中，替换可以这样做；变量拆分常用来迭代元组或列表序列。另一个常见用法是从函数返回多个值；Python 最近新增了更多高级的元组拆分功能，允许从元组的开头「摘取」几个元素。它使用了特殊的语法 *rest，这也用在函数签名中以抓取任意长度列表的位置参数。rest 的部分是想要舍弃的部分，rest 的名字不重要。作为惯用写法，许多 Python 程序员会将不需要的变量使用下划线；因为元组的大小和内容不能修改，它的实例方法都很轻量。其中一个很有用的就是 count（也适用于列表），它可以统计某个值得出现频率。

1『迭代元组和列表序列，例子是看懂了，但没摸清应用点；从函数返回多个值的应用也没弄懂。』

与元组不同，列表的长度是可变的，它所包含的内容也是可以修改的。你可以使用中括号 [] 或者 list 类型函数来定义列表；列表与元组非常相似（尽管元组不可修改），它们的很多函数用法是相似的；list 函数在数据处理中常用于将迭代器或者生成器转化为列表。

添加和删除元素。可以用 append 在列表末尾添加元素。insert 可以在特定的位置插入元素，插入的序号必须在 0 和列表长度之间。insert 的反操作是 pop，该操作会将特定位置的元素移除并返回。可以用 remove 去除某个值，remove 会先寻找第一个值并除去。如果不考虑性能，通过使用 append 和 remove，你可以将 Python 的列表用作一种完全合适的「多集合」数据结构。与字典、集合（后面会介绍）相比，检查列表中是否包含一个值是非常缓慢的。这是因为 Python 在列表中进行了线性逐个扫描，而在字典和集合中 Python 是同时检查所有元素的（基于哈希表）；用 in 可以检查列表是否包含某个值，否定 in 可以再加一个 not。

『警告：insert 与 append 相比，计算代价更高。因为子序列元素不得不在内部移动为新元素提供空间。如果你想要在序列的头部和尾部都插入元素，那你应该探索下 collections.deque，它是一个双端队列，可以满足头尾部都增加的要求。』

串联和组合列表。与元组类似，可以用加号将两个列表串联起来。如果已经定义了一个列表，用 extend 方法可以追加多个元素。请注意通过添加内容来连接列表是一种相对高代价的操作，这是因为连接过程中创建了新列表，并且还要复制对象。使用 extend 将元素添加到已经存在的列表是更好的方式，尤其是在你需要构建一个大型列表时。因此 everything = [] for chunk in list_of_lists: everything.extend(chunk) 要比串联方法快：everything = [] for chunk in list_of_lists: everything = everything + chunk。

排序。你可以调用列表的 sort 方法对列表进行内部排序（无须新建一个对象）。sort 有一些选项偶尔会派上用场。其中一项是传递一个二级排序 key —— 一个用于生成排序值的函数。例如，我们可以通过字符串的长度进行排序；稍后，我们会学习 sorted 函数，该方法可以针对通用序列产生一个排序后的拷贝。

1『key 是方法 sort 的一个实参，要传递进去。b.sort(key=len)』

二分搜索和维护已排序的列表。bisect 模块支持二分查找，和向已排序的列表插入值。bisect.bisect 可以找到插入值后仍保证排序的位置，bisect.insort 是向这个位置插入值。注意：bisect 模块不会检查列表是否已排好序，进行检查的话会耗费大量计算。因此，对未排序的列表使用 bisect 不会产生错误，但结果不一定正确。

切片。用切边可以选取大多数序列类型的一部分，切片的基本形式是在方括号中使用 start:stop；切片也可以被序列赋值；切片的起始元素是包括的，不包含结束元素。因此，结果中包含的元素个数是 stop - start；start 或 stop 都可以被省略，省略之后，分别默认序列的开头和结尾；负数表明从后向前切片；需要一段时间来熟悉使用切片，尤其是当你之前学的是 R 或 MATLAB。图 3-1 展示了正整数和负整数的切片。在图中，指数标示在边缘以表明切片是在哪里开始哪里结束的；在第二个冒号后面使用 step，可以隔一个取一个元素，一个聪明的方法是使用 - 1，它可以将列表或元组颠倒过来。

序列函数。Python 有很多有用的序列函数，你应当熟悉并择机使用。

enumerate 函数。我们经常需要在遍历一个序列的同时追踪当前元素的索引。一种自行实现的方法像下面的示例。由于这种场景很常见，所以 Python 内建了 enumerate 函数，返回了（i, value）元组的序列，其中 value 是元素的值，i 是元素的索引。当你需要对数据建立索引时，一种有效的模式就是使用 enumerate 构造一个字典，将序列值（假设是唯一的）映射到索引位置上。

sorted 函数。sorted 函数可以从任意序列的元素返回一个新的排好序的列表。sorted 函数可以接受和 sort 相同的参数。

zip 函数。zip 将列表、元组或其他序列的元素配对，组合成一个元组列表；zip 可以处理任意长度的序列，它生成列表长度由最短的序列决定；zip 的常用场景为同时遍历多个序列，有时候会和 enumerate 同时使用；给定一个已「配对」的序列时，zip 函数有一种机智的方式去「拆分」序列。这种方式的另一种思路就是将行的列表转换为列的列表。语法看上去略显魔幻。

```
for i, (a, b) in enumerate(zip(seq1, seq2)): 
    print('{0}: {1}, {2}'.format(i, a, b)) 
```

```
In [96]: pitchers = [('Nolan', 'Ryan'), ('Roger', 'Clemens'), ('Schilling', 'Curt')] 
In [97]: first_names, last_names = zip(*pitchers) 
In [98]: first_names 
Out[98]: ('Nolan', 'Roger', 'Schilling') 
In [99]: last_names 
Out[99]: ('Ryan', 'Clemens', 'Curt')
```

reversed 函数。reversed 可以从后向前迭代一个序列。要记住 reversed 是一个生成器（后面详细介绍），只有实体化（即列表或 for 循环）之后才能创建翻转的序列。

字典。dict（字典）可能是Python内建数据结构中最重要的。它更为常用的名字是哈希表或者是关联数组。字典是拥有灵活尺寸的键值对集合，其中键和值都是 Python 对象。用大括号 {} 是创建字典的一种方式，在字典中用逗号将键值对分隔；你可以像访问列表或元组中的元素一样，访问、插入或设定字典中的元素；你可以用检查列表和元组是否包含某个值得方法，检查字典中是否包含某个键；可以用 del 关键字或 pop 方法（返回值得同时删除键）删除值；eys 和 values 是字典的键和值的迭代器方法。虽然键值对没有顺序，这两个方法可以用相同的顺序输出键和值；用 update 方法可以将一个字典与另一个合并。update 方法改变了字典中元素位置，因此对于任何原字典中已经存在的键，如果传给 update 方法的数据也含有相同的键，则它的值将会被覆盖。

用序列创建字典。常常，你可能想将两个序列配对组合成字典。下面是一种写法。由于字典本质上是2-元组（含有 2 个元素的元组）的集合，字典是可以接受一个2-元组的列表作为参数的。稍后我们将会讨论字典推导式，那是另一种构建字典的方法。

默认值。下面的逻辑很常见。不过字典的 get 方法和 pop 方法可以返回一个默认值，因此上述的 if-else 代码块可以被简写为；带有默认值的 get 方法会在 key 参数不是字典的键时返回 None，而 pop 会抛出异常。

    value = some_dict.get(key, default_value)

一个常见的场景是字典中的值集合通过设置，成为另一种集合，比如列表。举个例子，你可以想象一下将字词组成的列表根据首字母分类为包含列表的字典。setdefault 方法就正是干这个的。上面的 for 循环可以改写为：

    for word in words: letter = word[0] by_letter.setdefault(letter, []).append(word)

内建的集合模块有一个非常有用的类，defaultdict。这个类使得上述目的实现更为简单。想要生成符合要求的字典，你可以向字典中传入类型或能在各位置生成默认值的函数：

```
from collections import defaultdict 
by_letter = defaultdict(list) 
for word in words: 
    by_letter[word[0]].append(word)
```

1『原文中的这几段代码没有完全消化掉，多琢磨琢磨。』

有效的键类型。字典的值可以是任意 Python 对象，而键通常是不可变的标量类型（整数、浮点型、字符串）或元组（元组中的对象必须是不可变的）。这被称为「可哈希性」。可以用 hash 函数检测一个对象是否是可哈希的（可被用作字典的键）。要用列表当做键，一种方法是将列表转化为元组，只要内部元素可以被哈希，它也就可以被哈希。

集合。集合是一种无序且元素唯一的容器。你可以认为集合也像字典，但是只有键没有值。集合可以有两种创建方式：通过 set 函数或者是用字面值集与大括号的语法；集合支持合并、交集、差分和对称差等数学集合运算。考虑两个示例集合。合并是取两个集合中不重复的元素。可以用 union 方法，或者 | 运算符。交集的元素包含在两个集合中。可以用 intersection 或 & 运算符。表 3-1 列出了常用的集合方法；所有的逻辑集合运算都有对应操作，允许你用操作的结果替代操作左边的集合内容。对于大型集合，下面的代码效率更高；和字典类似，集合的元素必须是不可变的。如果想要包含列表型的元素，必须先转换为元组；你还可以检测一个集合是否是另一个集合的子集或父集；当且仅当两个集合的内容一模一样时，两个集合才相等。

列表、集合和字典的推导式。列表推导式是 Python 最受喜爱的特性之一。它允许用户方便的从一个集合过滤元素，形成列表，在传递参数的过程中还可以修改元素。形式如下：

    [expr for val in collection if condition]

它等同于下面的 for 循环；

```
result = [] 
for val in collection: 
    if condition: 
        result.append(expr)
```

filter 条件可以被忽略，只留下表达式就行。例如，给定一个字符串列表，我们可以过滤出长度在 2 及以下的字符串，并将其转换成大写：

```
In [154]: strings = ['a', 'as', 'bat', 'car', 'dove', 'python'] 
In [155]: [x.upper() for x in strings if len(x) > 2] 
Out[155]: ['BAT', 'CAR', 'DOVE', 'PYTHON']
```

用相似的方法，还可以推导集合和字典。字典的推导式如下所示：

    dict_comp = {key-expr : value-expr for value in collection if condition}

集合的推导式与列表很像，只不过用的是尖括号：

    set_comp = {expr for value in collection if condition}

与列表推导式类似，集合与字典的推导也很方便，而且使代码的读写都很容易。来看前面的字符串列表。假如我们只想要字符串的长度，用集合推导式的方法非常方便：

```
In [156]: unique_lengths = {len(x) for x in strings} 
In [157]: unique_lengths 
Out[157]: {1, 2, 3, 4, 6}
```

map 函数可以进一步简化：

```
In [158]: set(map(len, strings)) 
Out[158]: {1, 2, 3, 4, 6}
```

作为一个字典推导式的例子，我们可以创建一个字符串的查找映射表以确定它在列表中的位置：

```
In [159]: loc_mapping = {val : index for index, val in enumerate(strings)} 
In [160]: loc_mapping 
Out[160]: {'a': 0, 'as': 1, 'bat': 2, 'car': 3, 'dove': 4, 'python': 5}
```

假设我们有一个包含列表的列表，包含了一些英文名和西班牙名；你可能是从一些文件得到的这些名字，然后想按照语言进行分类。现在假设我们想用一个列表包含所有的名字，这些名字中包含两个或更多的 e。可以用 for 循环来做；可以用嵌套列表推导式的方法，将这些写在一起，如下所示：

```
In [162]: result = [name for names in all_data for name in names 
.....:     if name.count('e') >= 2] 
In [163]: result 
Out[163]: ['Steven']
```

嵌套列表推导式可能会让你有点晕头转向。列表推导式的for循环部分是根据嵌套的顺序排列的，所有的过滤条件像之前一样被放在尾部。下面的例子是将含有整数元组的列表扁平化为一个简单的整数列表：

```
In [164]: some_tuples = [(1, 2, 3), (4, 5, 6), (7, 8, 9)] 
In [165]: flattened = [x for tup in some_tuples for x in tup] 
In [166]: flattened 
Out[166]: [1, 2, 3, 4, 5, 6, 7, 8, 9]
```

记住，for 表达式的顺序是与嵌套 for 循环的顺序一样（而不是列表推导式的顺序）：

```
flattened = [] 
for tup in some_tuples:
    for x in tup: 
        flattened.append(x)
```

你当然可以嵌套多层的列表推导式，但超过两到三层之后你很可能开始疑惑这种做法是否会有利于代码可读性。嵌套推导式的语法要和列表推导式中的列表推导式区分开，列表推导式中的列表推导式也是非常有效的：


```
In [167]: [[x for x in tup] for tup in some_tuples] 
Out[167]: [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
```

这段代码产生了一个列表的列表，而不是扁平化的只包含元素的列表。

函数是 Python 中最主要也是最重要的代码组织和复用手段。作为最重要的原则，如果你要重复使用相同或非常类似的代码，就需要写一个函数。通过给函数起一个名字，还可以提高代码的可读性。函数使用 def 关键字声明，用 return 关键字返回值；同时拥有多条 return 语句也是可以的。如果到达函数末尾时没有遇到任何一条 return 语句，则返回 None；函数可以有一些位置参数（positional）和一些关键字参数（keyword）。关键字参数通常用于指定默认值或可选参数。在上面的函数中，x 和 y 是位置参数，而 z 则是关键字参数。也就是说，该函数可以下面这两种方式进行调用；函数参数的主要限制在于：关键字参数必须位于位置参数（如果有的话）之后。你可以任何顺序指定关键字参数。也就是说，你不用死记硬背函数参数的顺序，只要记得它们的名字就可以了；也可以用关键字传递位置参数。前面的例子，也可以写为。

命名空间、作用域和局部函数。函数可以访问两种不同作用域中的变量：全局（global）和局部（local）。Python 有一种更科学的用于描述变量作用域的名称，即命名空间（namespace）。任何在函数中赋值的变量默认都是被分配到局部命名空间（local namespace）中的。局部命名空间是在函数被调用时创建的，函数参数会立即填入该命名空间。在函数执行完毕之后，局部命名空间就会被销毁（会有一些例外的情况，具体请参见后面介绍闭包的那一节）。看看下面这个函数；调用 func () 之后，首先会创建出空列表 a，然后添加 5 个元素，最后 a 会在该函数退出的时候被销毁。假如我们像下面这样定义 a；虽然可以在函数中对全局变量进行赋值操作，但是那些变量必须用 global 关键字声明成全局的才行。

注意：我常常建议人们不要频繁使用 global 关键字。因为全局变量一般是用于存放系统的某些状态的。如果你发现自己用了很多，那可能就说明得要来点儿面向对象编程了（即使用类）。

返回多个值。在我第一次用 Python 编程时（之前已经习惯了 Java 和 C++），最喜欢的一个功能是：函数可以返回多个值。下面是一个简单的例子；在数据分析和其他科学计算应用中，你会发现自己常常这么干。该函数其实只返回了一个对象，也就是一个元组，最后该元组会被拆包到各个结果变量中。在上面的例子中，我们还可以这样写；这里的 return_value 将会是一个含有 3 个返回值的三元元组。此外，还有一种非常具有吸引力的多值返回方式 —— 返回字典；具体用哪种技术取决于你需要做什么。

由于 Python 函数都是对象，因此，在其他语言中较难表达的一些设计思想在 Python 中就要简单很多了。假设我们有下面这样一个字符串数组，希望对其进行一些数据清理工作并执行一堆转换；不管是谁，只要处理过由用户提交的调查数据，就能明白这种乱七八糟的数据是怎么一回事。为了得到一组能用于分析工作的格式统一的字符串，需要做很多事情：去除空白符、删除各种标点符号、正确的大写格式等。做法之一是使用内建的字符串方法和正则表达式 re 模块；其实还有另外一种不错的办法，将需要在一组给定字符串上执行的所有运算做成一个列表；这种多函数模式使你能在很高的层次上轻松修改字符串的转换方式。此时的 clean_strings 也更具可复用性！还可以将函数用作其他函数的参数，比如内置的 map 函数，它用于在一组数据上应用一个函数。

匿名（lambda）函数。Python 支持一种被称为匿名的、或 lambda 函数。它仅由单条语句组成，该语句的结果就是返回值。它是通过 lambda 关键字定义的，这个关键字没有别的含义，仅仅是说「我们正在声明的是一个匿名函数」。

```
def short_function(x): 
    return x * 2 
equiv_anon = lambda x: x * 2
```

本书其余部分一般将其称为 lambda 函数。它们在数据分析工作中非常方便，因为你会发现很多数据转换函数都以函数作为参数的。直接传入 lambda 函数比编写完整函数声明要少输入很多字（也更清晰），甚至比将 lambda 函数赋值给一个变量还要少输入很多字。看看下面这个简单得有些傻的例子：

```
def apply_to_list(some_list, f): 
    return [f(x) for x in some_list] 
ints = [4, 0, 1, 5, 6] 
apply_to_list(ints, lambda x: x * 2)
```

虽然你可以直接编写 [x *2for x in ints]，但是这里我们可以非常轻松地传入一个自定义运算给 apply_to_list 函数。再来看另外一个例子。假设有一组字符串，你想要根据各字符串不同字母的数量对其进行排序；这里，我们可以传入一个 lambda 函数到列表的 sort 方法：

```
In [177]: strings = ['foo', 'card', 'bar', 'aaaa', 'abab']
In [178]: strings.sort(key=lambda x: len(set(list(x)))) 
In [179]: strings 
Out[179]: ['aaaa', 'foo', 'abab', 'bar', 'card']
```

笔记：lambda 函数之所以会被称为匿名函数，与 def 声明的函数不同，原因之一就是这种函数对象本身是没有提供名称 name 属性。

柯里化：部分参数应用。柯里化（currying）是一个有趣的计算机科学术语，它指的是通过「部分参数应用」（partial argument application）从现有函数派生出新函数的技术。例如，假设我们有一个执行两数相加的简单函数；通过这个函数，我们可以派生出一个新的只有一个参数的函数 ——add_five，它用于对其参数加 5：

    add_five = lambda y: add_numbers(5, y)

add_numbers 的第二个参数称为「柯里化的」（curried）。这里没什么特别花哨的东西，因为我们其实就只是定义了一个可以调用现有函数的新函数而已。内置的 functools 模块可以用 partial 函数将此过程简化。

生成器。能以一种一致的方式对序列进行迭代（比如列表中的对象或文件中的行），是 Python 的一个重要特点。这是通过一种叫做迭代器协议（iterator protocol，它是一种使对象可迭代的通用方式）的方式实现的，一个原生的使对象可迭代的方法。比如说，遍历一个字典，获得字典的键；当你编写 for key in some_dict 时，Python 解释器首先会尝试从 some_dict 创建一个迭代器；迭代器是一种特殊对象，它可以在诸如 for 循环之类的上下文中向 Python 解释器输送对象。大部分能接受列表之类的对象的方法也都可以接受任何可迭代对象。比如 min、max、sum 等内置方法以及 list、tuple 等类型构造器。

生成器（generator）是构造新的可迭代对象的一种简单方式。一般的函数执行之后只会返回单个值，而生成器则是以延迟的方式返回一个值序列，即每返回一个值之后暂停，直到下一个值被请求时再继续。要创建一个生成器，只需将函数中的 return 替换为 yeild 即可；调用该生成器时，没有任何代码会被立即执行；直到你从该生成器中请求元素时，它才会开始执行其代码。

生成器表达式。另一种更简洁的构造生成器的方法是使用生成器表达式（generator expression）。这是一种类似于列表、字典、集合推导式的生成器。其创建方式为，把列表推导式两端的方括号改成圆括号；它跟下面这个冗长得多的生成器是完全等价的；生成器表达式也可以取代列表推导式，作为函数参数。

```
In [191]: sum(x ** 2 for x in range(100)) 
Out[191]: 328350 
In [192]: dict((i, i **2) for i in range(5)) 
Out[192]: {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}
```

itertools 模块。标准库 itertools 模块中有一组用于许多常见数据算法的生成器。例如，groupby 可以接受任何序列和一个函数。它根据函数的返回值对序列中的连续元素进行分组。下面是一个例子；表 3-2 中列出了一些我经常用到的 itertools 函数。建议参阅 Python 官方文档，进一步学习。

错误和异常处理。优雅地处理 Python 的错误和异常是构建健壮程序的重要部分。在数据分析中，许多函数函数只用于部分输入。例如，Python 的 float 函数可以将字符串转换成浮点数，但输入有误时，有 ValueError 错误；假如想优雅地处理 float 的错误，让它返回输入值。我们可以写一个函数，在 try/except 中调用 float。当 float (x) 抛出异常时，才会执行 except 的部分；你可能注意到 float 抛出的异常不仅是 ValueError。你可能只想处理 ValueError，TypeError 错误（输入不是字符串或数值）可能是合理的 bug。可以写一个异常类型；可以通过将多个异常类型写成元组的方式同时捕获多个异常（小括号是必不可少的）；某些情况下，你可能想要处理一个异常，但是你希望一部分代码无论 try 代码块是否报错都要执行。为了实现这个目的，使用 finally 关键字；这样，我们可以让 f 在程序结束后总是关闭。类似地，你可以使用 else 来执行当 try 代码块成功执行时才会执行的代码。

比标准 Python 解释器提供更多额外的上下文是 IPython 的一大进步（标准 Python 解释器不提供任何额外的上下文）。你可以使用 %xmode 命令来控制上下文的数量，可以从 Plain（普通）模式（与标准 Python 解释器一致）切换到 Verbose（复杂）模式（可以显示函数的参数值以及更多有用信息）。你将会在下一章看到如何在错误发生后进入异常堆栈（使用 %debug 或 %pdb 命令）进行交互式事后调试。

文件和操作系统。本书的代码示例大多使用诸如 pandas.read_csv 之类的高级工具将磁盘上的数据文件读入 Python 数据结构。但我们还是需要了解一些有关 Python 文件处理方面的基础知识。好在它本来就很简单，这也是 Python 在文本和文件处理方面的如此流行的原因之一。为了打开一个文件以便读写，可以使用内置的 open 函数以及一个相对或绝对的文件路径；默认情况下，文件是以只读模式（'r'）打开的。然后，我们就可以像处理列表那样来处理这个文件句柄 f 了，比如对行进行迭代；行内容会在行结尾标识（EOL）完整的情况下从文件中全部读出，所以你会经常看到一些代码，功能是将文件中的内容形成不带 EOL 的列表；如果使用 open 创建文件对象，一定要用 close 关闭它。关闭文件可以返回操作系统资源。用 with 语句可以可以更容易地清理打开的文件，这样可以在退出代码块时，自动关闭文件；如果输入 f =open (path,'w')，就会有一个新文件被创建在 examples/segismundo.txt，并覆盖掉该位置原来的任何数据。另外有一个 x 文件模式，它可以创建可写的文件，但是如果文件路径存在，就无法创建。表 3-3 列出了所有的读 / 写模式。

1『译者说的迭代其实指遍历。』

对于可读文件，一些常用的方法是 read、seek 和 tell。read 会从文件返回字符。字符的内容是由文件的编码决定的（如 UTF-8），如果是二进制模式打开的就是原始字节；read 方法通过读取的字节数来推进文件句柄的位置。tell 方法可以给出句柄当前的位置；尽管我们从文件读取了 10 个字符，位置却是 11，这是因为用默认的编码用了这么多字节才解码了这 10 个字符。你可以用 sys 模块检查默认的编码；seek 将文件位置更改为文件中的指定字节。

```
In [219]: import sys 
In [220]: sys.getdefaultencoding() 
Out[220]: 'utf-8'
```

1『sys 是 Python 的标准模块，又体会到标准模块的丰富。』

3『

编码的有关概念：对于英语我们一般不会遇到什么编码问题，因为一个英文字符在系统里占一个位置。对于中文的话，微软设置了一个编码 gbk，一个汉字在系统里占有 2 个位置，gbk 编码虽然节省空间，但只包含有常用字。对于那些生僻的字，就用 utf-8 编码，这个编码一个汉字是占有 3 个位置。如果 resquest 得到的对象直接 read() 的话，它会按一个字符占一个位置的规则去读，那么读出来的就是乱码。解决的办法是在 read() 方法里指定解码的方式。

	response.read().decode('utf-8')

read() 读出来的信息是 html 语法形式的。

』

向文件写入，可以使用文件的 write 或 writelines 方法。例如，我们可以创建一个无空行版的 prof_mod.py。

文件的字节和 Unicode。Python 文件的默认操作是「文本模式」，也就是说，你需要处理 Python 的字符串（即 Unicode）。它与「二进制模式」相对，文件模式加一个 b。我们来看上一节的文件（UTF-8 编码、包含非 ASCII 字符）；UTF-8 是长度可变的 Unicode 编码，所以当我从文件请求一定数量的字符时，Python 会从文件读取足够多（可能少至 10 或多至 40 字节）的字节进行解码。如果以「rb」模式打开文件，则读取确切的请求字节数；取决于文本的编码，你可以将字节解码为 str 对象，但只有当每个编码的 Unicode 字符都完全成形时才能这么做；文本模式下，利用 open 方法的选项参数 encoding, Python 提供了一种方便的方法将文件内容从 Unicode 编码转换为其他类型的编码。

除了二进制模式，在打开文件时使用 seek 要当心。如果文件的句柄位置恰好在一个 Unicode 符号的字节中间时，后续的读取会导致错误；如果你经常要对非 ASCII 字符文本进行数据分析，通晓 Python 的 Unicode 功能是非常重要的。更多内容，参阅 Python 官方文档。

## 04. NumPy 基础：数组和矢量计算

### 1. 逻辑脉络

NumPy 的核心是对数组（矩阵）的强大操作。

### 2. 摘录及评论

NumPy（Numerical Python 的简称）是 Python 数值计算最重要的基础包。大多数提供科学计算的包都是用 NumPy 的数组作为构建基础。NumPy 的部分功能如下：ndarray，一个具有矢量算术运算和复杂广播能力的快速且节省空间的多维数组；用于对整组数据进行快速运算的标准数学函数（无需编写循环）；用于读写磁盘数据的工具以及用于操作内存映射文件的工具；线性代数、随机数生成以及傅里叶变换功能；用于集成由 C、C++、Fortran 等语言编写的代码的 C 语言 API。

由于 NumPy 提供了一个简单易用的 C 语言 API，因此很容易将数据传递给由低级语言编写的外部库，外部库也能以 NumPy 数组的形式将数据返回给 Python。这个功能使 Python 成为一种包装 C/C++/Fortran 历史代码库的选择，并使被包装库拥有一个动态的、易用的接口。NumPy 本身并没有提供多么高级的数据分析功能，理解 NumPy 数组以及面向数组的计算将有助于你更加高效地使用诸如 pandas 之类的工具。因为 NumPy 是一个很大的题目，我会在附录 A 中介绍更多 NumPy 高级功能，比如广播。对于大部分数据分析应用而言，我最关注的功能主要集中在：用于数据整理和清理、子集构造和过滤、转换等快速的矢量化数组运算；常用的数组算法，如排序、唯一化、集合运算等；高效的描述统计和数据聚合/摘要运算；用于异构数据集的合并/连接运算的数据对齐和关系型数据运算；将条件逻辑表述为数组表达式（而不是带有 if-elif-else 分支的循环）；数据的分组运算（聚合、转换、函数应用等）。

虽然 NumPy 提供了通用的数值数据处理的计算基础，但大多数读者可能还是想将 pandas 作为统计和分析工作的基础，尤其是处理表格数据时。pandas 还提供了一些 NumPy 所没有的更加领域特定的功能，如时间序列处理等。

笔记：Python 的面向数组计算可以追溯到 1995 年，Jim Hugunin 创建了 Numeric 库。接下来的 10 年，许多科学编程社区纷纷开始使用 Python 的数组编程，但是进入 21 世纪，库的生态系统变得碎片化了。2005 年，Travis Oliphant 从 Numeric 和 Numarray 项目整了出了 NumPy 项目，进而所有社区都集合到了这个框架下。NumPy 之于数值计算特别重要的原因之一，是因为它可以高效处理大数组的数据。这是因为：NumPy 是在一个连续的内存块中存储数据，独立于其他 Python 内置对象。NumPy 的 C 语言编写的算法库可以操作内存，而不必进行类型检查或其它前期工作。比起 Python 的内置序列，NumPy 数组使用的内存更少；NumPy 可以在整个数组上执行复杂的计算，而不需要 Python 的 for 循环。

基于 NumPy 的算法要比纯 Python 快 10 到 100 倍（甚至更快），并且使用的内存更少。NumPy 最重要的一个特点就是其 N 维数组对象（即 ndarray），该对象是一个快速而灵活的大数据集容器。你可以利用这种数组对整块数据执行一些数学运算，其语法跟标量元素之间的运算一样。要明白 Python 是如何利用与标量值类似的语法进行批次计算，我先引入 NumPy，然后生成一个包含随机数据的小数组：

笔记：在本章及全书中，我会使用标准的 NumPy 惯用法 import numpy as np。你当然也可以在代码中使用 from numpy import *，但不建议这么做。numpy 的命名空间很大，包含许多函数，其中一些的名字与 Python 的内置函数重名（比如 min 和 max）。

ndarray 是一个通用的同构数据多维容器，也就是说，其中的所有元素必须是相同类型的。每个数组都有一个 shape（一个表示各维度大小的元组）和一个 dtype（一个用于说明数组数据类型的对象）。

1『ndarray 对象太重要了，是 numpy 的核心基础。shape 和 dtype 是多维数组对象里的属性。比如 data.shape 返回的是数组各个维度的大小，比如是 (2, 3)；data.dtype 返回的是多维数组数据类型，比如是 dtype('float64')。』

本章将会介绍 NumPy 数组的基本用法，这对于本书后面各章的理解基本够用。虽然大多数数据分析工作不需要深入理解 NumPy，但是精通面向数组的编程和思维方式是成为 Python 科学计算牛人的一大关键步骤。

笔记：当你在本书中看到「数组」、「NumPy 数组」、「ndarray」时，基本上都指的是同一样东西，即 ndarray 对象。

创建数组最简单的办法就是使用 array 函数。它接受一切序列型的对象（包括其他数组），然后产生一个新的含有传入数据的 NumPy 数组。以一个列表的转换为例；嵌套序列（比如由一组等长列表组成的列表）将会被转换为一个多维数组。因为 data2 是列表的列表，NumPy 数组 arr2 的两个维度的 shape 是从 data2 引入的。可以用属性 ndim 和 shape 验证；除非特别说明（稍后将会详细介绍），np.array 会尝试为新建的这个数组推断出一个较为合适的数据类型。数据类型保存在一个特殊的 dtype 对象中。除 np.array 之外，还有一些函数也可以新建数组。比如，zeros 和 ones 分别可以创建指定长度或形状的全 0 或全 1 数组。empty 可以创建一个没有任何具体值的数组。要用这些方法创建多维数组，只需传入一个表示形状的元组即可。

想要使用 np.empty 来生成一个全 0 数组，并不安全，有些时候它可能会返回未初始化的垃圾数值。arange 是 Python 内置函数 range 的数组版；表 4-1 列出了一些数组创建函数。由于 NumPy 关注的是数值计算，因此，如果没有特别指定，数据类型基本都是 float64（浮点数）。

数据类型，即 dytpe，是一个特殊的对象，它包含了 ndarray 需要为某一种类型数据所申明的内存块信息（也称为元数据，即表示数据的数据）；dtype 是 NumPy 能够与其他系统数据灵活交互的原因。通常，其他系统提供一个硬盘或内存与数据的对应关系（直接映射到相应的机器表示），使得利用 C 或 Fortran 等底层语言读写数据变得十分方便。数据的 dtype 通常都是按照一个方式命名：类型名，比如 float 和 int，后面再接上表明每个元素位数的数字。一个标准的双精度浮点值（Python 中数据类型为 float），将使用 8 字节或 64 位。因此，这个类型在 NumPy 中称为 float64。表 4-2 将展现所有的 NumPy 所支持的数据类型。

笔记：记不住这些 NumPy 的 dtype 也没关系，新手更是如此。通常只需要知道你所处理的数据的大致类型是浮点数、复数、整数、布尔值、字符串，还是普通的 Python 对象即可。当你需要控制数据在内存和磁盘中的存储方式时（尤其是对大数据集），那就得了解如何控制存储类型。

可以通过 ndarray 的 astype 方法明确地将一个数组从一个 dtype 转换成另一个 dtype；在本例中，整数被转换成了浮点数。如果将浮点数转换成整数，则小数部分将会被截取删除；如果你有一个数组，里面的元素都是表达数字含义的字符串，也可以通过 astype 将字符串转换为数字。

在 NumPy 中，当使用 numpy.string_ 类型作字符串数据要小心，因为 NumPy 会修正它的大小或删除输入且不发出警告。pandas 在处理非数值数据时有更直观的开箱型操作；如果因为某些原因导致转换类型失败（比如字符串无法转换为 float64 位时），将会抛出一个ValueError。这里我偷懒地使用 float 来代替 np.float64，是因为 NumPy 可以使用相同别名来表征与 Python 精度相同的 Python 数据类型。你也可以使用另一个数组的 dtype 属性；也可以使用类型代码来传入数据类型。

笔记：使用 astype 时总是生成一个新的数组，即使你传入的 dtype 与之前一样。

NumPy 数组的运算。数组很重要，因为它使你不用编写循环即可对数据执行批量运算。NumPy 用户称其为向量化（vectorization）。大小相等的数组之间的任何算术运算都会将运算应用到元素级；数组与标量的算术运算会将标量值传播到各个元素；大小相同的数组之间的比较会生成布尔值数组；不同大小的数组之间的运算叫做广播（broadcasting），将在附录 A 中对其进行详细讨论。本书的内容不需要对广播机制有多深的理解。

1『广播的定义：不同大小的数组之间的运算。』

基本的索引和切片。NumPy 数组的索引是一个内容丰富的主题，因为选取数据子集或单个元素的方式有很多。一维数组很简单。从表面上看，它们跟 Python 列表的功能差不多；如上所示，当你将一个标量值赋值给一个切片时（如 arr [5:8]=12），该值会自动传播（也就说后面将会讲到的「广播」）到整个选区。跟列表最重要的区别在于，数组切片是原始数组的视图。这意味着数据不会被复制，视图上的任何修改都会直接反映到源数组上。作为例子，先创建一个 arr 的切片；现在，当我修稿 arr_slice 中的值，变动也会体现在原始数组 arr 中；切片 [:] 会给数组中的所有值赋值。

如果你刚开始接触 NumPy，可能会对此感到惊讶（尤其是当你曾经用过其他热衷于复制数组数据的编程语言）。由于 NumPy 的设计目的是处理大数据，所以你可以想象一下，假如 NumPy 坚持要将数据复制来复制去的话会产生何等的性能和内存问题。注意：如果你想要得到的是 ndarray 切片的一份副本而非视图，就需要明确地进行复制操作，例如 arr [5:8].copy ()。

对于高维度数组，能做的事情更多。在一个二维数组中，各索引位置上的元素不再是标量而是一维数组；因此，可以对各个元素进行递归访问，但这样需要做的事情有点多。你可以传入一个以逗号隔开的索引列表来选取单个元素。也就是说，下面两种方式是等价的；图 4-1 说明了二维数组的索引方式。轴 0 作为行，轴 1 作为列。

```
In [74]: arr2d[0][2] 
Out[74]: 3 
In [75]: arr2d[0, 2] 
Out[75]: 3
```

在多维数组中，你可以省略后续索引值，返回的对象将是降低一个维度的数组。因此在一个 2×2×3 的数组 arr3d 中。arr3d [0] 是一个 2×3 数组；标量值和数组都可以被赋值给 arr3d [0]；相似的，arr3d [1,0] 可以访问索引以 (1,0) 开头的那些值（以一维数组的形式返回）；上面的表达式可以分解为下面两步，注意在上面所有这些选取数组子集的例子中，返回的数组都是视图。

切片索引。ndarray 的切片语法跟 Python 列表这样的一维对象差不多；对于之前的二维数组 arr2d，其切片方式稍显不同。如你所见，数组沿着轴 0 进行了切片。表达式 arrzd[:2] 的含义为选择 arr2d 的前两「行」。你可以进行多组切片，与多组索引类似；像这样进行切片时，只能得到相同维数的数组视图。通过将整数索引和切片混合，可以得到低维度的切片。例如，我可以选取第二行的前两列；图 4-2 对此进行了说明。注意，「只有冒号」表示选取整个轴，因此你可以像下面这样只对高维轴进行切片；自然，对切片表达式的赋值操作也会被扩散到整个选区。

1『切片操作时，末尾索引值无效，倒数第二个才有效。比如对 arr2d 切片，arr2d[:2] 是切前 2 行，arr2d[1:3] 是切第二行到第三行；arr2d[:2, 1:] 表示行切前 2 行，列切从第 2 列到最后一列；arr2d[2, :2] 表示第二行的前两列。』

布尔型索引。来看这样一个例子，假设我们有一个用于存储数据的数组以及一个存储姓名的数组（含有重复项）。在这里，我将使用 numpy.random 中的 randn 函数生成一些正态分布的随机数据；假设每个名字都对应 data 数组中的一行，而我们想要选出对应于名字 "Bob" 的所有行。跟算术运算一样，数组的比较运算（如 ==）也是可以向量化的。因此，对 names 和字符串 "Bob" 的比较运算将会产生一个布尔型数组；这个布尔型数组可用于数组索引；布尔型数组的长度必须跟被索引的轴长度一致。此外，还可以将布尔型数组跟切片、整数（或整数序列，稍后将对此进行详细讲解）混合使用。注意：当布尔值数组的长度不正确时，布尔值选择数据的方法并不会报错，因此我建议在使用该特性的时候要小心。下面的例子，我选取了 names == 'Bob' 的行，并索引了列。

要选择除 "Bob" 以外的其他值，既可以使用不等于符号（!=），也可以通过 ～ 对条件进行否定；～ 符号可以在你想要对一个通用条件进行取反时使用；选取这三个名字中的两个需要组合应用多个布尔条件，使用 &（和）、|（或）之类的布尔算术运算符即可；通过布尔型索引选取数组中的数据，将总是创建数据的副本，即使返回一模一样的数组也是如此。

注意：Python 关键字 and 和 or 在布尔型数组中无效。要是用 & 与 |。

通过布尔型数组设置值是一种经常用到的手段。为了将 data 中的所有负值都设置为 0，我们只需；通过一维布尔数组设置整行或列的值也很简单。后面会看到，这类二维数据的操作也可以用 pandas 方便的来做。

神奇索引。神奇索引是 NumPy 中的术语，用于描述使用整数数组进行数据索引。假设我们有一个 8×4 的数组；为了选出一个符合特定顺序的子集，你可以简单地通过传递一个包含指明所需顺序的列表或数组来完成；这段代码确实达到我们的要求了！使用负数索引将会从末尾开始选取行；一次传入多个索引数组会有一点特别。它返回的是一个一维数组，其中的元素对应各个索引元组。附录 A 中会详细介绍 reshape 方法。

最终选出的是元素 (1,0)、(5,3)、(7,1) 和 (2,2)。无论数组是多少维的，神奇索引总是一维的。这个神奇索引的行为可能会跟某些用户的预期不一样（包括我在内），选取矩阵的行列子集应该是矩形区域的形式才对。下面是得到该结果的一个办法；请牢记神奇索引与切片不同，它总是将数据复制到一个新的数组中。

数组转置和轴对换。转置是重塑的一种特殊形式，它返回的是源数据的视图（不会进行任何复制操作）。数组不仅有 transpose 方法，还有一个特殊的 T 属性；在进行矩阵计算时，经常需要用到该操作，比如利用 np.dot 计算矩阵内积；对于高维数组，transpose 需要得到一个由轴编号组成的元组才能对这些轴进行转置（比较费脑子）。这里，第一个轴被换成了第二个，第二个轴被换成了第一个，最后一个轴不变。简单的转置可以使用 .T，它其实就是进行轴对换而已；ndarray 还有一个 swapaxes 方法，它需要接受一对轴编号。swapaxes 也是返回源数据的视图（不会进行任何复制操作）。

1『如何计算 2 个矩阵内积的源码。』

```
In [129]: arr = np.random.randn(6, 3) 
In [130]: arr 
Out[130]: 
array([[-0.8608, 0.5601, -1.2659], [ 0.1198, -1.0635, 0.3329], [-2.3594, -0.1995, -1.542 ], [-0.9707, -1.307 , 0.2863], [ 0.378 , -0.7539, 0.3313], [ 1.3497, 0.0699, 0.2467]]) 
In [131]: np.dot(arr.T, arr) 
Out[131]: 
array([[ 9.2291, 0.9394, 4.948 ], [ 0.9394, 3.7662, -1.3622], [ 4.948 , -1.3622, 4.3437]])
```

通用函数：快速的逐元素数组函数。通用函数，也可以称为 ufunc，是一种在 ndarray 数据中进行逐元素操作的函数。某些简单函数接收一个或多个标量数值，并产生一个或多个标量结果，而通用函数就是对这些简单函数的向量化封装。有很多 ufunc 是简单的逐元素转换，比如 sqrt 或 exp 函数；这些都是一元（unary）ufunc。另外一些（如 add 或 maximum）接受 2 个数组（因此也叫二元（binary）ufunc），并返回一个结果数组；这里，numpy.maximum 计算了 x 和 y 中元素级别最大的元素；虽然并不常见，但有些 ufunc 的确可以返回多个数组。modf 就是一个例子，它是 Python 内置函数 divmod 的矢量化版本，它会返回浮点数数组的小数和整数部分；通用函数接收一个可选参数out，允许对数组按位置操作；表 4-3 和表 4-4 分别列出了一些一元和二元 ufunc。

利用数组进行数据处理。使用 NumPy 数组可以使你利用简单的数组表达式完成多种数据操作任务，而无须写些大量循环。这种利用数组表达式来替代显式循环的方法，称为向量化。通常，向量化的数组操作会比纯 Python 的等价实现在速度上快一到两个数量级（甚至更多），这对所有种类的数值计算产生了最大的影响。附录 A 中我解释的广播机制，就是向量化计算的有效方式。作为一个简单的示例，假设我们想要对一些网格数据来计算函数 sqrt(x^2+y^2) 的值。np.meshgrid 函数接收两个一维数组，并根据两个数组的所有 (x, y) 对生成一个二维矩阵；现在，你可以用和两个坐标值同样的表达式来使用函数；作为第 9 章的先导，我用 matplotlib 创建了这个二维数组的可视化。见图 4-3。这张图是用 matplotlib 的 imshow 函数创建的。

将条件逻辑表述为数组运算。numpy.where 函数是三元表达式 x if condition else y 的向量化版本。假设我们有一个布尔数组和两个值数组；假设我们想要根据 cond 中的值选取 xarr 和 yarr 的值：当 cond 中的值为 True 时，选取 xarr 的值，否则从 yarr 中选取。列表推导式的写法应该如下所示；这有几个问题。第一，它对大数组的处理速度不是很快（因为所有工作都是由纯 Python 完成的）。第二，无法用于多维数组。若使用 np.where，则可以将该功能写得非常简洁：

```
In [170]: result = np.where(cond, xarr, yarr) 
In [171]: result 
Out[171]: array([ 1.1, 2.2, 1.3, 1.4, 2.5])
```

np.where 的第二个和第三个参数不必是数组，它们都可以是标量值。在数据分析工作中，where 通常用于根据另一个数组而产生一个新的数组。假设有一个由随机数据组成的矩阵，你希望将所有正值替换为 2，将所有负值替换为－2。若利用 np.where，则会非常简单；使用 np.where，可以将标量和数组结合起来。例如，我可用常数 2 替换 arr 中所有正的值；传递给 where 的数组大小可以不相等，甚至可以是标量值。

数学和统计方法。许多关于计算整个数组统计值或关于轴向数据的数学函数，可以作为数组类型的方法被调用。你可以使用聚合函数（通常也叫缩减函数），比如 sum、mean 和 std（标准差），既可以直接调用数组实例的方法，也可以使用顶层的 NumPy 函数。这里，我生成了一些正态分布随机数据，然后做了聚类统计；像 mean、sum 等函数可以接收一个可选参数 axis，这个参数可以用于计算给定轴向上的统计值，形成一个下降一维度的数组；这里，arr.mean (1) 是「计算行的平均值」，arr.sum (0) 是「计算每列的和」。其他如 cumsum 和 cumprod 之类的方法则不聚合，而是产生一个由中间结果组成的数组；在多维数组中，累加函数（如 cumsum）返回的是同样大小的数组，但是会根据每个低维的切片沿着标记轴计算部分聚类。表 4-5 列出了全部的基本数组统计方法。后续章节中有很多例子都会用到这些方法。

用于布尔型数组的方法。在上面这些方法中，布尔值会被强制转换为 1（True）和 0（False）。因此，sum 经常被用来对布尔型数组中的 True 值计数；另外还有两个方法 any 和 all，它们对布尔型数组非常有用。any 用于测试数组中是否存在一个或多个 True，而 all 则检查数组中所有值是否都是 True；这两个方法也能用于非布尔型数组，所有非 0 元素将会被当做 True。

排序。跟 Python 内置的列表类型一样，NumPy 数组也可以通过 sort 方法就地排序；多维数组可以在任何一个轴向上进行排序，只需将轴编号传给 sort 即可；顶级方法 np.sort 返回的是数组的已排序副本，而就地排序则会修改数组本身。计算数组分位数最简单的办法是对其进行排序，然后选取特定位置的值；更多关于 NumPy 排序方法以及诸如间接排序之类的高级技术，请参阅附录 A。在 pandas 中还可以找到一些其他跟排序有关的数据操作（比如根据一列或多列对表格型数据进行排序）。

唯一值以及其它的集合逻辑。NumPy 提供了一些针对一维 ndarray 的基本集合运算。最常用的可能要数 np.unique 了，它用于找出数组中的唯一值并返回已排序的结果；拿跟 np.unique 等价的纯 Python 代码来对比一下；另一个函数 np.in1d 用于测试一个数组中的值在另一个数组中的成员资格，返回一个布尔型数组；NumPy 中的集合函数请参见表 4-6。

用于数组的文件输入输出。NumPy 能够读写磁盘上的文本数据或二进制数据。这一小节只讨论 NumPy 的内置二进制格式，因为更多的用户会使用 pandas 或其它工具加载文本或表格数据（见第 6 章）。np.save 和 np.load 是读写磁盘数组数据的两个主要函数。默认情况下，数组是以未压缩的原始二进制格式保存在扩展名为 .npy 的文件中的；如果文件路径末尾没有扩展名 .npy，则该扩展名会被自动加上。然后就可以通过 np.load 读取磁盘上的数组；通过 np.savez 可以将多个数组保存到一个未压缩文件中，将数组以关键字参数的形式传入即可；加载 .npz 文件时，你会得到一个类似字典的对象，该对象会对各个数组进行延迟加载；如果数据压缩的很好，就可以使用 numpy.savez_compressed。

线性代数（如矩阵乘法、矩阵分解、行列式以及其他方阵数学等）是任何数组库的重要组成部分。不像某些语言（如 MATLAB），通过 * 对两个二维数组相乘得到的是一个元素级的积，而不是一个矩阵点积。因此，NumPy 提供了一个用于矩阵乘法的 dot 函数（既是一个数组方法也是 numpy 命名空间中的一个函数）；x.dot (y) 等价于 np.dot (x, y)；一个二维数组跟一个大小合适的一维数组的矩阵点积运算之后将会得到一个一维数组；@符（类似 Python 3.5）也可以用作中缀运算符，进行矩阵乘法。

numpy.linalg 中有一组标准的矩阵分解运算以及诸如求逆和行列式之类的东西。它们跟 MATLAB 和 R 等语言所使用的是相同的行业标准线性代数库，如 BLAS、LAPACK、Intel MKL（Math Kernel Library，可能有，取决于你的 NumPy 版本）等。表达式 X.T.dot (X) 计算 X 和它的转置 X.T 的点积。表 4-7 中列出了一些最常用的线性代数函数。

1『这里有矩阵各种运算的函数信息。』

伪随机数生成。numpy.random 模块对 Python 内置的 random 进行了补充，增加了一些用于高效生成多种概率分布的样本值的函数。例如，你可以用 normal 来得到一个标准正态分布的 4×4 样本数组；而 Python 内置的 random 模块则只能一次生成一个样本值。从下面的测试结果中可以看出，如果需要产生大量样本值，numpy.random 快了不止一个数量级；我们说这些都是伪随机数，是因为它们都是通过算法基于随机数生成器种子，在确定性的条件下生成的。你可以用 NumPy 的 np.random.seed 更改随机数生成种子；numpy.random 的数据生成函数使用了全局的随机种子。要避免全局状态，你可以使用 numpy.random.RandomState，创建一个与其它隔离的随机数生成器；表 4-8 列出了 numpy.random 中的部分函数。在下一节中，我将给出一些利用这些函数一次性生成大量样本值的范例。

1『「2019018Python编程」里也有个随机漫步的例子，不过跟用矩阵实现相比，矩阵的源码要简洁太多。』

示例：随机漫步。我们通过模拟随机漫步来说明如何运用数组运算。先来看一个简单的随机漫步的例子：从 0 开始，步长 1 和 -1 出现的概率相等。下面是一个通过内置的 random 模块以纯 Python 的方式实现 1000 步的随机漫步；不难看出，这其实就是随机漫步中各步的累计和，可以用一个数组运算来实现。因此，我用 np.random 模块一次性随机产生 1000 个「掷硬币」结果（即两个数中任选一个），将其分别设置为 1 或 -1，然后计算累计和；有了这些数据之后，我们就可以沿着漫步路径做一些统计工作了，比如求取最大值和最小值。

现在来看一个复杂点的统计任务 —— 首次穿越时间，即随机漫步过程中第一次到达某个特定值的时间。假设我们想要知道本次随机漫步需要多久才能距离初始 0 点至少 10 步远（任一方向均可）。np.abs (walk)>=10 可以得到一个布尔型数组，它表示的是距离是否达到或超过 10，而我们想要知道的是第一个 10 或－10 的索引。可以用 argmax 来解决这个问题，它返回的是该布尔型数组第一个最大值的索引（True 就是最大值）。

注意，这里使用 argmax 并不是很高效，因为它无论如何都会对数组进行完全扫描。在本例中，只要发现了一个 True，那我们就知道它是个最大值了。

一次模拟多个随机漫步。如果你希望模拟多个随机漫步过程（比如 5000 个），只需对上面的代码做一点点修改即可生成所有的随机漫步过程。只要给 numpy.random 的函数传入一个二元元组就可以产生一个二维数组，然后我们就可以一次性计算 5000 个随机漫步过程（一行一个）的累计和了；现在，我们来计算所有随机漫步过程的最大值和最小值；得到这些数据之后，我们来计算 30 或－30 的最小穿越时间。这里稍微复杂些，因为不是 5000 个过程都到达了 30。我们可以用 any 方法来对此进行检查；然后我们利用这个布尔型数组选出那些穿越了 30（绝对值）的随机漫步（行），并调用 argmax 在轴 1 上获取穿越时间。

```
In [258]: nwalks = 5000 
In [259]: nsteps = 1000 
In [260]: draws = np.random.randint(0, 2, size=(nwalks, nsteps)) # 0 or 1 
In [261]: steps = np.where(draws > 0, 1, -1) 
In [262]: walks = steps.cumsum(1) 
In [263]: walks 
Out[263]: 
array([[ 1, 0, 1, ..., 8, 7, 8], [ 1, 0, -1, ..., 34, 33, 32], [ 1, 0, -1, ..., 4, 5, 4], ..., [ 1, 2, 1, ..., 24, 25, 26], [ 1, 2, 3, ..., 14, 13, 14], [ -1, -2, -3, ..., -24, -23, -22]])

In [264]: walks.max() 
Out[264]: 138
In [265]: walks.min() 
Out[265]: -133

In [266]: hits30 = (np.abs(walks) >= 30).any(1) 
In [267]: hits30 
Out[267]: array([False, True, False, ..., False, True, False], dtype=bool) 
In [268]: hits30.sum() # Number that hit 30 or -30 
Out[268]: 3410

In [269]: crossing_times = (np.abs(walks[hits30]) >= 30).argmax(1) 
In [270]: crossing_times.mean() 
Out[270]: 498.88973607038122
```

请尝试用其他分布方式得到漫步数据。只需使用不同的随机数生成函数即可，如 normal 用于生成指定均值和标准差的正态分布数据：

```
In [271]: steps = np.random.normal(loc=0, scale=0.25, 
.....:    size=(nwalks, nsteps))
```

## 05. pandas 入门

### 1. 逻辑脉络

pandas 入门概念：基于 numpy 数组构建的；专门处理表格和混杂数据设计；Series 数据结构和 DataFrame 数据结构。

### 2. 摘录及评论

pandas 是本书后续内容的首选库。它含有使数据清洗和分析工作变得更快更简单的数据结构和操作工具。pandas 经常和其它工具一同使用，如数值计算工具 NumPy 和 SciPy，分析库 statsmodels 和 scikit-learn，和数据可视化库 matplotlib。

pandas 是基于 NumPy 数组构建的，特别是基于数组的函数和不使用 for 循环的数据处理。虽然 pandas 采用了大量的 NumPy 编码风格，但二者最大的不同是 pandas 是专门为处理表格和混杂数据设计的。而 NumPy 更适合处理统一的数值数组数据。

1『pandas 与 numpy 的区别在于：pandas 是专门为处理表格和混杂数据，而 numPy 更适合处理同质型的数值类数组数据。』

自从 2010 年 pandas 开源以来，pandas 逐渐成长为一个非常大的库，应用于许多真实案例。开发者社区已经有了 800 个独立的贡献者，他们在解决日常数据问题的同时为这个项目提供贡献。在本书后续部分中，我将使用下面这样的 pandas 引入约定：

    In [1]: import pandas as pd

因此，只要你在代码中看到 pd.，就得想到这是 pandas。因为 Series 和 DataFrame 用的次数非常多，所以将其引入本地命名空间中会更方便：

    In [2]: from pandas import Series, DataFrame

要使用 pandas，你首先就得熟悉它的两个主要数据结构：Series 和 DataFrame。虽然它们并不能解决所有问题，但它们为大多数应用提供了一种可靠的、易于使用的基础。

Series 是一种类似于一维数组的对象，它由一组数据（各种 NumPy 数据类型）以及一组与之相关的数据标签（即索引）组成。Series 的字符串表现形式为：索引在左边，值在右边。由于我们没有为数据指定索引，于是会自动创建一个 0 到 N-1（N 为数据的长度）的整数型索引。你可以通过 Series 的 values 和 index 属性获取其数组表示形式和索引对象。

通常，我们希望所创建的 Series 带有一个可以对各个数据点进行标记的索引；与普通 NumPy 数组相比，你可以通过索引的方式选取 Series 中的单个或一组值。['c', 'a', 'd'] 是索引列表，即使它包含的是字符串而不是整数；使用 NumPy 函数或类似 NumPy 的运算（如根据布尔型数组进行过滤、标量乘法、应用数学函数等）都会保留索引值的链接；还可以将 Series 看成是一个定长的有序字典，因为它是索引值到数据值的一个映射。它可以用在许多原本需要字典参数的函数中；如果数据被存放在一个 Python 字典中，也可以直接通过这个字典来创建 Series：

```
In [26]: sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000} 
In [27]: obj3 = pd.Series(sdata) 
```

如果只传入一个字典，则结果 Series 中的索引就是原字典的键（有序排列）。你可以传入排好序的字典的键以改变顺序；在这个例子中，sdata 中跟 states 索引相匹配的那 3 个值会被找出来并放到相应的位置上，但由于 "California" 所对应的 sdata 值找不到，所以其结果就为 NaN（即「非数字」（not a number），在 pandas 中，它用于表示缺失或 NA 值）。因为 ‘Utah’ 不在 states 中，它被从结果中除去。我将持续使用缺失（missing）或 NA 表示缺失数据。pandas 的 isnull 和 notnull 函数可用于检测缺失数据。Series 也有类似的实例方法。我将在第 7 章详细讲解如何处理缺失数据；对于许多应用而言，Series 最重要的一个功能是，它会根据运算的索引标签自动对齐数据；数据对齐功能将在后面详细讲解。如果你使用过数据库，你可以认为是类似 join 的操作。

1『两个字典合并后自动剔除重复的元素；「自动对齐」是个关键点。』

Series 对象本身及其索引都有一个 name 属性，该属性跟 pandas 其他的关键功能关系非常密切；Series 的索引可以通过赋值的方式就地修改。

1『通说 obj2.name = 'population' 给 Series 对象的 name 赋值，通过 obj2.index.name = 'state' 给其索引的 name 复制。这个知识点感觉后面有大用。』

DataFrame 是一个表格型的数据结构，它含有一组有序的列，每列可以是不同的值类型（数值、字符串、布尔值等）。DataFrame 既有行索引也有列索引，它可以被看做由 Series 组成的字典（共用同一个索引）。DataFrame 中的数据是以一个或多个二维块存放的（而不是列表、字典或别的一维数据结构）。有关 DataFrame 内部的技术细节远远超出了本书所讨论的范围。

笔记：虽然 DataFrame 是以二维结构保存数据的，但你仍然可以轻松地将其表示为更高维度的数据（层次化索引的表格型结构，这是 pandas 中许多高级数据处理功能的关键要素，我们会在第 8 章讨论这个问题）。

建 DataFrame 的办法有很多，最常用的一种是直接传入一个由等长列表或 NumPy 数组组成的字典；结果 DataFrame 会自动加上索引（跟 Series 一样），且全部列会被有序排列；如果你使用的是 Jupyter notebook，pandas DataFrame 对象会以对浏览器友好的 HTML 表格的方式呈现。

2『已经在 conda 环境里面安装了 Jupyter，「conda install jupyter」，然后用命名 jupyter-notebook 启动。』

对于特别大的 DataFrame，head 方法会选取前五行；如果指定了列序列，则 DataFrame 的列就会按照指定顺序进行排列。如果传入的列在数据中找不到，就会在结果中产生缺失值；通过类似字典标记的方式或属性的方式，可以将 DataFrame 的列获取为一个 Series。

1『比如 frame['year'] 即可获得 Series 的 name 属性为 year 的一个 Series 对象。Series 可以理解为一维的向量，而 DataFrame 可以理解为多维的矩阵。』

1『这里展现的数据形式，跟 pdms 三维抽出来的材料数据是一样的。如何把材料数据转化为一个 DataFrame 表格型的数据结构即为下一步的动作。』

笔记：在 IPython 中，属性型连接（比如 frame2.year）和列名的 tab 补全是非常方便的。

frame2 [column] 适用于任何列的名，但是 frame2.column 只有在列名是一个合理的 Python 变量名时才适用。注意，返回的 Series 拥有原 DataFrame 相同的索引，且其 name 属性也已经被相应地设置好了。行也可以通过位置或名称的方式进行获取，比如用 loc 属性（稍后将对此进行详细讲解）「frame.loc[1]」；列可以通过赋值的方式进行修改。例如，我们可以给那个空的 "debt" 列赋上一个标量值或一组值。

将列表或数组赋值给某个列时，其长度必须跟 DataFrame 的长度相匹配。如果赋值的是一个 Series，就会精确匹配 DataFrame 的索引，所有的空位都将被填上缺失值；如果被赋值的列并不存在，则会生成一个新的列。del 关键字可以像在字典中那样对 DataFrame 删除列。作为 del 的例子，我先添加一个新的布尔值的列，state 是否为 'Ohio'。

1『frame['debt'] = np.arange(6.)，DataFrame 里新增一列数据。可以看到不用新增一列空值，直接命名新增列的名字「debt」；del frame['debt'] 删除名为 debt 的列。』

注意：不能用 frame2.eastern 创建新的列。del 方法可以用来删除这列；通过索引方式返回的列只是相应数据的视图而已，并不是副本。因此，对返回的 Series 所做的任何就地修改全都会反映到源 DataFrame 上。通过 Series 的 copy 方法即可指定复制列。

另一种常见的数据形式是嵌套字典；如果嵌套字典传给 DataFrame，pandas 就会被解释为：外层字典的键作为列，内层键则作为行索引；你也可以使用类似 NumPy 数组的方法，对 DataFrame 进行转置（交换行和列）；内层字典的键会被合并、排序以形成最终的索引。如果明确指定了索引，则不会这样；由 Series 组成的字典差不多也是一样的用法；表 5-1 列出了 DataFrame 构造函数所能接受的各种数据；如果设置了 DataFrame 的 index 和 columns 的 name 属性，则这些信息也会被显示出来；跟 Series 一样，values 属性也会以二维 ndarray 的形式返回 DataFrame 中的数据。

索引对象。pandas 的索引对象负责管理轴标签和其他元数据（比如轴名称等）。构建 Series 或 DataFrame 时，所用到的任何数组或其他序列的标签都会被转换成一个 Index；Index 对象是不可变的，因此用户不能对其进行修改；不可变可以使 Index 对象在多个数据结构之间安全共享。

注意：虽然用户不需要经常使用 Index 的功能，但是因为一些操作会生成包含被索引化的数据，理解它们的工作原理是很重要的。除了类似于数组，Index 的功能也类似一个固定大小的集合。

1『Index()、values() 都是 pandas 模块里，DataFrame 对象的方法；frame.columns，columns 是 DataFrame 对象的属性，每一列的列名，也等于行标签为 0 的数值；可以给 DataFrame 对象的 index 重新赋值，frame.index = ['one', 'two', 'three', 'four', 'five', 'six']。』

与 python 的集合不同，pandas 的 Index 可以包含重复的标签；根据重复标签进行筛选，会选取所有重复标签对应的数据。每个索引都有一些集合逻辑的方法和属性，这些方法和属性解决了关于它所包含的数据的其他常见问题。表 5-2 中总结了这些方法和属性中常用的一部分。

基本功能。将介绍操作 Series 和 DataFrame 中的数据的基本手段。后续章节将更加深入地挖掘 pandas 在数据分析和处理方面的功能。本书不是 pandas 库的详尽文档，主要关注的是最重要的功能，那些不大常用的内容（也就是那些更深奥的内容）就交给你自己去摸索吧。

重新索引。pandas 对象的一个重要方法是 reindex，其作用是创建一个新对象，它的数据符合新的索引。看下面的例子；对于时间序列这样的有序数据，重新索引时可能需要做一些插值处理。method 选项即可达到此目的，例如，使用 ffill 可以实现前向值填充；借助 DataFrame，reindex 可以修改（行）索引和列。只传递一个序列时，会重新索引结果的行；列可以用 columns 关键字重新索引。表 5-3 列出了 reindex 函数的各参数及说明。

1『简单说 reindex 就是根据新的索引序列将原来的数据重新排行；列用 columns 关键字重新索引，可以实现加表头的功能，做一个表头的列表 states，frame.reindex(columns=states)。』

表 5-3，reindex 方法的参数。参数 index：新建作为索引的序列，可以是索引实例或任意其他序列型 Python 数据结构，索引使用时无须复制。

丢弃指定轴上的项，即在轴向上删除条目。如果你已经拥有索引数组或不含条目的列表，在轴向上删除一个或更多的条目就非常容易，但这样需要一些数据操作和集合逻辑，drop 方法会返回一个含有指示值或轴向上删除值的新对象；用标签序列调用 drop 会从行标签（axis 0）删除值；通过传递 axis=1 或 axis='columns' 可以删除列的值；许多函数，如 drop，会修改 Series 或 DataFrame 的大小或形状，可以就地修改对象，不会返回新的对象；小心使用 inplace，它会销毁所有被删除的数据。

1『drop() 方法删除数据结构里任意轴线上的数据，一行或一列，多行或多列；删除行的时候，把行的索引或者多行的索引列表传递进 drop() 方法，frame.drop('two')，frame.drop(['two', 'three'])；经验证，删除行操作后只是返回一个新的数据对象，原来的对象是不变的，命令 frame 查看即可，可以将返回的新对象赋值给新变量，frame2 = frame.drop(['two', 'three'])；删除单列的操作，frame.drop('state', axis='columns')，删除多列，frame.drop(['state', 'pop'], axis='columns')；drop() 方法里的参数 inplace 设置为 True，那么删除操作直接作用于原数据源，frame.drop('one', inplace=True)。』

索引、选取和过滤。Series 索引（obj [...]）的工作方式类似于 NumPy 数组的索引，只不过 Series 的索引值不只是整数。下面是几个例子；利用标签的切片运算与普通的 Python 切片运算不同，其末端是包含的；用切片可以对 Series 的相应部分进行设置；用一个值或序列对 DataFrame 进行索引其实就是获取一个或多个列；这种索引方式有几个特殊的情况。首先通过切片或布尔型数组选取数据；选取行的语法 data [:2] 十分方便。向 [ ] 传递单一的元素或列表，就可选择列。另一种用法是通过布尔型 DataFrame（比如下面这个由标量比较运算得出的）进行索引；这使得 DataFrame 的语法与 NumPy 二维数组的语法很像。

1『末端包含的意思，对非数值的索引，切片末端包括进去了，比如 se_obj['b':'c'] = 4，4 也赋值给了索引为 c 的那行；frame['state']，可以直接获得 DataFrame 对象里列名称为 state 的那列数据；对于 DataFrame 对象，frame[:2] 是获得对象里前 2 行里的数据；frame[frame < 5] = 0，DataFrame 对象里小于 5 的数据全部赋值为 0。』

用 loc 和 iloc 进行选取。对于 DataFrame 的行的标签索引，我引入了特殊的标签运算符 loc 和 iloc。它们可以让你用类似 NumPy 的标记，使用轴标签（loc）或整数索引（iloc），从 DataFrame 选择行和列的子集。作为一个初步示例，让我们通过标签选择一行和多列；这两个索引函数也适用于一个标签或多个标签的切片；所以，在 pandas 中，有多个方法可以选取和重新组合数据。对于 DataFrame，表 5-4 进行了总结。后面会看到，还有更多的方法进行层级化索引。

1『选取对象里的单行多列，frame.loc['two', ['year', 'debt']]，选取行名为 two，列名为 year 和 debt 的数据，注意 loc 不是一个方法，后面跟的是方括号不是圆括号，而且 loc 选取针对的是使用索引名称；iloc 选取针对的是索引号，frame.iloc[1, [3, 0, 1]]，截取的是第 2 行，第 4、1、2 列的数据； loc 和 iloc 还能与切片结合起来使用。』

笔记：在一开始设计 pandas 时，我觉得用 frame[:, col] 选取列过于繁琐（也容易出错），因为列的选择是非常常见的操作。我做了些取舍，将花式索引的功能（标签和整数）放到了 ix 运算符中。在实践中，这会导致许多边缘情况，数据的轴标签是整数，所以 pandas 团队决定创造 loc 和 iloc 运算符分别处理严格基于标签和整数的索引。ix 运算符仍然可用，但并不推荐。

1『无法实现用 frame[:, col] 选取列，mark 下待解决。』

表 5-4 DataFrame 的索引选项。df[val]：从 Dataframe 中选择单列或列序列；特殊情况的便利：布尔数组（过滤行），切片（切片行）或布尔值 Dataframe。

整数索引。处理整数索引的 pandas 对象常常难住新手，因为它与 Python 内置的列表和元组的索引语法不同。例如，你可能认为下面的代码会产生错误；在上面的例子中，pandas 可以「回退」到整数索引，但是这样的方式难免会引起一些微小的错误。假设我们有一个索引，它包含了 0、1、2，但是推断用户所需要的索引方式（标签索引或位置索引）是很难的；另一方面，对于非整数索引，则不会有潜在的歧义；为了保持一致性，如果你有一个包含整数的轴索引，数据选择时请始终使用标签索引。为了更精确地处理，可以使用 loc（用于标签）或 iloc（用于整数）。

1『个人理解，产生歧义的原因在于索引名里有可能含有数字，这样的话非数字索引与数字索引就混了。』

算术运算和数据对齐。pandas 最重要的一个功能是，它可以对不同索引的对象进行算术运算。在将对象相加时，如果存在不同的索引对，则结果的索引就是该索引对的并集。对于有数据库经验的用户，这就像在索引标签上进行自动外连接（outer join）。看一个简单的例子，它们相加就会产生；没有交叠的标签位置上，内部数据对齐会产生缺失值。缺失值会在后续的算术操作上产生影响。在 DataFrame 的示例中，行和列上都会执行对齐；将这些对象加在一起，返回一个 DataFrame，它的索引、列是每个 DataFrame 的索引、列的并集；因为 'c' 和 'e' 列均不在两个 DataFrame 对象中，在结果中以缺省值呈现。行也是同样。如果 DataFrame 对象相加，没有共用的列或行标签，结果都会是空。

1『发现新建一个 DataFrame 对象简洁的代码，df1 = pd.DataFrame(np.arange(12.).reshape(3, 4), columns=list('abcd'))，新建了一个 3 行 4 列的DataFrame 对象；df2.loc[1, 'b'] = np.nan，将 df2 对象里行名为 1，列名为 b 的数据赋值为空值 NaN。』

使用填充值的算术方法。在对不同索引的对象进行算术运算时，你可能希望当一个对象中某个轴标签在另一个对象中找不到时填充一个特殊值（比如 0）；将它们相加时，没有重叠的位置就会产生 NA 值；在 df1 上使用 add 方法，我将 df2 和一个 fill_value 作为参数传入；与此相关的一点，当对 Series 或 DataFrame 重建索引时，你也可以指定一个不同的填充值。

1『df1.add(df2, fill_value=0)，还是 df1 + df2，但是在没有重叠的位置上，df1 当 0 处理的，所以没有重叠的位置的值还是 df2 的值。fill_value=2 的话，没有重叠的位置上，df1 当 2 处理。』

表 5-5 列出了 Series 和 DataFrame 的算术方法。它们每个都有一个副本，以字母 r 开头，它会翻转参数。因此这两个语句是等价的。

DataFrame 和 Series 之间的操作。跟不同维度的 NumPy 数组一样，DataFrame 和 Series 之间算术运算也是有明确规定的。先来看一个具有启发性的例子，计算一个二维数组与其某行之间的差；当我们从 arr 减去 arr [0]，每一行都会执行这个操作。这就叫做广播（broadcasting），附录 A 将对此进行详细讲解。DataFrame 和 Series 之间的运算差不多也是如此；默认情况下，DataFrame 和 Series 的数学操作中会将 Series 的索引和 DataFrame 的列进行匹配，并广播到各行；如果一个索引值不在 DataFrame 的列中，也不在 Series 的索引中，则对象会重建索引并形成联合；如果你想改为在列上进行广播，在行上匹配，你必须使用算术方法中的一种。例如；传入的轴号就是希望匹配的轴。在本例中，我们的目的是匹配 DataFrame 的行索引（axis='index' or axis=0）并进行广播。

1『frame - frame[0]，frame 对象里每行的数据都要减去第一行的数据，对于行是可以直接减的，再比如 df1 - series1，series1 = df1['b']，但对于列就不行；series2 = df1.iloc[1]，然后的操作是这样的：df1.sub(series1, axis='index')。』

函数应用和映射。NumPy 的通用函数（逐元素数组方法）对 pandas 对象也有效；另一个常用的操作是将函数应用到一行或一列的一维数组上。DataFrame 的 apply 方法可以实现这个功能；这里的函数 f，计算了一个 Series 的最大和虽小的差，在 frane 的每列都执行了一次。结果是一个 Series，使用 frame 的列作为索引。如果传递 axis='columns' 到 apply，这个函数会在每行执行；许多最为常见的数组统计功能都被实现成 DataFrame 的方法（如 sum 和 mean），因此无需使用 apply 方法。传递到 apply 的函数不是必须返回一个标量，还可以返回由多个值组成的 Series；逐元素的 Python 函数也可以使用。假设你想要根据 frame 中的每个浮点数计算一个格式化字符串，可以使用 applymap 方法；使用 applymap 作为函数名是因为 Series 有 map 方法，可以将一个逐元素的函数应用到 Series 上。

```
In [193]: f = lambda x: x.max() - x.min() 
In [194]: frame.apply(f) 
In [195]: frame.apply(f, axis='columns') 
```

1『np.abs(frame) ，DataFrame 对象里的所有负数取正；frame['e'].map(format)。』

排序和排名。根据条件对数据集排序（sorting）也是一种重要的内置运算。要对行或列索引进行排序（按字典顺序），可使用 sort_index 方法，它将返回一个已排序的新对象；在 DataFrame 中，你可以在各个轴上按索引排序；数据默认是按升序排序的，但也可以降序排序；如果要根据 Series 的值进行排序，使用 sort_values 方法，默认情况下，所有的缺失值都会被排序至 Series 的尾部。

1『注意，这些只是对索引进行排序。df2.sort_index()，按行索引升序排序；df2.sort_index(axis=1)，按列索引升序排序；df2.sort_index(ascending=False)，按行索引降序排序；df2.sort_index(axis=1, ascending=False)，按列索引降序排序；sort_values 方法进行 Series 对象里的数据值排序，series.sort_values()，行数据升序排序，其余雷同，注意只是针对一维的 Series 对象，DataFrame 对象的话需要传入参数 by。』

当对 DataFrame 排序时，你可以使用一列或多列作为排序键。为了实现这个功能，传递一个或多个列名给 sort_values 的可选参数 by；要根据多个列进行排序，传入名称的列表即可；排名是指对数组从 1 到有效数据点总数分配名次的操作。Series 和 DataFrame 的 rank 方法是实现排名的方法，默认情况下，rank 通过将平均排名分配到每个组来打破平级关系；排名也可以根据他们在数据中的观察顺序进行分配；在上面的例子中，对条目 0 和 2 设置的名次为 6 和 7，而不是之前的平均排名 6.5，是因为在数据中标签 0 在标签 2 的前面。你可以按降序排名；DataFrame 可以对行或列计算排名；表 5-6 是可用的平级关系打破方法列表。

1『df2.sort_values(by='c', ascending=False)，df2.sort_values(by=['a', 'b'])，根据多个列进行排序；frame.rank(axis='columns')，对 DataFrame 对象的行或列计算排名。』

带有重复标签的轴索引。直到目前为止，我所介绍的所有范例都有着唯一的轴标签（索引值）。虽然许多 pandas 函数（如 reindex）都要求标签唯一，但这并不是强制性的。我们来看看下面这个简单的带有重复索引值的 Series；索引的 is_unique 属性可以告诉你它的值是否是唯一的；对于带有重复值的索引，数据选取的行为将会有些不同。如果某个索引对应多个值，则返回一个 Series；而对应单个值的，则返回一个标量值；这样会使代码变复杂，因为索引的输出类型会根据标签是否有重复发生变化。对 DataFrame 的行进行索引时也是如此。

描述性统计的概述与计算.pandas 对象装配了一个常用数学、统计学方法的集合。其中大部分属于归约或汇总统计的类别，这些方法从 Dataframe 的行或列中抽取一个 Series 或一系列值的单个值（如总和或平均值）。与 Numpy 数组中的类似方法相比，它们内建了处理缺失值的功能。考虑一个小型 DataFrame；调用 DataFrame 的 sum 方法将会返回一个含有列的和的 Series；传入 axis='columns' 或 axis=1 将会按行进行求和运算；除非整个切片上（在本例中是行或列）都是 NA，否则 NA 值是被自动排除的。可以通过禁用 skipna 来实现不排除 NA 值；表 5-7 列出了这些约简方法的常用选项。

1『发现了，只要是传入参数 axis='columns' 或 axis=1 的，就是对行进行计算处理。axis 参数是归约轴，0 为行向，1 为列向。』

有些方法（如 idxmin 和 idxmax）返回的是间接统计（比如达到最小值或最大值的索引）；另一些方法则是累计型的（比如 cumsum() 方法）；还有一种方法，它既不是约简型也不是累计型。describe 就是一个例子，它用于一次性产生多个汇总统计；对于非数值型数据，describe 会产生另外一种汇总统计；表 5-8 列出了所有与描述统计相关的方法。

相关系数与协方差。有些汇总统计（如相关系数和协方差）是通过参数对计算出来的。我们来看几个 DataFrame，它们的数据来自 Yahoo! Finance 的股票价格和成交量，使用的是 pandas-datareader 包（可以用 conda 或 pip 安装）；使用 pandas_datareader 模块下载了一些股票数据；注意：此时 Yahoo! Finance 已经不存在了，因为 2017 年 Yahoo! 被 Verizon 收购了。参阅 pandas-datareader 文档，可以学习最新的功能；现在计算价格的百分数变化，时间序列的操作会在第 11 章介绍。

Series 的 corr 方法用于计算两个 Series 中重叠的、非 NA 的、按索引对齐的值的相关系数。与此类似，cov 用于计算协方差；因为 MSTF 是一个合理的 Python 属性，我们还可以用更简洁的语法选择列；另一方面，DataFrame 的 corr 和 cov 方法将以 DataFrame 的形式分别返回完整的相关系数或协方差矩阵；利用 DataFrame 的 corrwith 方法，你可以计算其列或行跟另一个 Series 或 DataFrame 之间的相关系数。传入一个 Series 将会返回一个相关系数值 Series（针对各列进行计算）；传入一个 DataFrame 则会计算按列名配对的相关系数。这里，我计算百分比变化与成交量的相关系数；传入 axis='columns' 即可按行进行计算。无论如何，在计算相关系数之前，所有的数据项都会按标签对齐。

唯一值、值计数以及成员资格。还有一类方法可以从一维 Series 的值中抽取信息。看下面的例子；第一个函数是 unique，它可以得到 Series 中的唯一值数组；返回的唯一值是未排序的，如果需要的话，可以对结果再次进行排序（uniques.sort ()）。相似的，value_counts 用于计算一个 Series 中各值出现的频率；为了便于查看，结果 Series 是按值频率降序排列的。value_counts 还是一个顶级 pandas 方法，可用于任何数组或序列；isin 执行向量化的成员属性检查，还可以将数据集以 Series 或 DataFrame 一列的形式过滤为数据集的值子集；与 isin 类似的是 Index.get_indexer 方法，它可以给你一个索引数组，从可能包含重复值的数组到另一个不同值的数组；表 5-9 给出了这几个方法的一些参考信息。

有时，你可能希望得到 DataFrame 中多个相关列的一张柱状图。例如；将 pandas.value_counts 传给该 DataFrame 的 apply 函数，就会出现；这里，结果中的行标签是所有列的唯一值。这些值是每个列中这些值的相应计数。

## 02. Python 语法基础，IPython 和 Jupyter Notebooks

### 1. 逻辑脉络

Python 语法基础。

### 2. 摘录及评论

2011 年和 2012 年的时候，并没有多少 Python 数据分析资源。这是个蛋生鸡鸡生蛋的问题：很多我们现在觉得理所当然的库，比如 pandas、scikit- learn 和 statsmodels 在当时并不成熟。2017 年，出现了大量关于数据科学、数据分析以及机器学习的文献，补充了先前仅面向计算机科学家、物理学家和其他研究领域的专业人员的通用科学计算工作。此外，还出现了大量非常优秀的书籍，这些书主要是关于 Python 编程语言自身以及如何成为高效的 Python 软件工程师。

本书是介绍如何使用 Python 处理数据的，我认为独立地概述一下 Python 内建数据结构的特性以及数据操作方面的库是很有必要的。在我看来，在 Pythons 中高效地分析数据并不需要完全精通如何利用 Python 语言开发软件。推荐使用 Ipython 命令行和 Jupyter notebook 来实验代码示例，以及探索各种类型、函数和方法的文档。尽管我已经尽量按照增量方式来展现书中的内容，但可能还会偶尔遇到一些没有完全介绍的内容。

本书的大部分内容是关于如何基于数据表进行分析以及用于大型数据集的数据准备工具。为了使用这些工具，通常必须先把凌乱数据整理为更好看的（或者说更结构化的）表格形式。幸运的是，Python 就是一个将数据快速规整为合理形式的理想语言。使用 Python 语言的能力越强，准备待分析数据集的工作就越轻松。

本书的一些工具最好是通过 Python 或者 Jupyter 会话来探索。一旦学会了如何使用 Python 和 Jupyter 来探索数据，我推荐你实验本书的示例并且可以再实验尝试一些别的内容。和其他键盘控制的命令行环境一样，练就常用命令的肌肉记忆也是学习曲线的一部分。有一些 Pythons 中的概念在本章并未提及，比如类和面向对象编程，你会发现这些概念其实在 Python 数据分析中也是有用的。为了加深你的 Python 知识，建议通过 Python 官方教程或者一本优秀的通用 Python 编程书籍来补充本章没有介绍的内容。推荐的入门书籍包括：《Python Cookbook》，第 3 版，David Beazley 和 Brian K. Jones 著（O’Reilly）；《Fluent Python》，Luciano Ramalho 著 (O’Reilly)；《Effective Python》，Brett Slatkin 著 (Pearson)

2『已下载书籍「2020031Python_Cookbook3Ed」、「2020032Effective_Python」、「2019088Fluent_Python」以及其中文版。』

一些 Python 程序员总是这样执行 Python 代码的，从事数据分析和科学计算的人却会使用 IPython，一个强化的 Python 解释器，或 Jupyter notebooks，一个网页代码笔记本，它原先是 IPython 的一个子项目。在本章中，我介绍了如何使用 IPython 和 Jupyter，在附录 A 中有更深入的介绍。当你使用 %run 命令，IPython 会同样执行指定文件中的代码，结束之后，还可以与结果交互。

    In [1]: %run hello_world.py 

Jupyter 项目中的主要组件就是 notebook，这是一种交互式的文档类型，可以用于编写代码、文本（可以带标记）、数据可视化以及其他输出。Jupyter notebook 与内核交互，内核是编程语言的交互式计算协议的实现。Pythons 的 Jupyter 内核使用 Ipython 系统进行内部活动。需要启动 Jupyterg 时，可以在终端中运行 jupyter notebook 命令。

在多数平台上，Jupyter 会自动打开默认的浏览器（除非指定了 --no-browser）。或者，可以在启动 notebook 之后，手动打开网页 http://localhost:8888/。图 2-1 展示了 Google Chrome 中的 notebook。笔记：许多人使用 Jupyter 作为本地的计算环境，但它也可以部署到服务器上远程访问。这里不做介绍，如果需要的话，鼓励读者自行到网上学习。

2『部署到服务器上远程访问。研究相关的资料。』

当保存 notebook 时（File 目录下的 Save and Checkpoint），会创建一个后缀名为.ipynb 的文件。这是一个自包含文件格式，包含当前笔记本中的所有内容（包括所有已评估的代码输出）。可以被其它 Jupyter 用户加载和编辑。要加载存在的 notebook，把它放到启动 notebook 进程的相同目录内。你可以用本书的示例代码练习，见图 2-3。虽然 Jupyter notebook 和 IPython shell 使用起来不同，本章中几乎所有的命令和工具都可以通用。

从外观上，IPython shell 和标准的 Python 解释器只是看起来不同。IPython shell 的进步之一是其它 IDE 和交互计算分析环境都有的 tab 补全功能。在 shell 中输入表达式，按下 Tab，会搜索已输入变量（对象、函数等等）的命名空间；同样也适用于模块；除了补全命名、对象和模块属性，Tab 还可以补全其它的。当输入看似文件路径时（即使是 Python 字符串），按下 Tab 也可以补全电脑上对应的文件信息；结合 %run，tab 补全可以节省许多键盘操作；另外，tab 补全可以补全函数的关键词参数（包括等于号 =）。见图 2-4。

笔记：注意，默认情况下，IPython 会隐藏下划线开头的方法和属性，比如魔术方法和内部的「私有」方法和属性，以避免混乱的显示（和让新手迷惑！）这些也可以 tab 补全，但是你必须首先键入一个下划线才能看到它们。如果你喜欢总是在 tab 补全中看到这样的方法，你可以 IPython 配置中进行设置。可以在 IPython 文档中查找方法。

自省。在变量后面使用问号？，可以显示对象的信息；这可以作为对象的自省。如果对象是一个函数或实例方法，定义过的文档字符串，也会显示出信息。假设我们写了一个如下的函数。然后使用？符号，就可以显示如下的文档字符串；使用 ?? 会显示函数的源码；? 还有一个用途，就是像 Unix 或 Windows 命令行一样搜索 IPython 的命名空间。字符与通配符结合可以匹配所有的名字。例如，我们可以获得所有包含 load 的顶级 NumPy 命名空间。

你可以用 % run 命令运行所有的 Python 程序。假设有一个文件 ipython_script_test.py，可以如下运行；这段脚本运行在空的命名空间（没有 import 和其它定义的变量），因此结果和普通的运行方式 python script.py 相同。文件中所有定义的变量（import、函数和全局变量，除非抛出异常），都可以在 IPython shell 中随后访问；如果一个 Python 脚本需要命令行参数（在 sys.argv 中查找），可以在文件路径之后传递，就像在命令行上运行一样。

笔记：如果想让一个脚本访问 IPython 已经定义过的变量，可以使用 %run -i。在 Jupyter notebook 中，你也可以使用 %load，它将脚本导入到一个代码格中。

代码运行时按 Ctrl-C，无论是 %run 或长时间运行命令，都会导致 KeyboardInterrupt。这会导致几乎左右 Python 程序立即停止，除非一些特殊情况。警告：当 Python 代码调用了一些编译的扩展模块，按 Ctrl-C 不一定将执行的程序立即停止。在这种情况下，你必须等待，直到控制返回 Python 解释器，或者在更糟糕的情况下强制终止 Python 进程。

IPython 有许多键盘快捷键进行导航提示（类似 Emacs 文本编辑器或 UNIX bash Shell）和交互 shell 的历史命令。表 2-1 总结了常见的快捷键。图 2-5 展示了一部分，如移动光标；Jupyter notebooks 有另外一套庞大的快捷键。因为它的快捷键比 IPython 的变化快，建议你参阅 Jupyter notebook 的帮助文档。

魔术命令。IPython 中特殊的命令（Python 中没有）被称作「魔术」命令。这些命令可以使普通任务更便捷，更容易控制 IPython 系统。魔术命令是在指令前添加百分号 % 前缀。例如，可以用 %timeit（这个命令后面会详谈）测量任何 Python 语句，例如矩阵乘法的执行时间；魔术命令可以被看做 IPython 中运行的命令行。许多魔术命令有「命令行」选项，可以通过？查看；魔术函数默认可以不用百分号，只要没有变量和函数名相同。这个特点被称为「自动魔术」，可以用 %automagic 打开或关闭；一些魔术函数与 Python 函数很像，它的结果可以赋值给一个变量；IPython 的文档可以在 shell 中打开，我建议你用 %quickref 或 %magic 学习下所有特殊命令。表 2-2 列出了一些可以提高生产率的交互计算和 Python 开发的 IPython 指令。

集成 Matplotlib。IPython 在分析计算领域能够流行的原因之一是它非常好的集成了数据可视化和其它用户界面库，比如 matplotlib。不用担心以前没用过 matplotlib，本书后面会详细介绍。%matplotlib 魔术函数配置了 IPython shell 和 Jupyter notebook 中的 matplotlib。这点很重要，其它创建的图不会出现（notebook）或获取 session 的控制，直到结束（shell）。在 IPython shell 中，运行 %matplotlib 可以进行设置，可以创建多个绘图窗口，而不会干扰控制台 session；在 Jupyter 中，命令有所不同（图 2-6）；

语言的语义。Python 的语言设计强调的是可读性、简洁和清晰。有些人称 Python 为「可执行的伪代码」。

使用缩进，而不是括号。Python 使用空白字符（tab 和空格）来组织代码，而不是像其它语言，比如 R、C++、JAVA 和 Perl 那样使用括号。看一个排序算法的 for 循环；冒号标志着缩进代码块的开始，冒号之后的所有代码的缩进量必须相同，直到代码块结束。不管是否喜欢这种形式，使用空白符是 Python 程序员开发的一部分，在我看来，这可以让 python 的代码可读性大大优于其它语言。虽然期初看起来很奇怪，经过一段时间，你就能适应了；你应该已经看到，Python 的语句不需要用分号结尾。但是，分号却可以用来给同在一行的语句切分。Python 不建议将多条语句放到一行，这会降低代码的可读性。

笔记：我强烈建议你使用四个空格作为默认的缩进，可以使用 tab 代替四个空格。许多文本编辑器的设置是使用制表位替代空格。某些人使用 tabs 或不同数目的空格数，常见的是使用两个空格。大多数情况下，四个空格是大多数人采用的方法，因此建议你也这样做。

万物皆对象。Python 语言的一个重要特性就是它的对象模型的一致性。每个数字、字符串、数据结构、函数、类、模块等等，都是在 Python 解释器的自有「盒子」内，它被认为是 Python 对象。每个对象都有类型（例如，字符串或函数）和内部数据。在实际中，这可以让语言非常灵活，因为函数也可以被当做对象使用。





## 06. 数据加载、存储与文件格式

### 1. 逻辑脉络

pandas 的数据输入与输出。

### 2. 摘录及评论

访问数据是使用本书所介绍的这些工具的第一步。我会着重介绍 pandas 的数据输入与输出，虽然别的库中也有不少以此为目的的工具。输入输出通常可以划分为几个大类：读取文本文件和其他更高效的磁盘存储格式，加载数据库中的数据，利用 Web API 操作网络资源。pandas 提供了一些用于将表格型数据读取为 DataFrame 对象的函数。表 6-1 对它们进行了总结，其中 read_csv 和 read_table 可能会是你今后用得最多的。我将大致介绍一下这些函数在将文本数据转换为 DataFrame 时所用到的一些技术。这些函数的可选参数主要有以下几种类型：

1、索引：将一个或多个列当做返回的 DataFrame 处理，以及是否从文件、用户获取列名。

2、类型推断和数据转换：包括用户定义值的转换、和自定义的缺失值标记列表等。

3、日期解析：包括组合功能，比如将分散在多个列中的日期时间信息组合成结果中的单个列。

4、迭代：支持对大文件进行逐块迭代。

5、不规整数据问题：跳过一些行、页脚、注释或其他一些不重要的东西（比如由成千上万个逗号隔开的数值数据）。

因为工作中实际碰到的数据可能十分混乱，一些数据加载函数（尤其是 read_csv）的选项逐渐变得复杂起来。面对不同的参数，感到头痛很正常（read_csv 有超过 50 个参数）。pandas 文档有这些参数的例子，如果你感到阅读某个文件很难，可以通过相似的足够多的例子找到正确的参数。其中一些函数，比如 pandas.read_csv，有类型推断功能，因为列数据的类型不属于数据类型。也就是说，你不需要指定列的类型到底是数值、整数、布尔值，还是字符串。其它的数据格式，如 HDF5、Feather 和 msgpack，会在格式中存储数据类型。

日期和其他自定义类型的处理需要多花点工夫才行。首先我们来看一个以逗号分隔的（CSV）文本文件；里，我用的是 Unix 的 cat shell 命令将文件的原始内容打印到屏幕上。如果你用的是 Windows，你可以使用 type 达到同样的效果。由于该文件以逗号分隔，所以我们可以使用 read_csv 将其读入一个 DataFrame；还可以使用 read_table，并指定分隔符；并不是所有文件都有标题行。读入该文件的办法有两个。你可以让 pandas 为其分配默认的列名，也可以自己定义列名。

1『pd.read_table('data.csv', sep=', ')，使用 read_table 并指定分隔符的实现方式。』

假设你希望将 message 列做成 DataFrame 的索引。你可以明确表示要将该列放到索引 4 的位置上，也可以通过 index_col 参数指定 "message"；如果希望将多个列做成一个层次化索引，只需传入由列编号或列名组成的列表即可。

```
In [15]: names = ['a', 'b', 'c', 'd', 'message'] 
In [16]: pd.read_csv('examples/ex2.csv', names=names, index_col='message') 
```

有些情况下，有些表格可能不是用固定的分隔符去分隔字段的（比如空白符或其他模式）。有些表格可能不是用固定的分隔符去分隔字段的（比如空白符或其他模式来分隔字段）。看看下面这个文本文件；虽然可以手动对数据进行规整，这里的字段是被数量不同的空白字符间隔开的。这种情况下，你可以传递一个正则表达式作为 read_table 的分隔符。可以用正则表达式表达为 \s+，于是有；本例中，由于列名的数量比数据的列数少一个，因此 read_table 推断第一列应当作为 DataFrame 的索引。

1『用上面的正则表达式处理材料数据失败，必须好好研究正则表达式的知识。』

这些解析器函数还有许多参数可以帮助你处理各种各样的异形文件格式（表 6-2 列出了一些）。比如说，你可以用 skiprows 跳过文件的第一行、第三行和第四行；缺失值处理是文件解析任务中的一个重要组成部分。缺失数据经常是要么没有（空字符串），要么用某个标记值表示。默认情况下，pandas 会用一组经常出现的标记值进行识别，比如 NA 及 NULL；na_values 选项可以传入一个列表或一组字符串来处理缺失值；在字典中，每列可以指定不同的缺失值标识；表 6-2 列举了 pandas.read_csv 和 pandas.read_table 中常用的选项。

1『一个大的感触，read_table() 方法的可选参数真是丰富；skiprows 可选参数可以用来剔除不要的行数据；』

逐块读取文本文件。当处理大型文件或找出正确的参数集来正确处理大文件时，你可能需要读入文件的一个小片段或者按小块遍历文件。在尝试大文件之前，我们可以先对 pandas 的显示设置进行调整，使之更为紧凑；如果只想读取几行（避免读取整个文件），通过 nrows 进行指定即可；为了分块读入文件，可以指定 chunksize 作为每一块的行数；read_csv 返回的 TextParser 对象允许你根据 chunksize 遍历文件。例如，我们可以遍历 ex6.csv，并对 'key' 列聚合获得计数值；TextParser 还具有 get_chunk 方法，允许你按照任意大小读取数据块。

```
chunker = pd.read_csv('examples/ex6.csv', chunksize=1000) 
tot = pd.Series([]) 
for piece in chunker: 
    tot = tot.add(piece['key'].value_counts(), fill_value=0) 
tot = tot.sort_values(ascending=False)
In [40]: tot[:10] 
Out[40]: 
```

将数据写出到文本格式。数据也可以被输出为分隔符格式的文本。我们再来看看之前读过的一个 CSV 文件；利用 DataFrame 的 to_csv 方法，我们可以将数据写到一个以逗号分隔的文件中；当然，还可以使用其他分隔符（由于这里直接写出到 sys.stdout，所以仅仅是打印出文本结果而已）；缺失值在输出结果中会被表示为空字符串。你可能希望将其表示为别的标记值；如果没有设置其他选项，则会写出行和列的标签。当然，它们也都可以被禁用；此外，你还可以只写出一部分的列，并以你指定的顺序排列；Series 也有一个 to_csv 方法。

```
In [45]: import sys 
In [46]: data.to_csv(sys.stdout, sep='|') 
```

使用分隔格式。绝大多数的表型数据都可以使用函数 pandas.read_table 从硬盘中读取。然而，在某些情况下，一些手动操作可能是必不可少的。接收一个带有一行或多行错误的文件并不少见，read_table 也无法解决这种情况。为了介绍一些基础工具，考虑如下的小型 CSV 文件；对于任何带有单字符分隔符的文件，你可以使用 Python 的内建 csv 模块。要使用它，需要将任一打开的文件或文件型对象传给 csv.reader；像遍历文件那样遍历 reader 会产生元组，元组的值为删除了引号的字符。

之后，你就可以自行做一些必要处理，以将数据整理为你需要的形式。让我们按部就班，首先将文件读取为行的列表；然后，我们将数据拆分为列名行和数据行；再然后，我们使用字典推导式和表达式 zip(*values) 生成一个包含数据列的字典，字典中行转置成列；CSV 文件有多种不同风格。如需根据不同的分隔符、字符串引用约定或行终止符定义一种新的格式时，我们可以使用 csv.Dialect 定义一个简单的子类。我们也可以不必定义子类，直接将 CSV 方言参数传入 csv.reader 的关键字参数；表 6-3 中列出了 csv.Dialect 中的一些属性及其用途。

1『使用字典推导式和表达式 zip(*values) 生成一个包含数据列的字典，字典中行转置成列。这个是很关键的信息。』

```
In [57]: with open('examples/ex7.csv') as f: 
....:    lines = list(csv.reader(f))
In [58]: header, values = lines[0], lines[1:]
In [59]: data_dict = {h: v for h, v in zip(header, zip(*values))} 
In [60]: data_dict 
```

笔记：对于那些使用复杂分隔符或多字符分隔符的文件，csv 模块就无能为力了。这种情况下，你就只能使用字符串的 split 方法或正则表达式方法 re.split 进行行拆分和其他整理工作了。需要手动写入被分隔的文件时，你可以使用 csv.writer。这个函数接收一个已经打开的可写入文件对象以及和 csv.reader 相同的 CSV 方言、格式选项。

JSON 数据。JSON（JavaScript Object Notation 的简称）已经成为通过 HTTP 请求在 Web 浏览器和其他应用程序之间发送数据的标准格式之一。它是一种比表格型文本格式（如 CSV）灵活得多的数据格式。下面是一个例子；除其空值 null 和一些其他的细微差别（如列表末尾不允许存在多余的逗号）之外，JSON 非常接近于有效的 Python 代码。基本类型有对象（字典）、数组（列表）、字符串、数值、布尔值以及 null。对象中所有的键都必须是字符串。许多 Python 库都可以读写 JSON 数据。我将使用 json，因为它是构建于 Python 标准库中的。通过 json.loads 即可将 JSON 字符串转换成 Python 形式；json.dumps 则将 Python 对象转换成 JSON 格式；

```
In [62]: import json 
In [63]: result = json.loads (obj) 
In [65]: asjson = json.dumps(result)
```

如何将（一个或一组）JSON 对象转换为 DataFrame 或其他便于分析的数据结构就由你决定了。最简单方便的方式是：向 DataFrame 构造器传入一个字典的列表（就是原先的 JSON 对象），并选取数据字段的子集；pandas.read_json 可以自动将特别格式的 JSON 数据集转换为 Series 或 DataFrame。例如；pandas.read_json 的默认选项假设 JSON 数组中的每个对象是表格中的一行；如需了解读取、操作 JSON 数据（包括嵌套记录）的拓展示例，请参看第 7 章的 USDA 食品数据库示例。如果你需要从 pandas 中将数据导出为 JSON，一种办法是对 Series 和 DataFrame 使用 to_json 方法。

XML 和 HTML：Web 信息收集（网络抓取）。Python 有许多可以读写常见的 HTML 和 XML 格式数据的库，包括 lxml、Beautiful Soup 和 html5lib。lxml 的速度比较快，但其他库可以更好地处理异常的 HTML 或 XML 文件；pandas 有一个内置的功能，read_html，它可以使用 lxml 和 Beautiful Soup 自动将 HTML 文件中的表格解析为 DataFrame 对象。为了进行展示，我从美国联邦存款保险公司下载了一个 HTML 文件（pandas 文档中也使用过），它记录了银行倒闭的情况。首先，你需要安装 read_html 用到的库：

```
conda install lxml 
pip install beautifulsoup4 html5lib
```

pandas.read_html 有一些选项，默认条件下，它会搜索、尝试解析 <table> 标签内的的表格数据。结果是一个列表的 DataFrame 对象；因为 failures 有许多列，pandas 插入了一个换行符 \；这里，我们可以做一些数据清洗和分析（后面章节会进一步讲解），比如计算按年份计算倒闭的银行数。

利用 lxml.objectify 解析 XML。XML（Extensible Markup Language）是另一种常见的支持分层、嵌套数据以及元数据的结构化数据格式。本书实际上也是从一系列大型XML文档中生成的。

前面，我介绍了 pandas.read_html 函数，它可以使用 lxml 或 Beautiful Soup 从 HTML 解析数据。XML 和 HTML 的结构很相似，danXML 更为通用。这里，我会用一个例子演示如何利用 lxml 从 XML 格式解析数据；纽约大都会运输署发布了一些有关其公交和列车服务的数据资料。这里，我们将看看包含在一组 XML 文件中的运行情况数据。每项列车或公交服务都有各自的文件（如 Metro-North Railroad 的文件是 Performance_MNR.xml），其中每条 XML 记录就是一条月度数据，如下所示；我们先用 lxml.objectify 解析该文件，然后通过 getroot 得到该 XML 文件的根节点的引用；root.INDICATOR 返回一个生成器，可以产生每一个 <INDICATOR>XML 元素。对于每条记录，我们可以将标签名称的字典（如YTD_ACTUAL）填充为数据值（不包括几个标签）。最后，将这组字典转换为一个 DataFrame。

XML 数据可以比例子更复杂。每个标签也可以包含元数据。考虑一个 HTML 连接标签，也是有效的 XML；现在可以访问标签或链接文本中的任何字段（如 href）。

二进制数据格式。实现数据的高效二进制格式存储最简单的办法之一是使用 Python 内置的 pickle 序列化。pandas 对象都有一个用于将数据以 pickle 格式保存到磁盘上的 to_pickle 方法；你可以通过 pickle 直接读取被 pickle 化的数据，或是使用更为方便的 pandas.read_pickle。注意：pickle 仅建议用于短期存储格式。其原因是很难保证该格式永远是稳定的；今天 pickle 的对象可能无法被后续版本的库 unpickle 出来。虽然我尽力保证这种事情不会发生在 pandas 中，但是今后的某个时候说不定还是得「打破」该 pickle 格式。

1『frame.to_pickle('examples/frame_pickle')，数据以 pickle 格式存储；』

pandas 内置支持两个二进制数据格式：HDF5 和 MessagePack。下一节，我会给出几个 HDF5 的例子，但我建议你尝试下不同的文件格式，看看它们的速度以及是否适合你的分析工作。pandas 或 NumPy 数据的其它存储格式有：bcolz，一种可压缩的列存储二进制格式，基于 Blosc 压缩库；Feather，我与 R 语言社区的 Hadley Wickham 设计的一种跨语言的列存储文件格式。Feather 使用了 Apache Arrow 的列式内存格式。Feather 使用 Apache 箭头列式存储器格式。

3『

[Welcome to bcolz’s documentation! — bcolz 1.2.0 documentation](https://bcolz.readthedocs.io/en/latest/)

[wesm/feather: Feather: fast, interoperable binary data frame storage for Python, R, and more powered by Apache Arrow](https://github.com/wesm/feather)

[Hadley Wickham](http://hadley.nz/)

[Apache Arrow](http://arrow.apache.org/)

』

使用 HDF5 格式。HDF5 是一种存储大规模科学数组数据的非常好的文件格式。它可以被作为 C 库，带有许多语言的接口，如 Java、Python 和 MATLAB 等。HDF5 中的 HDF 指的是层次型数据格式（hierarchical data format）。每个 HDF5 文件都含有一个文件系统式的节点结构，它使你能够存储多个数据集并支持元数据。与其他简单格式相比，HDF5 支持多种压缩器的即时压缩，还能更高效地存储重复模式数据。对于那些非常大的无法直接放入内存的数据集，HDF5 就是不错的选择，因为它可以高效地分块读写。虽然可以用 PyTables 或 h5py 库直接访问 HDF5 文件，pandas 提供了更为高级的接口，可以简化存储 Series 和 DataFrame 对象。HDFStore 类可以像字典一样，处理低级的细节：

```
In [92]: frame = pd.DataFrame({'a': np.random.randn(100)}) 
In [93]: store = pd.HDFStore('mydata.h5') 
In [94]: store['obj1'] = frame 
In [95]: store['obj1_col'] = frame['a'] 
In [96]: store 
```

1『pd.DataFrame({'a':np.random.randn(100)})，100 行，1 列，列名为 a 的数据结构；』

HDF5 文件中的对象可以通过与字典一样的 API 进行获取；HDFStore 支持两种存储模式，'fixed' 和 'table'。后者通常会更慢，但是支持使用特殊语法进行查询操作；put 是 store ['obj2'] = frame 方法的显示版本，允许我们设置其它的选项，比如格式。pandas.read_hdf 函数可以快捷使用这些工具。

笔记：如果你要处理的数据位于远程服务器，比如 Amazon S3 或 HDFS，使用专门为分布式存储（比如 Apache Parquet）的二进制格式也许更加合适。Python 的 Parquet 和其它存储格式还在不断的发展之中，所以这本书中没有涉及。如果需要本地处理海量数据，我建议你好好研究一下 PyTables 和 h5py，看看它们能满足你的需求。由于许多数据分析问题都是 IO 密集型（而不是 CPU 密集型），利用 HDF5 这样的工具能显著提升应用程序的效率。

注意：HDF5 不是数据库。它最适合用作「一次写多次读」的数据集。虽然数据可以在任何时候被添加到文件中，但如果同时发生多个写操作，文件就可能会被破坏。

读取 Microsoft Excel 文件。pandas 的 ExcelFile 类或 pandas.read_excel 函数支持读取存储在 Excel 2003（或更高版本）中的表格型数据。这两个工具分别使用扩展包 xlrd 和 openpyxl 读取 XLS 和 XLSX 文件。你可以用 pip 或 conda 安装它们。要使用 ExcelFile，通过传递 xls 或 xlsx 路径创建一个实例；存储在表中的数据可以通过 pandas.read_excel 读取到 DataFrame 中；如果你读取的是含有多个表的文件，生成 ExcelFile 更快，但你也可以更简洁地将文件名传入 pandas.read_excel；如需将 pandas 数据写入到 Excel 格式中，你必须先生成一个 ExcelWriter，然后使用 pandas 对象的 to_excel 方法将数据写入；你也可以将文件路径传给 to_excel，避免直接调用 ExcelWriter。

```
In [104]: xlsx = pd.ExcelFile('examples/ex1.xlsx')
In [105]: pd.read_excel(xlsx, 'Sheet1') 
In [106]: frame = pd.read_excel('examples/ex1.xlsx', 'Sheet1') 
In [108]: writer = pd.ExcelWriter('examples/ex2.xlsx') 
In [109]: frame.to_excel(writer, 'Sheet1') 
In [110]: writer.save()
In [111]: frame.to_excel('examples/ex2.xlsx')
```

与 Web API 交互。许多网站都有一些通过 JSON 或其他格式提供数据的公共 API。通过 Python 访问这些 API 的办法有不少。一个简单易用的办法（推荐）是 requests 包。为了搜索最新的 30 个 GitHub 上的 pandas 主题，我们可以发一个 HTTP GET 请求，使用 requests 扩展库；Response（响应）对象的 json 方法将返回一个包含解析为本地 Python 对象的 JSON 的字典；data 中的每个元素都是一个包含 GitHub 问题页面上的所有数据的字典（注释除外）。我们可以将 data 直接传给 DataFrame，并提取感兴趣的字段；通过一些复杂操作，你可以创建一些更高阶的接口来访问常用的 Web API，以返回 DataFrame 对象以便于分析。

```
In [113]: import requests 
In [114]: url = 'https://api.github.com/repos/pandas-dev/pandas/issues' 
In [115]: resp = requests.get(url) 
In [116]: resp 
In [117]: data = resp.json() 
In [118]: data[0]['title'] 
In [119]: issues = pd.DataFrame(data, columns=['number', 'title', .....: 'labels', 'state']) 
In [120]: issues 
```

与数据库交互。在商业场景下，大多数数据可能不是存储在文本或 Excel 文件中。基于 SQL 的关系型数据库（如 SQL Server、PostgreSQL 和 MySQL 等）使用非常广泛，其它一些数据库也很流行。数据库的选择通常取决于性能、数据完整性以及应用程序的伸缩性需求。将数据从 SQL 加载到 DataFrame 的过程很简单，此外 pandas 还有一些能够简化该过程的函数。例如，我将使用 SQLite 数据库（通过 Python 内置的 sqlite3 驱动器）；然后插入几行数据；从表中选取数据时，大部分 Python SQL 驱动器（PyODBC、psycopg2、MySQLdb、pymssql 等）都会返回一个元组列表；你可以将元组的列表传给 DataFrame 构造函数，但你还需要包含在游标的 description 属性中的列名；

这种数据规整操作相当多，你肯定不想每查一次数据库就重写一次。SQLAlchemy 项目是一个流行的 Python SQL 工具，它抽象出了 SQL 数据库中的许多常见差异。pandas 有一个 read_sql 函数，可以让你轻松的从 SQLAlchemy 连接读取数据。这里，我们用 SQLAlchemy 连接 SQLite 数据库，并从之前创建的表读取数据。

```
In [135]: import sqlalchemy as sqla 
In [136]: db = sqla.create_engine('sqlite:///mydata.sqlite') 
In [137]: pd.read_sql('select * from test', db) 
```

3『[SQLAlchemy - The Database Toolkit for Python](https://www.sqlalchemy.org/)』

## 07. 数据清洗和准备

### 1. 逻辑脉络

用 pandas 进行数据的清洗。讨论处理缺失数据、重复数据、字符串操作和其它分析数据转换的工具。

处理缺失数据

数据转换

字符串操作

### 2. 摘录及评论

在数据分析和建模的过程中，相当多的时间要用在数据准备上：加载、清理、转换以及重塑。这些工作会占到分析师时间的 80% 或更多。有时，存储在文件和数据库中的数据的格式不适合某个特定的任务。许多研究者都选择使用通用编程语言（如 Python、Perl、R 或 Java）或 UNIX 文本处理工具（如 sed 或 awk）对数据格式进行专门处理。幸运的是，pandas 和内置的 Python 标准库提供了一组高级的、灵活的、快速的工具，可以让你轻松地将数据规变为想要的格式。

如果你发现了一种本书或 pandas 库中没有的数据操作方式，请尽管在邮件列表或 GitHub 网站上提出。实际上，pandas 的许多设计和实现都是由真实应用的需求所驱动的。在本章中，我会讨论处理缺失数据、重复数据、字符串操作和其它分析数据转换的工具。下一章，我会关注于用多种方法合并、重塑数据集。























