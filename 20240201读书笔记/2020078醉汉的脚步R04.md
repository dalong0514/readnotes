## 记忆时间

## 目录

0701 Measurement and the Law of Errors

0801 The Order in Chaos

## 0701. Measurement and the Law of Errors

ONE DAY not long ago my son Alexei came home and announced the grade on his most recent English essay. He had received a 93. Under normal circumstances I would have congratulated him on earning an A. And since it was a low A and I know him to be capable of better, I would have added that this grade was evidence that if he put in a little effort, he could score even higher next time. But these were not normal circumstances, and in this case I considered the grade of 93 to be a shocking underestimation of the quality of the essay. At this point you might think that the previous few sentences tell you more about me than about Alexei. If so, you're right on target. In fact, the above episode is entirely about me, for it was I who wrote Alexei's essay.

Okay, shame on me. In my defense I should point out that I would normally no sooner write Alexei's essays than take a foot to the chin for him in his kung fu class. But Alexei had come to me for a critique of his work and as usual presented his request late on the night before the paper was due. I told him I'd get back to him. Proceeding to read it on the computer, I first made a couple of minor changes, nothing worth bothering to note. Then, being a relentless rewriter, I gradually found myself sucked in, rearranging this and rewriting that, and before I finished, not only had he fallen asleep, but I had made the essay my own. The next morning, sheepishly admitting that I had neglected to perform a「save as」on the original, I told him to just go ahead and turn in my version.

He handed me the graded paper with a few words of encouragement.「Not bad,」he told me.「A 93 is really more of an A- than an A, but it was late and I'm sure if you were more awake, you would have done better.」I was not happy. First of all, it is unpleasant when a fifteen-year-old says the very words to you that you have previously said to him, and nevertheless you find his words inane. But beyond that, how could my material — the work of a person whom my mother, at least, thinks of as a professional writer — not make the grade in a high school English class? Apparently I am not alone. Since then I have been told of another writer who had a similar experience, except his daughter received a B. Apparently the writer, with a PhD in English, writes well enough for Rolling Stone, Esquire, and The New York Times but not for English 101. Alexei tried to comfort me with another story: two of his friends, he said, once turned in identical essays. He thought that was stupid and they'd both be suspended, but not only did the overworked teacher not notice, she gave one of the essays a 90 (an A) and the other a 79 (a C). (Sounds odd unless, like me, you've had the experience of staying up all night grading a tall stack of papers with Star Trek reruns playing in the background to break the monotony.)

Numbers always seem to carry the weight of authority. The thinking, at least subliminally, goes like this: if a teacher awards grades on a 100-point scale, those tiny distinctions must really mean something. But if ten publishers could deem the manuscript for the first Harry Potter book unworthy of publication, how could poor Mrs. Finnegan (not her real name) distinguish so finely between essays as to award one a 92 and another a 93? If we accept that the quality of an essay is somehow definable, we must still recognize that a grade is not a description of an essay's degree of quality but rather a measurement of it, and one of the most important ways randomness affects us is through its influence on measurement. In the case of the essay the measurement apparatus was the teacher, and a teacher's assessment, like any measurement, is susceptible to random variance and error.

Voting is also a kind of measurement. In that case we are measuring not simply how many people support each candidate on election day but how many care enough to take the trouble to vote. There are many sources of random error in this measurement. Some legitimate voters might find that their name is not on the rolls of registered voters. Others mistakenly vote for a candidate other than the one intended. And of course there are errors in counting the votes. Some ballots are improperly accepted or rejected; others are simply lost. In most elections the sum of all these factors doesn't add up to enough to affect the outcome. But in close elections it can, and then we usually go through one or more recounts, as if our second or third counting of the votes will be less affected by random errors than our first.

In the 2004 governor's race in the state of Washington, for example, the Democratic candidate was eventually declared the winner although the original tally had the Republican winning by 261 votes out of about 3 million.1 Since the original vote count was so close, state law required a recount. In that count the Republican won again, but by only 42 votes. It is not known whether anyone thought it was a bad sign that the 219-vote difference between the first and second vote counts was several times larger than the new margin of victory, but the upshot was a third vote count, this one entirely「by hand.」The 42-vote victory amounted to an edge of just 1 vote out of each 70,000 cast, so the hand-counting effort could be compared to asking 42 people to count from 1 to 70,000 and then hoping they averaged less than 1 mistake each. Not surprisingly, the result changed again. This time it favored the Democrat by 10 votes. That number was later changed to 129 when 700 newly discovered「lost votes」were included.

Neither the vote-counting process nor the voting process is perfect. If, for instance, owing to post office mistakes, 1 in 100 prospective voters didn't get the mailer with the location of the polling place and 1 in 100 of those people did not vote because of it, in the Washington election that would have amounted to 300 voters who would have voted but didn't because of government error. Elections, like all measurements, are imprecise, and so are the recounts, so when elections come out extremely close, perhaps we ought to accept them as is, or flip a coin, rather than conducting recount after recount.

The imprecision of measurement became a major issue in the mid-eighteenth century, when one of the primary occupations of those working in celestial physics and mathematics was the problem of reconciling Newton's laws with the observed motions of the moon and planets. One way to produce a single number from a set of discordant measurements is to take the average, or mean. It seems to have been young Isaac Newton who, in his optical investigations, first employed it for that purpose.2 But as in many things, Newton was an anomaly. Most scientists in Newton's day, and in the following century, didn't take the mean. Instead, they chose the single「golden number」from among their measurements — the number they deemed mainly by hunch to be the most reliable result they had. That's because they regarded variation in measurement not as the inevitable by-product of the measuring process but as evidence of failure — with, at times, even moral consequences. In fact, they rarely published multiple measurements of the same quantity, feeling it would amount to the admission of a botched process and raise the issue of trust. But in the mid-eighteenth century the tide began to change. Calculating the gross motion of heavenly bodies, a series of nearly circular ellipses, is a simple task performed today by precocious high school students as music blares through their headphones. But to describe planetary motion in its finer points, taking into account not only the gravitational pull of the sun but also that of the other planets and the deviation of the planets and the moon from a perfectly spherical shape, is even today a difficult problem. To accomplish that goal, complex and approximate mathematics had to be reconciled with imperfect observation and measurement.

There was another reason why the late eighteenth century demanded a mathematical theory of measurement: beginning in the 1780s in France a new mode of rigorous experimental physics had arisen.3 Before that period, physics consisted of two separate traditions. On the one hand, mathematical scientists investigated the precise consequences of Newton's theories of motion and gravity. On the other, a group sometimes described as experimental philosophers performed empirical investigations of electricity, magnetism, light, and heat. The experimental philosophers — often amateurs — were less focused on the rigorous methodology of science than were the mathematics-oriented researchers, and so a movement arose to reform and mathematize experimental physics. In it Pierre-Simon de Laplace again played a major role.

Laplace had become interested in physical science through the work of his fellow Frenchman Antoine-Laurent Lavoisier, considered the father of modern chemistry.4 Laplace and Lavoisier worked together for years, but Lavoisier did not prove as adept as Laplace at navigating the troubled times. To earn money to finance his many scientific experiments, he had become a member of a privileged private association of state-protected tax collectors. There is probably no time in history when having such a position would inspire your fellow citizens to invite you into their homes for a nice hot cup of gingerbread cappuccino, but when the French Revolution came, it proved an especially onerous credential. In 1794, Lavoisier was arrested with the rest of the association and quickly sentenced to death. Ever the dedicated scientist, he requested time to complete some of his research so that it would be available to posterity. To that the presiding judge famously replied,「The republic has no need of scientists.」The father of modern chemistry was promptly beheaded, his body tossed into a mass grave. He had reportedly instructed his assistant to count the number of words his severed head would attempt to mouth.

Laplace's and Lavoisier's work, along with that of a few others, especially the French physicist Charles-Augustin de Coulomb, who experimented on electricity and magnetism, transformed experimental physics. Their work also contributed to the development, in the 1790s, of a new rational system of units, the metric system, to replace the disparate systems that had impeded science and were a frequent cause of dispute among merchants. Developed by a group appointed by Louis XVI, the metric system was adopted by the revolutionary government after Louis's downfall. Lavoisier, ironically, had been one of the group's members.

The demands of both astronomy and experimental physics meant that a great part of the mathematician's task in the late eighteenth and early nineteenth centuries was understanding and quantifying random error. Those efforts led to a new field, mathematical statistics, which provides a set of tools for the interpretation of the data that arise from observation and experimentation. Statisticians sometimes view the growth of modern science as revolving around that development, the creation of a theory of measurement. But statistics also provides tools to address real-world issues, such as the effectiveness of drugs or the popularity of politicians, so a proper understanding of statistical reasoning is as useful in everyday life as it is in science.

IT IS ONE OF THOSE CONTRADICTIONS of life that although measurement always carries uncertainty, the uncertainty in measurement is rarely discussed when measurements are quoted. If a fastidious traffic cop tells the judge her radar gun clocked you going thirty-nine in a thirty-five-mile-per-hour zone, the ticket will usually stick despite the fact that readings from radar guns often vary by several miles per hour.5 And though many students (along with their parents) would jump off the roof if doing so would raise their 598 on the math SAT to a 625, few educators talk about the studies showing that, if you want to gain 30 points, there's a good chance you can do it simply by taking the test a couple more times.6 Sometimes meaningless distinctions even make the news. One recent August the Bureau of Labor Statistics reported that the unemployment rate stood at 4.7 percent. In July the bureau had reported the rate at 4.8 percent. The change prompted headlines like this one in The New York Times:「Jobs and Wages Increased Modestly Last Month.」7 But as Gene Epstein, the economics editor of Barron's, put it,「Merely because the number has changed it doesn't necessarily mean that a thing itself has changed. For example, any time the unemployment rate moves by a tenth of a percentage point…that is a change that is so small, there is no way to tell whether there really was a change.」8 In other words, if the Bureau of Labor Statistics measures the unemployment rate in August and then repeats its measurement an hour later, by random error alone there is a good chance that the second measurement will differ from the first by at least a tenth of a percentage point. Would The New York Times then run the headline「Jobs and Wages Increased Modestly at 2 P.M.」?

The uncertainty in measurement is even more problematic when the quantity being measured is subjective, like Alexei's English-class essay. For instance, a group of researchers at Clarion University of Pennsylvania collected 120 term papers and treated them with a degree of scrutiny you can be certain your own child's work will never receive: each term paper was scored independently by eight faculty members. The resulting grades, on a scale from A to F, sometimes varied by two or more grades. On average they differed by nearly one grade.9 Since a student's future often depends on such judgments, the imprecision is unfortunate. Yet it is understandable given that, in their approach and philosophy, the professors in any given college department often run the gamut from Karl Marx to Groucho Marx. But what if we control for that — that is, if the graders are given, and instructed to follow, certain fixed grading criteria? A researcher at Iowa State University presented about 100 students' essays to a group of doctoral students in rhetoric and professional communication whom he had trained extensively according to such criteria.10 Two independent assessors graded each essay on a scale of 1 to 4. When the scores were compared, the assessors agreed in only about half the cases. Similar results were found by the University of Texas in an analysis of its scores on college-entrance essays.11 Even the venerable College Board expects only that, when assessed by two raters,「92% of all scored essays will receive ratings within ± 1 point of each other on the 6-point SAT essay scale.」12

Another subjective measurement that is given more credence than it warrants is the rating of wines. Back in the 1970s the wine business was a sleepy enterprise, growing, but mainly in the sales of low-grade jug wines. Then, in 1978, an event often credited with the rapid growth of that industry occurred: a lawyer turned self-proclaimed wine critic, Robert M. Parker Jr., decided that, in addition to his reviews, he would rate wines numerically on a 100-point scale. Over the years most other wine publications followed suit. Today annual wine sales in the United States exceed `$`20 billion, and millions of wine aficionados won't lay their money on the counter without first looking to a wine's rating to support their choice. So when Wine Spectator awarded, say, the 2004 Valentín Bianchi Argentine cabernet sauvignon a 90 rather than an 89, that single extra point translated into a huge difference in Valentín Bianchi's sales.13 In fact, if you look in your local wine shop, you'll find that the sale and bargain wines, owing to their lesser appeal, are often the wines rated in the high 80s. But what are the chances that the 2004 Valentín Bianchi Argentine cabernet that received a 90 would have received an 89 if the rating process had been repeated, say, an hour later?

In his 1890 book The Principles of Psychology, William James suggested that wine expertise could extend to the ability to judge whether a sample of Madeira came from the top or the bottom of a bottle.14 In the wine tastings that I've attended over the years, I've noticed that if the bearded fellow to my left mutters「a great nose」(the wine smells good), others certainly might chime in their agreement. But if you make your notes independently and without discussion, you often find that the bearded fellow wrote,「Great nose」the guy with the shaved head scribbled,「No nose」and the blond woman with the perm wrote,「Interesting nose with hints of parsley and freshly tanned leather.」

From the theoretical viewpoint, there are many reasons to question the significance of wine ratings. For one thing, taste perception depends on a complex interaction between taste and olfactory stimulation. Strictly speaking, the sense of taste comes from five types of receptor cells on the tongue: salty, sweet, sour, bitter, and umami. The last responds to certain amino acid compounds (prevalent, for example, in soy sauce). But if that were all there was to taste perception, you could mimic everything — your favorite steak, baked potato, and apple pie feast or a nice spaghetti Bolognese — employing only table salt, sugar, vinegar, quinine, and monosodium glutamate. Fortunately there is more to gluttony than that, and that is where the sense of smell comes in. The sense of smell explains why, if you take two identical solutions of sugar water and add to one a (sugar-free) essence of strawberry, it will taste sweeter than the other.15 The perceived taste of wine arises from the effects of a stew of between 600 and 800 volatile organic compounds on both the tongue and the nose.16 That's a problem, given that studies have shown that even flavor-trained professionals can rarely reliably identify more than three or four components in a mixture.17

Expectations also affect your perception of taste. In 1963 three researchers secretly added a bit of red food color to white wine to give it the blush of a rosé. They then asked a group of experts to rate its sweetness in comparison with the untinted wine. The experts perceived the fake rosé as sweeter than the white, according to their expectation. Another group of researchers gave a group of oenology students two wine samples. Both samples contained the same white wine, but to one was added a tasteless grape anthocyanin dye that made it appear to be red wine. The students also perceived differences between the red and the white corresponding to their expectations.18 And in a 2008 study a group of volunteers asked to rate five wines rated a bottle labeled `$`90 higher than another bottle labeled `$`10, even though the sneaky researchers had filled both bottles with the same wine. What's more, this test was conducted while the subjects were having their brains imaged in a magnetic resonance scanner. The scans showed that the area of the brain thought to encode our experience of pleasure was truly more active when the subjects drank the wine they believed was more expensive.19 But before you judge the oenophiles, consider this: when a researcher asked 30 cola drinkers whether they preferred Coke or Pepsi and then asked them to test their preference by tasting both brands side by side, 21 of the 30 reported that the taste test confirmed their choice even though this sneaky researcher had put Coke in the Pepsi bottle and vice versa.20 When we perform an assessment or measurement, our brains do not rely solely on direct perceptional input. They also integrate other sources of information — such as our expectation.

Wine tasters are also often fooled by the flip side of the expectancy bias: a lack of context. Holding a chunk of horseradish under your nostril, you'd probably not mistake it for a clove of garlic, nor would you mistake a clove of garlic for, say, the inside of your sneaker. But if you sniff clear liquid scents, all bets are off. In the absence of context, there's a good chance you'd mix the scents up. At least that's what happened when two researchers presented experts with a series of sixteen random odors: the experts misidentified about 1 out of every 4 scents.21

Given all these reasons for skepticism, scientists designed ways to measure wine experts' taste discrimination directly. One method is to use a wine triangle. It is not a physical triangle but a metaphor: each expert is given three wines, two of which are identical. The mission: to choose the odd sample. In a 1990 study, the experts identified the odd sample only two-thirds of the time, which means that in 1 out of 3 taste challenges these wine gurus couldn't distinguish a pinot noir with, say,「an exuberant nose of wild strawberry, luscious blackberry, and raspberry,」from one with「the scent of distinctive dried plums, yellow cherries, and silky cassis.」22 In the same study an ensemble of experts was asked to rank a series of wines based on 12 components, such as alcohol content, the presence of tannins, sweetness, and fruitiness. The experts disagreed significantly on 9 of the 12 components. Finally, when asked to match wines with the descriptions provided by other experts, the subjects were correct only 70 percent of the time.

Wine critics are conscious of all these difficulties.「On many levels…[the ratings system] is nonsensical,」says the editor of Wine and Spirits Magazine.23 And according to a former editor of Wine Enthusiast,「The deeper you get into this the more you realize how misguided and misleading this all is.」24 Yet the rating system thrives. Why? The critics found that when they attempted to encapsulate wine quality with a system of stars or simple verbal descriptors such as good, bad, and maybe ugly, their opinions were unconvincing. But when they used numbers, shoppers worshipped their pronouncements. Numerical ratings, though dubious, make buyers confident that they can pick the golden needle (or the silver one, depending on their budget) from the haystack of wine varieties, makers, and vintages.

If a wine — or an essay — truly admits some measure of quality that can be summarized by a number, a theory of measurement must address two key issues: How do we determine that number from a series of varying measurements? And given a limited set of measurements, how can we assess the probability that our determination is correct? We now turn to these questions, for whether the source of data is objective or subjective, their answers are the goal of the theory of measurement.

THE KEY to understanding measurement is understanding the nature of the variation in data caused by random error. Suppose we offer a number of wines to fifteen critics or we offer the wines to one critic repeatedly on different days or we do both. We can neatly summarize the opinions employing the average, or mean, of the ratings. But it is not just the mean that matters: if all fifteen critics agree that the wine is a 90, that sends one message; if the critics produce the ratings 80, 81, 82, 87, 89, 89, 90, 90, 90, 91, 91, 94, 97, 99, and 100, that sends another. Both sets of data have the same mean, but they differ in the amount they vary from that mean. Since the manner in which data points are distributed is such an important piece of information, mathematicians created a numerical measure of variation to describe it. That number is called the sample standard deviation. Mathematicians also measure the variation by its square, which is called the sample variance.

The sample standard deviation characterizes how close to the mean a set of data clusters or, in practical terms, the uncertainty of the data. When it is low, the data fall near the mean. For the data in which all wine critics rated the wine 90, for example, the sample standard deviation is 0, telling you that all the data are identical to the mean. When the sample standard deviation is high, however, the data are not clustered around the mean. For the set of wine ratings above that ranges from 80 to 100, the sample standard deviation is 6, meaning that as a rule of thumb most of the ratings fall within 6 points of the mean. In that case all you can really say about the wine is that it is probably somewhere between an 84 and a 96.

In judging the meaning of their measurements, scientists in the eighteenth and nineteenth centuries faced the same issues as the skeptical oenophile. For if a group of researchers makes a series of observations, the results will almost always differ. One astronomer might suffer adverse atmospheric conditions; another might be jostled by a breeze; a third might have just returned from a Madeira tasting with William James. In 1838 the mathematician and astronomer F. W. Bessel categorized eleven classes of random errors that occur in every telescopic observation. Even if a single astronomer makes repeated measurements, variables such as unreliable eyesight or the effect of temperature on the apparatus will cause the observations to vary. And so astronomers must understand how, given a series of discrepant measurements, they can determine a body's true position. But just because oenophiles and scientists share a problem, it doesn't mean they can share its solution. Can we identify general characteristics of random error, or does the character of random error depend on the context?

One of the first to imply that diverse sets of measurements share common characteristics was Jakob Bernoulli's nephew Daniel. In 1777 he likened the random errors in astronomical observation to the deviations in the flight of an archer's arrows. In both cases, he reasoned, the target — true value of the measured quantity, or the bull's-eye — should lie somewhere near the center, and the observed results should be bunched around it, with more reaching the inner bands and fewer falling farther from the mark. The law he proposed to describe the distribution did not prove to be the correct one, but what is important is the insight that the distribution of an archer's errors might mirror the distribution of errors in astronomical observations.

That the distribution of errors follows some universal law, sometimes called the error law, is the central precept on which the theory of measurement is based. Its magical implication is that, given that certain very common conditions are satisfied, any determination of a true value based on measured values can be solved employing a single mathematical analysis. When such a universal law is employed, the problem of determining the true position of a heavenly body based on a set of astronomers' measurements is equivalent to that of determining the position of a bull's-eye given only the arrow holes or a wine's「quality」given a series of ratings. That is the reason mathematical statistics is a coherent subject rather than merely a bag of tricks: whether your repeated measurements are aimed at determining the position of Jupiter at 4 A.M. on Christmas Day or the weight of a loaf of raisin bread coming off an assembly line, the distribution of errors is the same.

This doesn't mean random error is the only kind of error that can affect measurement. If half a group of wine critics liked only red wines and the other half only white wines but they all otherwise agreed perfectly (and were perfectly consistent), then the ratings earned by a particular wine would not follow the error law but instead would consist of two sharp peaks, one due to the red wine lovers and one due to the white wine lovers. But even in situations where the applicability of the law may not be obvious, from the point spreads of pro football games25 to IQ ratings, the error law often does apply. Many years ago I got hold of a few thousand registration cards for a consumer software program a friend had designed for eight- and nine-year-olds. The software wasn't selling as well as expected. Who was buying it? After some tabulation I found that the greatest number of users occurred at age seven, indicating an unwelcome but not unexpected mismatch. But what was truly striking was that when I made a bar graph showing how the number of buyers diminished as the buyers' age strayed from the mean of seven, I found that the graph took a very familiar shape — that of the error law.

It is one thing to suspect that archers and astronomers, chemists and marketers, encounter the same error law; it is another to discover the specific form of that law. Driven by the need to analyze astronomical data, scientists like Daniel Bernoulli and Laplace postulated a series of flawed candidates in the late eighteenth century. As it turned out, the correct mathematical function describing the error law — the bell curve — had been under their noses the whole time. It had been discovered in London in a different context many decades earlier.

OF THE THREE PEOPLE instrumental in uncovering the importance of the bell curve, its discoverer is the one who least often gets the credit. Abraham De Moivre's breakthrough came in 1733, when he was in his mid-sixties, and wasn't made public until his book The Doctrine of Chances came out in its second edition five years later. De Moivre was led to the curve while searching for an approximation to the numbers that inhabit the regions of Pascal's triangle far beneath the place where I truncated it, hundreds or thousands of lines down. In order to prove his version of the law of large numbers, Jakob Bernoulli had had to grapple with certain properties of the numbers that appeared in those lines. The numbers can be very large — for instance, one coefficient in the 200th row of Pascal's triangle has fifty-nine digits! In Bernoulli's day, and indeed in the days before computers, such numbers were obviously very hard to calculate. That's why, as I said, Bernoulli proved his law of large numbers employing various approximations, which diminished the practical usefulness of his result. With his curve, De Moivre was able to make far better approximations to the coefficients and therefore greatly improve on Bernoulli's estimates.

The approximation De Moivre derived is evident if, as I did for the registration cards, you represent the numbers in a row of the triangle by the height of the bars on a bar graph. For instance, the three numbers in the third line of the triangle are 1, 2, 1. In their bar graph the first bar rises one unit; the second is twice that height; and the third is again just one unit. Now look at the five numbers in the fifth line: 1, 4, 6, 4, 1. That graph will have five bars, again starting low, rising to a peak at the center, and then falling off symmetrically. The coefficients very far down in the triangle lead to bar graphs with very many bars, but they behave in the same manner. The bar graphs in the case of the 10th, 100th, and 1,000th lines of Pascal's triangle are shown on chapter 07.

If you draw a curve connecting the tops of all the bars in each bar graph, it will take on a characteristic shape, a shape approaching that of a bell. And if you smooth the curve a bit, you can write a mathematical expression for it. That smooth bell curve is more than just a visualization of the numbers in Pascal's triangle; it is a means for obtaining an accurate and easy-to-use estimate of the numbers that appear in the triangle's lower lines. This was De Moivre's discovery.

Today the bell curve is usually called the normal distribution and sometimes the Gaussian distribution (we'll see later where that term originated). The normal distribution is actually not a fixed curve but a family of curves, in which each depends on two parameters to set its specific position and shape. The first parameter determines where its peak is located, which is at 5, 50, and 500 in the graphs on chapter 7. The second parameter determines the amount of spread in the curve. Though it didn't receive its modern name until 1894, this measure is called the standard deviation, and it is the theoretical counterpart of the concept I spoke of earlier, the sample standard deviation. Roughly speaking, it is half the width of the curve at the point at which the curve is about 60 percent of its maximum height. Today the importance of the normal distribution stretches far beyond its use as an approximation to the numbers in Pascal's triangle. It is, in fact, the most widespread manner in which data have been found to be distributed.

When employed to describe the distribution of data, the bell curve describes how, when you make many observations, most of them fall around the mean, which is represented by the peak of the curve. Moreover, as the curve slopes symmetrically downward on either side, it describes how the number of observations diminishes equally above and below the mean, at first rather sharply and then less drastically. In data that follow the normal distribution, about 68 percent (roughly two-thirds) of your observations will fall within 1 standard deviation of the mean, about 95 percent within 2 standard deviations, and 99.7 percent within 3.

The bars in the graphs above represent the relative magnitudes of the entries in the 10th, 100th, and 1,000th rows of Pascal's triangle (see chapter 04). The numbers along the horizontal axis indicate to which entry the bar refers. By convention, that labeling begins at 0, rather than 1 (the middle and bottom graphs have been truncated so that the entries whose bars would have negligible height are not shown).

In order to visualize this, have a look at the graph on chapter 07. In this table the data marked by squares concern the guesses made by 300 students, each observing a series of 10 coin flips.26 Along the horizontal axis is plotted the number of correct guesses, from 0 to 10. Along the vertical axis is plotted the number of students who achieved that number of correct guesses. The curve is bell shaped, centered at 5 correct guesses, at which point its height corresponds to about 75 students. The curve falls to about two-thirds of its maximum height, corresponding to about 51 students, about halfway between 3 and 4 correct guesses on the left and between 6 and 7 on the right. A bell curve with this magnitude of standard deviation is typical of a random process such as guessing the result of a coin toss.

The same graph also displays another set of data, marked by circles. That set describes the performance of 300 mutual fund managers. In this case the horizontal axis represents not correct guesses of coin flips but the number of years (out of 10) that a manager performed above the group average. Note the similarity! We'll get back to this in chapter 9.

A good way to get a feeling for how the normal distribution relates to random error is to consider the process of polling, or sampling. You may recall the poll I described in chapter 5 regarding the popularity of the mayor of Basel. In that city a certain fraction of voters approved of the mayor, and a certain fraction disapproved. For the sake of simplicity we will now assume each was 50 percent. As we saw, there is a chance that those involved in the poll would not reflect exactly this 50/50 split. In fact, if N voters were questioned, the chances that any given number of them would support the mayor are proportional to the numbers on line N of Pascal's triangle. And so, according to De Moivre's work, if pollsters poll a large number of voters, the probabilities of different polling results can be described by the normal distribution. In other words about 95 percent of the time the approval rating they observe in their poll will fall within 2 standard deviations of the true rating, 50 percent. Pollsters use the term margin of error to describe this uncertainty. When pollsters tell the media that a poll's margin of error is plus or minus 5 percent, they mean that if they were to repeat the poll a large number of times, 19 out of 20 (95 percent) of those times the result would be within 5 percent of the correct answer. (Though pollsters rarely point this out, that also means, of course, that about 1 time in 20 the result will be wildly inaccurate.) As a rule of thumb, a sample of 100 yields a margin of error that is too great for most purposes. A sample of 1,000, on the other hand, usually yields a margin of error in the ballpark of 3 percent, which for most purposes suffices.

Coin toss guessing compared to stock-picking success

It is important, whenever assessing any kind of survey or poll, to realize that when it is repeated, we should expect the results to vary. For example, if in reality 40 percent of registered voters approve of the way the president is handling his job, it is much more likely that six independent surveys will report numbers like 37, 39, 39, 40, 42, and 42 than it is that all six surveys will agree that the president's support stands at 40 percent. (Those six numbers are in fact the results of six independent polls gauging the president's job approval in the first two weeks of September 2006.)27 That's why, as another rule of thumb, any variation within the margin of error should be ignored. But although The New York Times would not run the headline「Jobs and Wages Increased Modestly at 2 P.M.,」analogous headlines are common in the reporting of political polls. For example, after the Republican National Convention in 2004, CNN ran the headline「Bush Apparently Gets Modest Bounce.」28 The experts at CNN went on to explain that「Bush's convention bounce appeared to be 2 percentage points…. The percentage of likely voters who said he was their choice for president rose from 50 right before the convention to 52 immediately afterward.」Only later did the reporter remark that the poll's margin of error was plus or minus 3.5 percentage points, which means that the news flash was essentially meaningless. Apparently the word apparently, in CNN-talk, means「apparently not.」

For many polls a margin of error of more than 5 percent is considered unacceptable, yet in our everyday lives we make judgments based on far fewer data points than that. People don't get to play 100 years of professional basketball, invest in 100 apartment buildings, or start 100 chocolate-chip-cookie companies. And so when we judge their success at those enterprises, we judge them on just a few data points. Should a football team lavish `$`50 million to lure a guy coming off a single record-breaking year? How likely is it that the stockbroker who wants your money for a sure thing will repeat her earlier successes? Does the success of the wealthy inventor of sea monkeys mean there is a good chance he'll succeed with his new ideas of invisible goldfish and instant frogs? (For the record, he didn't.)29 When we observe a success or a failure, we are observing one data point, a sample from under the bell curve that represents the potentialities that previously existed. We cannot know whether our single observation represents the mean or an outlier, an event to bet on or a rare happening that is not likely to be reproduced. But at a minimum we ought to be aware that a sample point is just a sample point, and rather than accepting it simply as reality, we ought to see it in the context of the standard deviation or the spread of possibilities that produced it. The wine might be rated 91, but that number is meaningless if we have no estimate of the variation that would occur if the identical wine were rated again and again or by someone else. It might help to know, for instance, that a few years back, when both The Penguin Good Australian Wine Guide and On Wine's Australian Wine Annual reviewed the 1999 vintage of the Mitchelton Blackwood Park Riesling, the Penguin guide gave the wine five stars out of five and named it Penguin Best Wine of the Year, while On Wine rated it at the bottom of all the wines it reviewed, deeming it the worst vintage produced in a decade.30 The normal distribution not only helps us understand such discrepancies, but also has enabled a myriad of statistical applications widely employed today in both science and commerce — for example, whenever a drug company assesses whether the results of a clinical trial are significant, a manufacturer assesses whether a sample of parts accurately reflects the proportion of those that are defective, or a marketer decides whether to act on the results of a research survey.

THE RECOGNITION that the normal distribution describes the distribution of measurement error came decades after De Moivre's work, by that fellow whose name is sometimes attached to the bell curve, the German mathematician Carl Friedrich Gauss. It was while working on the problem of planetary motion that Gauss came to that realization, at least regarding astronomical measurements. Gauss's「proof,」however, was, by his own later admission, invalid.31 Moreover, its far-reaching consequences also eluded him. And so he slipped the law inconspicuously into a section at the end of a book called The Theory of the Motion of Heavenly Bodies Moving about the Sun in Conic Sections. There it may well have died, just another in the growing pile of abandoned proposals for the error law.

It was Laplace who plucked the normal distribution from obscurity. He encountered Gauss's work in 1810, soon after he had read a memoir to the Académie des Sciences proving a theorem called the central limit theorem, which says that the probability that the sum of a large number of independent random factors will take on any given value is distributed according to the normal distribution. For example, suppose you bake 100 loaves of bread, each time following a recipe that is meant to produce a loaf weighing 1,000 grams. By chance you will sometimes add a bit more or a bit less flour or milk, or a bit more or less moisture may escape in the oven. If in the end each of a myriad of possible causes adds or subtracts a few grams, the central limit theorem says that the weight of your loaves will vary according to the normal distribution. Upon reading Gauss's work, Laplace immediately realized that he could use it to improve his own and that his work could provide a better argument than Gauss's to support the notion that the normal distribution is indeed the error law. Laplace rushed to press a short sequel to his memoir on the theorem. Today the central limit theorem and the law of large numbers are the two most famous results of the theory of randomness.

To illustrate how the central limit theorem explains why the normal distribution is the correct error law, let's reconsider Daniel Bernoulli's example of the archer. I played the role of the archer one night after a pleasant interlude of wine and adult company, when my younger son, Nicolai, handed me a bow and arrow and dared me to shoot an apple off his head. The arrow had a soft foam tip, but still it seemed reasonable to conduct an analysis of my possible errors and their likelihood. For obvious reasons I was mainly concerned with vertical errors. A simple model of the errors is this: Each random factor — say, a sighting error, the effect of air currents, and so on — would throw my shot vertically off target, either high or low, with equal probability. My total error in aim would then be the sum of my errors. If I was lucky, about half the component errors would deflect the arrow upward and half downward, and my shot would end up right on target. If I was unlucky (or, more to the point, if my son was unlucky), the errors would all fall one way and my aim would be far off, either high or low. The relevant question was, how likely was it that the errors would cancel each other, or that they would add up to their maximum, or that they would take any other value in between? But that was just a Bernoulli process — like tossing coins and asking how likely it is that the tosses will result in a certain number of heads. The answer is described by Pascal's triangle or, if many trials are involved, by the normal distribution. And that, in this case, is precisely what the central limit theorem tells us. (As it turned out, I missed both apple and son, but did knock over a glass of very nice cabernet.)

By the 1830s most scientists had come to believe that every measurement is a composite, subject to a great number of sources of deviation and hence to the error law. The error law and the central limit theorem thus allowed for a new and deeper understanding of data and their relation to physical reality. In the ensuing century, scholars interested in human society also grasped these ideas and found to their surprise that the variation in human characteristics and behavior often displays the same pattern as the error in measurement. And so they sought to extend the application of the error law from physical science to a new science of human affairs.

### Notes

1 Sarah Kershaw and Eli Sanders,「Recounts and Partisan Bickering Bring Election Fatigue to Washington Voters,」New York Times, December 26, 2004; and Timothy Egan,「Trial for Governor's Seat Set to Start in Washington,」New York Times, May 23, 2005.

2 Jed Z. Buchwald,「Discrepant Measurements and Experimental Knowledge in the Early Modern Era,」Archive for History of Exact Sciences 60, no. 6 (November 2006): 565–649.

3 Eugene Frankel,「J. B. Biot and the Mathematization of Experimental Physics in Napoleonic France,」in Historical Studies in the Physical Sciences, ed. Russell McCormmach (Princeton, N.J.: Princeton University Press, 1977).

4 Charles Coulston Gillispie, ed., Dictionary of Scientific Biography (New York: Charles Scribner's Sons, 1981), p. 85.

5 For a discussion of the errors made by radar guns, see Nicole Weisensee Egan,「Takin' Aim at Radar Guns,」Philadelphia Daily News, March 9, 2004.

6 Charles T. Clotfelter and Jacob L. Vigdor,「Retaking the SAT」(working paper SAN01-20, Terry Sanford Institute of Public Policy, Duke University, Durham, N.C., July 2001).

7 Eduardo Porter,「Jobs and Wages Increased Modestly Last Month,」New York Times, September 2, 2006.

8 Gene Epstein on「Mathemagicians,」On the Media, WNYC radio, broadcast August 25, 2006.

9 Legene Quesenberry et al.,「Assessment of the Writing Component within a University General Education Program,」November 1, 2000; http://wac.colostate.edu/aw/articles/quesenberry2000/quesenberry2000.pdf.

10 Kevin Saunders,「Report to the Iowa State University Steering Committee on the Assessment of ISU Comm-English 105 Course Essays,」September 2004; www.iastate.edu/-isucomm/InYears/ISUcomm_essays.pdf (accessed 2005; site now discontinued).

11 University of Texas, Office of Admissions,「Inter-rater Reliability of Holistic Measures Used in the Freshman Admissions Process of the University of Texas at Austin,」February 22, 2005; http://www.utexas.edu/student/admissions/research/Inter-raterReliability2005.pdf.

12 Emily J. Shaw and Glenn B. Milewski,「Consistency and Reliability in the Individualized Review of College Applicants,」College Board, Office of Research and Development, Research Notes RN-20 (October 2004): 3; http://www.collegeboard.com/research/pdf/RN-20.pdf.

13 Gary Rivlin,「In Vino Veritas,」New York Times, August 13, 2006.

14 William James, The Principles of Psychology (New York: Henry Holt, 1890), p. 509.

15 Robert Frank and Jennifer Byram,「Taste-Smell Interactions Are Tastant and Odorant Dependent,」Chemical Senses 13 (1988): 445–55.

16 A. Rapp,「Natural Flavours of Wine: Correlation between Instrumental Analysis and Sensory Perception,」Fresenius' Journal of Analytic Chemistry 337, no. 7 (January 1990): 777–85.

17 D. Laing and W. Francis,「The Capacity of Humans to Identify Odors in Mixtures,」Physiology and Behavior 46, no. 5 (November 1989): 809–14; and D. Laing et al.,「The Limited Capacity of Humans to Identify the Components of Taste Mixtures and Taste-Odour Mixtures,」Perception 31, no. 5 (2002): 617–35.

18 For the rosé study, see Rose M. Pangborn, Harold W. Berg, and Brenda Hansen,「The Influence of Color on Discrimination of Sweetness in Dry Table-Wine,」American Journal of Psychology 76, no. 3 (September 1963): 492–95. For the anthocyanin study, see G. Morrot, F. Brochet, and D. Dubourdieu,「The Color of Odors,」Brain and Language 79, no. 2 (November 2001): 309–20.

19 Hilke Plassman, John O'Doherty, Baba Shia, and Antonio Rongel,「Marketing Actions Can Modulate Neural Representations of Experienced Pleasantness,」Proceedings of the National Academy of Sciences, January 14, 2008; http://www.pnas.org.

20 M. E. Woolfolk, W. Castellan, and C. Brooks,「Pepsi versus Coke: Labels, Not Tastes, Prevail,」Psychological Reports 52 (1983): 185–86.

21 M. Bende and S. Nordin,「Perceptual Learning in Olfaction: Professional Wine Tasters Versus Controls,」Physiology and Behavior 62, no. 5 (November 1997): 1065–70.

22 Gregg E. A. Solomon,「Psychology of Novice and Expert Wine Talk,」American Journal of Psychology 103, no. 4 (Winter 1990): 495–517.

23 Rivlin,「In Vino Veritas.」

24 Ibid.

25 Hal Stern,「On the Probability of Winning a Football Game,」American Statistician 45, no. 3 (August 1991): 179–82.

26 The graph is from Index Funds Advisors,「Index Funds.com: Take the Risk Capacity Survey,」http://www.indexfunds3.com/step3page2.php, where it is credited to Walter Good and Roy Hermansen, Index Your Way to Investment Success (New York: New York Institute of Finance, 1997). The performance of 300 mutual fund managers was tabulated for ten years (1987–1996), based on the Morningstar Principia database.

27 Polling Report,「President Bush — Overall Job Rating,」http://pollingreport.com/BushJob.htm.

28.「Poll: Bush Apparently Gets Modest Bounce,」CNN, September 8, 2004, http://www.cnn.com/2004/ALLPOLITICS/09/06/presidential.poll/index.htm.

29「Harold von Braunhut,」Telegraph, December 23, 2003; http://www.telegraph.co.uk/news/main.jhtml?xml=/news/2003/12/24/db2403.xml.

30 James J. Fogarty,「Why Is Expert Opinion on Wine Valueless?」(discussion paper 02.17, Department of Economics, University of Western Australia, Perth, 2001).

31 Stigler, The History of Statistics, p. 143.

0701测量与误差定律

不久前，我的儿子阿列克谢回到家中，向我通报了他最近一次作文的成绩 —— 93 分。我一般会祝贺他得了个 A，然后鼓励他争取下次得到更高的分数。所以我往往会再多句嘴，告诫他再努力一点点，但这次的情况有点儿不同。这个 93 分相较于文章的质量，实在是太低了。这个说法是不是让你觉得我是在维护自己的文章，而不是阿列克谢的作文？啊，你可说到点子上了：实际上，前面这段话的确完全是在说我自己，因为那篇作文是我写的……

对对对，我的做法真是可耻。不过我还是要为自己辩护一下，我一般不会帮阿列克谢写作文，我也不会到他的武术课用自己的脸替他挨那一脚。不过那天的情况有点儿不同。阿列克谢照例又是在交作业的头天深夜来找我，希望我点评一下他的作品。我答应帮他看看。最开始我只是在一两个地方做了一点儿小小的改动，但接着，我发现自己渐渐深陷无情的改写之中：这里的文字调一下顺序，那一段干脆重新写。等到修改完成后，儿子早已上床睡觉了，于是他的作文成了我自己的作品。第二天早上，我睡眼惺忪地承认，忘了把他原来的文章另存一份，所以我让他把我修改后的那版直接交上去。

他把成绩单递给了我。成绩单上写着几句鼓励的话语。「还不算糟，」他告诉我，「93 分确实更接近 A- 而不是 A，不过那天也确实是挺晚了。我相信，如果那天你更清醒一点儿，肯定会写得更好。」这些安慰的话可一点儿没让我更加高兴。首先，一个 15 岁的孩子把本该是你的台词甩到你的脸上，这种情况实在无法让人舒心，更何况我在他的话里听不出一星半点的真诚。另外，我，一个至少在我妈妈看来是个职业作家的人写的作文，怎么可能在高中英语课上拿不到高分？显然，在这个问题上我并不孤单。后来我听说另一个有着类似经历的作家的故事，不过他的女儿拿的可是个 B。很明显，这位拥有英语博士学位的作家，他的文笔好得至少能够满足《滚石》、《时尚先生》和《纽约时报》的要求，却对付不了英语 101 网络电台。阿列克谢还试着用另一个故事来安慰我：他有两个朋友，有一次把完全相同的两篇作文一起交了上去。他觉得这两个家伙很蠢，而且觉得他们肯定会被抓抄袭。但操劳过度的老师不仅没有注意到这两篇文章完全雷同，而且给其中一个打了 90 分（A），给另一个打了 79 分（C）。（听起来很怪，不过你如果跟我一样，要熬通宵给高高一摞卷子打分，而排遣这个枯燥无聊的工作的，只有旁边重播的《星际迷航》，你就会明白了。）

数字似乎总是自带权威性。人们几乎总是，或者至少是下意识地认为，如果老师按百分制打分，那么即使 1 分、2 分的微小差别，也一定意味着某种真实的差距。但如果连续 10 个出版商都相信第一部《哈利·波特》的手稿不值得出版的话，那么可怜的芬尼根太太（可不是我儿子英语老师的真名啊）怎么可能如此精确地区分两篇作文的好坏，给一篇打 92 分而另一篇打 93 分呢？即使我们接受作文质量可以在一定程度上被定义的观点，我们也应该认识到，分数并不是对作文质量的描述，更大程度上是对作文质量的测量。而随机性对测量的影响，正是它影响我们的最重要的方式之一。在作文的这个例子中，测量装置是教师，正如任何测量值一样，教师给出的分数很容易受随机变化和误差的影响。

投票也是一种测量。投票所测量的，并不是每位候选人在投票那天得到了多少人的支持，而是有多少人在乎这个选举，还不嫌麻烦地跑去投票。有些合法选民也许会发现，登记选民名册中并没有他们的名字；另外一些人则可能误将选票投给了他们并不支持的人。当然计票也有误差。有些选票不该被接受却被收下，有些选票不该被拒收却被拒之门外，还有些选票干脆凭空消失了。在大多数选举中，这些因素累积起来的总后果并不足以影响选举结果。但如果候选人得票数相差不大，这个后果就可能产生实质影响。这时，我们常常进行一次或多次的重新计票，就好像第二次或第三次计票，受到的随机影响会比第一次更少。

在 2004 年的华盛顿州州长竞选中，尽管最初的计票结果表明，共和党候选人靠着总数约 300 万张选票中多出的 261 张获胜，但最终获胜的却是民主党候选人。由于第一次的得票数很接近，根据该州法律，这时要进行重新计票。第二次计票仍然是共和党获胜，但领先差距缩小到 42 票。两次计票的这 219 票的差别，已经是新的领先票数的好几倍了。结果是不是让什么人产生了不祥的预感，我们不得而知。但头两次计票的结果，带来了第三次纯「手工」计票。这个 42 票的优势，相当于在每 7 万张选票中领先 1 票，因此，手工计票的作用，实际上可以被比拟为让 42 个人从 1 数到 7 万，并希望平均数错的次数为每人 1 次。所以并不令人吃惊的是，选举结果再次变化，民主党人反赢了 10 票。当新发现的 700 张「丢失选票」被加进来之后，这个优势变成了 129 票。

上面的计票和投票过程并非尽善尽美。比如，如果邮局犯错，每 100 个原本打算投票的选民，就有 1 个未能收到通知投票地点的邮件，因而没有去参加投票，那么在华盛顿州的这场选举中，仅这一项错误就能产生 300 名有投票意愿却因政府的过失而没有参加的选民。与所有的测量一样，选举并非精确无误，重新计票也是如此。因此，当选举结果极其接近时，也许我们更应该接受这个事实，或者干脆就靠扔硬币决定胜负，而不是把选票数了又数。

测量的不精确性是 18 世纪中叶学术界讨论的一个主要问题。那时，天体物理和数学领域的研究者的首要任务，就是让牛顿定律与所观测到的月球与行星的运行轨迹相吻合。如果对同一个量有若干彼此不同的观测值，而现在要利用这些不同值产生一个单一的值，那么方法之一是对这些观测值求平均，或者取它们的均值。现在我们认为是年轻的牛顿在做光学研究的时候，最早将这个算平均的方法用在了刚才所讲的那个问题上。但如同牛顿在许多其他事情上的做法一样，这种求平均的处理方式在当时可是个另类。从牛顿的时代到其后一个世纪的时间里，大多数科学家都不使用均值产生多个观测结果对应的最终结果，他们的做法是从这些观测结果中挑出一个「黄金数」，也就是他们的第六感认为最可靠的那个结果。他们之所以这么做，是因为同一个量的多次测量值的变化并未被他们看作测量过程中不可避免的副产品，而是被他们视为失败的象征，有时甚至还会给他们带来道德问题。实际上，他们很少公开同一个量的多个测量结果，因为如果这么做了，就等于承认他们对结果进行了修补，而这会给他们带来信任危机。但到了 18 世纪中期，情况有所变化。今天，计算天体的完整运行轨迹（一系列接近圆形的椭圆）是一件十分简单的事情，天赋高一点儿的高中生甚至可以一边戴着耳机听音乐，一边就把它给解决了。但是如果要更精细地描述行星的运动，不仅需要考虑太阳引力，还需要考虑其他行星的引力，以及行星和月球的形状与完美球体之间的偏差。时至今日，这仍是一个巨大的难题。为了完成这个任务，人们不得不将复杂的近似数学公式与不完美的观测结果加以调和。

18 世纪晚期出现的这个对于测量的数学理论的需求，还另有一个原因：18 世纪 80 年代，法国兴起一种新的、严密的实验物理学。在此之前，物理学包括两种相互分离的实践方式。一方面，数学家研究牛顿的运动和重力理论的精确推论；另一方面，一群有时被称为实验哲学家的人，采用经验主义的方式来研究电、磁、光和热。这些实验哲学家常常是一些业余爱好者，相较于关注数学的研究者，他们不那么看重严密的科学方法论。因此，一场对实验物理学进行改革并将其数学化的运动就此展开。而拉普拉斯再次扮演了主角。

拉普拉斯之所以对物理学产生兴趣，是由于被尊为现代化学之父的他的同胞安托万 - 洛朗·拉瓦锡的研究工作。拉普拉斯和拉瓦锡共事多年，但在幸免政治动乱这方面，拉瓦锡却不像拉普拉斯那样成功。为了赚钱支持自己的科学实验，拉瓦锡成为受国家保护的收税官这个享有特权的法国国王私人组织中的一员。这个职位的工作，本来也不可能让你的公民同胞们突然迸发出热情，并邀请你去他家享用美味可口的姜饼配咖啡。而等法国大革命到来时，它更有可能成为一项足以带来非常严重的后果的铁证。1794 年，拉瓦锡与组织的其他成员一同被捕，并很快被判处死刑。一直以来都是一名专注的科学家的拉瓦锡，请求法官再给他一点儿时间，以便他能把手头的几项研究完成，好留给后人。对他的这个请求，主审法官给出一个著名的答复：「共和国不需要科学家。」现代化学之父很快被处斩，尸体被扔进一个合葬墓。按传说所言，他在临刑前还指示他的助手要数一数，看他的头颅被砍下来之后还能说出几个字。

拉普拉斯、拉瓦锡和其他一些人，特别是进行电磁实验的法国物理学家查利 - 奥古斯丁·库仑的成果，改变了实验物理学的面貌。而在 18 世纪 90 年代，他们还对一种新的、合理的单位制的建立做出了贡献。这个新的单位制 —— 国际单位制 —— 的目的，是替代那些各自为政、阻碍科学发展还经常导致贸易纠纷的多个现有的单位制。国际单位制是由法国国王路易十六指派的一个小组建立的，在路易十六被推翻后被革命政府采用。具有讽刺意味的是，拉瓦锡就曾是这个小组的成员之一。

天文学和实验物理学方面的需求，意味着在 18 世纪晚期到 19 世纪早期，很大一部分数学家的任务，就是去理解和量化随机误差。他们的研究工作催生了一个新领域：数学统计学。这个分支学科提供了一整套工具，用于解释观测值和实验数据。统计学家有时认为，现代科学就是围绕着测量理论的建立成长起来的。统计学同样为解决诸如药物有效性或政治家受欢迎程度等现实问题提供了工具。正确地理解统计推理，不仅对科学研究十分有帮助，而且对我们在日常生活中碰到的很多问题作用很大。

尽管测量总是伴随着不确定性，但在给出测量结果时，测量中的不确定性很少被提及。这本身就是生活中众多的自相矛盾之一。如果一个挑剔的交警告诉法官，她的雷达枪测出你在限速 35 英里 / 小时的路段开到了 39 英里的时速，那么你一般是无法逃过被开超速罚单的命运的，尽管雷达枪的读数常常会发生多达数英里的变化。对许多学生（及其家长）而言，只要能让 SAT 数学考试成绩从 598 分提高到 625 分，就是让他们跳楼都没问题。不过却没有几位教育者告诉他们，有研究表明，你只需要多考几次，就有很大把握多拿 30 分。有时候，一些毫无意义的差别甚至能变成新闻。最近的某个 8 月，美国劳工统计局的就业统计数据显示失业率为 4.7%，而在 7 月，这个比例是 4.8%。这一改变马上就上了头条，比如《纽约时报》上的这个「上月的职位与薪水温和增长」。但用《巴伦周刊》经济版编辑吉恩·爱泼斯坦的话来说：「单纯的数字变化，并不意味着事情本身真的发生了变化。比如，失业率在任何时候都可能发生 0.1 个百分点的变化…… 这个变化如此之小，以至我们无法确定改变是否确实发生了。」换句话说，如果美国劳工统计局在 8 月统计了失业率，并在一小时后重新统计一次，那么仅仅由于随机误差，就很有可能使两次结果相差至少 0.1 个百分点。如果是这样的话，《纽约时报》的头条是不是该换成「下午两点的职位与薪水温和增长」？

当测量对象是一个如阿列克谢的英语课作文质量那样的主观量时，测量中的不确定性造成的问题就更大了。宾夕法尼亚州克莱瑞恩大学的一群研究者收集了 120 个学期的试卷，并对它们进行了非常仔细的审查。可以确定的是，我们自家孩子的作业永远享受不到如此细致程度的评价：每张试卷都由 8 名教师独立按 A 到 F 的七级分数制打分。对于同一张试卷，这 8 位教师给出的分数有时会相差 2 个甚至更多等级。而平均来说，不同教师给出的这个评分差别，差不多是 1 个等级。由于学生的前途常常取决于这类评价，因此，评分中存在的这个不精确性是十分不幸的。但我们应该知道，不论从方法上或理念上来看，任何一所大学中都有着从卡尔·马克思式到格劳乔·马克斯式的各式教授。如此一来，分数上的差别也就可以理解了。不过让我们再来考虑一下，如果我们在一定程度上控制这些影响打分的因素，比如固定打分的教师，并让他们按某固定的打分依据阅卷，结果又会如何呢？艾奥瓦州立大学的一名研究者，交给一群主修修辞与专业沟通的博士生大约 100 名学生的作文，并事先按评分标准对他们进行了高强度训练。每篇作文由两名独立评分者打分，等级为 1 等到 4 等。当比较他们的分数时，仅仅在大约一半的试卷上，两名打分者的意见相互一致。得克萨斯大学在本校的入学作文考试分数上也发现了类似的结果。即使是可敬的美国大学理事会，它所期望的评分稳定程度也不过是，当由两位打分者进行评判时，「在按 6 分制打分的 SAT 作文考试中，92% 的作文所得的两个分数分差在 ±1 分之内」。

另一个被赋予超出其合理可信程度的主观性测量，就是葡萄酒的评分。回溯 20 世纪 70 年代，当时的葡萄酒业还是个死气沉沉的行业，虽然有增长，但这个增长主要来自廉价的低等佐餐酒的销售。1978 年发生了一件通常被认为是葡萄酒业迅猛发展的事件：罗伯特·帕克这位律师出身的自封的酒评师，决定在文字的评语之外，再用 0 到 100 的数字给葡萄酒打分。之后，大多数其他酒类出版物都沿用这套体系。如今，美国的葡萄酒年销售额超过 200 亿美元，而那些数以百万计的葡萄酒迷，在没有看酒的评分之前，是绝不会把钱放到柜台上的。因此，比如那次《葡萄酒观察家》杂志给 2004 年份瓦伦丁比安奇酒庄的阿根廷赤霞珠打了 90 分而非 89 分时，这多出来的一分，就带来了瓦伦丁比安奇酒庄销售额上的巨大增长。实际上，如果你去看看你当地的葡萄酒店，那些因吸引力不够而常常沦为促销品和低价货的葡萄酒，得分一般都在 80 多分不到 90 分这一档。但如果我们过一个小时给那个 90 分的 2004 年份瓦伦丁比安奇酒庄的阿根廷赤霞珠重新打一次分，那么这个新分数是 89 分的可能性有多大呢？

威廉·詹姆斯在 1890 年出版的《心理学原理》中认为，品酒专家能将品酒能力发挥到如此的程度，他们甚至可以判断出某杯马德拉白葡萄酒是来自上半瓶还是下半瓶。我在这么多年来参加过的品酒会上也注意到，如果我左边那个长着大胡子的家伙嘟囔了一声「大酒香味」（这酒闻起来挺香），那么其他人肯定也会发出赞同的共鸣。但如果以各自独立品酒、不能相互讨论的方式各打各的分数，那么我们经常能看到，那个大胡子写的是「大酒香味」，另一个刚剃过头的家伙则草草写了个「没有酒香」，而这个烫发的金发美女写的却是「有意思的酒香，似乎有一丝欧芹和刚晒过的皮革的味道」。

根据理论观点，我们有许多理由质疑葡萄酒评分的统计显著性。首先，我们对味道的感知，依赖于味觉和嗅觉刺激复杂的相互作用。严格来说，味觉来自舌头上的 5 种感知细胞：咸、甜、酸、苦和鲜味感知细胞。最后一种细胞会对某些氨基酸成分（例如普遍存在于酱油中的某些成分）产发生反应。但如果这就是味觉的全部，我们就可以用食盐、蔗糖、醋、奎宁和味精，模仿你最爱的牛排、烤土豆、苹果派大餐，或美味的意大利肉酱面等任何东西。幸运的是，仅仅这些味道还不足以让我们胃口大开，还需要嗅觉发挥作用。饮用两杯浓度完全相同的糖水时，如果我们在其中一杯加入一些（无糖的）草莓香精，那么你会觉得这杯水更甜。这种情况可以通过嗅觉来解释。我们所感知的葡萄酒味道，是 600-800 种可挥发性的有机化合物构成的大杂烩，它们同时在舌头上和鼻子中产生混合效果。有研究证明，即使是受过品味训练的专业人员，也很少能够分辨出一种混合物中 3 到 4 种以上的组成成分。在这种情况下，要分辨产生葡萄酒味的大杂烩，可真是一个问题了。

2『人舌头上 5 类基本感知细胞，做一张信息数据卡片。（2021-02-28）』

我们对于味道的预期，同样能影响我们对味道的感知。1963 年，3 名研究者偷偷地在白葡萄酒中加入了一点儿红色食用色素，把酒弄成了玫瑰红色。然后，他们请一群专家给染了色的酒的甜度打分，再把这个分数和未加色素的酒进行比较。这些专家都觉得红葡萄酒应该比白葡萄酒更甜，因此他们打出的分数也都表明假的玫瑰红葡萄酒比白葡萄酒更甜。另一组研究者把两杯葡萄酒样品提供给一群品酒学专业的学生。这两杯样品是相同的白葡萄酒，但其中一杯加入了无味的葡萄花青素，看上去像红葡萄酒。同样，由于对不同品种的酒的味道抱有预期，这些学生觉得红葡萄酒和白葡萄酒在味道上存在差别。在 2008 年的一项研究中，研究者让受试者给 5 瓶酒打分。他们给标价 90 美元的酒打出的分数，要高于另一瓶标价 10 美元的酒，但实际上，狡猾的研究者在两个瓶子里灌的是完全相同的酒。研究者还在实验中对受试者的大脑活动进行了磁共振成像。结果表明，当受试者品尝他们相信是更贵的酒时，那个普遍被认为对快感产生响应的大脑区域，确实处于更加兴奋的状态。但如果我们现在想对这些品酒行家指指点点的话，就让我们再来看看下面这个例子吧。一位研究者首先询问了 30 个人各自对可乐的偏好，看他们是更喜欢可口可乐还是百事可乐。然后，这些人品尝了并排放着的两个牌子的可乐，看看他们对这些可乐口味的评价，是不是与他们事先的偏好吻合。品鉴结束后，有 21 个人都说试喝更加确认了他们的选择。可实际上，这个鬼祟的研究者把瓶子里的内容对调了：百事可乐瓶子里装的是可口可乐，而百事可乐则倒进了可口可乐的瓶子里。在进行评价或测量时，我们的大脑并非单纯依赖于直接的感知输入，而是额外结合了其他的信息源 —— 比如我们抱有的期望。

期望偏误的反面则是我们因为缺乏相关背景知识，对结果无法做出准确预估，品酒师也经常被这种期望偏误的另一面欺骗。你不大可能把一大块放在你的鼻子下面的山葵跟一瓣大蒜搞混；当然，你基本上也不会把大蒜的味道和你运动鞋里的味道搞混（仅仅是打个比方）。但如果现在你面对的是一杯清澈的液体，那么试图分辨它的气味，根本就是徒劳的。在缺乏其他相关因素时，你把气味搞混的可能性相当大。至少当两名研究者让专家们判断一系列随机的 16 种气味时，情况就是如此：大概每 4 种气味中，这些专家就会认错 1 种。

这些情况足以令人对品酒这件事起疑，因此受此推动，科学家设计了多种方式，直接测量品酒专家对味道的分辨力。一种方法是酒味三角。它并不是什么真正的三角形，而是个比拟：每位专家都得到 3 杯酒，其中 2 杯完全相同，他们的任务是找出那杯不同的。1990 年的一项研究表明，专家能正确识别这个不同样品的比例仅为 2/3，也就是说，差不多每 3 次中就会有 1 次，这些品酒界的宗师级人物，会在品酒挑战中区别不出比如有着「野草莓、甜黑莓和覆盆子的馥郁醇香」的黑皮诺，和有着「干李子、黄樱桃和丝滑黑醋栗的独特气味」的黑皮诺。同样，在这项研究中有若干专家被要求在包括酒精含量、是否含鞣酸、甜度和果味度等 12 个评分指标上，给若干种酒打分。专家们的意见在 9 个指标上存在明显差异。最后，在一项根据其他专家的描述指出相应的酒的测试中，受试者的正确率仅仅为 70%。

酒评家们其实很清楚上面这些困难。「从许多层面而言……（这个给酒评分的体系）是毫无意义的。」《葡萄酒与烈酒杂志》的编辑就这样说过。而按《葡萄酒爱好者》杂志某前编辑的话来说：「你对它了解越深，越能了解这个东西是多么被人误导又在误导别人。」但评分体系仍然蓬勃发展。为什么会这样呢？酒评家们发现，当他们用星级制或简单的好、坏、糟透了这样的词语描述酒的质量时，消费者对他们的意见并不十分信服；但如果他们给出的是数值的评定结果，那么购买者表现出的态度简直可以用崇拜来形容。数字评级尽管十分可疑，却能让购买者相信，他们从不同种类、不同酿酒商和不同年份的葡萄酒的大海中捞到了那枚金针（或银针，这就要看他们的预算了）。

如果一种葡萄酒或一篇文章的质量确实可以用一个数字来衡量，那么测量理论必须解决两个关键问题：如何根据一系列不同的测量值求出这个最后的数？给定一组有限的测量值时，如何评估这个所得的数就是正确答案的概率？我们现在就来看看这两个问题，无论问题中的测量值是通过主观还是客观方式获得的，对这两个问题的回答，都是测量理论希望达到的目标。

要理解测量，关键在于理解随机误差造成的数据变化的性质。我们可以把几种酒提供给 15 个酒评家，或者在不同时间重复提供给某位酒评家，或者把这两种方式结合使用，我们接着可以把每种酒所得的多个得分求平均值或取均值，这样就可以简单明了地得到酒评家对这些酒的总看法。但重要的不仅仅是均值：某种酒在所有 15 次酒评中都得到 90 分，这传达的是一种信息；而如果它的 15 个分数是 80、81、82、87、89、89、90、90、90、91、91、94、97、99 和 100，这传达的又是另一种信息。这两组数据的均值相同，但数据偏离均值的程度不同。数据点的分布方式非常重要，因此数学家创造了一个数值量度描述数据中的波动。这个数值被称为样本标准差。数学家有时还会使用这个值的平方，即样本方差。

样本标准差描述了一组数据与其均值的接近程度，或者实际上，描述了数据不确定性程度的高低。样本标准差较小时，数据都落在均值附近。例如那组所有酒评分都是 90 分的数据，其样本标准差为 0，而这个样本标准差就告诉我们，所有数据都与均值相同。但当样本标准差较大时，数据就没有密集地分布在均值附近。那个取值在 80 分到 100 分的酒评分数集，其样本标准差为 6。利用这个样本标准差，我们可以通过一个经验性规则，判断有超过半数的评分，它们与均值的差落在 6 分之内。在这种情况下，你真正能说的，是这种葡萄酒的分数可能在 84 分到 96 分之间。

18 世纪和 19 世纪的科学家在试图解读测量数据的真实意义时，也面临着持怀疑态度的酒评家所面临的相同问题。如果现在有一群研究人员对同一个量进行了一系列观测，那么他们得到的测量结果几乎总是不同的。一个天文学家可能碰到了不适合进行观测的气象条件；另一个人的望远镜有可能被微风给吹动了；而第三个人说不定刚与威廉·詹姆斯品完马德拉葡萄酒之后才到家。1838 年，数学家与天文学家贝塞尔就总结出每一次望远镜观测过程中可能会出现的 11 类随机误差。即使是同一名天文学家进行重复测量，诸如视力不佳或温度对测量仪器的影响之类的变数，也会导致测量结果发生改变。因此，天文学家必须知道，在给定了一系列不完全相同的测量结果时，如何才能确定天体的真实位置。尽管酒评家和科学家面对的问题是一样的，我们却不能仅凭这一点就认为解决方法也是相同的。那么我们能不能识别随机误差的一般特征？还是说随机误差的特征取决于环境？

雅各布·伯努利的侄子丹尼尔是最早认识到不同类型的测量方法具有共同特征的人之一。1777 年，他将天文观测中的随机误差，与弓箭手射箭时的偏差进行了类比。他推断，在两种情况下，目标（被测量的真实值或箭靶靶心）应该落在中心附近的某个位置，而观测结果应该围绕着它，而且，离目标较近的观测结果应该比远离目标的更多。他用来描述这个分布的定律并不正确，但重要的是他洞察了如下事实，即描述弓箭手误差的分布，也能用来描述天文观测误差。

测量理论的基本原理，就是误差分布遵循某种普遍规律，这个规律有时被称为误差定律。神奇的是，由它还可以得出如下推论，即当数据满足某些十分常见的条件时，通过单一的数学分析，就能根据测量值确定任意类型的真值。根据这一普遍规律，由天文学家的观测数据确定某天体真实位置的问题，跟仅知道箭支落点的位置确定靶心位置的问题，或是根据一系列评酒分数确定葡萄酒「品质」的问题，就是等价的。数学统计之所以是一个连贯体系的学科，并非在于它仅仅是一堆技巧，而在于，无论我们现在的测量对象是圣诞节凌晨 4 点时木星的位置，还是某条生产线生产的提子面包的重量，重复测量所得的结果中，误差的分布都是一样的。

这并不是说只有随机误差会影响测量结果。如果一群酒评家中的一半只爱红葡萄酒，而另一半对白葡萄酒情有独钟，但是除此之外他们的观点完全一致，那么一种特定葡萄酒所得的分数，其分布将不会遵循误差定律，而是会形成两个高峰，其中一个对应着红葡萄酒爱好者所给的分数，另一个则对应着白葡萄酒爱好者所给的分数。即使在一些定律的适用性不那么明显的场合，例如职业足球比赛中获胜方领先的分数 ，或是 IQ（智商）得分，误差定律也适用。多年以前，我曾经掌握了某软件几千名用户的注册数据。这个软件是一位朋友为 8 岁和 9 岁的孩子设计的，但软件的销售情况不如预期。到底是什么人购买了这个软件呢？我根据注册数据制作了一些表格，发现最大的购买人群是 7 岁的孩子。这个期望目标人群和实际目标人群之间的错位当然令人不快，却也并非完全出乎意料。真正令我震惊的是，当我绘制条形图想看看购买者数量随着年龄逐渐偏离 7 岁这个均值时的趋势时，我发现，画出来的条形图看上去十分眼熟 —— 它就是误差定律中的那条曲线。

能够质疑弓箭手和天文学家、化学家和市场销售经理遵从的是同一条误差定律是一回事，能找出定律的具体形式却是另一回事。在天文数据分析这个需求的驱动下，18 世纪晚期，丹尼尔·伯努利和拉普拉斯这样的科学家，提出了一系列假设。事实证明，正确描述误差定律的数学函数即钟形曲线，其实一直就在他们的眼皮子底下。而在几十年前，在一个不同的场合，它就已经在伦敦被发现了。

揭示钟形曲线重要性的有三个人，但被认为贡献最小的人，恰好就是钟形曲线的发现者。亚伯拉罕·棣莫弗是在 1733 年取得这个突破的，当时他 65 岁左右。但这个突破要为人所知，还得等到 5 年后他的《机会的学说》第二版的出版。之前我们把帕斯卡三角形在第 10 行就拦腰斩断了，如果这个三角形继续向下延伸，直到数百行甚至数千行，这时帕斯卡三角形区域的近似值是多少呢？这就是棣莫弗探索的东西。正是这个探索，让他发现了钟形曲线。雅各布·伯努利在证明自己的大数定律时，也必须研究这些数列的某些性质。这些数字可以非常大，例如，在帕斯卡三角形第 200 行中的一个系数，有 59 个数字！在伯努利那个时代，这样巨大的数字的计算显然异常困难。实际上，直到计算机出现之前，这都是一个无比艰巨的任务。我前面提到过，伯努利在证明他的大数定律的过程中使用了近似，而正是这些近似削弱了他的结果的实用性。但他之所以这样做，是因为他需要处理这些巨大的帕斯卡三角形的系数。不过，棣莫弗利用他发现的曲线，进行了更好的近似计算，从而极大地改进了伯努利的估计。

如果按照我在那个软件用户注册数据上的做法，把帕斯卡三角形中某行的数字，以条形图绘制出来，那么棣莫弗推导出的近似就是显而易见的。例如，帕斯卡三角形第 3 行中的 3 个数字分别是 1、2、1。绘制成条形图的话，第一个条形高 1 个单位；第二个条形的高度是第一个的一倍；而第三个条形的高度又变为 1 个单位。现在来看第 5 行中的 5 个数字：1、4、6、4、1。图中将有 5 个条形，且同样由最矮的条形开始，在中间上升到顶点，然后又对称地降下来。那些很靠下的行中的系数会形成有非常多条形的条形图，但这些条形的高矮变化方式是一样的。帕斯卡三角形的第 10 行、第 100 行和第 1000 行所对应的条形图见图 7-1。

图 7-1

注：以上图中的条形高度代表帕斯卡三角形（见图 4-1）中第 10、第 100 和第 1000 行中各系数的相对幅值。横轴上的数字代表对应系数的序号。按照惯例，这些序号从 0 而不是 1 开始。（中图和下图的横轴被截断了，省略了那些条形高度可以忽略的系数。）

如果用一条曲线将这些条形的顶部连起来，就会出现一个很有特征的形状，一个近似钟形的形状。如果把曲线再弄平滑些，你就可以写出它所对应的数学表达式。这条平滑的钟形曲线不仅仅是对帕斯卡三角系数的可视化描述，它更提供了一种既精确又易于使用的估计方法，让我们可以估计帕斯卡三角中那些较大行数中的系数。这就是棣莫弗的发现。

钟形曲线如今常被称为正态分布，有时也被称为高斯分布（我们将在后面看到这个名称的由来）。正态分布实际上并不是一条固定的曲线，而是一系列曲线，其具体位置与形状由两个参数来确定。第一个参数确定了曲线峰值出现的位置，图 7-1 中分别为 5、50 和 500。第二个参数则确定了曲线的延展程度。这个延展程度的现代名称 —— 标准差 —— 要到 1894 年才会出现，它也是我们早先提到的样本标准差这个概念的理论对应物。大体而言，在曲线最大高度的 60% 处，它是曲线宽度的一半。如今，正态分布的重要性已远远超出它作为帕斯卡三角形中数字的近似值的用途。实际上，它是我们发现的最常见的数据分布方式之一。

2『才知道正态分布并不是一条固定的曲线，而是一系列曲线。做一张反常识卡片。（2021-02-28）』

钟形曲线能描述数据分布这一点表明，在多次重复观测后，大多数结果将落在均值附近。曲线中的波峰就表明了这一点。不仅如此，当曲线对称地朝两边逐渐降低时，它同时也给出那些大于或小于均值的特定观测结果出现的频次逐渐减小的规律。在遵循正态分布的数据中，大约 68%（差不多 2/3）的观测值将落在均值 1 个标准差的范围内，大约 95% 落在 2 个标准差的范围内，而 3 个标准差的范围则囊括了 99.7% 的观测值。

我们用图 7-2 进行说明。我们扔了 10 次硬币，并让 300 名学生猜测每次的结果，图中的正方形标出的就是猜测的结果。水平轴上的数字代表在 10 次扔硬币中猜对的次数，分别是从 0 次到 10 次。垂直方向绘制的则是猜对的学生的数量。曲线的形状像一口钟，中心落在了猜中 5 次的位置上，这个位置的曲线高度对应了 75 名学生。在其左侧，差不多是 3 次到 4 次中间的地方，曲线高度约为 51 名学生，即大致为最高值的 2/3；而在右侧，这个高度所对应的点落在了 6 次与 7 次中间。对于猜测扔硬币的结果而言，具有这种标准差大小的钟形曲线，是典型的随机过程。

图 7-2 测量与误差定律

在同一张图上，我们还用圆圈标出了另一组数据。这是 300 名共同基金经理的业绩数据。此时，水平轴表示的不是 10 次扔硬币中猜对的次数，而是 10 年中某经理的表现高于这个群体平均水平的年份的数量。注意两条曲线之间的相似性！在第 9 章中，我们将回到这个问题上来。

要更直观地认识正态分布与随机误差的关系，一个好例子是民意调查或抽样的过程。读者也许还能记起第 5 章那个巴塞尔市长支持度的调查。有确定的一部分选民支持市长，也有确定的另一部分不支持他。为了简单起见，假设这两种人各占 50%。我们已经知道，被调查人群可能无法正好反映这种对半分的情况。实际上，如果询问 N 名选民，他们之中支持市长的人数为某个给定数量的可能性，就正比于帕斯卡三角第 N 行中对应的系数。因此棣莫弗的成果告诉我们，如果调查者对大量选民进行调查，那么各种调查结果的出现概率，就可以用正态分布来描述。换言之，在民意调查中观察到的 95% 的支持率会落在 50% 这个真实支持率 2 个标准差的范围内。民意调查方用误差幅度来表示这一不确定性。如果调查者对媒体说某次调查的误差幅度是正负 5%，他们的意思是指，如果将同样的调查重复多次，那么在 20 次中有 19 次（95%），所得结果会在正确结果的 5% 的误差范围内（尽管调查者很少明说，但实际上这句话也意味着，每 20 次调查中大概就有 1 次的结果错得离谱）。根据经验，样本容量为 100 时的误差幅度，对大多数调查目的而言都太大了；而样本容量为 1000 时的误差幅度大约是 3%，在大多数情况下就已足够了。

不管是什么类型的调查或民意测验，我们在对调查结果进行评价时，一个关键在于我们必须认识到，如果这个调查或测验重新来过的话，调查结果将会毫无意外地发生变化。举个例子，如果已登记选民对于总统的真实认可率为 40%，那么对选民进行的 6 次独立调查，更可能给出诸如 37%、39%、39%、40%、42% 和 42% 这样的结果，而不会是所有这 6 次调查得到的认可率都是 40%（前面这 6 个数字，实际上就是 2006 年 9 月的头两周，对总统任职情况认可率进行的 6 次独立调查的结果）。这也就是我们应该忽略所有那些没有超出误差幅度的差异的原因，这同样是一条经验性的规则。但是，《纽约时报》虽然没有登出「下午两点的职位与薪水温和增长」这样的头条，但在报道政治方面的民意调查时，类似的头条却随处可见。例如在 2004 年美国共和党全国代表大会之后，CNN（美国有线电视新闻网）的头条是「布什的支持度明显温和反弹」。CNN 的专家们接着解释道：「布什在大会上的支持率看来反弹了约 2 个百分点…… 潜在选民中表示要选他为总统的比例，从大会前的 50% 立刻上升到之后的 52%。」只是到了后面，报道者才指出，调查的误差幅度为正负 3.5 个百分点，实际上这等于宣告了上述新闻根本毫无意义。很明显，「明显」这个词在 CNN 评论节目中的真正含义是「明显没有」。

对于许多调查而言，大于 5% 的误差幅度都属于不可接受的范围。但在日常生活中，我们进行判断所依据的数据量往往远少于必需。人们不可能打 100 年的职业棒球，或投资 100 幢公寓楼，或办 100 家巧克力曲奇饼公司。因此，如果要评价别人在这些方面取得的成就，我们的判断依据就仅仅是少数几个数据点。橄榄球队是否应该大手笔甩出 5000 万美元，引诱那个上赛季刚刚取得了创纪录成绩的家伙呢？那个想要你掏钱投资的股票经纪人，她之前的成功能够复现的可能性有多高？而那个富有的发明者在「海猴子」这个发明上取得的成功，是不是意味着他那些「隐身金鱼」和「即活蛙」的新点子的成功率也挺大呢？（据考，他的这些新点子并不成功。）一次成功或失败，只是我们观察到的一个数据点，或者钟形曲线上的一个采样，它仅仅代表一直存在着的许多可能性中的一个。对于这一单独的观测值，我们无法知道它只是代表着均值，还是某「异常值」；也无法知道这个观测所对应的，到底是我们有把握赌一把的事情，还是某个不大可能再次出现的罕见情况。但是我们至少应该了解，一个采样点就是一个采样点，相较于简单地视其为真实值，我们更应该在产生该采样点的分布标准差或离散程度这个语境中看待它。一瓶葡萄酒的得分是 91 分，但如果没有足够的信息估计这种酒被多次评分或由多人评分时得分的波动，91 这个数字就毫无意义。下面的例子也许能帮助我们理解这一点：几年前，《企鹅版澳大利亚优质葡萄酒指南》（ The Penguin Good Australian Wine Guide ）以及《葡萄酒》（ Wine ）杂志的《澳大利亚葡萄酒年鉴》（ Australian Wine Annual ），都对 1999 年份米其顿黑森林公园雷司令酒进行了点评。「企鹅版指南」按五星制评分给了该酒五星，并称其为「企鹅版」年度最佳葡萄酒；而《葡萄酒》杂志则将其列在所有参评酒的末位，并确信这是十年来最差的酒。正态分布不仅可以帮助我们理解这样的矛盾，而且使无数的统计应用成为现实，而这些统计应用广泛出现在科学和商业领域，比如制药公司评判临床试验结果的显著性，或者制造商评判部件的一个抽样检测是否准确反映了不合格产品的比例，或者市场经销商是否应该接受一项研究调查的结果并据此对其行动做出决策，等等。

认识到正态分布描述了测量误差的分布，则是棣莫弗发现钟形曲线之后数十年的事了。发现这一点的人是德国数学家卡尔·弗里德里希·高斯，人们常常将他的名字与钟形曲线联系在一起。高斯在解决行星运动问题时开始认识到正态分布的这个性质，至少是认识到天文测量中的误差可以通过正态分布来描述。但高斯的「证明」是有问题的，他自己后来也承认了这一点。而且，他也未能发现正态分布的许多更加深刻的推论。因此，他将这条定律毫不起眼地插在了《天体运动论》结尾部分的一个小节中。该定律本来很可能就此被埋没，并最终成为那些数量不断增多的被摈弃的误差定律之一。

将正态分布从这个阴暗的角落中拉出来的人是拉普拉斯。他在 1810 年读到了高斯的著作，而此前不久，他刚刚在法兰西科学院宣读了一份备忘，证明了一条被称为中心极限的定理。该定理指出，大数量独立随机因素总和的值为任意给定值的概率，遵循正态分布。比如，你准备烤 100 条 1 千克重的面包，每条面包你都严格按照配方来做，但由于随机性，你有时可能会多加一点儿或少加一点儿面粉或牛奶，或者在烘烤时多烤掉一点儿或少烤掉一点儿水分。这些各种各样的因素，最终会让你烤出来的每条面包跟 1 千克的期望重量相比，都多几克或少几克。但中心极限定理告诉我们，这些面包的重量将遵循正态分布。读了高斯的著作后，拉普拉斯立刻意识到，他可以用正态分布改进自己的工作，并可以给出一个比高斯更好的论证，来说明正态分布的确就是人们孜孜以求的误差定律。拉普拉斯忙不迭地给备忘加了个小尾巴，而中心极限定理和大数定律现在也成为随机性理论中最负盛名的两个结论。

2『这里有关「中心极限定理」的信息做一张主题卡片，而且这里的信息还不够，需要补充。（2021-02-28）』

要理解中心极限定理是怎样说明正态分布就是正确的误差定律的，让我们再来看看丹尼尔·伯努利的弓箭手例子。我就曾扮演过这个弓箭手的角色。那是在一个晚上，在令人欣欣然的葡萄美酒与有大人们参与的幕间节目之后，小儿子尼古拉递给我一张弓和一支箭，问我敢不敢试试射落他头上放着的苹果。箭头是用柔软的泡沫塑料做成的，但如果先来分析一下我可能出现的失误和概率，应该也不可谓不合理。出于大家显然都能理解的原因，我主要考虑垂直方向的误差。我对这个误差建立了如下的简单模型：每个随机因素，比如瞄准的偏差、气流的影响等等，都会使我射出的箭在垂直方向上偏离目标，或者高些或者低些，两者出现的可能性相等。如果我走运的话，这些影响因素中大概有一半将使箭的落点偏高，而另一半使之偏低，总的来说，结果就是箭能准确命中目标。但如果我不走运（或者更准确点儿说，我儿子不走运）的话，所有这些误差都会使箭朝同一个方向偏离，从而使箭或高或低地远离目标。现在的问题是，当所有引起偏差的因素被综合起来后，误差正好相互抵消，或者正好都朝一个方向累加并形成最大误差，或者最终的总和落在前两者中间的任意一点时，各自的可能性有多大？这些影响因素构成了一个伯努利过程，因此上面的问题就等同于问，在扔多次硬币时得到特定次数的正面朝上的可能性有多大。帕斯卡三角形给出了问题的答案，或者当试验次数很多时，正态分布给出了答案。这也是中心极限定理所说的内容。（至于那次射箭挑战，最后我既没有射中苹果，也没有射中我的儿子，却射翻了一杯上好的红葡萄酒。）

19 世纪 30 年代，大多数科学家已经开始相信，每个测量值都是一个复合的产物，受到众多误差源的影响，并因此遵循误差定律。从此以后，利用误差定律和中心极限定理，我们便获得了对数据及其与物理现实之间的关系的崭新且更为深刻的理解。在紧接而来的下一个世纪中，研究兴趣放在人类社会本身的那些学者也掌握了这个观念，并惊奇地发现，个人的性格与行为的变化，也常常表现出跟测量误差一样的模式。因此，他们试图将误差定律的应用，从物理科学拓展到与人类本身的事务有关的一门新科学之中。

## 0801. The Order in Chaos

IN THE MID-1960S, some ninety years old and in great need of money to live on, a Frenchwoman named Jeanne Calment made a deal with a forty-seven-year-old lawyer: she sold him her apartment for the price of a low monthly subsistence payment with the agreement that the payments would stop upon her death, at which point she would be carried out and he could move in.1 The lawyer must have known that Ms. Calment had already exceeded the French life expectancy by more than ten years. He may not have been aware of Bayes's theory, however, nor known that the relevant issue was not whether she should be expected to die in minus ten years but that her life expectancy, given that she had already made it to ninety, was about six more years.2 Still, he had to feel comfortable believing that any woman who as a teenager had met Vincent van Gogh in her father's shop would soon be joining van Gogh in the hereafter. (For the record, she found the artist「dirty, badly dressed, and disagreeable.」)

Ten years later the attorney had presumably found an alternative dwelling, for Jeanne Calment celebrated her 100th birthday in good health. And though her life expectancy was then about two years, she reached her 110th birthday still on the lawyer's dime. By that point the attorney had turned sixty-seven. But it was another decade before the attorney's long wait came to an end, and it wasn't the end he expected. In 1995 the attorney himself died while Jeanne Calment lived on. Her day of reckoning finally came on August 4, 1997, at the age of 122. Her age at death exceeded the lawyer's age at his death by forty-five years.

Individual life spans — and lives — are unpredictable, but when data are collected from groups and analyzed en masse, regular patterns emerge. Suppose you have driven accident-free for twenty years. Then one fateful afternoon while you're on vacation in Quebec with your spouse and your in-laws, your mother-in-law yells,「Look out for that moose!」and you swerve into a warning sign that says essentially the same thing. To you the incident would feel like an odd and unique event. But as the need for the sign indicates, in an ensemble of thousands of drivers a certain percentage of drivers can be counted on to encounter a moose. In fact, a statistical ensemble of people acting randomly often displays behavior as consistent and predictable as a group of people pursuing conscious goals. Or as the philosopher Immanuel Kant wrote in 1784,「Each, according to his own inclination follows his own purpose, often in opposition to others; yet each individual and people, as if following some guiding thread, go toward a natural but to each of them unknown goal; all work toward furthering it, even if they would set little store by it if they did know it.」3

According to the Federal Highway Administration, for example, there are about 200 million drivers in the United States.4 And according to the National Highway Traffic Safety Administration, in one recent year those drivers drove a total of about 2.86 trillion miles.5 That's about 14,300 miles per driver. Now suppose everyone in the country had decided it would be fun to hit that same total again the following year. Let's compare two methods that could have been used to achieve that goal. In method 1 the government institutes a rationing system employing one of the National Science Foundation's supercomputing centers to assign personal mileage targets that meet each of the 200 million motorists' needs while maintaining the previous annual average of 14,300. In method 2 we tell drivers not to stress out over it and to drive as much or as little as they please with no regard to how far they drove the prior year. If Uncle Billy Bob, who used to walk to work at the liquor store, decides instead to log 100,000 miles as a shotgun wholesaler in West Texas, that's fine. And if Cousin Jane in Manhattan, who logged most of her mileage circling the block on street-cleaning days in search of a parking space, gets married and moves to New Jersey, we won't worry about that either. Which method would come closer to the target of 14,300 miles per driver? Method 1 is impossible to test, though our limited experience with gasoline rationing indicates that it probably wouldn't work very well. Method 2, on the other hand, was actually instituted — that is, the following year, drivers drove as much or as little as they pleased without attempting to hit any quota. How did they do? According to the National Highway Traffic Safety Administration, that year American drivers drove 2.88 trillion miles, or 14,400 miles per person, only 100 miles above target. What's more, those 200 million drivers also suffered, within less than 200, the same number of fatalities in both years (42,815 versus 42,643).

We associate randomness with disorder. Yet although the lives of 200 million drivers vary unforeseeably, in the aggregate their behavior could hardly have proved more orderly. Analogous regularities can be found if we examine how people vote, buy stocks, marry, are told to get lost, misaddress letters, or sit in traffic on their way to a meeting they didn't want to go to in the first place — or if we measure the length of their legs, the size of their feet, the width of their buttocks, or the breadth of their beer bellies. As nineteenth-century scientists dug into newly available social data, wherever they looked, the chaos of life seemed to produce quantifiable and predictable patterns. But it was not just the regularities that astonished them. It was also the nature of the variation. Social data, they discovered, often follow the normal distribution.

That the variation in human characteristics and behavior is distributed like the error in an archer's aim led some nineteenth-century scientists to study the targets toward which the arrows of human existence are aimed. More important, they sought to understand the social and physical causes that sometimes move the target. And so the field of mathematical statistics, developed to aid scientists in data analysis, flourished in a far different realm: the study of the nature of society.

STATISTICIANS have been analyzing life's data at least since the eleventh century, when William the Conqueror commissioned what was, in effect, the first national census. William began his rule in 1035, at age seven, when he succeeded his father as duke of Normandy. As his moniker implies, Duke William II liked to conquer, and in 1066 he invaded England. By Christmas Day he was able to give himself the present of being crowned king. His swift victory left him with a little problem: whom exactly had he conquered, and more important, how much could he tax his new subjects? To answer those questions, he sent inspectors into every part of England to note the size, ownership, and resources of each parcel of land.6 To make sure they got it right, he sent a second set of inspectors to duplicate the effort of the first set. Since taxation was based not on population but on land and its usage, the inspectors made a valiant effort to count every ox, cow, and pig but didn't gather much data about the people who shoveled their droppings. Even if population data had been relevant, in medieval times a statistical survey regarding the most vital statistics about humans — their life spans and diseases — would have been considered inconsistent with the traditional Christian concept of death. According to that doctrine, it was wrong to make death the object of speculation and almost sacrilegious to look for laws governing it. For whether a person died from a lung infection, a stomachache, or a rock whose impact exceeded the compressive strength of his skull, the true cause of his or her death was considered to be simply God's will. Over the centuries that fatalistic attitude gradually gave way, yielding to an opposing view, according to which, by studying the regularities of nature and society, we are not challenging God's authority but rather learning about his ways.

A big step in that transformation of views came in the sixteenth century, when the lord mayor of London ordered the compilation of weekly「bills of mortality」to account for the christenings and burials recorded by parish clerks. For decades the bills were compiled sporadically, but in 1603, one of the worst years of the plague, the city instituted a weekly tally. Theorists on the Continent turned up their noses at the data-laden mortality bills as peculiarly English and of little use. But to one peculiar Englishman, a shopkeeper named John Graunt, the tallies told a gripping tale.7

Graunt and his friend William Petty have been called the founders of statistics, a field sometimes considered lowbrow by those in pure mathematics owing to its focus on mundane practical issues, and in that sense Graunt in particular makes a fitting founder. For unlike some of the amateurs who developed probability — Cardano the doctor, Fermat the jurist, or Bayes the clergyman — Graunt was a seller of common notions: buttons, thread, needles, and other small items used in a household. But Graunt wasn't just a button salesman; he was a wealthy button salesman, and his wealth afforded him the leisure to pursue interests having nothing to do with implements for holding garments together. It also enabled him to befriend some of the greatest intellectuals of his day, including Petty.

One inference Graunt gleaned from the mortality bills concerned the number of people who starved to death. In 1665 that number was reported to be 45, only about double the number who died from execution. In contrast, 4,808 were reported to have died from consumption, 1,929 from「spotted fever and purples,」2,614 from「teeth and worms,」and 68,596 from the plague. Why, when London was「teeming with beggars,」did so few starve? Graunt concluded that the populace must be feeding the hungry. And so he proposed instead that the state provide the food, thereby costing society nothing while ridding seventeenth-century London streets of their equivalent of panhandlers and squeegee men. Graunt also weighed in on the two leading theories of how the plague is spread. One theory held that the illness was transmitted by foul air; the other, that it was transmitted from person to person. Graunt looked at the week-to-week records of deaths and concluded that the fluctuations in the data were too great to be random, as he expected they would be if the person-to-person theory were correct. On the other hand, since weather varies erratically week by week, he considered the fluctuating data to be consistent with the foul-air theory. As it turned out, London was not ready for soup kitchens, and Londoners would have fared better if they had avoided ugly rats rather than foul air, but Graunt's great discoveries lay not in his conclusions. They lay instead in his realization that statistics can provide insights into the system from which the statistics are derived.

Petty's work is sometimes considered a harbinger of classical economics.8 Believing that the strength of the state depends on, and is reflected by, the number and character of its subjects, Petty employed statistical reasoning to analyze national issues. Typically his analyses were made from the point of view of the sovereign and treated members of society as objects to be manipulated at will. Regarding the plague, he pointed out that money should be spent on prevention because, in saving lives, the realm would preserve part of the considerable investment society made in raising men and women to maturity and therefore would reap a higher return than it would on the most lucrative of alternative investments. Regarding the Irish, Petty was not as charitable. He concluded, for example, that the economic value of an English life was greater than that of an Irish one, so the wealth of the kingdom would be increased if all Irishmen except a few cowherds were forcibly relocated to England. As it happened, Petty owed his own wealth to those same Irish: as a doctor with the invading British army in the 1650s, he had been given the task of assessing the spoils and assessed that he could get away with grabbing a good share for himself, which he did.9

If, as Petty believed, the size and growth of a population reflect the quality of its government, then the lack of a good method for measuring the size of a population made the assessment of its government difficult. Graunt's most famous calculations addressed that issue — in particular the population of London. From the bills of mortality, Graunt knew the number of births. Since he had a rough idea of the fertility rate, he could infer how many women were of childbearing age. That datum allowed him to guess the total number of families and, using his own observations of the mean size of a London family, thereby estimate the city's population. He came up with 384,000 — previously it was believed to be 2 million. Graunt also raised eyebrows by showing that much of the growth of the city was due to immigration from outlying areas, not to the slower method of procreation, and that despite the horrors of the plague, the decrease in population due to even the worst epidemic was always made up within two years. In addition, Graunt is generally credited with publishing the first「life table,」a systematic arrangement of life-expectancy data that today is widely employed by organizations — from life insurance companies to the World Health Organization — that are interested in knowing how long people live. A life table displays how many people, in a group of 100, can be expected to survive to any given age. To Graunt's data (the column in the table below labeled「London, 1662」), I've added columns exhibiting the same data for a few countries today.10

Graunt's life table extended

In 1662, Graunt published his analyses in Natural and Political Observations…upon the Bills of Mortality. The book met with acclaim. A year later Graunt was elected to the Royal Society. Then, in 1666, the Great Fire of London, which burned down a large part of the city, destroyed his business. To add insult to injury, he was accused of helping to cause the destruction by giving instructions to halt the water supply just before the fire started. In truth he had no affiliation with the water company until after the fire. Still, after that episode, Graunt's name disappeared from the books of the Royal Society. Graunt died of jaundice a few years later.

Largely because of Graunt's work, in 1667 the French fell in line with the British and revised their legal code to enable surveys like the bills of mortality. Other European countries followed suit. By the nineteenth century, statisticians all over Europe were up to their elbows in government records such as census data — 「an avalanche of numbers.」11 Graunt's legacy was to demonstrate that inferences about a population as a whole could be made by carefully examining a limited sample of data. But though Graunt and others made valiant efforts to learn from the data through the application of simple logic, most of the data's secrets awaited the development of the tools created by Gauss, Laplace, and others in the nineteenth and early twentieth centuries.

THE TERM statistics entered the English language from the German word Statistik through a 1770 translation of the book Bielfield's Elementary Universal Education, which stated that「the science that is called statistics teaches us what is the political arrangement of all the modern states in the known world.」12 By 1828 the subject had evolved such that Noah Webster's American Dictionary defined statistics as「a collection of facts respecting the state of society, the condition of the people in a nation or country, their health, longevity, domestic economy, arts, property and political strength, the state of their country, &c.」13 The field had embraced the methods of Laplace, who had sought to extend his mathematical analysis from planets and stars to issues of everyday life.

The normal distribution describes the manner in which many phenomena vary around a central value that represents their most probable outcome; in his Essai philosophique sur les probabilités, Laplace argued that this new mathematics could be employed to assess legal testimony, predict marriage rates, calculate insurance premiums. But by the final edition of that work, Laplace was in his sixties, and so it fell to a younger man to develop his ideas. That man was Adolphe Quételet, born in Ghent, Flanders, on February 22, 1796.14

Quételet did not enter his studies spurred by a keen interest in the workings of society. His dissertation, which in 1819 earned him the first doctorate in science awarded by the new university in Ghent, was on the theory of conic sections, a topic in geometry. His interest then turned to astronomy, and around 1820 he became active in a movement to found a new observatory in Brussels, where he had taken a position. An ambitious man, Quételet apparently saw the observatory as a step toward establishing a scientific empire. It was an audacious move, not least because he knew relatively little about astronomy and virtually nothing about running an observatory. But he must have been persuasive, because not only did his observatory receive funding, but he personally received a grant to travel to Paris for several months to remedy the deficiencies in his knowledge. It proved a sound investment, for Quételet's Royal Observatory of Belgium is still in existence today.

In Paris, Quételet was affected in his own way by the disorder of life, and it pulled him in a completely different direction. His romance with statistics began when he made the acquaintance of several great French mathematicians, including Laplace and Joseph Fourier, and studied statistics and probability with Fourier. In the end, though he learned how to run an observatory, he fell in love with a different pursuit, the idea of applying the mathematical tools of astronomy to social data.

When Quételet returned to Brussels, he began to collect and analyze demographic data, soon focusing on records of criminal activity that the French government began to publish in 1827. In Sur l'homme et le développement de ses facultés, a two-volume work he published in 1835, Quételet printed a table of annual murders reported in France from 1826 to 1831. The number of murders, he noted, was relatively constant, as was the proportion of murders committed each year with guns, swords, knives, canes, stones, instruments for cutting and stabbing, kicks and punches, strangulation, drowning, and fire.15 Quételet also analyzed mortality according to age, geography, season, and profession, as well as in hospitals and prisons. He studied statistics on drunkenness, insanity, and crime. And he discovered statistical regularities describing suicide by hanging in Paris and the number of marriages between sixty-something women and twenty-something men in Belgium.

Statisticians had conducted such studies before, but Quételet did something more with the data: he went beyond examining the average to scrutinizing the manner in which the data strayed from its average. Wherever he looked, Quételet found the normal distribution: in the propensities to crime, marriage, and suicide and in the height of American Indians and the chest measurements of Scottish soldiers (he came upon a sample of 5,738 chest measurements in an old issue of the Edinburgh Medical and Surgical Journal). In the height of 100,000 young Frenchmen called up for the draft he also found meaning in a deviation from the normal distribution. In that data, when the number of conscripts was plotted against their height, the bell-shaped curve was distorted: too few prospects were just above five feet two and a compensating surplus was just below that height. Quételet argued that the difference — about 2,200 extra「short men」 — was due to fraud or, you might say friendly fudging, as those below five feet two were excused from service.

Decades later the great French mathematician Jules-Henri Poincaré employed Quételet's method to nab a baker who was shortchanging his customers. At first, Poincaré, who made a habit of picking up a loaf of bread each day, noticed after weighing his loaves that they averaged about 950 grams instead of the 1,000 grams advertised. He complained to the authorities and afterward received bigger loaves. Still he had a hunch that something about his bread wasn't kosher. And so with the patience only a famous — or at least tenured — scholar can afford, he carefully weighed his bread every day for the next year. Though his bread now averaged closer to 1,000 grams, if the baker had been honestly handing him random loaves, the number of loaves heavier and lighter than the mean should have — as I mentioned in chapter 7 — diminished following the bell-shaped pattern of the error law. Instead, Poincaré found that there were too few light loaves and a surplus of heavy ones. He concluded that the baker had not ceased baking underweight loaves but instead was seeking to placate him by always giving him the largest loaf he had on hand. The police again visited the cheating baker, who was reportedly appropriately astonished and presumably agreed to change his ways.16

Quételet had stumbled on a useful discovery: the patterns of randomness are so reliable that in certain social data their violation can be taken as evidence of wrongdoing. Today such analyses are applied to reams of data too large to have been analyzed in Quételet's time. In recent years, in fact, such statistical sleuthing has become popular, creating a new field, called forensic economics, perhaps the most famous example of which is the statistical study suggesting that companies were backdating their stock option grants. The idea is simple: companies grant stock options — the right to buy shares later at the price of the stock on the date of the grant — as an incentive for executives to improve their firms' share prices. If the grants are backdated to times when the shares were especially low, the executives' profits will be correspondingly high. A clever idea, but when done in secret it violates securities laws. It also leaves a statistical fingerprint, which has led to the investigation of such practices at about a dozen major companies.17 In a less publicized example, Justin Wolfers, an economist at the Wharton School, found evidence of fraud in the results of about 70,000 college basketball games.18

Wolfers discovered the anomaly by comparing Las Vegas bookmakers' point spreads to the games' actual outcomes. When one team is favored, the bookmakers offer point spreads in order to attract a roughly even number of bets on both competitors. For instance, suppose the basketball team at Caltech is considered better than the team at UCLA (for college basketball fans, yes, this was actually true in the 1950s). Rather than assigning lopsided odds, bookies could instead offer an even bet on the game but pay out on a Caltech bet only if their team beat UCLA by, say, 13 or more points.

Though such point spreads are set by the bookies, they are really fixed by the mass of bettors because the bookies adjust them to balance the demand. (Bookies make their money on fees and seek to have an equal amount of money bet on each side so that they can't lose, whatever the outcome.) To measure how well bettors assess two teams, economists use a number called the forecast error, which is the difference between the favored team's margin of victory and the point spread determined by the marketplace. It may come as no surprise that forecast error, being a type of error, is distributed according to the normal distribution. Wolfers found that its mean is 0, meaning that the point spreads don't tend to either overrate or underrate teams, and its standard deviation is 10.9 points, meaning that about two thirds of the time the point spread is within 10.9 points of the margin of victory. (In a study of professional football games, a similar result was found, with a mean of 0 and a standard deviation of 13.9 points.)19

When Wolfers examined the subset of games that involved heavy favorites, he found something astonishing: there were too few games in which the heavy favorite won by a little more than the point spread and an inexplicable surplus of games in which the favorite won by just less. This was again Quételet's anomaly. Wolfers's conclusion, like Quételet's and Poincaré's, was fraud. His analysis went like this: it is hard for even a top player to ensure that his team will beat a point spread, but if the team is a heavy favorite, a player, without endangering his team's chance of victory, can slack off enough to ensure that the team does not beat the spread. And so if unscrupulous bettors wanted to fix games without asking players to risk losing, the result would be just the distortions Wolfers found. Does Wolfers's work prove that in some percentage of college basketball games, players are taking bribes to shave points? No, but as Wolfers says,「You shouldn't have what's happening on the court reflecting what's happening in Las Vegas.」And it is interesting to note that in a recent poll by the National Collegiate Athletic Association, 1.5 percent of players admitted knowing a teammate「who took money for playing poorly.」20

QUÉTELET DID NOT PURSUE the forensic applications of his ideas. He had bigger plans: to employ the normal distribution in order to illuminate the nature of people and society. If you made 1,000 copies of a statue, he wrote, those copies would vary due to errors of measurement and workmanship, and that variation would be governed by the error law. If the variation in people's physical traits follows the same law, he reasoned, it must be because we, too, are imperfect replicas of a prototype. Quételet called that prototype l'homme moyen, the average man. He felt that a template existed for human behavior too. The manager of a large department store may not know whether the spacey new cashier will pocket that half-ounce bottle of Chanel Allure she was sniffing, but he can count on the prediction that in the retail business, inventory loss runs pretty steadily from year to year at about 1.6 percent and that consistently about 45 percent to 48 percent of it is due to employee theft.21 Crime, Quételet wrote, is「like a budget that is paid with frightening regularity.」22

Quételet recognized that l'homme moyen would be different for different cultures and that it could change with changing social conditions. In fact, it is the study of those changes and their causes that was Quételet's greatest ambition.「Man is born, grows up, and dies according to certain laws,」he wrote, and those laws「have never been studied.」23 Newton became the father of modern physics by recognizing and formulating a set of universal laws. Modeling himself after Newton, Quételet desired to create a new「social physics」describing the laws of human behavior. In Quételet's analogy, just as an object, if undisturbed, continues in its state of motion, so the mass behavior of people, if social conditions remain unchanged, remains constant. And just as Newton described how physical forces deflect an object from its straight path, so Quételet sought laws of human behavior describing how social forces transform the characteristics of society. For example, Quételet thought that vast inequalities of wealth and great fluctuations in prices were responsible for crime and social unrest and that a steady level of crime represented a state of equilibrium, which would change with changes in the underlying causes. A vivid example of such a change in social equilibrium occurred in the months after the attacks of September 11, 2001, when travelers, afraid to take airplanes, suddenly switched to cars. Their fear translated into about 1,000 more highway fatalities in that period than in the same period the year before — hidden casualties of the September 11 attack.24

But to believe that a social physics exists is one thing, and to define one is another. In a true science, Quételet realized, theories could be explored by placing people in a great number of experimental situations and measuring their behavior. Since that is not possible, he concluded that social science is more like astronomy than physics, with insights deduced from passive observation. And so, seeking to uncover the laws of social physics, he studied the temporal and cultural variation in l'homme moyen.

Quételet's ideas were well received, especially in France and Great Britain. One physiologist even collected urine from a railroad-station urinal frequented by people of many nationalities in order to determine the properties of the「average European urine.」25 In Britain, Quételet's most enthusiastic disciple was a wealthy chess player and historian named Henry Thomas Buckle, best known for an ambitious multivolume book called History of Civilization in England. Unfortunately, in 1861, when he was forty, Buckle caught typhus while traveling in Damascus. Offered the services of a local physician, he refused because the man was French, and so he died. Buckle hadn't finished his treatise. But he did complete the initial two volumes, the first of which presented history from a statistical point of view. It was based on the work of Quételet and was an instant success. Read throughout Europe, it was translated into French, German, and Russian. Darwin read it; Alfred Russel Wallace read it; Dostoyevsky read it twice.26

Despite the book's popularity, the verdict of history is that Quételet's mathematics proved more sensible than his social physics. For one thing, not all that happens in society, especially in the financial realm, is governed by the normal distribution. For example, if film revenue were normally distributed, most films would earn near some average amount, and two-thirds of all film revenue would fall within a standard deviation of that number. But in the film business, 20 percent of the movies bring in 80 percent of the revenue. Such hit-driven businesses, though thoroughly unpredictable, follow a far different distribution, one for which the concepts of mean and standard deviation have no meaning because there is no「typical」performance, and megahit outliers, which in an ordinary business might occur only once every few centuries, happen every few years.27

More important than his ignoring other probability distributions, though, is Quételet's failure to make much progress in uncovering the laws and forces he sought. So in the end his direct impact on the social sciences proved modest, yet his legacy is both undeniable and far-reaching. It lies not in the social sciences but in the「hard」sciences, where his approach to understanding the order in large numbers of random events inspired many scholars and spawned revolutionary work that transformed the manner of thinking in both biology and physics.

IT WAS CHARLES DARWIN'S FIRST COUSIN who introduced statistical thinking to biology. A man of leisure, Francis Galton had entered Trinity College, Cambridge, in 1840.28 He first studied medicine but then followed Darwin's advice and changed his field to mathematics. He was twenty-two when his father died and he inherited a substantial sum. Never needing to work for a living, he became an amateur scientist. His obsession was measurement. He measured the size of people's heads, noses, and limbs, the number of times people fidgeted while listening to lectures, and the degree of attractiveness of girls he passed on the street (London girls scored highest; Aberdeen, lowest). He measured the characteristics of people's fingerprints, an endeavor that led to the adoption of fingerprint identification by Scotland Yard in 1901. He even measured the life spans of sovereigns and clergymen, which, being similar to the life spans of people in other professions, led him to conclude that prayer brought no benefit.

In his 1869 book, Hereditary Genius, Galton wrote that the fraction of the population in any given range of heights must be nearly uniform over time and that the normal distribution governs height and every other physical feature: circumference of the head, size of the brain, weight of the gray matter, number of brain fibers, and so on. But Galton didn't stop there. He believed that human character, too, is determined by heredity and, like people's physical features, obeys in some manner the normal distribution. And so, according to Galton, men are not「of equal value, as social units, equally capable of voting, and the rest.」29 Instead, he asserted, about 250 out of every 1 million men inherit exceptional ability in some area and as a result become eminent in their field. (As, in his day, women did not generally work, he did not make a similar analysis of them.) Galton founded a new field of study based on those ideas, calling it eugenics, from the Greek words eu (good) and genos (birth). Over the years, eugenics has meant many different things to many different people. The term and some of his ideas were adopted by the Nazis, but there is no evidence that Galton would have approved of the Germans' murderous schemes. His hope, rather, was to find a way to improve the condition of humankind through selective breeding.

Much of chapter 9 is devoted to understanding the reasons Galton's simple cause-and-effect interpretation of success is so seductive. But we'll see in chapter 10 that because of the myriad of foreseeable and chance obstacles that must be overcome to complete a task of any complexity, the connection between ability and accomplishment is far less direct than anything that can possibly be explained by Galton's ideas. In fact, in recent years psychologists have found that the ability to persist in the face of obstacles is at least as important a factor in success as talent.30 That's why experts often speak of the「ten-year rule,」meaning that it takes at least a decade of hard work, practice, and striving to become highly successful in most endeavors. It might seem daunting to think that effort and chance, as much as innate talent, are what counts. But I find it encouraging because, while our genetic makeup is out of our control, our degree of effort is up to us. And the effects of chance, too, can be controlled to the extent that by committing ourselves to repeated attempts, we can increase our odds of success.

Whatever the pros and cons of eugenics, Galton's studies of inheritance led him to discover two mathematical concepts that are central to modern statistics. One came to him in 1875, after he distributed packets of sweet pea pods to seven friends. Each friend received seeds of uniform size and weight and returned to Galton the seeds of the successive generations. On measuring them, Galton noticed that the median diameter of the offspring of large seeds was less than that of the parents, whereas the median diameter of the offspring of small seeds was greater than that of the parents. Later, employing data he obtained from a laboratory he had set up in London, he noticed the same effect in the height of human parents and children. He dubbed the phenomenon — that in linked measurements, if one measured quantity is far from its mean, the other will be closer to its mean — regression toward the mean.

Galton soon realized that processes that did not exhibit regression toward the mean would eventually go out of control. For example, suppose the sons of tall fathers would on average be as tall as their fathers. Since heights vary, some sons would be taller. Now imagine the next generation, and suppose the sons of the taller sons, grandsons of the original men, were also on average as tall as their fathers. Some of them, too, would have to be taller than their fathers. In this way, as generation followed generation, the tallest humans would be ever taller. Because of regression toward the mean, that does not happen. The same can be said of innate intelligence, artistic talent, or the ability to hit a golf ball. And so very tall parents should not expect their children to be as tall, very brilliant parents should not expect their children to be as brilliant, and the Picassos and Tiger Woodses of this world should not expect their children to match their accomplishments. On the other hand, very short parents can expect taller offspring, and those of us who are not brilliant or can't paint have reasonable hope that our deficiencies will be improved upon in the next generation.

At his laboratory, Galton attracted subjects through advertisements and then subjected them to a series of measurements of height, weight, even the dimensions of certain bones. His goal was to find a method for predicting the measurements of children based on those of their parents. One of Galton's plots showed parents' heights versus the heights of their offspring. If, say, those heights were always equal, the graph would be a neat line rising at 45 degrees. If that relationship held on average but individual data points varied, then the data would show some scatter above and below that line. Galton's graphs thus exhibited visually not just the general relationship between the heights of parent and offspring but also the degree to which the relationship holds. That was Galton's other major contribution to statistics: defining a mathematical index describing the consistency of such relationships. He called it the coefficient of correlation.

The coefficient of correlation is a number between -1 and 1; if it is near ± 1, it indicates that two variables are linearly related; a coefficient of 0 means there is no relation. For example, if data revealed that by eating the latest McDonald's 1,000-calorie meal once a week, people gained 10 pounds a year and by eating it twice a week they gained 20 pounds, and so on, the correlation coefficient would be 1. If for some reason everyone were to instead lose those amounts of weight, the correlation coefficient would be -1. And if the weight gain and loss were all over the map and didn't depend on meal consumption, the coefficient would be 0. Today correlation coefficients are among the most widely employed concepts in statistics. They are used to assess such relationships as those between the number of cigarettes smoked and the incidence of cancer, the distance of stars from Earth and the speed with which they are moving away from our planet, and the scores students achieve on standardized tests and the income of the students' families.

Galton's work was significant not just for its direct importance but because it inspired much of the statistical work done in the decades that followed, in which the field of statistics grew rapidly and matured. One of the most important of these advances was made by Karl Pearson, a disciple of Galton's. Earlier in this chapter, I mentioned many types of data that are distributed according to the normal distribution. But with a finite set of data the fit is never perfect. In the early days of statistics, scientists sometimes determined whether data were normally distributed simply by graphing them and observing the shape of the resulting curve. But how do you quantify the accuracy of the fit? Pearson invented a method, called the chi-square test, by which you can determine whether a set of data actually conforms to the distribution you believe it conforms to. He demonstrated his test in Monte Carlo in July 1892, performing a kind of rigorous repeat of Jagger's work.31 In Pearson's test, as in Jagger's, the numbers that came up on a roulette wheel did not follow the distribution they would have followed if the wheel had produced random results. In another test, Pearson examined how many 5s and 6s came up in 26,306 tosses of twelve dice. He found that the distribution was not one you'd see in a chance experiment with fair dice — that is, in an experiment in which the probability of a 5 or a 6 on one roll were 1 in 3, or 0.3333. But it was consistent if the probability of a 5 or a 6 were 0.3377 — that is, if the dice were skewed. In the case of the roulette wheel the game may have been rigged, but the dice were probably biased owing to variations in manufacturing, which my friend Moshe emphasized are always present.

Today chi-square tests are widely employed. Suppose, for instance, that instead of testing dice, you wish to test three cereal boxes for their consumer appeal. If consumers have no preference, you would expect about 1 in 3 of those polled to vote for each box. As we've seen, the actual results will rarely be distributed so evenly. Employing the chi-square test, you can determine how likely it is that the winning box received more votes due to consumer preference rather than to chance. Similarly, suppose researchers at a pharmaceutical company perform an experiment in which they test two treatments used in preventing acute transplant rejection. They can use a chi-square test to determine whether there is a statistically significant difference between the results. Or suppose that before opening a new outlet, the CFO of a rental car company expects that 25 percent of the company's customers will request subcompact cars, 50 percent will want compacts, and 12.5 percent each will ask for cars in the midsize and「other」categories. When the data begin to come in, a chi-square test can help the CFO quickly decide whether his assumption was correct or the new site is atypical and the company would do well to alter the mix.

Through Galton, Quételet's work infused the biological sciences. But Quételet also helped spur a revolution in the physical sciences: both James Clerk Maxwell and Ludwig Boltzmann, two of the founders of statistical physics, drew inspiration from Quételet's theories. (Like Darwin and Dostoyevsky, they read of them in Buckle's book.) After all, if the chests of 5,738 Scottish soldiers distribute themselves nicely along the curve of the normal distribution and the average yearly mileage of 200 million drivers can vary by as little as 100 miles from year to year, it doesn't take an Einstein to guess that the 10 septillion or so molecules in a liter of gas might exhibit some interesting regularities. But actually it did take an Einstein to finally convince the scientific world of the need for that new approach to physics. Albert Einstein did it in 1905, the same year in which he published his first work on relativity. And though hardly known in popular culture, Einstein's 1905 paper on statistical physics proved equally revolutionary. In the scientific literature, in fact, it would become his most cited work.32

EINSTEIN'S 1905 WORK on statistical physics was aimed at explaining a phenomenon called Brownian motion. The process was named for Robert Brown, botanist, world expert in microscopy, and the person credited with writing the first clear description of the cell nucleus. Brown's goal in life, pursued with relentless energy, was to discover through his observations the source of the life force, a mysterious influence believed in his day to endow something with the property of being alive. In that quest, Brown was doomed to failure, but one day in June 1827, he thought he had succeeded.

Peering through his lens, Brown noted that the granules inside the pollen grains he was observing seemed to be moving.33 Though a source of life, pollen is not itself a living being. Yet as long as Brown stared, the movement never ceased, as if the granules possessed some mysterious energy. This was not intentioned movement; it seemed, in fact, to be completely random. With great excitement, Brown concluded at first that he had bagged his quarry, for what could this energy be but the energy that powers life itself?

In a string of experiments he performed assiduously over the next month, Brown observed the same kind of movement when suspending in water, and sometimes in gin, as wide a variety of organic particles as he could get his hands on: decomposing fibers of veal, spider's web「blackened with London dust,」even his own mucus. Then, in a deathblow to his wishful interpretation of the discovery, Brown also observed the motion when looking at inorganic particles — of asbestos, copper, bismuth, antimony, and manganese. He knew then that the movement he was observing was unrelated to the issue of life. The true cause of Brownian motion would prove to be the same force that compelled the regularities in human behavior that Quételet had noted — not a physical force but an apparent force arising from the patterns of randomness. Unfortunately, Brown did not live to see this explanation of the phenomenon he observed.

The groundwork for the understanding of Brownian motion was laid in the decades that followed Brown's work, by Boltzmann, Maxwell, and others. Inspired by Quételet, they created the new field of statistical physics, employing the mathematical edifice of probability and statistics to explain how the properties of fluids arise from the movement of the (then hypothetical) atoms that make them up. Their ideas did not catch on for several more decades, however. Some scientists had mathematical issues with the theory. Others objected because at the time no one had ever seen an atom and no one believed anyone ever would. But most physicists are practical, and so the most important roadblock to acceptance was that although the theory reproduced some laws that were known, it made few new predictions. And so matters stood until 1905, when long after Maxwell was dead and shortly before a despondent Boltzmann would commit suicide, Einstein employed the nascent theory to explain in great numerical detail the precise mechanism of Brownian motion.34 The necessity of a statistical approach to physics would never again be in doubt, and the idea that matter is made of atoms and molecules would prove to be the basis of most modern technology and one of the most important ideas in the history of physics.

The random motion of molecules in a fluid can be viewed, as we'll note in chapter 10, as a metaphor for our own paths through life, and so it is worthwhile to take a little time to give Einstein's work a closer look. According to the atomic picture, the fundamental motion of water molecules is chaotic. The molecules fly first this way, then that, moving in a straight line only until deflected by an encounter with one of their sisters. As mentioned in the Prologue, this type of path — in which at various points the direction changes randomly — is often called a drunkard's walk, for reasons obvious to anyone who has ever enjoyed a few too many martinis (more sober mathematicians and scientists sometimes call it a random walk). If particles that float in a liquid are, as atomic theory predicts, constantly and randomly bombarded by the molecules of the liquid, one might expect them to jiggle this way and that owing to the collisions. But there are two problems with that picture of Brownian motion: first, the molecules are far too light to budge the visible floating particles; second, molecular collisions occur far more frequently than the observed jiggles. Part of Einstein's genius was to realize that those two problems cancel each other out: though the collisions occur very frequently, because the molecules are so light, those frequent isolated collisions have no visible effect. It is only when pure luck occasionally leads to a lopsided preponderance of hits from some particular direction — the molecular analogue of Roger Maris's record year in baseball — that a noticeable jiggle occurs. When Einstein did the math, he found that despite the chaos on the microscopic level, there was a predictable relationship between factors such as the size, number, and speed of the molecules and the observable frequency and magnitude of the jiggling. Einstein had, for the first time, connected new and measurable consequences to statistical physics. That might sound like a largely technical achievement, but on the contrary, it represented the triumph of a great principle: that much of the order we perceive in nature belies an invisible underlying disorder and hence can be understood only through the rules of randomness. As Einstein wrote,「It is a magnificent feeling to recognize the unity of a complex of phenomena which appear to be things quite apart from the direct visible truth.」35

In Einstein's mathematical analysis the normal distribution again played a central role, reaching a new place of glory in the history of science. The drunkard's walk, too, became established as one of the most fundamental — and soon one of the most studied — processes in nature. For as scientists in all fields began to accept the statistical approach as legitimate, they recognized the thumbprints of the drunkard's walk in virtually all areas of study — in the foraging of mosquitoes through cleared African jungle, in the chemistry of nylon, in the formation of plastics, in the motion of free quantum particles, in the movement of stock prices, even in the evolution of intelligence over eons of time. We'll examine the effects of randomness on our own paths through life in chapter 10. But as we're about to see, though in random variation there are orderly patterns, patterns are not always meaningful. And as important as it is to recognize the meaning when it is there, it is equally important not to extract meaning when it is not there. Avoiding the illusion of meaning in random patterns is a difficult task. It is the subject of the following chapter.

### Notes

1 Holland, What Are the Chances? p. 51.

2 This is only an approximation, based on more recent American statistics. See U.S. Social Security Administration,「Actuarial Publications: Period Life Table.」The most recent table is available at http://www.ssa.gov/OACT/STATS/table4c6.htm.

3 Immanuel Kant, quoted in Theodore Porter, The Rise of Statistical Thinking: 1820–1900 (Princeton, N.J.: Princeton University Press, 1988), p. 51.

4 U.S. Department of Transportation, Federal Highway Administration,「Licensed Drivers, Vehicle Registrations and Resident Population,」http://www.fhwa.dot.gov/policy/ohim/hs03/htm/dlchrt.htm.

5 U.S. Department of Transportation, Research and Innovative Technology Administration, Bureau of Transportation Statistics,「Motor Vehicle Safety Data,」http://www.bts.gov/publications/national_transportation_statistics/2002/html/table_02_17.htm.

6「The Domesday Book,」History Magazine, October/November 2001.

7 For Graunt's story, see Hacking, The Emergence of Probability, pp. 103–9; David, Gods, Games and Gambling, pp. 98–109; and Newman, The World of Mathematics, 3:1416–18.

8 Hacking, The Emergence of Probability, p. 102.

9 Theodore Porter, The Rise of Statistical Thinking, p. 19.

10 For Graunt's original table, see Hacking, The Emergence of Probability, p. 108. For the current data, see World Health Organization,「Life Tables for WHO Member States,」http://www.who.int/whosis/database/life_tables/life_tables.cfm. The figures quoted were taken from abridged tables and rounded.

11 Ian Hacking, The Taming of Chance (Cambridge: Cambridge University Press, 1990), p. vii.

12 H. A. David,「First (?) Occurrence of Common Terms in Statistics and Probability,」in Annotated Readings in the History of Statistics, ed. H. A. David and A.W.F. Edwards (New York: Springer, 2001), appendix B and pp. 219–28.

13 Noah Webster, American Dictionary of the English Language (1828; facsimile of the 1st ed., Chesapeake, Va.: Foundation for American Christian Education, 1967).

14 The material on Quételet is drawn mainly from Stigler, The History of Statistics, pp. 161–220; Stephen Stigler, Statistics on the Table: The History of Statistical Concepts and Methods (Cambridge, Mass.: Harvard University Press, 1999), pp. 51–66; and Theodore Porter, The Rise of Statistical Thinking, pp. 100–9.

15 Louis Menand, The Metaphysical Club (New York: Farrar, Straus & Giroux, 2001), p. 187.

16 Holland, What Are the Chances? pp. 41–42.

17 David Yermack,「Good Timing: CEO Stock Option Awards and Company News Announcements,」Journal of Finance 52, no. 2 (June 1997): 449–76; and Erik Lie,「On the Timing of CEO Stock Option Awards,」Management Science 51, no. 5 (May 2005): 802–12. See also Charles Forelle and James Bandler,「The Perfect Payday — Some CEOs Reap Millions by Landing Stock Options When They Are Most Valuable: Luck — or Something Else?」Wall Street Journal, March 18, 2006.

18 Justin Wolfers,「Point Shaving: Corruption in NCAA Basketball,」American Economic Review 96, no. 2 (May 2006): 279–83.

19 Stern,「On the Probability of Winning a Football Game.」

20 David Leonhardt,「Sad Suspicions about Scores in Basketball,」New York Times, March 8, 2006.

21 Richard C. Hollinger et al., National Retail Security Survey: Final Report (Gainesville: Security Research Project, Department of Sociology and Center for Studies in Criminal Law, University of Florida, 2002–2006).

22 Adolphe Quételet, quoted in Theodore Porter, The Rise of Statistical Thinking, p. 54.

23 Quételet, quoted in Menand, The Metaphysical Club, p. 187.

24 Jeffrey Kluger,「Why We Worry about the Things We Shouldn't…and Ignore the Things We Should,」Time, December 4, 2006, pp. 65–71.

25 Gerd Gigerenzer, Empire of Chance: How Probability Changed Science and Everyday Life (Cambridge: Cambridge University Press, 1989), p. 129.

26 Menand, The Metaphysical Club, p. 193.

27 De Vany, Hollywood Economics; see part IV,「A Business of Extremes.」

28 See Derek William Forrest, Francis Galton: The Life and Work of a Victorian Genius (New York: Taplinger, 1974); Jeffrey M. Stanton,「Galton, Pearson, and the Peas: A Brief History of Linear Regression for Statistics Instructors,」Journal of Statistics Education 9, no. 3 (2001); and Theodore Porter, The Rise of Statistical Thinking, pp. 129–46.

29 Francis Galton, quoted in Theodore Porter, The Rise of Statistical Thinking, p. 130.

30 Peter Doskoch,「The Winning Edge,」Psychology Today, November/ December 2005, pp. 44–52.

31 Deborah J. Bennett, Randomness (Cambridge, Mass.: Harvard University Press, 1998), p. 123.

32 Abraham Pais, The Science and Life of Albert Einstein (London: Oxford University Press, 1982), p. 17; see also the discussion on p. 89.

33 On Brown and the history of Brownian motion, see D. J. Mabberley, Jupiter Botanicus: Robert Brown of the British Museum (Braunschweig, Germany, and London: Verlag von J. Cramer / Natural History Museum, 1985); Brian J. Ford,「Brownian Movement in Clarkia Pollen: A Reprise of the First Observations,」Microscope 40, no. 4 (1992): 235–41; and Stephen Brush,「A History of Random Processes. I. Brownian Movement from Brown to Perrin,」Archive for History of Exact Sciences 5, no. 34 (1968).

34 Pais, Albert Einstein, pp. 88–100.

35 Albert Einstein, quoted in Ronald William Clark, Einstein: The Life and Times (New York: HarperCollins, 1984), p. 77.

0801混沌中的秩序

20 世纪 60 年代中期，一名 90 多岁高龄的法国妇女让娜·卡尔梅，因急需生活费而跟一个 47 岁的律师做了笔交易：她将自己的公寓低价卖给律师，律师则按月给她提供生活费，直到她过世。等到了那一天，她横着出去，律师竖着进来。律师肯定知道卡尔梅女士的寿命已超出法国人期望寿命达 10 年之多。不过看来他大概并不了解贝叶斯定理，所以也不知道他所了解的这位女士在超过预期寿命的 10 年中去世的概率其实跟这笔买卖没啥关系，而真正跟买卖有关的，是在这位女士已经活到 90 岁的前提下，她的期望寿命大概还有 6 年。不过即便如此，律师也觉得这笔买卖不必担心会亏本。因为他相信，不管是哪个女人，如果她十来岁时就已经在父亲的店中遇见过文森特·凡高，那么他完全有理由相信，她跟凡高的再次会面一定用不了多长时间。（据说该女士认为我们的大艺术家「邋里邋遢，穿衣没品，惹人讨厌」。）

果不其然，在 10 年后，律师不得不另找一处住所栖身，而让娜则用自己良好的健康状况庆祝了她的第 100 个生日。尽管这时她的期望寿命只有差不多 2 年，但靠着律师的奉养，她又度过了 110 岁的生日，而律师已经 67 岁了。再过 10 年，律师漫长的等待终于到了头，却是他没有猜中的结局。1995 年，律师去世了，让娜还活着。她自己的那一日最终于 1997 年 8 月 4 日到来了，这时她已是 122 岁高龄，比律师的寿命多了整整 45 个年头。

单个个体的期望寿命以及命运是无法被预计的。但如果从大群体采集数据，并在大规模数据上进行总体分析，具有规律性的模型就会浮现出来。假设你已经 20 年没有出过交通事故了。然后在那个命中注定的下午，你跟妻子和她的娘家人一道在魁北克度假，你的岳母突然大叫一声：「当心驼鹿！」你一打方向盘，直接撞向了写着同样内容的警示牌。对你来说，这一定是一起你不打算重现的特殊事件。但是，警示牌存在本身，反映的就是一种客观需要。在那些数以千计的驾驶员中，肯定有一定比例的人会碰上驼鹿。实际上，随机行动的个人所构成的统计集合，常常表现出自洽且可以预测的行为，就好像这是一群有意识地追求某个共同目标的人。或者如哲学家康德在 1784 年所写：「每个个人，按照自己的喜好，追逐着自己的目标，并常与他人的目标相左；但每个个人或民族，都通向一个自然的但对他们而言都属未知的目标，就好像追随着某条引领之线；所有人都为这个目标做着努力，哪怕当他们知道目标的存在却对它不加重视时，也一样在努力。」

根据美国联邦公路总署的资料，美国大约有 2 亿名司机。美国国家公路交通安全管理局的数据显示，在最近的某一年中，这些司机共行驶了约 2.86 万亿英里的里程 ，差不多是每人 1.43 万英里。现在，假设美国人民想搞个好玩的事情，就是让司机们在下一年中跑出同样的总里程数。那我们来比较两种方法，它们都可以达到这个目的。方法一是由政府在国家自然科学基金会的一个超级计算中心，建立一个定量配给系统，将每人的目标行驶里程分配下去，以便在满足这 2 亿名司机各自需要的同时，还保持前一年那 1.43 万英里的平均里程。而在方法二中，我们告诉司机，不要有任何压力，你爱开多远就开多远，不用去管前一年到底跑了多少里程。如果从来都是步行去酒铺上班的比利·鲍勃叔叔，这次决定要当个霰弹枪推销员，到西得克萨斯跑上 10 万英里，没问题；住在曼哈顿的简表妹总是把大部分里程花在在道路清洁日围着她住的街区团团乱转找车位上，但现在她结了婚并搬到了新泽西，那么我们也不用担心她的里程数是否因此而改变。那么在这两种方法中，哪个能更准确地达到 1.43 万英里这个人均年里程数目标呢？我们没法对方法一进行实验，不过，根据以往在一段有限的时间内实施汽油配给政策的经验来看，这个方法大概不会很奏效；另一方面，方法二其实就是我们在实际中采用的方法：在下一年中，司机们高兴开多远就开多远，根本没想过要去达到什么配额之类的问题。那他们的表现如何呢？根据美国国家公路交通安全管理局的数据，在接下来的一年中，美国司机开了 2.88 万亿英里，或者每人 1.44 万英里，不过比目标多了 100 英里而已。而且，这 2 亿名司机出现了几乎同样多的交通死亡人数（42815 人对 42643 人），与上一年相比相差不到 200 人。

我们总是将随机性与无序联系在一起。尽管 2 亿名司机的生活总是在发生着无法预知的变化，我们却很难证明，他们作为一个整体的行为，能够比上面描述的行为更为有序。我们如果去看看诸如投票、买股票、结婚、人口失踪、信件写错地址，或者在去一个原本不打算参加的会议途中碰到堵车之类的情况，或者我们在测量人们腿的长度、脚的尺码、臀部的宽度或啤酒肚的厚度时，我们就能发现类似的规律性。当 19 世纪的科学家一头扎进前不久才开始能够获取的社会学数据时，他们发现不管在什么类型的数据中，生活中的混沌似乎总是会形成可以被量化描述也可以被预测的模式。但这些规律性并非唯一令人震惊的地方，同样令人惊讶的还有这些数据变化的性质，因为科学家发现，社会学中的数据也常常遵循正态分布。

人类的特征与行为的差异，以及弓箭手瞄准的误差，这两者竟然具有相似的分布情况，这个事实促使 19 世纪的一些科学家开始思索，如果将我们人类本身的存在比喻为一支箭，那么这支箭瞄准的目标到底是什么？而比找到这个目标更加重要的，是了解到底有哪些社会和自然方面的因素，使得这个目标有时会发生改变。因此，数学统计学这个为帮助科学家分析数据而发展起来的学科，在一个大相径庭的领域中蓬勃发展起来，这个领域就是对社会运行规律的本质的研究。

统计学家对日常生活中的数据进行分析的做法，出现时间至迟不会晚于 11 世纪。当时，威廉一世派人进行了事实上的史上首次全英格兰普查。威廉的统治始于 1035 年，时年 7 岁的他继承了父亲诺曼底公爵之位。正如其绰号「征服者威廉」所示，威廉一世喜欢征服，并在 1066 年入侵了英格兰。圣诞节那天他送给自己的礼物，就是英格兰的王冠。这种迅速的胜利给他留下了一点儿小问题：这些被他征服的到底是什么人？而更重要的问题是，他能从这些新臣民身上收多少税？为了获得这些问题的答案，他派人调查英格兰的每个角落，记录每一块土地的大小，土地的所有者，以及土地上有哪些资源。为了确保结果的正确性，在第一批巡察员的工作结束后，他又派出第二批巡察员。当时的税并不是按人头来收取的，收税的依据是土地的面积及用途。因此，巡察员付出了称得上勇气可嘉的努力，清点了公牛、奶牛和猪的数量，不过关于为它们清理排泄物的人，却没有什么记录被留下来。不过就算当时的税收和人口数据有关，就算想在中世纪对与人有关的最重要的寿命和疾病数据进行调查，也会被认为与基督教传统的死亡观念相悖。根据基督教教义，将死亡作为理性思考的对象本身就是错误的，而试图寻找掌管死亡规律的行为，简直就是亵渎神灵。不管人们是死于肺部感染、胃痛，还是石头的冲击力超过了头骨的承受力，他或她的真正死因，都被简单地认为是上帝的意旨。需要经过许多个世纪，这种宿命的态度才会慢慢退出历史舞台，而接替它的是相反的观点。根据新观点，研究自然和社会的规律性并非挑战上帝的权威，而是试图理解上帝的行事方式。

这个观念转变过程在 16 世纪迈出了一大步。当时的伦敦市长下令编撰每周一期的「死亡表」，以归总教区教士所记录的洗礼和葬礼。编撰工作在数十年中偶尔进行着，但在 1603 年这个瘟疫最为猖獗的年份之一，伦敦创立了周记制度。欧洲大陆的神学家们对这些满页都是数据的死亡表不屑一顾，这被视为典型的英国做派，几乎毫无用处。但是，对店主约翰·格朗特这个英国人来说，表中的数据却叙述了一个引人入胜的故事。

格朗特和朋友威廉·配第被称为统计学的奠基人。纯数学领域的研究者有时会觉得这个领域十分浅薄，因为它关注的是凡夫俗子的实用问题。从这个意义来说，让格朗特作为这个领域的创建者，其身份跟这个领域给人留下的印象尤其相配。与建立概率论的业余爱好者如医生卡尔达诺、法官费马或神父贝叶斯这些人不同，格朗特是个买卖纽扣、针线之类家居小饰品的商人。但格朗特不仅仅是纽扣商，他还是个有钱的纽扣商。财富使他有足够的闲暇，这让他可以去追求一些跟如何扣衣服毫无关系的兴趣与爱好。财富还使他和一些当代最伟大的知识分子成为朋友，其中就包括配第。

格朗特从死亡表中得出的推论之一，是关于饿死的人数。1665 年，这个数字据称是 45 人，大概只是被处决人数的一倍；相比之下，有 4808 人死于肺病，1929 人死于斑点热和紫癜，2614 人死于牙病和寄生虫病，68596 人死于瘟疫。那是什么原因使得伦敦在「乞丐成堆」的时候，却只有这么几个人是饿死的？格朗特认为，这肯定是因为普通民众乐于施舍食物，才使得乞丐们免于饿死。他因此提出，国家应该给缺衣少食的穷人提供食物，这样一来，一方面社会大众不必付出太多，另一方面 17 世纪伦敦的街道也不至于被与之相称的乞丐和擦车仔们给堵个满满当当。格朗特还参加了瘟疫传播方式的两种理论的争论。一种理论认为这种疾病是通过污浊的空气传播的，而另一种则认为传播是通过人与人的接触完成的。格朗特观察了一周又一周的死亡记录，最后认为，因瘟疫致死人数的数据，其变动幅度太大，令人无法相信这种变动是完全随机的。根据他的观点，人与人之间接触的模式基本上是恒定的，因此人传人的理论如果是正确的，那么死亡人数的波动应该表现出随机性。而另一方面，每周的天气变化都可能相当大，超出一般的随机波动范围，所以他觉得，这些波动的数据与污浊空气理论更为一致。当然，后来的事情表明，伦敦并没有为开设粥场做好准备，而伦敦市民如果远离丑陋的老鼠而不是污浊的空气，那么他们也会活得更好些。但是格朗特的伟大发现并不在于得出某个具体问题的结论，而在于他认识到，我们可以通过统计数据，更好地洞察产生这些数据的那个体系。

配第被一些人认为是古典经济学的先驱。配第相信，国家的力量有赖于臣民的数量和性格，并最终通过臣民的数量和性格得以体现。因此，他运用统计推理的方法分析国家大事。他的分析通常都是从统治者的视角出发，而将社会成员当作可随意摆布的物品。他指出，为了对付瘟疫，应该把钱花在预防上，因为拯救生命就等于省下一笔抚养费，而国家通过拯救更多的生命可以省出数量可观的金钱。而且这种做法的回报，即使相较于最有利可图的其他投资方式，也有过之而无不及。不过配第对爱尔兰人可就没那么仁慈了。例如，他认为一个英格兰人的性命，其经济价值要大于一个爱尔兰人的性命，因此，如果除了留几个人在爱尔兰放牛，把其余爱尔兰人都强迁到英格兰，那么王国的财富可以随之增加。实事求是地说，配第自己的财富就得益于这些爱尔兰人：作为 17 世纪 50 年代入侵爱尔兰英军的随军医生，他被指派了一项任务，就是对战争造成的损失进行评估。经过仔细评估他得出的结论是，他可以从中大捞一笔，而且能安然脱身。而他也正是这样做的。

如果一如配第认为的那样，人口及其增长反映了政府的优劣，那么在缺少一个测量人口的好方法的情况下，对政府优劣的评价就会变得十分困难。正是在这个问题上，格朗特进行了他最为著名的计算，特别是对伦敦人口的计算。格朗特从死亡表中获取了出生人口的数据。结合对出生率的粗略估计，他进一步推算出育龄女性的大概数量，然后估计出家庭数量，再通过他自己观察到的伦敦家庭成员的平均人数，就能估计出全城人口。他的计算结果是 38.4 万，而之前人们都觉得这个数应该有 200 万。格朗特还证明，伦敦人口增长的主要贡献，来自从边远地区过来的移民，而非更为缓慢的人口繁衍；此外，尽管瘟疫仍然会造成恐慌，但即使是最严重的疫病所导致的人口减少，也会在两年之内有所缓和。这一结论颇为令人吃惊。此外，人们相信正是格朗特发布了历史上首份「寿命表」。时至今日，这个以系统的方式排列预期寿命数据的做法，仍被从人寿保险公司到世界卫生组织等对人们的寿命感兴趣的机构或团体使用着。表 8-1 显示了在每 100 人中能活到任意指定寿命的人数。在格朗特所列的数据（下表中标着「伦敦，1662」这一列）之外，我额外增加了某些国家现在的数据。

表 8-1 格朗特寿命表扩展版

![](./res/2020002.png)

1662 年，格朗特在《死亡表的…… 自然与政治观察》（ Natural and Political Observations … upon the Bills of Mortality ）中发表了他的分析。该书获得了广泛赞誉。一年后，格朗特入选英国皇家学会。但在 1666 年，伦敦发生了大火。城市的很大一部分被全部烧毁，格朗特的生意也毁于一旦。为了在他的伤口上再撒一把羞辱的盐，有人指控他在大火发生前下令停止供水，所以他是火灾的帮凶。实际上，直到火灾发生之后，他才与供水公司有过联系。不管怎样，在这段插曲之后，格朗特的名字从英国皇家学会名册上消失了。几年后，格朗特死于黄疸。

很大程度上，正是由于格朗特的成果，法国才在 1667 年与英国站到了同一条战线上，修订法律给诸如死亡表这样的统计调查放行。其他欧洲国家紧随其后。到了 19 世纪，整个欧洲的统计学家，都深深地陷在诸如普查数据这样的政府记录之中，这简直就是「一场数字的大雪崩」。格朗特的贡献在于，他证明了我们可以通过仔细分析数据的有限样本，推理得出关于人群总体的结论。尽管格朗特与其他人付出了卓绝的努力，以求能够利用简单的逻辑了解数据中蕴含的信息，但要解释这些数据所隐藏的大部分秘密，还有待高斯、拉普拉斯和其他 19 世纪及 20 世纪早期的科学家所建立的工具。

「统计」一词源自德语单词 Statistik，是由《比尔菲尔德初等普及教育》（ Bielfield’s Elementary Universal Educatio ）一书 1770 年的翻译版引入英语的。书中写道：「被称为统计的这门学科告诉我们，在已知世界中的现代国家，其政治组成是怎样的。」到 1828 年，统计学科已经发生了变化，此时诺亚·韦伯斯特的《美国英语词典》（今简称《韦氏大词典》）所定义的统计学是：「有关社会状态，民族或国家中人们的情况，他们的健康状况、寿命、国内经济、艺术、财产与政治影响力以及国家的状态等事实的集合。」拉普拉斯一直都在探索，如何将他的数学分析方法的应用对象，从行星与恒星等天体扩展到日常生活，在韦伯斯特定义统计学的含义时，统计学已经将拉普拉斯的方法包含在内了。

正态分布描述了许多系统中，系统的表现围绕某中心值发生改变的行为方式，这个中心值就代表了系统最可能的输出值。在《概率的哲学导论》（ Essai philosophique sur les probabilités ）一书中，拉普拉斯声称这个新的数学分支，能用于评判法庭证言、预测结婚率、计算保险费等问题。但当该书最后一版问世时，拉普拉斯已是六十多岁了，因此继承和发扬其构想的重任，就落到了 1796 年 2 月 22 日出生于佛兰德根特的阿道夫·凯特勒这名年轻人的身上。

2『电子书籍没找到，已购买纸版书籍「2021116关于概率的哲学随笔」。（2021-02-28）』

凯特勒的研究工作并非肇因于对社会运转方式的兴趣。1819 年为他赢得根特大学颁发的第一个数学博士学位的论文，是与圆锥曲线理论有关的一个几何学课题。然后，他的兴趣转向天文学。1820 年左右，他积极运作，以寻求在他当时任职的布鲁塞尔建立一个新天文台。在充满野心的凯特勒看来，这个天文台显然是建立一个科学帝国的第一步。他的这一步走得颇为莽撞，因为他对天文学所知不多，而对天文台运作更是几乎一无所知。但他说服别人的能力肯定相当不错，因为不仅天文台的建设得到资助，他个人也获得一笔基金，从而能够用几个月的时间到巴黎去弥补知识上的不足。结果表明，这笔投资收益显著，因为凯特勒的比利时皇家天文台直到今天仍然存在。

在巴黎，凯特勒以其独有的方式，受到了生活之无常的影响，并被命运带到一个完全不同的方向。他与统计学的罗曼史，始于与几位伟大的法国数学家的结识，其中包括拉普拉斯和约瑟夫·傅立叶。他师从傅立叶，学习了统计与概率。尽管这时他已经学会如何运作天文台，但凯特勒陷入对另一个目标的热烈追求之中，这个目标就是运用天文学中的数学工具处理社会数据。

凯特勒回到布鲁塞尔后，开始收集和分析人口统计数据，并很快将注意力集中在法国政府于 1827 年开始发布的犯罪活动记录上。在《论人类及其能力之发展》（ Sur l’homme et le développement de ses facultés ）这部发表于 1835 年的两卷本著作中，凯特勒印上了一张 1826 年至 1831 年所报道的法国各年谋杀案数量的表格。他注意到，谋杀案的数量是相对恒定的，不仅如此，每年用火枪、剑、小刀、木棒、石头、其他砍刺器具、拳脚、绳勒、溺毙和火烧等方式进行的谋杀案，它们的比例也相对恒定。凯特勒还按年龄、地域、季节、职业以及是否在医院和监狱里等因素，对死亡率进行分析。他还对酗酒、精神病和犯罪进行了统计学研究。此外，他在巴黎自缢者人数以及比利时 60 多岁老妪配 20 多岁小伙子的婚姻数量上，也发现了各自的统计规律。

统计学家之前就进行过类似的研究，但凯特勒比他们做得更多：他不仅观察了数据的平均值，还仔细研究了它们与平均值之间的偏离情况。不管研究对象是什么样的，凯特勒都遇到了正态分布：在犯罪、结婚和自杀的倾向上，在美洲原住民的身高上，乃至在苏格兰士兵的胸围测量值上（从《爱丁堡医学和外科杂志》的一篇旧文中，他得到 5738 个胸围测量数据的样本），正态分布无处不在。在为该书草稿所准备的 10 万名年轻法国人的身高中，他还发现一种现象，那就是一组数据的分布与正态分布之间的偏离，本身可能意味着一些不为人知的信息。在这些数据中，当把准备应征入伍的士兵的数量与他们的身高进行对比时，所得的钟形曲线是扭曲的：身高刚刚超过 5 英尺 2 英寸的人数比钟形曲线所预测的数量要少很多，而恰好小于这个身高的人数又太多了，就好像后者的出现是为了补偿前者的不足。凯特勒论述道，这个多出来的约 2200 个「矮子」的差异，应该是造假或善意的捏造造成的，因为身高不到 5 英尺 2 英寸的人可以免服兵役。

1-2『关键信息：一组数据的分布于正态分布之间的偏差，可能意味着不为人知的信息。做一张金句卡片。（2021-02-28）』——已完成

几十年后，伟大的法国数学家亨利·庞加莱，用凯特勒的方法逮到一个欺骗顾客的面包师。庞加莱每天都要买一条面包，买回来后，他会给面包称一下重量，结果发现这些面包的平均重量大约为 950 克，而非广告中所称的 1000 克。他向管理部门投诉了此事。之后，他买到的面包变大了些。可他还是觉得有什么地方不太对劲。凭着只有著名学者或至少是获得了终身教职的人才有的耐心，他在接下来的一年中，每天都仔细地称量面包。尽管这些面包现在的平均重量十分接近 1000 克，但如果这个面包师的确是老老实实地随机挑出一条面包卖给他，那么比这个平均重量更重些或更轻些的面包，其数量应如第 7 章所说的那样，按误差定律的钟形曲线逐渐减少。可庞加莱发现，他的面包里偏轻的比例太少，而偏重的相应过多。庞加莱由此得出结论，那个面包师其实并没有停止制作缺斤少两的面包，只不过总是拿手头最大的一条面包打发他罢了。警察再次造访了骗人的面包师，据报道所言，他表现出可想而知的震惊，并不出所料地同意改正自己的行为。

凯特勒无意中得到一个有用的发现：随机性的模式非常值得我们信赖，因此如果我们在某些社会学数据中发现它们的分布与钟形曲线不匹配，我们甚至可以把它作为证据来使用。如今，这种分析被用于凯特勒的时代不可能处理的大量数据。实际上，在近几年中，类似的统计侦察术逐渐流行，并创造了一个被称为法律经济学的新领域。在这个领域最著名的案例中，统计学研究证明，一些公司将其优先认股权的获准时间谎报为更早的时刻。这种做法的目的很简单：公司会以优先认股权（事后以认股权获准时的价格购买股票的权利）的方式激励管理人员努力推动公司股价上涨。如果认股权获准时间提前到某股价特别低的时刻，管理人员的利润就会大涨。这个点子很机灵，但是如果秘密地采取这种做法，就违反了证券法。同时，这种做法也留下了统计学意义上的「指纹」，最终导致司法部门对 10 多家大公司展开了调查。在另一个不那么出名的例子里，沃顿商学院的经济学家贾斯廷·沃尔弗斯在大约 7 万场大学篮球比赛中发现了打假球的证据。

沃尔弗斯是在比较拉斯韦加斯赌球的分差与球赛真实结果时，注意到这个不正常的地方的。对于大家看好的球队，赌球庄家会给出一个分差，引导人们以差不多对半开的比例对两支队伍下注。比如，假设大家都认为加州理工学院篮球队比加州大学洛杉矶分校篮球队要强（对于大学篮球赛的球迷们来说，是的，在 20 世纪 50 年代时的确如此），那么为了避免赌球双方获胜的机会不等，庄家只会在加州理工学院胜出加州大学洛杉矶分校 13 分以上的时候，才会付钱给那些押宝在加州理工学院的人。通过这个分差，赌博双方的输赢机会大致均等。

尽管这个分差是由庄家设定的，但实际上，它最后还是由广大赌徒确定，因为庄家会根据赌徒下注的情况调整分差，从而达到双方需求上的平衡。（庄家是从下注的钱中抽取抽头获利的，因此他们追求的就是双方所下赌注的相等，这样一来，不管比赛结果怎样，庄家都稳赚不赔。）经济学家用一个被称为预测误差的值，来衡量赌徒们对两支球队评估的准确程度，它等于被看好一方的真实胜出分差与赌博市场所设定的分差之间的差值。作为一种误差，预测误差的分布毫不令人惊讶地遵循正态分布。沃尔弗斯发现，预测误差的均值为 0，即分差既不倾向于过高评价球队，也不倾向于过低评价它们；其标准差为 10.9 分，也就是说，在大约 2/3 的比赛中，赌场设置的分差与真实比分，相差不会超过 10.9 分（在一项针对职业橄榄球赛的研究中，研究者也发现类似的结果，其均值为 0，标准差为 13.9 分）。

然后，沃尔弗斯考察了对阵双方强弱悬殊的那些比赛，他发现了一个令人吃惊的情况：在这些比赛中，强队胜出的比分恰恰超过庄家设置的分差的场次过分地少，而恰恰不到这个分差的场次又无法解释地多了许多。这又是一个凯特勒异常。与凯特勒和庞加莱一样，沃尔弗斯给出的结论是有人打假球。他的分析如下：即使是顶尖的球员，要保证球队胜出的分数比分差更多，也是十分困难的；但如果一支队伍明显强于对手，那么球员便可以在不危及球队获胜的前提下，很容易通过马虎的比赛表现，确保球队不会赢得太差的比分。因此，如果有些缺德的赌徒想要改变赌博的结果，又不想要求球员故意输掉比赛，那么结果正好是沃尔弗斯发现的这个偏离情况。那么，沃尔弗斯的结果能否证明，在一定比例的大学篮球赛中，球员们会因受贿而故意压低得分？他倒没有这么说。但沃尔弗斯也说过：「我们不能认为法庭上发生的事情，就是拉斯韦加斯发生的事情的一个如实反映。」有意思的是，在一次由美国大学体育联盟进行的调查中，1.5% 的运动员承认知道有队友「因为收了钱而故意发挥得差些」。

凯特勒并不打算把他的想法用到法庭上。他有个更宏伟的计划：利用正态分布说明人与社会的本原。他写道，如果给一尊雕像做 1000 个复制品，那么由于测量和手艺的误差，这些复制品将各有不同，而这些不同将遵循误差定律。由此推断，如果人们的物质特性遵循同样的定律，那么必然是因为我们同样是某原型的不完美复制品。凯特勒称这个原型为「平均人」。他还觉得，对人类行为而言，同样存在着一个模板。一家大百货公司的经理可能无法知道，那个好像吸毒后飘飘欲仙的出纳，会不会把他正在嗅着的那瓶半盎司 香奈儿 Allure 香水揣到自己兜里。但下面这个预测是靠得住的：在零售业中，库存商品的年损失率相当稳定地维持在 1.6% 左右，而同样稳定的是，45-48% 的损失是员工盗窃造成的。犯罪就「像一笔以令人毛骨悚然的规律性来支付的预算」，凯特勒这样写道。

凯特勒认识到，不同文化下的「平均人」也会不同，而且这个「平均人」会随着社会环境的改变而改变。实际上，研究「平均人」的改变及改变的原因，才是凯特勒最大的野心。「人遵循特定的定律出生、成长和死亡」，他这样写道，而这些定律「从未得到过研究」。牛顿认识了一套宇宙定律，并将其公式化，从而成为现代物理学之父。以牛顿为榜样的凯特勒，则希望创造一门新的「社会物理学」，以描述人类行为的定律。正如一个物体在不受干扰时会保持其运动状态一样，在凯特勒的类比中，人类群体的行为在社会环境中保持不变时，同样保持恒定状态。而且，正如牛顿描述了物理力如何使物体偏离其直线路径一样，凯特勒期望找到人类行为的定律，以描述社会力量是如何改变社会特性的。举例来说，凯特勒认为财富的巨大悬殊以及价格的巨大波动，对犯罪和社会的不稳定负有责任，而一个稳定的犯罪率水平则体现了一种平衡状态，当造成这个平衡状态的那些影响因素发生改变时，平衡状态随之改变。2001 年 9·11 恐怖袭击事件发生后的几个月，涌现出很多社会平衡发生变化的生动例子。当时，出行者由于害怕搭乘飞机而纷纷转为驾车出行。正是 9·11 恐怖袭击事件带来的人们不敢搭乘飞机的恐慌，造成了比前一年增加了约 1000 名车祸丧生者的事实，这就是 9·11 恐怖袭击事件造成的隐形伤亡。

但是，相信这样一门社会物理学的存在是一回事，真正建立起这门学科则是另一回事。凯特勒认识到，我们可以将大量的人置于实验环境中并测量其行为，然后据此检视一门真正的科学中的理论。由于这个方法实际上并不可行，所以他相信社会科学更像天文学而非物理学，也就是对社会科学理论的理解，需要通过被动的观测来获得。因此，为了发现社会物理学的定律，他对「平均人」随时间和文化的变化而变化的现象进行了研究。

人们普遍接受了凯特勒的观点，法国和英国尤其如此。一名生理学家甚至从一个不同国籍的人频繁光顾的火车站洗手间中收集尿液，以确定「平均欧洲人尿液」的性质。凯特勒在英国最热情的信徒，是名叫亨利·托马斯·巴克尔的富有的棋手和历史学家，他最为人所知的，就是他野心勃勃的多卷本著作《英国文明史》。不幸的是，1861 年巴克尔 40 岁的那年，他在大马士革旅行期间患上了斑疹伤寒。一名当地医生前来为他治病，他拒绝了，因为这个医生是个法国人，他因此病故。巴克尔未能完成他的著作，不过他还是完成了前两卷，在第一卷中，他通过统计观点阐述历史。这部分内容基于凯特勒的成果，并立刻大获成功。整个欧洲都在阅读这本书。它被译成法文、德文和俄文。达尔文读过它，阿尔弗雷德·拉塞尔·华莱士读过它，陀思妥耶夫斯基更是读了两遍。

尽管该书大受欢迎，历史却证明，凯特勒的数学成就比他的社会物理学成就更高。一方面，并非所有的社会事件都遵循正态分布，在金融领域中这一点特别明显。比如，如果电影票房遵循正态分布，那么大多数电影赚到的钱都应该落在某平均值附近，而有 2/3 的电影票房，将落在这个平均值周围一个标准差的范围内。但在电影行业中，20% 的电影带来了 80% 的票房收入。这类由热点驱动的产业，尽管完全无法预测，却遵循了一个完全不同的分布，其中均值和标准差的概念毫无意义，因为根本就没有什么「典型」的票房表现，而那些类似百万票房的例外，在一般的行当中可能几个世纪才出现一次，在电影行业中却隔几年就会发生一次。

相较于凯特勒忽略了其他类型的概率分布这个缺点，更重要的是，他未能在寻找希望的定律和驱动力的道路上前进多远。因此，他对社会物理学的直接影响是有限的。不过，他的遗产却是不可否认且意义深远的。他的遗产并不在社会科学，而在「硬」科学上。正是在这些「硬」科学中，以大量随机事件理解其中秩序的思路，启发了众多学者，产生了革命性成果，并改变了生物学和物理学的思维方式。

将统计学思想引入生物学的，是达尔文的表弟，弗朗西斯·高尔顿，他是一个闲适的人。他在 1840 年进入剑桥大学三一学院 ，先是学习医学，随后接受了达尔文的建议改学数学。当他父亲去世时，22 岁的他继承了一笔可观的财产。由于不需要靠工作来糊口，他成了一名业余科学家。他着迷于测量，他测量人们头颅的大小、鼻子的大小、四肢的长短；他测量人们在听课时坐立不安的次数；他测量那些在路上看到的女孩的吸引力（伦敦姑娘得分最高，而阿伯丁的女孩垫了底）；他测量人们指纹的特征，并最终使伦敦警察厅于 1901 年采纳了指纹识别；他甚至还测量君主和教士的寿命，结果发现他们与其他职业人群的寿命差不多，因此他得出结论，祈祷其实并没带来什么好处。

高尔顿在 1869 年《遗传的天才》一书中写道：任何身高落在给定范围内的人数占总人口的比例，应保持恒定而不随时间的改变而改变，而且身高与所有其他物理特征 —— 头颅周长、大脑大小、灰质重量、大脑纤维数量等等 —— 都遵循正态分布。他相信人的性格也由遗传决定，并如同生理特征一样，以某种方式遵循正态分布。因此，按高尔顿的话说，人「作为社会之基本单元」，并不「在投票及其他方面的能力上具有相同的价值」。相反，他断言每 100 万男人中，约有 250 人继承了某方面的特殊才能，并因此在该领域出类拔萃（在他那个年代，妇女一般不参加工作，因此他没有对妇女进行类似的分析）。高尔顿根据这些想法创建了一门新的研究领域，即优生学。优生学一词来自希腊词 eu（好）和 genos（出生）。对于后来很长时间内各种不同的人来说，优生学的含义大不相同。纳粹后来也采纳了这个术语和观点，但我们并没有证据证明高尔顿赞同德国人的杀人计划。相反，他希望能找到一种途径，通过选择性繁殖改善人类的状况。

高尔顿关于成功所给出的简单因果解释很有诱惑性，本书第 9 章的大部分内容就是为了解释其缘由。但在第 10 章我们将看到，对于任何一项可以被称为复杂内容的任务，它的实现过程都充满了无数可预见的和不可预见的障碍，因此，能力和成就之间的联系，远不如高尔顿解释的那样直接。实际上，心理学家近年来发现，对于取得成功而言，在面对困难时毫不退缩的品质，至少与天分同等重要。这也就是专家们常常提及的「10 年规则」，也就是对于多数行为而言，要想把它做得相当成功，至少需要 10 年的刻苦、练习和奋斗。勤奋与机遇跟天分同等重要，这也许令人沮丧，但我觉得我们更应该因此而振奋，因为我们虽然不能控制自身的基因组成，但努力的程度完全取决于自己。我们也能在一定程度上控制机遇的影响：只要反复尝试，就能提高成功的机会。

无论优生学是好是坏，高尔顿在遗传方面进行的研究，都引导我们发现了现代统计学的两个核心数学概念。这两个概念一个出现于 1875 年，当时高尔顿把几包甜豌豆荚分给了 7 个朋友，他们每人都收到统一大小和重量的种子，他们要做的是将种出的下一代种子还给高尔顿。高尔顿对这些种子进行测量后注意到，大个头种子产生的后代，其直径的中值比父代种子小，而小个头种子产生的后代比父代更大。后来，从在伦敦所建的实验室获得的数据中，他发现人类父母和子女身高上也存在同样的情况。他将这种现象称为回归，即在相关联的测量中，如果一个测量值远离均值，那么另一个测量值将会更接近均值。

高尔顿很快意识到，如果一个过程没有这样的回归行为，那么这个过程将不可避免地失控。举例来说，假设高个子爸爸的儿子，平均而言与父亲一样高。既然身高各有不同，那么某些儿子就会比爸爸更高。到了下一代，假定这些更高的儿子，他们的儿子，即开始那群人的孙子，平均而言也跟他们的父亲一样高，其中又有某些人同样会高过他们的父亲。一代又一代之后，人类中最高的人将变得越来越高。但由于回归的存在，这种情况并没有发生。同样的论述也可用于与生俱来的智力、艺术才能或打高尔夫球的技术等方面。因此，个头很高的父母不应该指望孩子长得同样高，非常聪明的父母不应该指望孩子同样聪明，而毕加索们和泰格·伍兹们也不应该指望自己的孩子取得跟他们一样的成就。另一方面，非常矮的父母却可以期待后代会更高些，而我们这些并不聪明或不善于画画的人，也有理由期待这些方面的无能将被下一代改变。

高尔顿用广告为实验室招徕受试者，然后对他们进行一系列身高、体重乃至特定骨骼尺寸的测量。他希望发现一种根据父母的测量值预测孩子对应的测量值的方法。为了找到这种方法，高尔顿绘制了各种图形，其中之一就是以父母的身高为孩子的身高作图。打个比方，如果父母和孩子的身高总是相等，就应该得到一条呈 45 度的直线。如果这种关系平均来说是成立的，但个别数据点可能有变化，该图就会表现出某些点落在这条直线上方，而另一些落在下方。用这种方式，高尔顿的图不但显示了父母和子女身高之间的一般关系，而且显示了这种关系存在的程度。这就是高尔顿对统计学所做的另一大贡献：他定义了一种数学指标，用来描述类似关系的质量。该指标被称为相关系数。

相关系数是取值在 -1 到 1 之间的一个数。如果相关系数很接近 1 或 -1，就表明两个变量很好地遵循线性关系；相关系数为 0 则表示两者不相关。打个比方，如果数据显示每周吃一次麦当劳最新推出的 1000 卡路里 的套餐，一年下来体重会增加 10 磅；如果吃两次，就会增加 20 磅，以此类推，那么这时的相关系数为 1。如果由于某种原因，吃这个套餐的人不是增重，而是减掉同样的体重，那么相关系数为 -1。如果增加和减少的体重的值五花八门，并不依赖于吃了几次套餐，那么相关系数为 0。今天，相关系数已经成为统计学中使用最广泛的概念之一。人们用它评估诸如吸烟量和癌症的发生、恒星距地球的距离和它们远离我们的速度、学生在标准化考试中所得分数和他们的家庭收入等的关系。

高尔顿的成果十分重要，其重要性不仅体现在它的直接应用上，而且体现在它促成了随后几十年统计学所取得的进展上。正是在这段时期内，统计学迅速发展成为一门成熟的学科，而其中最重要的进展之一，是由高尔顿的信徒皮尔逊完成的。在本章早些时候，我曾给出许多遵循正态分布的数据类型。但对于容量有限的数据集而言，数据的分布不可能与正态分布曲线完美匹配。在统计学的早期，科学家们简单地将数据绘制成图，然后观察所得曲线的形状，以此判断这些数据是否遵循正态分布。但这个曲线匹配的精确程度我们该如何量化呢？皮尔逊发明了一种被称为 $χ^2$ 检验的方法，它可以确定一个数据集是否确实遵循我们所认为的那个分布。

2『$χ^2$ 检验方法，做一张术语卡片。（2021-02-28）』——已完成

1892 年 7 月，他在蒙特卡洛演示了这个检验方法。他的测试可以被看作贾格尔当年事迹的照搬。正如贾格尔那时碰到的情形一样，在皮尔逊的测试中，轮盘赌开出的数字并不遵循他所希望的分布，即轮盘产生的结果确实是随机时所应遵循的分布。在另一次测试中，皮尔逊观察了 12 枚骰子的 26306 次投掷，并统计了每把扔出 5 点与 6 点的数量。他发现这个分布同样不满足使用完美骰子进行公平赌赛时应该得到的分布 —— 在公平的情况下，一枚骰子扔出 5 点或 6 点的概率是 1/3，或 0.3333；但实际的分布却表明，这个概率似乎是 0.3377。换言之，骰子有问题。对于轮盘赌而言，这可能是有人暗中操纵的结果，但骰子的问题很可能出在制造过程中，正如我的朋友莫希强调的那样，这种不完美性总是存在着。

今天，$χ^2$ 检验得到广泛运用。假设现在我们检验的不是骰子，而是消费者对三种麦片的喜好程度。如果消费者没有特殊的偏好，那么可以期望，每盒麦片都会被近 1/3 的消费者选购。正如我们已经看到的，真实结果很难如此均匀地分布。这时我们就可以使用 $χ^2$ 检验来确定，那盒得票最多的麦片，确实是因为消费者的喜爱而非凑巧胜出的可能性有多大。与此类似，如果一家制药公司的研究人员想要测试两种用于防治急性器官移植排斥反应的治疗方法，就可以通过 $χ^2$ 检验确定两者是否存在显著性差异。再或者，在一个新的租车店开张之前，租车公司的首席财务官会认为有 25% 的顾客想租用微型汽车，50% 的想租小型汽车，还分别有 12.5% 的需要中型或其他类型的汽车。那么等到租赁数据逐渐累积起来之后，$χ^2$ 检验就能帮助这位首席财务官迅速判断之前的假设是否正确。如果新店是个偏离典型情况的例外，那么公司最好把这个店不同的车型做一下调整。

凯特勒的成果由高尔顿引入生物科学。但凯特勒同样对物理科学中一次革命性的发展做出了贡献：统计物理学创建者中的两位，詹姆斯·克拉克·麦克斯韦和路德维希·玻尔兹曼，他们都从凯特勒的理论中获得过灵感（跟达尔文和陀思妥耶夫斯基一样，他们是在巴克尔的书中读到这些内容的）。不管怎样，如果 5738 名苏格兰士兵的胸围能很好地按正态曲线分布，而 2 亿名司机每年驾驶里程之间的差距可以小到 100 英里，那么在想到一升气体中那 10^24 左右的分子会表现出某些有趣的规律性这一点上，就不需要一个爱因斯坦的大脑了。但为了让科学界最终接受这个物理学新方法的必要性，我们还是需要一位爱因斯坦。阿尔伯特·爱因斯坦完成这个任务是在 1905 年，也就是他发表第一篇关于相对论论文的那一年。尽管主流媒体对此知之甚少，但爱因斯坦 1905 年的那篇关于统计物理学的文章，跟他的相对论论文一样具有革命性意义。实际上，它是他在科学界被引用次数最多的论文。

爱因斯坦 1905 年关于统计物理学的论文，其写作目的是希望解释一种被称为布朗运动的现象。该过程以罗伯特·布朗的名字命名。布朗是一名植物学家，是使用显微镜的世界级专家，而且被认为是第一个对细胞核做出清晰描述的人。布朗孜孜不倦地追求着他的生活目标，那就是通过观察发现生命力的来源。在当时，人们相信这个所谓生命力是一种神秘的作用力，那些有生命者具有的那种性质，就是由这个生命力赋予的。在这一追求中，布朗注定要失败，但在 1827 年 6 月的一天，他认为自己成功了。

透过镜头，布朗注意到，他所观察的花粉颗粒内部的微粒似乎在动。尽管花粉是一种生命之源，但它本身不是什么有生命的物体。不过不管布朗盯上多久，这些微粒的运动都不曾停止，就好像它们拥有某种神秘的能量。这个运动似乎没有特定的目的地，实际上，运动看起来完全是随机的。无比激动的布朗马上得出结论，他已经找到他孜孜以求的东西，因为除了驱动生命本身的能量，这还能是什么别的东西吗？

在接下来的一个月里，布朗勤勉有加地进行了一系列实验，并发现所有他能搞到手的各色有机小颗粒在悬浮于水中或杜松子酒中时，都有着同样的运动，这些小颗粒包括经过分解的小牛肉纤维，「被伦敦的灰尘染黑的」蜘蛛网，甚至他自己的黏液，等等。接着，他一意孤行的解释遭到致命一击：当他用石棉、铜、铋、锑和锰之类的无机小颗粒进行实验时，也观察到同样的运动。这让他认识到，他看到的运动根本与生命无关。后来的进展证明，布朗运动产生的真实原因，并非某个实在的物理力，而是由随机模式产生的虚拟力。它与那个凯特勒注意到的强迫人类行为有规律性的力是相同的。不幸的是，当布朗运动的正确解释出现时，布朗已经不在人世了。

在布朗发现布朗运动之后的数十年间，玻尔兹曼、麦克斯韦与其他学者打下了理解布朗运动的基础。他们受到凯特勒观点的启发，建立了统计物理学这个新领域，用概率与统计的数学思想，解释构成流体的（当时仍然是假想的）原子的运动是如何造就流体性质的。但在之后的几十年中，这些想法并未取得什么进展。某些科学家对该理论所使用的数学工具心存疑虑；其他人则因为从未见过或根本不相信有人能见到这个所谓的原子而反对该理论。但大多数物理学家都是实用主义者，对于这种原则性的东西倒不是那么在意。可以说，该理论未被接受，最主要的原因就是它虽然能推导出若干已知的流体定律，却没有给出可验证的预言。事情就这么被拖到了 1905 年，这时麦克斯韦过世已久，而沮丧的玻尔兹曼也离自杀不远了。爱因斯坦使用这个新生的理论，通过详尽的数学公式，解释了布朗运动的准确机制。从此以后，不再有人对在物理学中使用统计学方法的必要性存有疑问，而物质是由原子和分子组成的观点，将成为大多数现代技术的基础，同时也成为物理学史上最为重要的思想之一。

正如我们将在第 10 章见到的那样，流体中分子的随机运动，可以成为我们自己的生命之路的一个类比。因此，爱因斯坦的工作值得我们多花点儿时间仔细看看。在原子论描绘的图景中，水分子的基本运动是完全杂乱无章的。分子先是沿着这条路径运动，然后沿着另一条路径飞走。它将保持直线运动，直到与另一个分子相撞改变自己的前进方向。本书前言已经提到过，这种路径 —— 在不同的时间和方向上是随机变化的 —— 常常被称为醉汉的脚步。那些喝了太多马提尼酒的人，都能非常清楚地理解这个名字的由来（不过头脑更冷静的数学家和科学家有时称其为「随机漫步」）。按原子理论的预测，如果悬浮于液体中的颗粒不断地被液体分子随机轰击，那么可以预计，颗粒将因为这些碰撞，一会儿朝这儿一会儿朝那儿动来动去。但布朗运动的这种解释存在两个问题：首先，相较于可见的悬浮颗粒，分子实在太小了，无法推动颗粒移动；其次，分子的碰撞远比所观察到的颗粒晃动要来得频繁。爱因斯坦的天才想法部分在于，他认识到这两者其实会相互抵消：尽管碰撞发生得非常频繁，但由于分子如此之轻，以至这些频繁而独立的碰撞不会产生明显的影响。只有在某些时候，纯粹的运气使得某方向上的撞击更占优势，才会发生足以使人注意的晃动，就好像分子世界中的马里斯创纪录之年一样。爱因斯坦通过计算发现，尽管在分子层面上看来是一片混沌，但在分子的参数（大小、数量和速度等）和可观察的颗粒晃动的频率与幅度之间，存在着可预测的数学关系。于是有史以来头一遭，新的可观测的结果与统计物理学被爱因斯坦联系在一起。虽然我这里说得好像这个成就不过是一些纯粹的技术性问题，但实际恰恰相反，爱因斯坦的这项成果是一个伟大原则的胜利：在众多我们能感知的自然界秩序的掩饰之下，其实都是不可见的无序，只有通过随机性的法则，我们才能理解这些表面上的秩序。正如爱因斯坦所言：「能认识到那些看上去与直接可见的真理相去甚远的现象，其实具有统一性，这种感觉无比美好。」

2『这里的信息知道了为啥本书名为「醉汉的脚步」。做一张主题卡片。（2021-02-28）』

在爱因斯坦的数学推导中，正态分布再次扮演了核心角色，并达到它在科学史上新的辉煌地位。醉汉的脚步就此确立了它的地位，即成为最具基础性并很快得到最多研究的自然过程之一。等到各领域的科学家承认了统计学方法的正统地位后，在几乎所有的领域，他们都发现了醉汉的脚步存在的证据，如在清理非洲丛林以消灭蚊虫的研究中，在制造尼龙的化学反应中，在塑料的形成过程中，在自由量子粒子的运动中，在股票价格的变化中，甚至在千古以来的智力进化中，都是如此。在第 10 章，我们将一起来看一看，随机性对我们自身的生命之路造成了哪些影响。正如我们将要看到的那样，尽管随机变化中存在着有规律的模式，但并非所有模式都是有意义的。当模式中存在意义时，我们应该去发现它。同样重要的是，当模式实际上并无意义时，我们就不该试图去抽取什么「意义」。要避免对随机模式产生「有意义」的错觉，可不是件容易的事情，而这构成了本书下一章的主题。