## 记忆时间

## 目录

0701 Speed: Parallel Computers

0801 Computers That Learn and Adapt

0901 Beyond Engineering

## 0701. Speed: Parallel Computers

B esides differing in the amount of memory, one type of universal computer can differ from another in the speed with which its operations are carried out. The speed of a computer is generally governed by the amount of time it takes to move data in and out of memory.

The sort of computers we have been talking about so far are sequential computers — they operate on one word of data at a time. The reason that conventional computers operate this way is largely historical. In the late 1940s and early 1950s, when computers were being developed, the switching elements (relays and vacuum tubes) were expensive but relatively fast. The memory elements (mercury delay lines, magnetic drums) were relatively cheap and slow. They were especially suitable for producing sequential streams of data. Computers were designed to keep the expensive processor switches as usefully busy as possible, while not placing too much demand on the speed of the memory. These early computers were room-size, with the expensive processor on one side and the slow memory on the other. A trickle of data flowed between.

As computer technology improved, software grew increasingly complex and expensive and it became increasingly difficult to train programmers, so in order to preserve the investment in software and training, the basic structure of computers remained unchanged. There was little motivation to rethink the two-part design, because technology was progressing at such a rapid rate that it was easy to build faster and cheaper machines just by re-creating the same kind of computer in a new technology.

The speed of computers tended to double every two years. Vacuum tubes were replaced by transistors and eventually by integrated circuits. Delay-line memories were replaced by magnetic-core memories, and then also by integrated circuits. The room-size machines shrank to a silicon chip the size of a thumbnail. Through all this changing technology, the simple processor-connected-to-memory design stayed the same. If you look at a modern single-chip computer under a microscope, you can still see the vestiges of the room full of vacuum tubes: one area of the chip is devoted to processing, another to memory. This is true in spite of the fact that the processor and memory parts of the computer are now made by the same methods, often on the same piece of silicon. The patterns of activity are still optimized for the earlier, two-part design. The section of the silicon chip that implements processing is kept very busy; the part that implements memory still trickles out data one word at a time.

The flow of data between processor and memory is the bottleneck of a sequential computer. The root problem is that the memory is designed to access a single location on each instruction cycle. As long as we stick with this fundamental design, the only way to increase the speed of the computer is to reduce the cycle time. For many years, the cycle time of computers could be reduced by increasing the speed of the switches: faster switches led to faster computers. Now this is no longer such an effective strategy. The speed of today's large computers is limited primarily by the time required to propagate information along their wires, and that in turn is limited by the finite speed of light. Light travels at about a foot per nanosecond (a billionth of a second). The cycle time of the fastest computers today is about a nanosecond, and it is no coincidence that their processors are less than a foot across. We are approaching the limits of how much we can speed up the computer without changing the basic design.

0701 速度：并行计算机

除了存储空间不同之外，不同类型的通用计算机的运算速度也有所不同。计算机的速度通常取决于将数据写入和移出存储器所花费的时间。

目前为止，我们所讨论的计算机都是顺序计算机，即它们每次只能处理一个计算机字长的数据。传统计算机之所以以这种方式运行，主要是历史原因使然。20 世纪 40 年代末和 50 年代初期，当时计算机刚被发明，开关元件（继电器和真空管）的价格非常昂贵，但速度相对较快；存储元件（汞延迟线和磁鼓）的价格低廉，但运行速度较慢。它们的组合特别适合处理连续的数据流。设计者让计算机昂贵的处理器始终处于忙碌状态，但对存储速度没有提出过多要求。这些早期的计算机有一间房那么大，其中一边是昂贵的处理器，另一边是运行缓慢的存储器，有少量数据在两者之间流动。

随着计算机技术的进步，软件变得越来越复杂和昂贵，培训程序员的难度也变得越来越大。因此，为了节省在软件和培训方面的投入，计算机的基本架构基本没变。重新思考这种双边设计结构的动力不够强大，因为技术的进步非常之快，通过新技术便能轻易地制造出运行速度更快、价格更便宜的计算机。

计算机的运行速度通常每两年就会翻一倍。晶体管取代了真空管，最后又被集成电路替代。磁芯存储器取代了延迟线存储器，最终也被集成电路替代。房间般大小的机器最终被缩小为只有指甲盖大小的硅芯片。在这一系列的工艺变革过程中，处理器与存储器相连的设计模式一直保持不变。如果你在显微镜下观察现代的单片机，依然可以看到「满屋」真空管的痕迹：芯片的一部分区域用于处理，另一部分区域用于存储。尽管现在计算机的处理器和存储器在制造工艺上是相同的，且两者都位于同一块硅芯片上，但其工作模式只是原设计的优化而已，仍为早期的两部分，彼此隔开。然而，用作处理功能的那部分芯片一直非常繁忙，而用于存储功能的那部分芯片仍然每次只能输出一个计算机字长的数据。

处理器和存储器之间的数据流速度是顺序计算机的瓶颈所在，其根本问题在于，存储器在每个指令周期内只能访问单个地址。只要我们维持这种基本的设计架构，那么提高计算机运行速度的唯一方法就是，缩短每一个指令周期的时间。多年以来，我们通过提高开关的速度来缩短计算机的指令周期，即更快的开关能使计算机的运行速度更快。然而，这种策略不再有效。如今，大型计算机的运算速度主要受限于信息在线路中的传播用时，而这一点又受到光速的限制。光以每纳秒（十亿分之一秒）一英尺（1 英尺 = 0.304 8 米）的速度传播。如今，速度最快的计算机的指令周期约为一纳秒，所以这些计算机的处理器尺寸小于一英尺并非巧合。在不修改基础设计架构的前提下，我们已经接近计算机的速度极限了。

### 7.1 Parallelism

To work any faster, today's computers need to do more than one operation at once. We can accomplish this by breaking up the computer memory into lots of little memories and giving each its own processor. Such a machine is called a parallel computer. Parallel computers are practical because of the low cost and small size of microprocessors. We can build a parallel computer by hooking together dozens, hundreds, or even thousands of these smaller processors. The fastest computers in the world are massively parallel computers, which use thousands or even tens of thousands of processors.

As I have earlier described, computers are constructed in a hierarchy of building blocks, with each level of the hierarchy serving as the step to the level above. Parallel computers are the obvious next level in this scheme, with the computers themselves as building blocks. Such a construction can be called either a parallel computer or a computer network. The distinction between the two is somewhat arbitrary and has more to do with how the system is used than how it works. Usually a parallel computer is in one location, whereas a network is spread out geographically, but there are exceptions in both these rules. Generally, if a group of connected computers is used in a coordinated fashion, we call it a parallel computer. If the computers are used somewhat independently, we call the connected computers a computer network.

Putting a large number of computers together to achieve greater speed seems like an obvious step, but for many years the consensus among computer scientists was that it would work for only a few applications. I spent a large part of my early career arguing with people who believed it was impractical, or even impossible, to build and program general-purpose massively parallel computers. This widespread skepticism was based on two misapprehensions — one about how complex such a system would have to be, and the other about how the components of such a system would work together.

Scientists tended to overestimate the complexity of parallel computers, because they underestimated — or at least underappreciated — the rate of improvement in microelectronics-fabrication technology. It was not so much that they were ignorant of this trend as that the rate of technological change was unprecedented in history; it was thus extremely difficult for expectations and intuitions to keep abreast of the change. I remember giving a talk at a computer conference at the New York Hilton in the mid 1970s, in which I pointed out that current trends indicated that there would soon be more microprocessors than people in the United States. This was considered an outrageous extrapolation at the time. Although microprocessors had already been produced, the popular view of a computer was still that of a large set of refrigerator-size cabinets with blinking lights. In the question period at the end of the talk, one of my ill-disposed listeners asked, in a voice laden with sarcasm,「Just what do you think people are going to do with all these computers? It's not as if you needed a computer in every doorknob!」The audience burst out laughing, and I was at a loss for an answer, but as a matter of fact in that same hotel today, every doorknob contains a microprocessor that controls the lock.

Another reason that people were skeptical about parallel computers was more subtle, and also more valid. It was the perceived inefficiency of breaking a computation into many concurrent parts. This problem continues to limit the application of parallel computers today, but it is not nearly as serious a limitation as it was once thought to be. Part of the reason for the overestimation of the difficulty was a series of misleading experiences with early parallel machines. The first parallel digital computers were built in the 1960s by connecting two or three large sequential computers. In most cases, the multiple processing units shared a single memory, so that each of the processors could access the same data. These early parallel computers were usually programmed to give each processor a different task: for example, in a database application, one processor would retrieve the records, another processor would tabulate statistics, and the third would print out the results. The processors were used much as different operators on an assembly line, each doing one stage of the computation.

There were several inefficiencies inherent in this scheme, all of which seemed to grow along with the number of processors. One inefficiency was that the task had to be divided up into more or less independent stages. While it was often possible to divide tasks into two or three stages, it was difficult to see how to divide one task into ten or a hundred stages. As one detractor of parallel processing explained it to a newspaper reporter,「Well, two reporters might be able to write a newspaper article faster by having one of them gather the news while another writes the story, but a hundred reporters working on the article together would probably not be able to get it written at all.」Such arguments were pretty convincing.

Another inefficiency stemmed from the shared access to a single memory. A typical memory could retrieve only one word at a time from a given area of memory, and this limitation on the rate of access created an obvious bottleneck in the system, which limited its performance. If more processors were added to a system already limited by its fetch rate, the processors would all end up spending more time waiting for data, and the efficiency of the system would decrease.

Moreover, processors had to be careful not to create an inconsistency by altering data that another processor was looking at. For example, consider an airline-reservation system. If one processor is working on the problem of reserving seats, it looks to see if a seat is empty and then reserves the seat if it is. If two processors are booking seats for two different passengers simultaneously, there is a potential problem: they may both notice that the same seat is empty and decide to reserve it before either has a chance to mark it as taken. To avoid this kind of mishap, a processor had to go through an elaborate sequence that locked out other processors from accessing data while that processor was looking at a data word. This further aggravated the inefficiencies attendant on the competition for the system's memory, and in the worst case it would reduce the speed of a multiprocessor system to the speed of a single processor — and even to less than the speed of a single processor. As noted, these inefficiencies worsened as the number of processors increased.

The final source of inefficiency appeared to be even more fundamental: the difficulty of balancing the tasks assigned to the various processors. To return to the assembly-line analogy: we can see that the rate of computation will be governed by the speed of the slowest step. If there is just one slow operation, the rate of computation is set by that single operation. It is not unreasonable to expect that in this case, too, the efficiency of the system will decrease as we increase the number of processors.

The best formulation of these inefficiency problems is known as Amdahl's Law, after Gene Amdahl, the computer designer who came up with it in the 1960s. Amdahl's argument went something like this: There will always be a part of the computation which is inherently sequential — work that can be done by only one processor at a time. Even if only 10 percent of the computation is inherently sequential, no matter how much you speed up the remaining 90 percent, the computation as a whole will never speed up by more than a factor of 10. The processors working on the 90 percent that can be done in parallel will end up waiting for the single processor to finish the sequential 10 percent of the task. This argument suggests that a parallel computer with 1,000 processors will be extremely inefficient, since it will be only about ten times faster than a single processor. When I was trying to get funding to build my first parallel computer — a massively parallel computer with 64,000 processors — the first question I usually got at the end of one of my pitches was「Haven't you ever heard of Amdahl's Law?」

Of course, I had heard of Amdahl's Law, and to tell you the truth I saw nothing wrong with the reasoning behind it. Yet I knew for certain, even though I couldn't prove it, that Amdahl's Law did not apply to the problems I was trying to solve. The reason I was so confident is that the problems I was working on had already been solved on a massively parallel processor — the human brain. I was a student at the Artificial Intelligence Laboratory at MIT, and I wanted to build a machine that could think.

When I first visited the MIT Artificial Intelligence Lab as a freshman undergraduate in 1974, the field of AI (as it was coming to be known) was in a state of explosive growth. The first programs that could follow simple instructions written in plain English were being developed, and a computer that understood human speech seemed just around the corner. Computers were excelling at games like chess, which had been considered too complicated for them just a few years earlier. Artificial vision systems were recognizing simple objects, like line drawings and piles of blocks. Computers were even passing calculus exams and solving simple analogy problems taken from IQ tests. Could general-purpose machine intelligence be all that far off?

But by the time I joined the laboratory as a graduate student a few years later, the problems were looking more difficult. The simple demonstrations had turned out to be just that. Although lots of new principles and powerful methods had been invented, applying them to larger, more complicated problems didn't seem to work. At least part of the problem lay with the speed limitations of computers. AI researchers found it unfruitful to extend their experiments to cases involving more data, because the computers were already slow, and adding more data just made them slower. It was frustrating, for example, to try to get a machine to recognize a pile of objects when recognizing a single object required hours of computing time.

The computers were slow because they were sequential; they could do only one thing at a time. A computer must look at a picture pixel by pixel; by contrast, a brain perceives an entire picture instantly and can simultaneously match what it sees to every image it knows. For this reason, a human being is much faster than a computer at recognizing objects, even though the neurons in the human visual system are much slower than the transistors in the computer. This difference in design inspired me, as it did many others, to look for ways of designing massively parallel computers — computers that could perform millions of operations concurrently and exploit parallelism more like the brain does. Because I knew that the brain was able to get fast performance from slow components, I also knew that Amdahl's Law did not always apply.

I now understand the flaw in Amdahl's argument. It lies in the assumption that a fixed portion of the computation, even just 10 percent, must be sequential. This estimate sounds plausible, but it turns out not to be true of most large computations. The false intuition came from a misunderstanding about how parallel processors would be used. The crux of the issue is in how the work of a computation is divided among the processors: it might seem at first that the best way to divide a computation among several processors is to give each a different part of the program to execute. This works to an extent, but (as the aforementioned journalistic analogy suggests) it suffers from the same drawbacks as assigning a task to a team of people: much of the potential concurrency is lost in the problems associated with coordination. Programming a computer by breaking up the program is like having a large team of people paint a fence by assigning one person to the job of opening the paint, another to preparing the surface, another to applying the paint, and another to cleaning the brushes. This functional breakdown requires a high degree of coordination, and after a certain point adding more people doesn't help speed up the task.

A more efficient way to use a parallel computer is to have each processor perform similar work, but on a different section of the data. This so-called data parallel decomposition of the task is like getting a fence painted by assigning each worker a separate section of fence. Not all problems break up as easily as painting a fence, but where large compuations are concerned this method works surprisingly well. For instance, image-processing tasks can be composed in concurrent parts by assigning a little patch of the image to each processor. A search problem, like playing chess, can be decomposed by having each processor simultaneously search different lines of play. In these examples, the degree of speed-up achieved is almost proportional to the number of processors — so the more of them the better. A little additional time must be spent dividing the problem among the processors and gluing the answers together, but if the problem is large the computation can be performed very efficiently, even on a parallel machine with tens of thousands of processors.

The computations just described can fairly obviously be decomposed to run in parallel, but data parallel decomposition also works on more complicated tasks. There are surprisingly few large problems that cannot be handled efficiently by parallel processing. Even problems that most people think are inherently sequential can usually be solved efficiently on a parallel computer. An example is the chain-following problem. My children play a game called Treasure Hunt, which is based on the chain-following problem. I give them a piece of paper with a clue about where the next clue is hidden. That clue leads to the next clue, and so on, until they reach the treasure at the end. In the computational version of this game, the program is given, as input, the address of a location in memory containing the address of another location. That location contains the address of still another, and so on. Eventually, the address specifies a memory location containing a special word that indicates it is the end of the chain. The problem is to find the last location from the first.

Initially, the chain-following problem looks like the epitome of an inherently sequential computation, because there seems to be no way for the computer to find the last location in the chain without following the linked addresses through the entire chain. The computer has to look at the first location to find the address of the second, then look at the second to find the address of the third, and so on. It turns out, however, that the problem can be solved in parallel. A parallel computer with a million processors can find the last element in a chain of a million addresses in twenty steps.

The trick is to divide the problem in half at every step, a bit like the approach used in the sorting algorithms in chapter 5. Assume that each of the million memory locations has its own processor, which can send a message to any other processor. To find the end of the chain, every processor begins by sending its own address to the processor that follows it in the chain — that is, the processor whose address is stored in its memory location. Each processor then knows the address not only of the processor that comes after it but also of the one that precedes it. The processor uses this information to send the address of its successor to its predecessor. Now each processor knows the address of the processor that lies two ahead of it in the chain, so now the chain connecting the first processor to the last processor is half the length it was originally. This reduction step is then repeated, and each time it is repeated, the length of the chain is halved. After twenty iterations of the reduction step, the first processor in a million-memory chain knows the address of the last. Similar methods can be applied to many other tasks that seem inherently sequential.

As of this writing, parallel computers are still relatively new, and it is not yet understood what other types of problems can be decomposed to efficiently take advantage of many processors. A rule of thumb seems to be that parallelism works best on problems with large amounts of data, because if there are lots of data elements there is plenty of similar work to divide among processors.

One reason that most computations can be decomposed into concurrent subproblems is that most computations are models of the physical world. Computations based on physical models can operate in parallel because the physical world operates in parallel. Computer graphics images, for example, are often synthesized by an algorithm that simulates the physical process of light reflecting off the surfaces of objects. The picture is drawn from a mathematical description of the shapes of the objects by calculating how each ray of light would bounce from surface to surface while traveling from the source to the eye. The calculation of all the light rays can proceed concurrently, because the bouncing of light proceeds concurrently in the physical world.

A typical example of the kind of computation well suited to a parallel computer is a simulation of the atmosphere, used in predicting the weather. The three-dimensional array of numbers that represents the atmosphere is analogous to three-dimensional physical space. Each number specifies the physical parameter of a certain volume of air — for example, the pressure in a cube of atmosphere 1 kilometer on a side. Each of these cubes will be represented by a few numbers specifying average temperature, pressure, wind velocity, and humidity. To predict how the volume of air in one such cube will evolve, the computer calculates how air flows between neighboring volumes: for instance, if more air flows into a volume than out of it, then the pressure in that volume will increase. The computer also calculates the changes attributable to factors such as sunlight and evaporation. The atmospheric simulation is calculated in a series of steps, each of which corresponds to a small increment of time — say, half an hour — so that the flow of simulated air and water among the cells in the array is analogous to the flow of real air and water in the pattern of weather. The result is a kind of three-dimensional moving picture inside the computer — a picture that behaves according to physical laws.

Of course, the accuracy of this simulation will depend both on the resolution and the accuracy of the three-dimensional image, which accounts for the notorious inaccuracy of weather predictions over time. If the resolution of the model is increased and the initial conditions are measured more accurately, then the prediction will be better — although even a very high resolution will never be perfect over a long period of time, because the initial state of the atmosphere cannot be measured with exact precision. Like the game of roulette, weather systems are chaotic, so a small change in initial conditions will produce a significant change in the outcome. On a parallel computer, each processor can be assigned the responsibility for predicting the weather in a tiny area. When wind blows from one area to the next, then the processors modeling those areas must communicate. Processors modeling geographically separated areas can proceed almost independently, in parallel, because the weather in these areas is almost independent. The computation is local and concurrent, because the physics that governs the weather is also local and concurrent.

While the weather simulation is linked to physical law in an obvious manner, many other computations are linked more subtly to the physical world. For instance, calculating telephone bills is concurrent, because telephones (and telephone customers) operate independently in the physical world. The only problems we don't know how to solve efficiently on a parallel computer are those for which the growing dimension of the problem is analogous to the passage of time. An example is the problem of predicting the future positions of the planets. (Ironically, this is the very problem for which many of our mathematical tools of computation were originally invented.)

The paths of the planets are the consequence of well-defined rules of momentum and gravitational interactions between the nine planets and the Sun. (For simplicity's sake, we will ignore the effects of small bodies, such as moons and asteroids.) All the information necessary to solve the problem can be represented by nine coordinates, so there isn't much data. The computational difficulty of the problem comes from the fact that the calculation must be made, as far as we know, by calculating the successive positions of the planets at each of billions of tiny steps, each representing a short period of time. The only way we know how to calculate the positions of the planets a million years in the future is to calculate their position at each intermediate time between now and then. If there is a trick to solving this problem concurrently, such as the one used in the chain-following problem, I am not aware of it. On the other hand, as far as I know, no one has proved that this orbit problem is inherently sequential. It remains an open question.

Highly parallel computers are now fairly common. They are used mostly in very large numerical calculations (like the weather simulation) or in large database calculations, such as extracting marketing data from credit card transactions. Since parallel computers are built of the same parts as personal computers, they are likely to become less expensive and more common with time. One of the most interesting parallel computers today is the one that is emerging almost by accident from the networking of sequential machines. The worldwide network of computers called the Internet is still used primarily as a communications system for people. The computers act mostly as a medium — storing and delivering information (like electronic mail) that is meaningful only to humans. I am convinced that this will change. Already standards are beginning to emerge that allow these computers to exchange programs as well as data. The computers on the Internet, working together, have a potential computational capability that far surpasses any individual computer that has ever been constructed.

I believe that eventually the Internet will grow to include the computers embedded in telephone systems, automobiles, and simple home appliances. Such machines will read their inputs directly from the physical world rather than relying on humans as intermediaries. As the information available on the Internet becomes richer, and the types of interaction among the connected computers become more complex, I expect that the Internet will begin to exhibit emergent behavior going beyond any that has been explicitly programmed into the system. In fact the Internet is already beginning to show signs of emergent behavior, but so far most of it is pretty simple: plagues of computer viruses and unpredicted patterns of message routing. As computers on the network begin to exchange interacting programs instead of just electronic mail, I suspect that the Internet will start to behave less like a network and more like a parallel computer. I suspect that the emergent behavior of the Internet will get a good deal more interesting.

并行性

为了达到更快的处理速度，现在的计算机需要同时执行多个操作。为了实现这个目标，我们可以将计算机存储器分为多份，并为每一份提供独立的处理器。这种计算机被称为并行计算机。由于微处理器的成本低廉、尺寸较小，所以并行计算机是实际可行的。我们可以将数十、数百甚至数千个小尺寸的处理器连接起来，组装成一台并行计算机。世界上最快的计算机是大规模并行计算机，它们所用的处理器数目高达数千甚至数万个。

如前所述，计算机是在构件的层次结构上搭建起来的，每层结构都是上层结构的基础。在这种模式中，计算机本身就属于基础构件，而并行计算机是其上面的一个层次。这种结构可被称为并行计算机或者计算机网络，两者之间并无明确的差别，也许其差别更多地与系统的应用有关，而非单个计算机的工作过程。一般来说，并行计算机位于一个地点，而计算机网络则在地理上是广泛分布的。不过，这两条规则都存在例外。通常，如果一组互联的计算机以协同的方式合作，我们便会称之为并行计算机。如果这些计算机在某种程度上是独立工作的，那么我们便将这些互联的计算机称为计算机网络。

将大量计算机连接起来获得更快的运算速度，这看起来似乎是显而易见的选择。不过，计算机科学家的共识是，这种结构只适用于少数几种应用场景。有些人认为，制造通用的大规模并行计算机并对其进行编程是不切实际，甚至是不可能实现的。在我职业生涯的早期，我花费了大量时间来反驳这种观点。这种普遍存在的怀疑源于两种误解，一种是误解了这个系统的复杂程度，另一种是误解这个系统各个组成部分之间的协同工作原理。

科学家倾向于高估并行计算机的复杂性，因为他们低估了或者至少没有重视微电子制造技术的进步速度。与其说他们对这一趋势一无所知，不如说这类技术变革的速度前所未有地快，致使人们很难跟上脚步。20 世纪 70 年代中期，我参加了纽约希尔顿酒店举行的一次计算机会议，我在发言中指出，当前的趋势表明，不久以后，美国的微处理器数目将会超过美国的人口总数。当时大家认为这是一种武断。尽管微处理器已经被生产出来了，但大众对计算机的普遍印象依然停留在闪着指示灯的冰箱大小的「柜子」上。在我报告结束后的提问环节，一名不怀善意的听众用充满讽刺的语气问道：「你认为人们将如何处理这么多计算机呢？看来不会是为每个门柄都安装一台计算机吧！」听众不禁大笑起来，我一下子回答不上来，事实上，现在这家酒店的每个门柄上都装有一个控制门锁的微处理器。

人们怀疑并行计算机的另一个理由更为微妙，也更有道理。大家都知道，将计算分成许多并行的部分，会导致运行效率低下。现在这个问题依然制约着并行计算机的应用效率，但它并没有想象中的那么严重。高估这个问题的难度的一部分原因源自对早期并行计算机的一系列误解。20 世纪 60 年代，第一批并行计算机是通过将两三个顺序计算机相连而组成的。在大多数情况下，多个并行处理单元共享一个存储器，以便每个处理器访问相同的数据。通过编程，这些早期的并行计算机可以给每个处理器分配一个不同的任务。例如，在数据库应用程序中，第一个处理器用于检索数据，第二个用于汇总统计数据，第三个则用于打印结果。这些处理器相当于生产线上的不同工人，每个工人负责完成一个环节的计算任务。

这种方法存在固有的低效率问题，其严重程度会随着处理器数目的增加而增加。第一个低效率问题源于任务必须被分解为或多或少的独立阶段。任务被分解为两三个阶段不成问题，但若想将其分解为 10 个或者 100 个阶段，就非常困难了。正如并行计算的质疑者向一名报纸记者解释的那样：「如果让一名记者收集新闻素材，让另一位记者撰写新闻文章，那么这两位记者可能会很快写完一篇文章。但如果 100 名记者都在撰写这篇文章，可能就根本无法完成工作。」这种说法非常具有说服力。

第二个低效率问题源于存储器的共享访问模式。典型的存储器每次只能从给定区域内检索一个计算机字，这种受限的访问速度造成了明显的瓶颈，并限制了系统的性能。如果在读取速率已经受限的系统中增加更多处理器，那么处理器将在等待数据方面花费更多时间，系统效率将会更低。

此外，处理器必须格外小心地避免某些情况造成的不一致性，比如修改另一个处理器正在查看的数据。以航空公司的订票系统为例，如果一个处理器正在进行订座操作，它会查看某个座位是否为空，如果为空就会预订此座位。如果两个处理器同时为不同的乘客预订座位，就会带来一个问题：它们可能会同时发现有一个座位是空的，并且在对方还未占据之前都决定将这个座位标记为预留状态。为了避免出现这种问题，处理器必须采取一系列周密的操作，使某个处理器在查询数据时避免其他处理器修改该数据。这种对系统存储器的争夺会进一步降低效率，可能导致的最坏情况是，多处理器系统的速度将会降低至单处理器的速度，甚至更低。如上所述，这些低效率问题会随着处理器数量的增加变得更为严峻。

第三个低效率问题的根源似乎更为本质，即如何将任务均衡地分配给不同的处理器。回到生产线的例子中，我们可以从中发现，计算的运行速度取决于速度最慢的环节。如果只存在一个慢速环节，那么计算的运行速度就会由该环节决定。在这种情况下，系统的效率会随着处理器数量的增加而降低。

安达尔定律完美地解释了这些低效率问题，该定律由计算机设计师吉恩·安达尔（Gene Amdahl）于 20 世纪 60 年代提出，并以其名命名。安达尔的结论如下：总有一部分计算具有内在的顺序性，它们每次只能由单个处理器完成。即使只有 10% 的计算任务，它们实质上也具有内在的顺序性，无论如何加速剩余 90% 的并行计算任务，整体计算速度的提升比例永远不会超过 10 倍。当处理器完成那 90% 的并行计算任务后，会继续等待单个处理器来完成按顺序执行的这 10% 的计算任务。这个结论表明，具有 1 000 个处理器的并行计算机的效率极低，因为它只会比单个处理器的速度快 10 倍左右。当我试图申请基金来建造我的第一个并行计算机（一台拥有 64000 个处理器的大型并行计算机）时，收到的第一个问题通常是：「你有没有听说过安达尔定律？」

2『安达尔定律，做一张术语卡片。（2021-10-11）』—— 已完成

我当然听说过安达尔定律，而且我认为这个定律背后的推理过程没有问题。然而，我也确信，安达尔定律并不适用于我试图解决的问题，即便我无法证明这一点。我之所以如此确信是因为，我正在研究的问题已经通过一台大规模并行计算机得到了解决，这台计算机便是人类的大脑。当我还是麻省理工学院人工智能实验室的一名学生时，就想制造一台可以思考的机器。

1974 年，当我以本科新生的身份第一次访问麻省理工学院人工智能实验室时，人工智能领域正处于爆炸性发展的阶段。第一代用简单的英文执行书写指令的程序正在开发中，能理解人类语言的计算机即将诞生；计算机在国际象棋等游戏中表现出色，而在几年前，这些游戏对它们来说还过于复杂；人工视觉系统能识别出简单的物体，例如线条画和成堆的积木；计算机甚至通过了简单的微积分测试，并解决了智商测试中的一些简单问题。通用人工智能真的离我们遥遥无期吗？

几年后，当我以研究生的身份加入人工智能实验室时，问题看起来变得更加复杂了。一些简单的演示表明情况确实如此。尽管研究人员发明了许多崭新的原理和强大的工具，但当应用于更大规模、更复杂的问题时，它们并不奏效。其中有部分问题的解决受限于计算机的运行速度。人工智能的研究人员发现：将实验推广至涉及更多数据的场合时常常徒劳无获，因为计算机的运行速度已经够慢了，增加更多的数据只会拖慢它们的速度。例如，在计算机识别单个物体就需要数小时的情况下，再让计算机去识别一堆物体，结果无疑会令人感到沮丧。

计算机的运行速度之所以很慢是因为，它们是按顺序执行的，也就是说，它们每次只能做一件事情，比如计算机必须逐个像素地查看一幅图像。相比之下，人类大脑能瞬间感知整幅图像，并立即将看到的图像和已知的图像进行匹配。正是由于这个原因，人类在识别物体方面比计算机快得多，即便人类视觉系统中的神经元比计算机中的晶体管慢得多。这种设计上的差异激发了我以及其他许多人去寻找大规模并行计算机的设计方法，这类计算机可以同时执行数百万次运算，并且能够像大脑那样利用并行性。既然大脑能够从低速的部件中获得高速的性能，因此，我认为安达尔定律并非适用于所有情形。

现在，我知道了安达尔定律的缺陷，那就是，它假设计算任务中有固定比例的任务一定是按顺序执行的，即使只有 10%。这个假设看似合理，但事实上，大多数大规模计算并非如此。这种错误的直觉来源于对并行处理器使用方式的误解。问题的关键在于，如何在处理器之间分配计算任务。初看起来，最佳分配方式是让每个处理器分别执行程序中的不同部分。这种方式在一定程度上是有效的。然而，这就类似于向一组队伍分配任务时遇到的问题（正如前面提到的记者写稿的类比），具有如下缺点：大部分潜在的并行性都会消失于与协调相关的问题中。通过分解程序的方式对计算机进行编程，就类似于协调一大群人粉刷篱笆，即让第一个人打开油漆桶，让第二个人来处理篱笆表面，让第三个人来粉刷油漆，让第四个人来清洗刷子。这个分解过程需要高度的协调性，而且到一定程度之后，增加更多的人手并不能加快任务的完成速度。

另一种更有效地使用并行计算机的方式是，让每个处理器执行相似的任务，但使用不同部分的数据。这种所谓的数据并行分解方法类似于在粉刷篱笆的任务中为每个工人分配一块独立的篱笆。虽然并非所有的问题都像粉刷篱笆一样容易分解，但这种方法在大型计算任务中的应用效果非常好。例如，通过给每个处理器分配一小块图像，图像处理任务就可以以并行的方式被完成；在国际象棋这样的搜索问题中，通过让每个处理器同时搜索不同的走法，便可以实现任务的并行分解。在这些例子中，速度的提升几乎与处理器的数量成正比，即处理器数量越多，效果越好。当然，给处理器分配任务以及收集处理器的答案会花费额外的时间。如果问题的规模很大，计算任务的完成情况会更加高效，即便有成千上万个处理器并行执行任务。

很明显，上述这些计算任务可以在分解后并行执行。对于更复杂的任务，数据并行分解的方法同样行之有效。令人感到惊讶的是，无法以并行计算的方式处理的大型问题寥寥无几。大部分人认为的具有顺序性的计算问题也能通过并行计算机得到有效解决。其中一个例子是追踪链问题。我的小孩曾玩过的寻宝游戏就是一个基于追踪链的问题。我给他们一张纸条，纸条上的线索与下一条线索的隐藏地点相关，而且那条线索又指向了下一条线索，以此类推，直到他们最后找到宝藏。在这个游戏的计算机版本中，对于给定的程序，其输入是存储器中的一个位置的地址，这个位置存储了另一个位置的地址，而后面这个位置存储的依然是下一个位置的地址，以此类推。最后，包含特殊的计算机字的地址会指定某个存储位置，这个特殊的计算机字会指示该地址就是地址链条的末端。这个问题就是要从第一个位置出发找到最后一个位置。

初看起来，追踪链问题是一个典型的具有顺序性的计算问题，因为如果计算机不沿着整个链条追踪相连的地址，就无法找到链条的最后一个位置。为了找到第二个位置的地址，计算机必须找到第一个位置，为了找到第三个位置的地址，计算机必须找到第二个位置，以此类推。然而事实证明，这个问题可以通过并行的方法解决。具有 100 万个处理器的并行计算机可以在 20 个步骤之内找到一条包含 100 万个地址的链条的最后一个位置。

上述过程的窍门在于，每一步都将问题的规模缩小一半，这和第 5 章介绍的排序算法有些类似。假设 100 万个存储位置都有自己的处理器，并且可以给任何其他处理器发送信息。为了找到链条的末端，每个处理器首先会将自己的地址发送给链条中紧跟它的处理器，而紧跟它的后一个处理器的地址存储于前一个处理器的内存位置中。这样，每个处理器不仅知道了它后面的处理器的地址，还知道了前面的处理器的地址。然后，处理器利用此信息将它后面的处理器的地址发送给它前面的处理器。此时，每个处理器都知道了沿此链条后继下一个处理器的地址。因此，此时连接第一个和最后一个处理器的链条长度与原来的相比缩短了一半。接着重复该简化步骤，每重复一次，链条长度就会减半。经过 20 步的简化过程之后，在包含 100 万个存储地址的链条中，第一个处理器便知道了最后一个处理器的地址。类似的方法也可以应用于完成许多其他看似具有顺序性的计算任务。

在撰写本书时，并行计算机仍然是一种相对较新的技术。目前，我们尚不清楚何种类型的任务能被分解且有效利用多处理器的优势。不过，这里有一条经验：数据量大的问题最适合用并行技术来解决，因为当数据量很大时，处理器之间就能分配到许多相似的计算任务。

大多数计算任务能被分解成并行处理的子问题，原因之一是，大多数计算都基于物理世界的模型。这类计算可以通过并行的方式运行，因为物理世界的运行方式也是并行的。例如，计算机表示的图像通常通过算法合成，该算法模拟了光线从物理表面反射的过程。我们可以计算出每条光线从光源传输到眼睛，以及从一个表面反射到另一个表面的过程，由此从物体形状的数学描述中获得图像。所有关于光线的计算都可以同时进行，因为光在真实物理世界中是同时完成反射的。

适合并行计算的典型例子还有天气预报所需的大气模拟。代表大气的三维数字矩阵类似于三维物理空间。每个数字代表了一定体积的大气的某个物理参数，比如 1 立方千米单位容量内的大气压强。每个立方体都可以由几个代表平均温度、压力、风速以及湿度等物理量的数字表示。为了预测这些立方体中的大气将如何变化，计算机需要计算相邻空间内空气的流动过程。例如，如果某个空间内的空气流入量大于空气流出量，那么这个空间内的空气压力就会上升。计算机还会计算日照和水汽蒸发等因素带来的变化量。大气的模拟过程由一系列计算而来，每个步骤都对应着一段时间，比如半小时，因此在矩阵的单元之间模拟出的空气和水的流动情况类似于真实天气中空气和水的流动情况。计算机的最终模拟结果便是一种三维的移动图像，一种按物理规律变化的图像。

当然，模拟精度取决于三维图像的分辨率和准确度，这些因素是导致天气预报随着时间的推移而变得不准确的罪魁祸首。如果模型分辨率越高，初始条件测量得越精确，则预测结果就会越准确，但即使分辨率再高，从长远来看，天气预报也永远不可能达到百分之百的准确度，因为天气的初始状态不可能被精准无误地测量出来。和轮盘机游戏一样，天气系统是一种混沌系统，初始条件的微小扰动就能使结果产生巨大变化。在并行计算机中，每个处理器都可以负责预测小块区域的天气。当风从一块区域吹向另一块区域时，对这些区域进行建模的处理器之间必须进行通信。那些对地理上分离的区域进行建模的处理器可以独立、并行地运行，因为这些区域的天气之间几乎没有关系。模拟计算既可以是局部的，也可以是并行的，因为控制天气的物理定律也具有这两种特性。

天气模拟和物理定律之间显然存在联系。许多其他计算任务和物理世界之间的联系更为微妙。例如，电话费用的结算方法是并行的，因为电话和所对应的客户在物理世界中是独立运行的。只有一类问题我们不知道如何在并行计算机中有效地解决，即那些规模随着时间的推移而不断变大的问题，比如预测行星的轨迹。具有讽刺意味的是，最初正是为了解决这个问题，许多数学计算工具才被发明出来。

行星的轨道是九大行星和太阳之间动量和引力相互作用法则的结果。为了简单起见，我们将忽略诸如月球和小行星等小型天体的影响。我们可以用 9 个坐标来表示解决这个问题所需的全部信息，因此数据量并不大。这个问题的计算难度在于如下事实：进行运算时需要计算出行星的连续轨迹。而这个过程由数十亿个小阶段组成，每个阶段都代表了一段很短的时间。我们知道的唯一能计算出行星在未来 100 万年后的位置的方法是，计算出它们从现在起每隔一个时间段后的位置。一方面，我不知道这个问题是否存在并行的解法，正如解决追踪链问题时使用的方法；另一方面，据我所知，没有人能证明，轨道预测问题在本质上是顺序性问题。因此，这是一个悬而未决的问题。

当今，高度并行的计算机已经相当普遍，主要应用于大型数值的计算（例如天气模拟）或者大型数据库的计算，例如从信用卡交易记录中提取市场营销数据。由于并行计算机和个人计算机的组成零件是相同的，因此随着时间的推移，它们会变得更加便宜和常见。最有趣的一种并行计算机碰巧是从顺序计算机网络中出现的。这个被称为互联网的全球计算机网络主要被用作通信系统。这些计算机主要起到了媒介的作用，即存储和发送那些仅对人们有意义的信息，比如电子邮件。我相信今后的情况会出现变化，因为现在已经开始出现允许计算机像交换数据那样交换程序的标准了。互联网上的计算机一起合作产生的潜在计算能力，远远超过了历史上的任何一台计算机。

我相信，互联网发展到最后，一定会将电话、汽车以及家用电器都嵌入计算机内。这些计算机将直接从物理世界中读取和输入信息，不再依赖人类作为中间人。随着互联网上的信息变得越来越丰富，以及相连的计算机之间的交互形式变得越来越复杂，我预测，互联网将会开始呈现出一些涌现行为（emergent behavior），这些行为会超出程序规定的系统行为范围。事实上，互联网已经开始显示出这种迹象。不过，到目前为止，大部分内容都十分简单，比如计算机病毒的蔓延、无法预测的信息路由模式等。随着网络中的计算机可以交换程序，而不仅是收发电子邮件，互联网的行为将会变得不像网络，而更像并行计算机。我还坚信，互联网中的涌现行为会变得更加有趣。

## 0801. Computers That Learn and Adapt

The computer programs I have so far described operate according to fixed rules supplied by the programmer. They have no way of inventing new rules themselves, or of improving the ones they are given. The chess-playing programs, if their programmers do not tinker with them, will keep making the same mistakes over and over, no matter how many games of chess they play. In this sense, computers are completely predictable; it is in this sense that computers can「do only what they are programmed to do」 — a point often raised by the defenders of humankind in the「man vs. machine」debate.

But not all software is this inflexible. It is possible to write programs that improve with experience. When they operate with such programs, computers can learn from their mistakes and correct their own errors. They accomplish this by making use of feedback. Any system based on feedback needs three kinds of information:

1 What is the desired state (the goal )?

2 What is the difference between the current state and the desired state (the error )?

3 What actions will reduce the difference between the current state and the goal state (the response ).

The feedback system adjusts the response, according to the error, to achieve the goal. The simplest and most familiar examples of feedback systems are not learning systems but control systems; the household thermostat is a good example. This feedback system recognizes only two possible errors and produces only two possible responses. The goal is to maintain a particular temperature, and the possible errors are that the temperature is either too hot or too cold. The responses are predetermined: if the temperature is too cold, the response is to turn on the furnace; if the temperature is too hot, the response is to turn the furnace off. Since the thermostat can only turn the furnace on or off, the response has nothing to do with the magnitude of the error. (This is a fact that I have repeatedly tried to explain to various members of my household, who insist on turning the thermostat all the way up to 90 degrees whenever the house is too cold, in hopes that somehow things will warm up faster; this tactic does not help. The thermostat can only turn the furnace on; it cannot turn it up.)

In principle, however, there is no reason why a home heating thermostat could not respond in proportion to the error. Such a system would require a method of adjusting the output of the furnace, rather than just turning it on or off. The apparatus would doubtless be more complicated and expensive, but it would ensure that the temperature was more precisely maintained. Such proportional control thermostats are used today in systems that control delicate industrial processes. Some household appliances — certain models of Japanese washing machines, for example — also use proportional control (or an approximation of it), a feature often advertised as fuzzy logic.

Another example of a system that uses proportional control is the automatic-pilot system that guides an airplane. The goal in this case is to keep the plane pointed in a given direction. A direction finder, such as a compass, measures the error in the direction the plane is traveling. The autopilot responds by making an adjustment in the position of the rudder of the plane in proportion to the size and direction of the error. Thus a slight deviation in course will result in only a slight change in the rudder position, but a major deviation, such as the swerving produced by a shift in the wind, will result in a large change in the setting of the rudder. If the automatic pilot system did not use proportional control but instead pushed the rudder all the way to the left or all the way to the right in the fashion of the home thermostat, the airplane would oscillate back and forth in an uncomfortable and probably dangerous manner.

In all of these feedback systems, the connection between error and response is fixed. The sensitivity of the response is predetermined by the design of the control system. But it is also possible to design an even more flexible feedback system, in which the response of the system adapts with time. In this case, the parameters of an initial feedback system are adjusted by a second feedback system. If the second feedback system adapts and improves over time, the system can be said to have「learned」the parameters of control.

Consider, for example, how a human pilot learns to fly an airplane. Typically, the student pilot oversteers at first — that is, overcorrects for every error. The pilot is using something like the system the thermostat uses to control the heat: if the plane is too far to the left, turn right; if it is too far to the right, turn left. Since there is a delay between turning the control rudder and the response of the plane, the system begins to oscillate. The pilot needs to learn how to move the rudder in proportion to the error, and this requires gauging the sensitivity of the response. The pilot learns this parameter through another feedback system; in this case, the goal of the feedback is to keep the plane on the correct course without oscillations, and the error is the degree of oscillation. The response is to adjust the response of the primary feedback system — that is, to adjust the amount that the control rudder is moved in order to correct a given erroneous angle in the plane's heading. Whenever the pilot's first feedback system is oscillating, he reduces its responsiveness. He increases its responsiveness if the plane begins to drift off course. Once the pilot learns the correct sensitivity, he can keep the plane on course without any oscillation.

It would be possible to build an automatic pilot that uses a second feedback system to adjust its own parameters, as described here. In this case, the autopilot could be said to have「learned」to fly the plane, in the same way that the human pilot learns. As far as I know, such adaptive autopilot systems are not used in real airplanes, but if they were they would have certain advantages. If the airplane sustained damage that caused a change in the responsiveness of the plane, such as a partly broken rudder, the autopilot would be able to adapt to this new situation. It might even be able to adapt if the connections to the rudder's control motor were accidentally reversed, so that the signal that normally turns the airplane right instead turned it left. Like a human pilot, the autopilot would require a fair amount of time to adjust to such a radical change in circumstances.

0801 能自我学习和进化的计算机

然而，并非所有的程序都是一成不变的。我们可以编写出随着经验的积累而不断完善的程序。当计算机运行这样的程序时，它们能够从错误中积累经验，并纠正问题。计算机可以通过反馈系统来实现这一功能。任何基于反馈机制的系统都需要如下三类信息：

1、什么是理想的状态（目标）？

2、当前状态和理想状态之间有什么差异（误差）？

3、采取什么样的行动会减少当前状态和理想状态之间的差异（响应）？

反馈系统根据误差来调整响应动作，以实现目标。最简单和最常见的反馈系统不是学习系统，而是控制系统，典型的例子就是家用暖气系统中的恒温器。该反馈系统只能识别两种类型的误差，以及采取两种类型的响应动作。该系统的目标是维持特定的室内温度，两种可能的误差分别是室温太高和室温太低。响应动作是预先确定的，即如果温度太低，启动电热装置；如果温度太高，则关闭电热装置。由于恒温器只能开启或者关闭电热装置，因此响应动作与误差的大小无关。我曾多次向我的家人解释这一事实。当房间里的温度变得很低时，他们坚持将恒温器调到最大值，希望让屋内更快暖和起来。然而，这个办法并不起作用，因为恒温器只能开启电热装置，并不能提高其温度。

然而，从原理上来说，家用暖气系统中的恒温器并不是不能按照误差比例来调节输出。若想实现这一点，系统还要能调节电热装置的输出大小，而不是只能开启或者关闭电热装置。这种装置无疑会变得更加复杂和昂贵，不过，它能够更加精准地控制室温。如今这种比例控制恒温器已用于控制复杂的工业过程系统。一些家用电器也采用了比例控制或者与之类似的方法，例如某些型号的日本洗衣机，这种特性通常被称为模糊逻辑（fuzzy logic）。

使用比例控制系统的另一个例子是飞机的自动驾驶系统，例如，我们的目标是让飞机保持固定的飞行方向。测向仪（例如罗盘）会测量飞机飞行方向的误差。自动驾驶仪会通过调整飞机的方向舵来做出响应，其大小和方向与误差的大小和方向成正比。因此，轻微的方向误差只会使方向舵发生轻微的变动，而巨大的方向误差会使方向舵发生较大的变动，例如，风向突然转变所导致的飞机大幅度转向。如果自动驾驶系统没有采用比例控制，而是和家用取暖系统中的恒温器一样，只能使方向舵左移或右移，那么飞机就会剧烈摇晃，而且也非常危险。

在上述的所有反馈系统中，误差和响应动作之间的关系是固定的。控制系统预先设定了响应的灵敏度。不过，也可以设计出一种更灵活的反馈系统，其响应动作可以随着时间的变化而变化。在这种情况下，第一个反馈系统的参数可以由第二个反馈系统来调整。如果第二个反馈系统随着时间的不断变化而改进，那么该系统便「学会」了控制参数。

我们以人类飞行员学习驾驶飞机的过程为例。在通常情况下，飞行员最初都会过度转向，也就是说，对每个误差都会矫枉过正。此时飞行员使用的策略和恒温器的控温系统类似：如果飞机向左偏得太远，则向右转；如果飞机向右偏得太远，就向左转。由于飞机转向舵的转向和飞机方向的改变之间存在时间上的延迟，因此系统开始左右摇晃。飞行员需要学习如何根据误差的大小按比例来改变方向舵，这需要估计方向舵的响应灵敏度。飞行员可以通过另一个反馈系统来掌握该灵敏度的参数。在这个反馈系统中，反馈的目标是让飞机保持正确的航向且不出现摇晃，系统和误差是摇晃的程度，系统的响应动作是调整第一个反馈系统的响应动作，也就是说，调整方向舵以纠正飞机航向偏离角所导致的位移量。当飞行员的第一个反馈系统开始摇晃时，第二个反馈系统就会减少响应输出；当飞机开始偏离航向时，就会增加响应输出。一旦飞行员适应了飞机的灵敏度，他就能让飞机在不出现任何摇晃的情况下保持正确航向。

综上所述，我们可以制造出利用第二个反馈系统来调整其自身参数的自动驾驶仪。在这种情况下，我们可以说这个自动驾驶仪「学会」了驾驶飞机，其学习方式与人类飞行员的学习方式一样。据我所知，真实的飞机上并没有使用这种自适应性自动驾驶系统。不过，如果它们能得到应用，一定具有优势。如果飞机受到了损坏，并导致飞机的响应性能发生故障，比如飞机转向舵部分失效，那么自动驾驶仪便能够应对这种特殊情况。如果飞机转向舵的控制电机意外反转，导致让飞机右转的信号反而让飞机左转，那么自动驾驶仪也能够应对这种情况。和人类飞行员一样，自动驾驶仪需要相当长的时间来适应环境中出现的剧烈变化。

### 8.1 Tranining the Computer

This basic notion of feedback is central to all learning systems, although it often takes a more complicated form than the self-adjusting automatic pilot. Often, feedback in computer programs is provided by training with the help of examples. The trainer (usually a human being) plays the role of a teacher, and the program becomes a student. A classical example of a trained learning system is a program written by the AI pioneer Patrick Winston, which learns the definition of concepts like「arch」from a series of positive and negative examples provided by an instructor. Winston's program learns new concepts by looking at simple line drawings of piles of blocks. The program is able to analyze such drawings and generate symbolic descriptions of the piles of blocks: for example,「Two touching cubes, supporting a wedge.」The trainer shows the program some examples of block configurations that form arches and another set of examples that do not, telling the program which are examples of「arch」and which are not. Initially, the program has no definition for the concept of「arch,」but as it is shown these positive and negative examples, it begins to formulate a working definition. Each time the program is shown a new example, it tests its working definition against the new example. If the definition sufficiently describes a positive example, or rules out a negative example, the program does not modify the definition. If the definition is in error, it is modified to fit the example.

Here is a scenario of how the program learns the definition of「arch」from a few examples. Assume that the first example the program is shown is a positive example: example A in Figure 25 , two upright rectangular blocks supporting a triangle. To start, the program will have to make an initial guess at formulating the definition of an arch. This initial guess does not need to be accurate, because it will be refined by future examples. Let's assume that the program uses the shapes of the blocks as its initial guess at a definition:「An arch is two rectangular blocks and a triangular block.」The second example the program is shown might be the same blocks, all lying down (example C in Figure 25 ). This is a negative example — that is, an example of something that is not an arch. Since the program's initial working definition mistakenly identifies this negative example as an arch, it will modify its definition to exclude the example. The program does this by identifying differences between the definition and the example and using them to add restrictions to the definition. In this case, the difference is in the relationships of the blocks, so an improved definition will include these relationships:「An arch is two upright rectangular blocks supporting a triangular block.」Now let's say the trainer supplies another positive example (B in Figure 25 ). This example uses a rectangular block at the top, instead of a triangular one. Since the program's working definition is not broad enough to include this positive example, the program will generalize its definition of「arch」to allow other shapes.

FIGURE 25 Positive and negative examples of arches

After being shown these examples and a few others, the program will converge on the following definition of an arch:「A prismatic body supported by two upright blocks that do not touch one another.」Each element of the definition has been learned by making some kind of mistake, and the definition has been adjusted accordingly. Once the program converges on the right definition, it stops making mistakes and leaves its definition unchanged. It can then correctly identify as an arch any arch it is shown, even if it has never seen that particular set of blocks before. It has learned the concept of「arch.」

训练计算机

反馈这一基本概念对所有学习系统来说都至关重要，尽管它通常比具备自动调整能力的自动驾驶系统更加复杂。通常来说，计算机程序中的反馈作用是通过不断训练样本而获得的。训练员（通常是人）扮演着教师的角色，程序则是学生。人工智能领域的先驱帕特里克·温斯顿（Patrick Winston）曾经编写过一个程序，它能从训练员提供的一系列正样本和负样本中学习「拱门」等概念，该程序是训练学习系统的经典案例。温斯顿设计的程序通过观察一组用简单的线条画成的块来学习新概念。该程序能够分析这些线条画的特点，并生成这些块的符号化描述。例如，「两个互相接触的立方体支撑着一个楔形体」。训练员会向程序展示一些构成拱门结构的模型示例，以及其他无法构成拱门结构的模型示例，以此告知程序哪些结构是「拱门」，哪些结构不是。最初，程序并没有关于「拱门」的定义，但当它看过这些正样本和负样本之后，便会开始形成有效的定义。每当给程序展示一个新样本时，它都会用这个定义来审核该样本。如果这个定义能准确地描述正样本或者排除负样本，那么程序就无须修改这个定义。如果这个定义出了错，就需要修改以匹配新样本。

下面这个例子说明了程序是如何从几个样本中学习「拱门」的定义的。假设展示给程序的第一个样本是一个正样本，即图 8-1 中的样本 A，其中，两个直立的长方体模块支撑着一个三角体模块。程序在形成拱门的定义时，首先需要做出一个初始猜测。这个初始猜测不必十分准确，因为它会根据未来的样本进行修改。假设程序将模块形状作为它对定义的初始猜测：拱门是两个长方体模块和一个三角体模块。假设展示给程序的第二个样本由相同的模块组成，但它们都倒在地面上，即图 8-1 中的样本 C，那么这便是一个负样本，也就是说，该样本不是一个拱门。然而，程序的初始定义却错误地将这个负样本识别为拱门，因此它需要改进定义，以将此样本排除在外。程序还可以识别出定义和样本之间的差异，并将此差异作为新的限制条件加入定义中。在这个例子中，两者之间的差异在于模块之间的位置关系，所以修正后的定义将包含这种关系：拱门是两个直立的长方体模块，上面支撑着一个三角体模块。假设现在训练员提供了另一个正样本，即图 8-1 中的样本 B。在这个样本中，顶部的模块为长方体模块，而非三角体模块。由于程序中的定义范围不够广泛，没有包含这个正样本，所以程序需要扩展其「拱门」的定义范围以涵盖其他形状。

广泛，没有包含这个正样本，所以程序需要扩展其「拱门」的定义范围以涵盖其他形状。

图 8-1 拱门的正样本和负样本

在给程序展示了各种样本之后，它会对拱门形成这样的定义：由两个互不接触的、直立的长方体支撑着一个棱形体。程序从错误中学习定义每个要素，并根据错误调整定义内容。一旦程序得到了正确的定义，它就不会再犯错误，定义也不会再有变动。此时，即使程序之前从未见过某个模块集，它也能够准确地识别出所有展示给它的拱门，因为它已经掌握了「拱门」的概念。

### 8.2 Neural Networks

Winston's program learns the concept of「arch,」but concepts like「touching,」「triangular block,」and「support」have been built into it from the beginning. Its representation of the world is specifically designed for piles of blocks. The search for a more general, universal representation scheme has led many researchers to computing systems with structures analogous to connected nets of biological neurons, such as occur in the brain. Such a system is called an artificial neural network.

A neural network is a simulated network of artificial neurons. This simulation may be performed on any kind of computer, but because the artificial neurons can operate concurrently, a parallel computer is the most natural place to execute it. Each artificial neuron has one output and a large number of inputs, perhaps hundreds or thousands. In the most common type of neural network, the signals between the neurons are binary — that is, either 1 or 0. The output of one neuron can be connected to the inputs of many others. Each input has a number associated with it, called its weight , which determines how much of an effect the input has upon the neuron's single output. This weight can be any number, positive or negative. The neuron's output is thus determined by a vote of the signals coming into its inputs, adjusted by the weights of the inputs. The neuron computes its output by multiplying each input signal by the input weight and summing the results; in other words, it adds up the weight of all the inputs that receive a 1 signal. If the weighted sum reaches a specific threshold, the output is 1; otherwise, the output is 0.

The function of an artificial neuron corresponds, very roughly, to the function of some types of real neurons in the brain. Real neurons also have one output and many inputs, and the input connections, called synapses , have different strengths (corresponding to the different input weights). A signal can either enhance or inhibit the firing of the neuron (corresponding to positive and negative weights), and the neuron will fire when the combined stimulation of the inputs is equal to or above some threshold. These are the senses in which an artificial neuron is analogous to a real one. There are also many ways in which a real neuron is much more complicated than an artificial one, but this simple artificial neuron is sufficient for building a system capable of learning.

The first thing to notice about artificial neurons is that they can be used to carry out the And, Or , and Invert operations. A neuron implements the Or function if the threshold is 1 and each of the input weights is equal to or greater than 1. A neuron with a threshold equal to the sum of the weights will implement the And function. Neurons with a single, negatively weighted input and a threshold of 0 will implement the Invert function. Since any logical function can be constructed by combining the And, Or , and Invert functions, a network of neurons can implement any Boolean function. Artificial neurons are universal building blocks.

We don't know very much about how the human brain works, but in some parts of the brain it seems that new information is learned by modifying the strength of the synapses that connect the neurons. This is certainly the case in the lower organisms on which we perform experiments — for example, sea snails. Sea snails can be taught certain conditioned responses, and it can be shown that they learn the response by changing the strength of the synaptic connections between neurons. Assuming that human learning works the same way, you are (I hope) adjusting the connections in your brain as you read this book.

A network of artificial neurons can「learn」by changing the weights of its connections. A good example is a very simple type of neural network called a perceptron , which can learn to recognize patterns. The way perceptrons learn is indicative of how most neural networks operate. A perceptron is a network with two layers of neurons and a single output. Each input in the first layer is connected to a sensing device like a light detector, which measures the brightness of one spot on an image. Each input of the second layer is connected to an output from the first layer, as shown in Figure 26.

Imagine that we are trying to teach the perception to recognize the letter A, which we will accomplish by showing it a large number of positive and negative examples of an A. The goal is for the perceptron to adjust the weights of the second layer so that its output will be 1 if, and only if, it is shown the image of an A. It accomplishes this by adjusting those weights whenever it makes an error. Each neuron in the first layer of the perceptron looks at a small patch of whatever example is being presented. Each of these first-layer neurons is programmed to recognize a specific local feature, such as a particular corner or a line at a particular orientation; it does so by means of the fixed weights of its own inputs. For example, here is a pattern of negative and positive input weights for the receptive field of a first-layer neuron programmed to recognize a corner, such as the point at the top of a capital A:

The first layer of the perception contains thousands of such feature-detecting neurons, each one programmed to recognize a particular kind of local feature in a particular part of the receptive field. This first layer of neurons detects features in the image which are useful for distinguishing between any letters; serifs are easy to detect, so they make letters more recognizable to the perceptron, just as they make a particular letter easier for the human eye to identify.

FIGURE 26 Perceptron

The local-feature detectors in the first layer provide the evidence, and the weights of the second layer determine how to weigh this evidence. For example, a corner pointing upward in the upper part of the image is evidence in favor of an A, while a corner pointing downward in the middle is evidence against. The perceptron learns by adjusting the weights on the inputs to the second layer. The learning algorithm is very simple: whenever the trainer indicates that the perceptron has made a mistake, the perceptron will adjust all of the weights of all the inputs that voted in favor of the mistake in such a way as to make future mistakes less likely. For instance, if the perceptron incorrectly identifies an image as an A, the weights of all the inputs that voted in favor of the false conclusion will be decreased. If the perceptron fails to identify a real A, then the inputs that voted in favor of the A will be increased. If the perceptron has enough feature detectors of the right type, this training method will eventually cause the perceptron to learn to recognize A's.

The learning procedure of the perceptron is another example of feedback. The goal is to set the weights correctly, the errors are misidentifications of the training examples, and the response is to adjust the weights. Notice that perceptrons, like Winston's arch program, learn only by making mistakes. This is a characteristic of all feedback-based learning systems. Given enough training, this particular procedure will always converge upon a correct choice of weights, assuming that there is a set of weights that does the job. This makes the perceptron seem like the perfect pattern-recognition machine, but the catch is the assumption that there exists some correct pattern of weights that will accomplish the task. To recognize the letter A in various sizes, fonts, and positions, the perceptron needs a very rich set of feature detectors in the first layer.

FIGURE 27 Perceptron spiral

Perceptrons can learn to recognize any letter if they are given enough features to work with, but there are some types of patterns, more complex than letters, that cannot be recognized by summing together local features in any way. For example, simply by summing up the evidence of local patches, a perceptron cannot tell whether or not all the dark spots in an image are connected, because connectedness is a global property; no local feature, by itself, can serve as evidence for or against connectedness. Figure 27 , adapted from Marvin Minsky and Seymour Papert's book Perceptrons , demonstrates that connectedness cannot always be assessed just by looking at local features.

For these and other reasons, two-layer perceptrons are not the most practical neural networks for recognizing most types of patterns. More general neural networks, with more layers, are able to recognize more complicated patterns. Such networks use similar procedures for learning. Trained neural networks of this type are often used for tasks like image recognition and speech recognition — tasks that are difficult to specify by a fixed set of rules. For instance, the simple word-recognition systems that are built into many children's toys today are based on neural networks.

神经网络

温斯顿设计的程序掌握了「拱门」的概念，不过，这是建立在其他诸如「接触」「三角体模块」「支撑」等概念从一开始就已输入程序的基础之上。这种表示方式是专为各种形状的模块设计的。为了寻求更通用的表示方案，许多研究人员开始研究类似于生物神经元网络（比如大脑中的神经网络）的计算系统，这种系统被称为人工神经网络。

人工神经网络是由人工神经元组成的模拟网络。该模拟任务可在任何类型的计算机上完成。由于人工神经元能够并行工作，因此并行计算机是执行该模拟任务的最佳工具。每个人工神经元都有一个输出和多个输入，其输入数目达到数百或者数千个。在最常见的神经网络类型中，神经元之间的信号是二进制的，即 1 或 0。一个神经元的输出可以连接至许多神经元的输入。神经元的每个输入有一个与之关联的数，被称为权重，它决定了该输入对神经元输出的影响程度。这个权重可以是任意数，正数和负数皆可。因此，神经元的输出由进入输入端的所有信号共同决定。神经元通过将每个输入信号值乘以输入权重，然后对所有的结果求和，最终得出输出。换句话说，它将所有接收到信号 1 的输入的权重都相加。如果权重之和达到某个阈值，则输出为 1；否则，输出为 0。

粗略地来说，人工神经元的功能相当于大脑中某类真正的神经元的功能。真正的神经元也拥有一个输出和多个输入，输入的连接点称为突触，它们的连接强度各不相同（对应于不同的输入权重）。信号可以强化或者抑制神经元的放电行为（对应于正数权重和负数权重），当输入的累积刺激等于或者高于某个阈值时，神经元就会放电。人工神经元和真正的神经元在上述几个方面是类似的。当然，真正的神经元比人工神经元复杂得多，不过，这种简单的人工神经元已经足以构建一种能够自我学习的系统了。

需要注意的是，人工神经元能用于执行「与」「或」「非」等逻辑运算。如果阈值为 1 且输入权重都等于或者大于 1，则人工神经元可以实现逻辑「或」的功能。如果一个人工神经元的输入权重之和等于阈值，便可以实现逻辑「与」的功能。如果人工神经元只有一个输入的权重为负且阈值为 0，则可以实现逻辑「非」的功能。由于通过「与」「或」「非」三种逻辑功能的组合可以构造出所有逻辑功能，因此神经元网络可以实现所有的布尔功能。人工神经元是一种通用构件。

我们还不太了解大脑是如何工作的，不过，某部分大脑似乎可以通过修改连接神经元的突触的强度来学习新知识。我们在低等生物（比如海螺）身上做了实验，情况就是如此。通过训练，海螺可以形成某些条件反射，并且证明了：它们是通过改变神经元之间突触的连接强度来获取这些条件反射的。假设人类的学习方式与之相同，那么当你阅读这本书时，你正在调整大脑神经元之间的连接，至少我希望是这样。

人工神经网络可以通过改变其连接的权重来进行学习。感知系统便是一个很好的例子，它是一种十分简单的神经网络，可以学会识别模式。感知系统的学习方式代表了大多数人工神经网络的运作方式。感知系统是一种具有两层神经元和单个输出的神经网络。第一层神经元的每个输入会连接到某个传感器上，例如用于测量图像上某点亮度的光线检测器；第二层神经元的每个输入与第一层神经元的输出相连，如图 8-2 所示。

图 8-2 感知系统

若想让感知系统识别出字母 A，我们就会通过给它展示字母「A」的大量正样本和负样本来实现。我们的目标是让感知系统调整第二层神经网络的权重，使得当且仅当给它展示字母 A 的图像时，其输出为 1。实现此目标的方法是，在它每次犯错后不断调整这些权重。无论给它展示何种样本，感知器第一层中的每个神经元都只关注其中的一小块区域。通过事先的设计，第一层神经网络中的每个神经元都能识别一种特定的局部特征，例如特定的角度或者特定方向的线条。神经元是依靠其本身输入的固定权重来实现这一点的。例如，图 8-3 展示的图像为第一层网络中某个神经元的感受野对应的权重模式，该感受野可以识别尖角，比如位于大写字母 A 顶部的一点。

图 8-3 某个神经元感受野对应的权重模式

感知系统中的第一层神经网络包含了数千个这样的特征检测神经元，每个神经元都能用于识别感受野中特定区域内的特定局部特征。第一层神经元检测图像中的特征，这些特征可用于区分不同的字母。字体中的衬线体易于检测，能让感知系统更容易地识别出字母，就像它们能使特定字母更容易被人眼识别出来一样。

第一层神经元中的局部特征检测器提供证据，第二层神经元中的权重决定如何衡量这些证据。例如，图像上部指向上方的尖角是有利于识别字母 A 的证据，而图像中部指向下方的尖角则是不利证据。感知系统通过调整第二层输入的权重进行学习。学习算法十分简单：一旦训练员向感知系统指出出现的错误，它就会调整引发此次错误的所有输入的权重，以降低未来出现同样错误的可能性。例如，如果感知系统错误地将其他图像识别为 A，那么在输入中所有支持这一错误结论的输入权重都将减小；如果感知系统没能识别出真正的字母 A，那么在输入中所有支持字母 A 的输入权重将会增加。如果感知系统拥有足够多合适的特征检测器，那么这种训练方法最终会教会它如何识别出真正的字母 A。

感知系统的学习过程是反馈系统的一个例子，其目标是设定正确的权重，误差是对培训样本的错判，响应动作是不断地调整权重。需要注意的是，与温斯顿设计的「拱门」识别程序一样，感知系统只能通过犯错来学习。这是所有基于反馈的自学习系统的共同特点。假设预期的权重（利用该权重即可准确地完成任务）确实存在，那么只要给予足够的训练，这个反馈过程将始终收敛于正确的权重。虽然从这一点来看，感知系统似乎是一种完美的模式识别器，但关键的问题在于，这是建立在假设（存在一组正确的、能完成任务的权重）成功的基础之上的。为了识别出各种大小、字体和位置的字母 A，感知系统需要在第一层神经网络中安装大量特征检测器。

如果给予感知系统足够多的特征，它们就能够学会识别所有字母。不过，对于一些比字母更复杂的模式，感知系统无法以任何方式将局部特征综合起来识别它们。例如，如果只是简单地综合局部的斑点信息，那么感知系统便无法判断图像中的所有斑点是不是连通的，因为连通性是全局属性，局部特征不能作为支持或者不支持连通性的证据。图 8-4 截自马文·明斯基和西摩·佩伯特（Seymour Papert）的著作《感知系统》（Perceptrons），这本书指出，仅通过观察局部特征不能判断连通性。

图 8-4 螺线感知系统

出于各种原因，在识别大多数模式时，双层感知系统并不是最实用的神经网络。拥有更多层数的更通用的神经网络能够识别更复杂的模式。这类神经网络采用了类似的学习策略，它们通常用于诸如图像和语音识别之类的任务，这些任务难以通过一组固定的规则来描述。例如，许多儿童玩具中的简易词语识别系统就是建立在神经网络基础之上的。

### 8.3 Self-Organizing Systems

The disadvantage of a learning system based on positive and negative examples is that it requires a trainer to classify the examples. There is another type of neural network, which does not require a trainer — or, to put it another way, there are networks in which the training signals are generated by the network itself. Such a self-training network is a self-organizing system. Self-organizing systems have been studied for years (Alan Turing published important work in this area), but there has been a recent renewal of research activity in such systems, and even some new progress, partly because of the availability of faster computers. Like trained neural networks, self-organizing systems are a natural fit for parallel computers.

As an example of a self-organizing system that works, consider the problem of transmitting an image from the eye to the brain (see Figure 28 ). The retina, on which the image is projected, is a two-dimensional sheet of light-sensitive neurons. The image on the retina is transformed into a similar projected image in the brain by a bundle of neurons that transfers the image. If this bundle is wired imperfectly, then the projected image will be slightly scrambled, with each pixel in slightly the wrong place. I will describe a self-organizing artificial neural network that can learn to unscramble such a picture, restoring each pixel to its proper position. The unscrambler consists of a single layer of neurons arranged in a two-dimensional array. The outputs of these neurons form the corrected image. If the picture is scrambled only slightly, then each pixel in the scrambled image will be in the general neighborhood of its correct position. Each neuron's inputs look at a neighborhood of pixels in the scrambled image, and the neuron learns which of these pixels should be connected to the output in order to produce the unscrambled image. The neuron forms the connection by setting the weight of the correct input to 1 and the weight of its other inputs to 0.

FIGURE 28 Eye, with scrambled nerve bundle and unscrambler

The training algorithm for the unscrambler is based on the fact that images have a nonrandom structure. As discussed earlier, real images are not just random arrays of dots but pictures of the world, so nearby areas of the image tend to look the same. The unscrambler turns this statement around, by assuming that pixels that tend to look the same ought therefore to be near one another. The neurons in the unscrambler work by measuring the correlation of each of their inputs with the outputs of the neighboring neurons during exposure to a series of images. Whenever a neuron makes an「error」by firing differently from its neighbors, the neuron increases the weight of the inputs that match the outputs of its neighbors and decreases the weights of the other inputs. Of course, its neighbors are also learning their connections at the same time, so in the beginning it is a case of (so to speak) the blind leading the blind, but eventually some of the unscrambler neurons begin locking onto their correct inputs and thus become effective trainers for their neighbors. Again, the only neurons that are adjusted are those that have made mistakes. As the neurons train one another, an unscrambled image begins to emerge in the outputs, and eventually the network organizes itself to produce an image of perfect clarity.

The self-adjusting autopilot, Patrick Winston's arch program, the perception, and the unscrambler are just a few examples of systems that learn. All these systems are based on either external or internal feedbacks, and all learn by correcting their mistakes. The design of each of these systems was inspired by a biological system of similar function. In harvesting these products of evolution, we are like the fool in Aesop's fable,「The Goose That Laid the Golden Egg,」who chooses the eggs instead of the goose. In the next chapter, we shall discuss the goose.

自组织系统

基于正样本和负样本的学习系统有一个缺点，即它需要训练员对这些样本进行分类。不过，存在一种不需要训练员的神经网络，这类网络可以自己生成训练信号。这种可以自动训练的神经网络是一种自组织系统。关于自组织系统的研究已经持续多年了，艾伦·图灵就在该领域取得过重要成果。最近，这个系统又出现了新的研究动向，并取得了一些进展，部分原因在于计算机的运行速度更快了。和神经网络一样，自组织系统非常适合并行计算机。

将图像从眼睛传输至大脑的过程是有效自组织系统的一个很好的例子（见图 8-5）。首先，图像被投影到视网膜上，视网膜是一张由感光神经元组成的薄膜。然后，视网膜上的图像被一束可以传递图像的神经元传导至大脑，产生一个相同的投射映象。如果这束神经元的连接存在问题，那么投影图像就会出现一些扭曲，即每个像素点都位于错误的位置。有一种自组织神经网络可以学会复原图像，将每个像素点复原至正确的位置。该复原系统由以二维阵列形式排列的单层神经元组成，这些神经元的输出形成了校正后的图像。如果图像的扭曲程度比较轻微，那么扭曲的图像中的像素点就位于正确的位置附近。每个神经元的输入会关注扭曲图像中的某个局部区域内的像素点，并学习应该将哪些像素点连接至输出以复原图像。神经元将正确输入的权重设置为 1，将其他输入的权重设置为 0，从而形成上述连接关系。

图 8-5 眼睛、偏离的视神经束以及复原系统

复原系统的训练算法基于如下事实：图像的结构是非随机的。如前所述，真实的图像并不是由像素点组成的随机阵列，而是物理世界的图景。因此，图像中的局部区域看起来是相同的。复原系统会反过来利用该结论，它假设，看起来相同的像素点应该彼此接近。在处理一系列图像的过程中，复原系统中的神经元通过测量其输入与相邻神经元的输出的相关性来进行工作。每当某个神经元的输出与其相邻神经元的输出有所不同时，即当它犯错时，这个神经元会增加与相邻神经元的输出相匹配的输入的权重，并降低其他输入的权重。当然，这个神经元的相邻神经元同时也在学习自己的连接关系。所以，开始阶段的情况就相当于盲人给盲人引路，但最终有一些神经元开始固定它们的正确输入，从而成为其相邻神经元的有效训练员。同样，唯一需要调整的神经元是那些犯错的神经元。当神经元彼此训练时，原始图像就开始出现在神经元的输出中，并且神经网络最后会进行自我组织，以生成一幅非常清晰的图像。

自动调整的自动驾驶仪、温斯顿设计的「拱门」识别程序、感知系统以及图像复原系统只是自我学习系统的其中几个例子。所有这些系统都是基于外部或者内部的反馈，并且可以通过纠正错误来不断学习。上述每个系统的设计都受到了具有类似功能的生物系统的启发。在收获这些科技发展成果时，我们就像伊索寓言《会下金蛋的鹅》中的傻瓜一样，选择了金蛋而非下金蛋的鹅。在下一章中，我们会讨论「鹅」的问题。

## 0901. Beyond Engineering

A ccording to legend, the thirteenth-century scientist and monk Roger Bacon was a dabbler in black magic, and once constructed a talking mechanical head. It is said that he wanted to defend England from invaders by building a wall around the kingdom, and he constructed the head in order to ask its advice about how to build the wall. Bacon fashioned the head out of brass, replicating the design of a human head in every detail. He heated it over a fire while uttering magical incantations — a process that went on for days. Eventually, the head awoke and began to talk. Unfortunately, Bacon was by that time so exhausted from casting spells that he had fallen asleep. His young assistant was unwilling to awaken the master for the mere ramblings of a brass head, and the head exploded over the fire before Bacon could ask it any questions.

The Bacon legend has elements in common with stories about other conjurers who constructed an artificial intelligence: Dedalus, Pygmalion, Albertus Magnus, the Rabbi of Prague. A theme common to many of these stories is that some form of cooking or ripening is necessary to make something start to think. In the days before computing machines, few imagined that a process as complex as thinking could ever be broken down into operations that could be implemented by mechanisms. Instead, the assumption was that if an intelligence were ever to be created, it would be by an emergent process — that is, by a process in which the complex behavior emerges as a global consequence of billions of tiny local interactions. It was assumed that what the conjurer needed was not the correct wiring diagram but the correct recipe, according to which the ingredients would organize themselves into an intelligence. Such a process would allow an intelligence to be created without the conjurer's understanding exactly how the process — or the intelligence itself — worked.

Oddly enough, I am in basic agreement with this prescientific notion: I believe that we may be able create an artificial intelligence long before we understand natural intelligence, and I suspect that the creation process will be one in which we arrange for intelligence to emerge from a complex series of interactions that we do not understand in detail — that is, a process less like engineering a machine and more like baking a cake or growing a garden. We will not engineer an artificial intelligence; rather, we will set up the right conditions under which an intelligence can emerge. The greatest achievement of our technology may well be the creation of tools that allow us to go beyond engineering — that allow us to create more than we can understand.

Before discussing how this emergent design process might work, let us consider our best example of intelligence: the human brain. Since the brain itself was「designed」 — by the emergent process of Darwinian evolution — it may be usefully compared with the engineered designs that we have considered so far.

0901 超越工程

传说在 13 世纪，有位名叫罗杰·培根（Roger Bacon）的科学家对巫术有所涉猎，他曾经制造了一颗会说话的机械头颅。传闻他想在王国周围建造一道城墙，用来抵御英格兰的侵略。他造此头颅就是为向它咨询如何修建这道城墙。培根的头颅是用黄铜做成的，其中每个细节都是模仿真人设计的。做成之后，他将这颗机械头颅架在火上烤，并口念咒语，这个过程持续了好几天。最终，这颗机械头颅苏醒了，开始说话了。不幸的是，此时培根由于施法而疲惫不堪，不由自主地睡着了，而年轻的徒弟不愿意因为一颗黄铜头颅的胡言乱语而叫醒沉睡的师父。最终，在培根向它提问之前，机械头颅在火中爆炸了。

除了培根，祈求于人工智能的人还有很多，包括德达拉斯（Dedalus）、皮格马利翁（Pygmalion）、艾伯塔斯·马格努斯（Albertus Magnus）以及布拉格的拉比学者等。这些祈求者的故事有一个共同的主题，即若想让某物具备思维能力，必须经过某种形式的熬炼和熟化。在计算机被发明之前，几乎没有人能想到，和思维一样复杂的过程能分解为可以通过装置实现的运算步骤。相反，当时的假设是，如果要创造出一种智能，就应该经历一个涌现的过程，即这样一种过程：复杂行为的涌现是数十亿局部而微小的交互行为的整体结果。这一假设的构想基础是：魔术师需要的不是正确的设计连线图，而是正确的配方。根据这个配方，各种成分可以自我组织形成智能，这个过程可以让魔术师在无法准确理解该过程或者智能本身是如何工作的情况下，创造出智能。

不过，我倒是基本上认同这种前科学的理念。我相信，远在理解自然智能之前，我们就能创造出人工智能。我也认为，智能最终会从一系列复杂的交互行为中产生，但我们可能并不了解这些交互行为本身的细节，也就是说，这个过程不同于设计工程机械，而更像是烤制蛋糕或者打理花园。我们不是去设计人工智能，而是营造一个有利于培育人工智能的良好环境。人类在技术上的最大成就可能就是，制造出了跳出工程设计思维的工具，使我们的创造力超越了理解力。

在讨论这个设计过程如何发挥作用之前，我们先来看看智能的最佳例子：人类的大脑。因为大脑本身是通过达尔文的进化过程「设计」出来的，所以将它和我们之前讨论的工程设计进行对比十分有意义。

### 9.1 The Brain

The human brain has about 10 12 neurons, and each neuron has, on average, 10 5 connections. The brain is to some degree a self-organizing system, but it would be wrong to think of it as a homogeneous mass. It contains hundreds of different types of neurons, many of which occur only in particular regions. Studies of brain tissue show that the patterns of neuronal connection, too, differ in the various regions of the brain: there are some fifty areas in which the pattern is recognizably different, and there are probably many more in which the differences in neural anatomy are too subtle for us to distinguish.

Each area of the brain is apparently specialized for a particular type of function, such as recognizing color in visual images, producing intonation in speech, or keeping track of the names of things. We know this because when specific areas are damaged by an accident or a stroke there is also a corresponding loss of function. For example, damage to areas 44 and 45 on the left side of the frontal lobe — together they are called Broca's area — often robs someone of the ability to produce grammatical speech. People so afflicted may still pronounce words clearly and they may understand the speech of others, but they will be unable to construct grammatical sentences. Damage to an area known as the annular gyrus, located a little farther toward the back of the head, causes difficulties in reading and writing; damage to yet other areas results in an inability to recall the names of familiar objects or recognize familiar faces.

It would be wrong to assume that the various areas of the brain are analogous to the functional building blocks of a computer. For one thing, damage to most areas will not cause a well-defined loss of function: removal of most of the right frontal lobe, for example, sometimes causes indefinable changes in personality and sometimes causes no noticeable change at all. Even in those cases in which the loss of function is well-defined, it is not at all evident that the function was performed entirely by the damaged area; it may be that the area just provided some minor element of support necessary for the function. An automobile with a dead battery will not be able to move, but we don't therefore assume that the battery is responsible for propelling the car.

There are certain areas of the brain — in particular, areas near the back of the head associated with visual processing — where we can actually make some sense of the pattern of connections: for instance, those involved in receiving the inputs from the left and right eye to create the sense of depth in stereo vision. But in most of the brain, the「wiring pattern」remains a mystery. Even the notion that most of the brain is hardwired for specific functions may turn out to be incorrect. Language, for instance, seems to be processed mostly on the left side, whereas spatial recognition, such as the ability to understand a map, seems to be performed primarily on the right. Yet under a microscope the pattern of left-brain and right-brain tissue looks pretty much the same. If there is a systematic difference between the wiring patterns in the brain's two hemispheres, it is too subtle for us to discern.

It may be that brain functions are learned in some sort of self-organizing process that changes the strength of various synaptic connections in order to fit an area for a certain kind of function. This is surely true to a degree. We know, for instance, that a monkey with a missing finger will continue to use the area of its brain that normally processes information from that finger: the idle neurons are recruited to perform processing for the animal's other fingers. Human beings probably rearrange the functions of the brain in a similar manner as they recover from a stroke. A stroke victim may initially have trouble with a specific function, such as recognizing faces, and then relearn the function with time. Since damaged neurons cannot regenerate themselves, the patient presumably relearns the function by recruiting neurons in a different part of the brain.

If functions like recognizing faces and understanding language are learned in different parts of the brain, then there must also be some sense in which these functions are already built in from the beginning. Newborn babies are particularly interested in faces in the first few days of life, and they learn to recognize them long before they learn to distinguish between much simpler shapes, such as letters. Similarly, babies seem predisposed to pay attention to certain kinds of patterns in speech that allow them to learn words and grammar. The functions that process language and recognize faces end up in different parts of the brain because, presumably, those parts of the brain are somehow primed to perform those dissimilar functions.

Even in those portions of the brain where functions seem to be hardwired, the pattern of wiring bears little resemblance to the hierarchical structure of functional blocks within a computer: there is no simple pattern of inputs going to outputs. Instead the connections are often bidirectional, with one set of neurons connecting in one direction and a complementary set connecting in reverse. Figure 29 shows the wiring diagram of the visual cortex of the macaque monkey, as best as can be determined by tracing the connections. Each of the lines in the diagram represents a bundle of many thousands of neurons, along with a complementary bundle in the reverse direction. At first glance, it appears as if everything is connected to everything else — unlike the neat, hierarchical circuit diagram of an engineered computer.

The important point here is that the brain is not only very complicated but also very different in structure from an engineered machine. That does not mean that we cannot ever engineer a machine to perform the functions of the human brain, but it does mean that we cannot expect to understand an intelligence by taking it apart and analyzing it as if it were a hierarchically designed machine.

It is possible that a satisfactory description of what the brain does will be almost as complex as a description of the structure of the brain — in which case, there is no meaningful sense in which we can understand it. In engineering, the way we deal with complexity is to break it into parts. Once we understand each part separately, we can understand the interactions between the parts. The way we understand each of the parts is to apply the engineering process recursively, breaking each part into a subpart, and so on. The design of an electronic computer, along with all its software, is impressive testimony to how far this process can be pushed. As long as the function of each part is carefully specified and implemented, and as long as the interactions between the parts are controlled and predictable, this system of「divide and conquer」works very well, but an evolved object like the brain does not necessarily have this kind of hierarchical structure.

FIGURE 29 Block diagram of the macaque visual cortex

大脑

人类大脑约有 10^12 个神经元，每个神经元平均拥有 105 个连接。在某种程度上，大脑是一个自组织系统。然而，如果将它视为一堆同质的物质，那就大错特错了。大脑包含数百种不同类型的神经元，其中许多神经元只出现在特定区域。针对脑组织的研究表明，不同大脑区域内神经元的连接模式各不相同：其中大约有 50 个区域的连接模式存在明显的不同，实际的区域数目可能更多。不过，它们在神经解剖学上的差异太小，我们难以区分。

很显然，大脑中的每块区域都具备特定的功能，例如辨别视觉图像里的颜色、发出语音中的语调或者记住事物的名字。我们之所以知道这些，是因为当特定区域因事故或者中风而受损时，与之对应的大脑功能将会丧失。例如，当大脑额叶左侧的 44 号和 45 号区域，也就是布罗卡区受到损伤时，人们就无法说出语法正确的句子。更糟糕的是，虽然他们可能吐字清晰，能理解别人的话，但无法再遣词造句了。位于大脑偏后方的环状脑回区一旦受到损伤，将导致人们读写困难。当其他某个区域受到损伤时，将导致人们无法回忆起熟悉的事物的名字，或者认出熟悉的面孔。

认为大脑的各个区域类似于计算机的功能构件，这种观点是错误的。首先，对于大脑中的大多数区域来说，一个区域的损伤并不会导致某种明显的功能障碍。例如，移除大部分右额叶有时只会改变这个人的某种性格，有时甚至不会引起任何明显的变化。其次，即使在那些功能丧失十分明显的情况下，也不能贸然断定该功能只由受损区域负责，这个区域有可能仅提供该功能所需的一些次要辅助条件。虽然一辆汽车在电池耗尽之后无法启动，但我们不能就此认为电池是驱动汽车前进的唯一原因。

实际上，我们可以弄清楚大脑某些区域的连接模式，尤其是靠近大脑后部、与视觉处理相关的区域。例如，有些大脑区域接受来自左眼和右眼的输入信息，并实现立体视感的连接模式。不过，在大脑的大部分区域，「布线模式」仍然是一个谜，甚至，大脑通过硬接线方式实现特定功能的观点也可能是不正确的。例如，左脑似乎负责语言功能，而右脑似乎主要负责空间认知功能，比如理解地图的能力。然而在显微镜下，左脑和右脑的组织模式看起来几乎相同。如果大脑的左右半球的连接模式存在系统差异，那么这种差异也非常微妙，我们难以辨别。

可能的情况是，大脑功能可以通过某种自组织过程形成，该过程能改变各种突触连接的强度，以便使这部分区域满足特定功能。从一定程度上来说，这一点肯定是正确的。例如，当猴子失去一根手指后，它会继续使用之前用来处理来自这根手指的信息的大脑区域。这些空闲的神经元会被重新用来处理来自其他手指的信息。人类在中风痊愈后，很可能采用了类似的方法来重新安排大脑功能。中风患者最初可能会失去特定能力，比如辨识面孔，但随着时间的推移，他们可以重新学会这种能力。由于受损的神经元无法再生，因此患者可能通过调用大脑其他区域的神经元来重新获得相应的能力。

如果诸如辨识面孔和语言理解等能力可以在大脑的不同区域内实现，那么从某种意义上来说，这些功能从一开始就被嵌入了大脑。新生婴儿在刚出生的前几天，对人脸特别感兴趣，在他们学会分辨简单的形状之前（比如字母），就学会了辨识人脸。同样，婴儿似乎倾向于留意声音中的某些特定模式，这使他们能够学会单词和语法。语言处理和辨识面孔等功能最终由大脑的不同区域来实现，这大概是因为这些大脑区域已经以某种方式做好了执行不同功能的准备。

即使大脑中某些区域的功能是通过硬接线的方式实现的，但布线模式也不同于计算机中功能块的层次结构，即没有输入到输出的简单模式。相反，连接关系通常是双向的，也就是一组神经元同向连接，另一组神经元反向连接。

关键的问题在于，大脑不仅具有很复杂的功能，而且其结构也与工程机器大不相同。这并不意味着我们无法设计出能执行大脑功能的机器，而是意味着我们不能将智能视为一种按层次结构设计的机器，指望通过分解和分析便能理解。

大脑的功能可能与大脑的结构一样复杂。在这种情况下，我们还不能真正地理解大脑。在工程设计中，我们处理复杂系统的方式就是，将整体分解为若干子部分，一旦我们理解了每个部分，就掌握了各部分之间的交互关系。我们理解每个子部分的方式就是，递归地应用这一分析过程，将每个子部分再分解为更小的子部分，以此类推。电子计算机及其所有软件的设计历程深刻地说明了这一过程可以进行到何种地步。只要详细规定并实现每个部分的功能，并且只要不同部分之间的交互行为是可控和可预测的，那么这种「分而治之」的工程方法就是有效的。然而，像大脑这样的进化产物并不具备这种层次结构。

### 9.2 The Problem with Moudularity

The reliance on a strict hierarchical structure is the Achilles heel of the engineering process, since of necessity it creates the kind of adamant inflexibility we associate with machines. As discussed in chapter 6 , hierarchical systems are fragile in the sense that they are prone to catastrophic failure. Products of engineering are inherently fragile, because each part of an engineered system must meet the design specifications of how it should interact with other parts. These specifications serve as a kind of contract between components. If one of the components breaks its part of the contract, the design assumptions of the systems are invalid, and the system breaks down in an unpredictable way. The failure of a single low-level component can percolate through the system with catastrophic effects. Of course, complex systems like computers and airplanes are engineered to avoid these so-called single-point failures, through the methods of redundancy described in chapter 6 , but such techniques can guard the system only against anticipated failures. All the potential consequences of a particular failure must be predicted and understood — a task that becomes increasingly difficult as the machine becomes more and more complex.

The problem goes beyond the failure of individual components. In a complicated system, even correctly functioning parts can produce unexpected behaviors when they interact. Often when a large software system malfunctions, the programmers responsible for each of the parts can convincingly argue that each of their respective subroutines is doing the right thing. Often they are all correct, in the sense that each subroutine is correctly implementing its own specified function. The flaw lies in the specifications of what the parts are supposed to do and how they are supposed to interact. Such specifications are difficult to write correctly without anticipating all possible interactions. Large complex systems, like computer operating systems or telephone networks, often exhibit puzzling and unanticipated behaviors even when every part is functioning as designed. You may recall that a few years ago the long-distance telephone lines of the eastern United States stopped routing calls for several hours. The system used a sophisticated fault-tolerant design, based on redundancy. All its components were functioning correctly, but an unanticipated interaction between two versions of the software running at different switching stations caused the entire system to fail.

It is amazing to me that the engineering process works as well as it does. Designing something as complicated as a computer or an operating system can require thousands of people. If the system is sufficiently complicated, no one person can have a complete view of the system. This situation generally leads to mistakes stemming from misunderstandings of interfaces and inefficiencies of design. Again, such interface difficulties get worse as the system becomes more complex.

It is important to note that the problems outlined above are not inherent weakness of machines or of software per se. They are weaknesses of the engineering design process. We know that not everything that is complex is fragile. The brain is much more complicated than a computer, yet it is much less prone to catastrophic failure. The contrast in reliability between the brain and the computer illustrates the difference between the products of evolution and those of engineering. A single error in a computer's program can cause it to crash, but the brain is usually able to tolerate bad ideas and incorrect information and even malfunctioning components. Individual neurons in the brain are constantly dying, and are never replaced; unless the damage is severe, the brain manages to adapt and compensate for these failures. (Ironically, as I was writing this chapter, my computer crashed and required rebooting.) Humans rarely crash.

模块化的问题

工程设计方法的致命弱点在于，它依赖于严密的层级结构，这必定会导致机器缺乏灵活性。正如第 6 章所讨论的那样，具有层级结构的系统容易发生灾难性故障，从这个角度来说，它们是脆弱的。从本质上来说，工程产品是脆弱的，因为工程系统中的每个子系统都必须符合规定它与其他子系统如何交互的设计规范。这些规范就像各个子系统之间的一种协议，如果有一个子系统违反了该协议，则系统设计依赖的假设便不再有效，系统就会以一种不可预测的方式崩溃。单个低级部件的故障可能会在整个系统中扩散，带来灾难性的后果。当然，在设计诸如计算机和飞机等复杂系统时，设计师会避免这些所谓的单点故障，方法就是利用第 6 章介绍的冗余技术。不过，这些技术只能防止系统出现预期范围内的故障。随着机器变得越来越复杂，若想准确地预测并完全掌握特定故障的所有潜在后果，则会异常困难。

然而，问题远不止单个部件失效这么简单。在复杂系统中，即使所有部分都运行正常，当它们之间进行交互时也会产生一些意料之外的行为。通常，当大型软件系统出现故障时，负责每个子系统的程序员都会据理力争地说明自己的子程序没有问题。他们的说法往往是正确的，即每个子程序都准确地实现了各自规定的功能。系统缺陷源自规定每个子系统应该做什么以及它们之间如何交互的标准规范。如果没有预料到所有可能的交互行为，就很难正确地编写这些标准规范。对于大型复杂系统来说，比如计算机操作系统或者电话网络，即使每个子系统都按设计运行，也经常会出现一些令人费解和意料之外的行为。你或许还记得，十几年前，美国东部的几条长途电话线路停止接通呼叫达数小时之久。该系统采用了基于冗余结构的复杂容错机制。当时系统中的所有部件都运行正常，但运行于不同转接站上的同一软件的两个不同版本在交互时出现了意外，导致整个系统瘫痪。

令我感到惊讶的是，工程设计方法在实践中仍然非常有效。设计像计算机或者操作系统这样复杂的对象时，可能需要数千人。如果系统非常复杂，就没有人能掌握系统的全貌。这种情况带来的后果是，弄错接口以及设计的低效性会导致许多错误，而随着系统变得越来越复杂，接口问题会变得更加严重。

需要注意的是，上述列举的问题并不是机器或者软件自身固有的缺陷，它们是在工程设计过程中产生的缺陷。我们知道，并非所有复杂的事物都是脆弱的，比如大脑远比计算机复杂，但不易发生灾难性的故障。大脑和计算机之间的可靠性对比说明了进化产物和工程设计产品之间的差别。计算机程序中的单个错误可能会导致整个计算机崩溃，而大脑通常可以容忍错误的想法和的信息，甚至部分功能失常。在大脑中，虽然不断有神经元在衰亡，并且再无新的神经元补缺，但除非遭到严重的损伤，否则大脑都会设法适应和弥补这些问题。具有讽刺意味的是，在我写这一章时，我的计算机崩溃了，需要重新启动，而人类大脑几乎不会崩溃。

### 9.3 Simulate Evolution

So, in creating an artificial intelligence, what is the alternative to engineering? One approach is to mimic within the computer the process of biological evolution. Simulated evolution gives us a different way to design complicated hardware and software — a way that avoids many of the problems of engineering. To understand how simulated evolution works, let's look at a specific example. Say that we want to design a piece of software that sorts numbers into descending order. The standard engineering approach would be to write such a program using one of the sorting algorithms discussed in chapter 5 , but let's consider how we might instead「evolve」the software.

The first step is to generate a「population」of random programs. We can create this population using a pseudorandom number generator to produce random sequences of instructions (see chapter 4 ). To speed up the process, we can use only those instructions useful for sorting, such as comparison and exchange instructions. Each of these random sequences of instructions is a program: the random population will contain, say, 10,000 such programs, each one a few hundred instructions long.

The next step is to test the population to find which programs are the most successful. This requires us to run each of the programs to see whether or not it can sort a test sequence correctly. Of course, since the programs are random, none are likely to pass the test — but by sheer luck some will come closer to a correct sorting than others. For instance, by chance, a program may move low numbers to the back of the sequence. By testing each program on a few different number sequences, we can assign a fitness score to each program.

The next step is to create new populations descended from the high-scoring programs. To accomplish this, programs with less than average scores are deleted; only the fittest programs survive. The new population is created by making copies of the surviving programs with minor random variations, a process analogous to asexual reproduction with mutation. Alternatively, we can「breed」new programs by pairing survivors in the previous generation — a process analogous to sexual reproduction. We accomplish this by combining instruction sequences from each of the「parent」programs to produce a「child.」The parents presumably survived because they contained useful instruction sequences, and there is a good chance that the child will inherit the most useful traits from each of the parents.

When the new generation of programs is produced, it is again subjected to the same testing and selection procedure, so that once again the fittest programs survive and reproduce. A parallel computer will produce a new generation every few seconds, so the selection and variation processes can feasibly be repeated many thousands of times. With each generation, the average fitness of the population tends to increase — that is, the programs get better and better at sorting. After a few thousand generations, the programs will sort perfectly.

I have used simulated evolution to evolve a program to solve specific sorting problems, so I know that the process works as described. In my experiments, I also favored the programs that sorted the test sequences quickly, so that faster programs were more likely to survive. This evolutionary process created very fast sorting programs. For the problems I was interested in, the programs that evolved were actually slightly faster than any of the algorithms described in chapter 5  — and, in fact, they were faster at sorting numbers than any program I could have written myself.

One of the interesting things about the sorting programs that evolved in my experiment is that I do not understand how they work. I have carefully examined their instruction sequences, but I do not understand them: I have no simpler explanation of how the programs work than the instruction sequences themselves. It may be that the programs are not understandable — that there is no way to break the operation of the program into a hierarchy of understandable parts. If this is true — if evolution can produce something as simple as a sorting program which is fundamentally incomprehensible — it does not bode well for our prospects of ever understanding the human brain.

I have used mathematical tests to prove that the evolved sorting programs are flawless sorters, but I have even more faith in the process that produced them than in the mathematical tests. This is because I know that each of the evolved sorting programs descends from a long line of programs whose survival depended on being able to sort.

The fact that evolved software cannot always be understood makes some people nervous about using it in real applications, but I think this nervousness is founded on false assumptions. One of the assumptions is that engineered systems are always well understood, but this is true only of relatively simple systems. As noted, no single person completely understands a complex operating system. The second false assumption is that systems are less trustworthy if they cannot be explained. Given the choice of flying in an airplane operated by an engineered computer program or one flown by a human pilot, I would pick the human pilot. And I would do so even though I don't understand how the human pilot works. I prefer to put my faith in the process that produced the pilot. As with the sorting programs, I know that a pilot is descended from a long line of survivors. If the safety of the airplane depended on sorting numbers correctly, I would rather depend on an evolved sorting program than on one written by a team of programmers.

模拟进化

在创造人工智能时，除了工程设计，还有什么其他途径呢？一种途径是在计算机中模拟生物进化的过程。模拟进化过程能为我们设计复杂的硬件和软件带来启发，还可以避免许多源自工程设计的弊端。为了理解模拟进化如何进行，我们来看一个具体的例子。假设我们要设计一款可以降序排序数字的软件。标准的工程设计方法是，使用第 5 章介绍的一种排序算法，编写对应的程序，但在这里，我们考虑如何「进化」出这款软件。

第一步，生成一组由随机程序组成的「种群」。我们可以根据伪随机数生成器生成随机指令序列的方式，创建出程序种群（见第 4 章）。为了加快这个过程，我们只能使用那些对排序有用的指令，比如比较和交换指令。这里的每一个随机指令序列都是一段程序：随机种群将包含 10 000 个这样的程序，每个程序包含几百条指令。

第二步是测试程序种群，找到哪些程序是最有用的。这需要我们运行每一个程序，判断它们能否将测试序列正确地排序。当然，由于程序是随机的，几乎没有一个可以通过测试，但如果运气足够好，有些程序会更接近正确的排序。例如，在偶然情况下，某个程序可能会将小数值数字移动至序列末端。通过在几个不同的数字序列上测试每个程序，我们可以为它们的适应度打分。

第三步是从得分高的程序中生成新的程序种群。为此，我们需要删除低于平均分数的程序；只有适应度最高的程序才能留存下来。然后，给幸存的程序增加微小的随机改动，并对其进行大量复制，新的程序种群就这样被创造出来了，这个过程类似于具有变异特性的无性繁殖过程。或者，我们可以通过将上一代种群中的幸存者进行配对来「培育」新程序，这个过程类似于有性繁殖，实现的方法是，融合来自每个「父母」程序的指令序列，再生成「孩子」程序。「父母」程序能够留存下来的原因可能是它们具有有用的指令序列，因此「孩子」程序很可能从「父母」程序那里继承了最有用的特征。

当新一代程序生成之后，它们会再次接受相同的测试和选择过程，那些最适合的程序会继续留存下来并进行繁殖。并行计算机每隔几秒就会生成新一代程序，因此选择和变异的过程可以重复数千次。每当生成新一代程序时，程序种群的平均适应度就会提高，也就是说，这些程序的排序结果将会越来越准确。经过几千次迭代后，程序将会得出完全准确的排序结果。

我曾经用模拟进化过程的方法设计了一款程序，用来解决特定的排序问题。因此，我知道这种过程是行之有效的。在我的实验中，我喜欢那些能快速地进行排序的程序，因此运行更快的程序更可能留存下来。这个进化过程创造出了速度很快的排序程序。实际上，对于我感兴趣的问题来说，通过进化得来的程序比第 5 章所讲的任意一类算法都要稍快一些，而且，它们的数字排序速度比我自己编写出的程序还要快。

很有趣的一点是，我并不知道在实验中进化得来的排序算法是如何工作的。我虽然仔细地检查了它们的指令序列，但还是无法理解它们，除了这些指令序列本身，我找不到关于这些程序如何工作的更简单的解释。可能这些程序是不可理解的，我们没有办法将程序的操作过程分解为一种可理解的层次结构。如果事实真是如此，即如果进化能够产生与排序程序一样简单但根本无法理解的事物，那么这对我们探究人类大脑的前景来说并不是什么好兆头。

我已经用数学检验的方法证明了通过进化的方式得到的排序程序是一种完美的排序机。不过，相比于数学测试，我更信任生成程序的过程。这是因为我知道，每个进化而来的程序都来自许多程序，这些程序能否留存下来取决于它们能否排序。

由进化产生的程序总是无法让人理解，这一事实会让人们在实际应用过程中感到不安。不过，我认为这种紧张不安源自错误的假设。第一个假设是通过工程方法设计的系统往往易于理解，但其实这只适用于相对简单的系统。如上所述，没有一个人能够完全理解一个复杂的操作系统。第二个错误的假设是，如果无法解释某个系统，这个系统就不太可靠。如果要选择乘坐一架由工程方法设计出的计算机程序或者人类飞行员驾驶的飞机，我会选择人类飞行员，即使我不明白人类飞行员是如何工作的，我也会这么选择，因为我更愿意相信飞行员的选拔过程。与排序程序一样，我知道合格的飞行员是从许多候选者中挑选出来的。如果飞机的安全性取决于对数列的正确排序，那我宁愿选择依靠进化得来的排序程序，而非由程序团队编写的程序。

### 9.4 Evolving A Thinking Machine

Simulated evolution is not in itself a solution to the problem of making a thinking machine, but it points us in the right direction. The key idea is to shift the burden of complexity away from the hierarchy of design and onto the combinatorial power of the computer. Essentially, simulated evolution is a kind of heuristic search technique that searches the space of possible designs. The heuristics it uses to search the space are Try a design similar to the best designs you have found so far and Combine elements of two successful designs. Both heuristics work well.

Simulated evolution is a good way to create novel structures, but it is an inefficient way to tune an existing design. Its weaknesses as well as its strengths stem from evolution's inherent blindness to the「Why」of a design. Unlike the feedback systems described in the last chapter, where specific changes were made to correct specific failures, evolution chooses variations blindly, without taking into account how the changes will affect the outcome.

The human brain takes advantage of both mechanisms: it is as much a product of learning as it is of evolution. Evolution paints the broad strokes, and the development of the individual in interaction with its environment completes the picture. In fact, the product of evolution is not so much a design for a brain as the design for a process that generates a brain — not so much a blueprint as a recipe. Thus, there are multiple levels of emergent processes operating at once. An evolutionary process creates a recipe for growing a brain, and the developmental process interacts with the environment to wire the brain. The developmental process includes both the internally driven processes of morphogenesis and the externally driven processes of learning. The maturational forces of morphogenesis cause nerve cells to grow in the right patterns, and the process of learning fine-tunes the connections. The ultimate stage in the brain's learning is a cultural process, in which knowledge acquired by other individuals over many generations is transferred into it.

I have described each of these emergent mechanisms (evolution, morphogenesis, learning) as if they were discrete processes, but in reality they are synergistically intertwined. There is no hard line between the maturational forces of morphogenesis and the instructional processes of culture. When a mother coos baby talk to her newborn child, this is both an instruction process and an aid in the maturation of the infant brain. The process of morphogenesis is itself an adaptive process, in which each cell develops in constant interaction with the rest of the cells in the organism, in a complex feedback process that tends to correct errors and keep the development of the organism on track.

There are also synergistic interactions between the evolutionary processes that create the species and the developmental processes that create the individual. The clearest example of the interaction between development and evolution is known as the Baldwin effect, first described by the evolutionary biologist James Baldwin in 1896 and rediscovered by the computer scientist Geoffrey Hinton almost a century later. The basic idea of the Baldwin effect is that when you combine evolution with development, evolution can happen faster; the adaptive processes of development can fix the flaws in an imperfect evolutionary design.

To understand the Baldwin effect, one must first appreciate the difficulty of evolving traits that require multiple mutations to occur together. Consider the evolution of the instinct for nest-building behavior in a bird. It is reasonable to assume that building a nest requires a few dozen individual steps, such as locating a twig, picking it up with the beak, carrying the twig back to the nest, and so on. Let's also assume, for the sake of the example, that each of these steps requires a different mutation and that the benefit to the bird (in the form of a completed nest) requires the complete set of mutations. In other words, if even a single step is missing, the nest will not get built at all, and therefore the bird will be no more fit than its peers and will derive no evolutionary advantage. Obviously, the problem with evolving such a trait is that evolution will select for one of its component mutations only if all the others are present: the simultaneous occurrence of all these mutations within a single individual is a highly improbable event. Since no single step is beneficial by itself, it is difficult to imagine how a behavior such as nest building could possibly evolve.

The Baldwin effect is synergistic interaction between evolution and learning. This interaction helps to solve this problem by offering the bird partial credit for a mutation that produces a single step of the task. A bird that is born knowing how to do some of the steps will have an advantage over a bird that does not, since it will have fewer steps to learn, so it's more likely to arrive at succesful nest-building behavior. Each single step that the bird is born with contributes to the possibility of learning, and therefore is valuable in itself. Viewed this way, each individual mutation will be favored independently, so that nest-building behavior will result from steps that are added to the bird's instinctual repertoire gradually, and in less time than it would take for a probabilistic fluke that produces the mutations all at once in a single individual In effect, the fact that the bird can learn makes the evolution happen faster. The Baldwin effect applies not just to learning but to any adaptive mechanism in the development of the individual.

Part of the reason that I'm optimistic about the prospects of evolving a thinking machine is that we do not have to start from scratch. We can「prime」the initial population of machines with patterns of structure that we observe in the brain. We can also start with whatever patterns of development and learning we observe in natural systems, even if we do not have a complete understanding of them. This should help even if our guesses are not quite right, since starting our search somewhere near a solution is probably much better than starting at random. By including some model of development in this process, the evolution of a thinking machine could take advantage of the Baldwin effect.

Another effect that radically reduces the time required to develop a complex behavior is instruction. A human baby develops intelligence at least in part because it has other humans to learn from. Part of this learning is acquired by sheer imitation, and part through explicit instruction. Human language is a spectacular mechanism for transferring ideas from one mind to another, allowing us to accumulate useful knowledge and behavior over many generations at a rate that far outpaces biological evolution. The「recipe」for human intelligence lies as much in human culture as it does in the human genome.

However, even starting with everything we know, I would not expect us to be able to evolve high-level artificial intelligence in a single step. Here is a rough outline of how the sequence of stages might progress. We would begin by evolving a design of a machine with the intelligence of, say, an insect by creating a simple environment in which insectlike intelligence would be favored, and by starting with an initial population predisposed through its developmental mechanisms to develop the kinds of neural structures we see in insects. Through a sequence of successively richer simulated environments, we might eventually evolve our insect intelligence into the intelligence of a frog, a mouse, and so on. Even going this far would doubtless take decades of work and involve many dead ends and false starts, but eventually this course of research could lead to the evolution of an artificial intelligence with the complexity and flexibility of the primate brain.

Should we ever manage to evolve a machine that can understand language, we would be able to skip ahead rapidly, by taking advantage of human culture. I imagine that we would need to teach an intelligent machine by much the same process that we would teach a human child, with the same mixture of skills, facts, morals, and stories. Since we would be incorporating human culture into the machine's recipe for intelligence, the resulting machine would not be an entirely artificial intelligence but rather a human intelligence supported by an artificial mind. For this reason, I expect that we would get along with it just fine.

I am aware, of course, that building such a machine will create a tangle of moral issues. For instance, once such a machine has been created, will it be immoral to turn it off? I would guess that turning it off would be wrong, but I do not pretend to be certain of the moral status of an intelligent artifact. Fortunately, we will have many years to work such questions out.

Most people are interested in not so much the practical moral questions of a hypothetical future as the philosophical issues that the mere possibility an artificial intelligence raises about ourselves. Most of us do not appreciate being likened to machines. This is understandable: we ought to be insulted to be likened to stupid machines, such as toasters and automobiles, or even to today's computers. Saying that the mind is a relative of a current-generation computer is as demeaning as saying that a human being is related to a snail. Yet both statements are true, and both can be helpful. Just as we can learn something about ourselves by studying the neural structure of the snail, we can learn something about ourselves by studying the simple caricature of thought within today's computers. We may be animals, but in a sense our brain is a kind of machine.

Many of my religious friends are shocked that I see the human brain as a machine and the mind as computation. On the other hand, my scientific friends accuse me of being a mystic because I believe that we may never achieve a complete understanding of the phenomenon of thought. Yet I remain convinced that neither religion nor science has everything figured out. I suspect that consciousness is a consequence of the action of normal physical laws, and a manifestation of a complex computation, but to me this makes consciousness no less mysterious and wonderful — if anything, it makes it more so. Between the signals of our neurons and the sensations of our thoughts lies a gap so great that it may never be bridged by human understanding. So when I say that the brain is a machine, it is meant not as an insult to the mind but as an acknowledgment of the potential of a machine. I do not believe that a human mind is less than what we imagine it to be, but rather that a machine can be much, much more.

进化出会思维的机器

模拟进化过程虽然并不能解决思维机器的问题，但为我们指出了一个正确的方向。关键的一点在于，将分层体系设计的复杂性问题转移到计算机的组合能力上。从本质上来说，模拟进化过程是一种启发式搜索技术，它可以搜索可能的设计空间，而用于搜索此空间的启发式方法就是：尝试与已有的最佳设计相似的设计方案，或者结合两个成功的设计方案的元素。这两种启发式方法都行之有效。

虽然模拟进化过程是一种创造新结构的好方法，但它在调整现有设计方面的效率很低。它的弱点和优势源于进化理论对「为什么」如此设计这个问题的内在盲目性。第 8 章描述的反馈系统会为矫正具体的故障做出相应的变化，但进化理论与之不同，它会盲目地选择和更改方案，而不会考虑这些变更会如何影响结果。

人类大脑同时利用了这两种机制，它既是学习的产物，也是进化的产物。进化勾画出了笼统的框架，而个体在与环境的相互作用中完善了细节。事实上，进化的产物并不是大脑的设计方案，而是过程（过程产生了大脑）的设计方案，它并非蓝图，而是秘方。多个层次的进化过程在同时进行，进化的过程为大脑的形成提供了秘方，推动了发育过程与环境的相互作用，并进一步激发了大脑。发育过程既包括内部驱动的形态发育过程，也包括外部驱动的学习过程。形态发育的力量促使神经细胞以正确的模式生长，而学习的过程则对神经连接进行微调。大脑学习的最终阶段是一种文化过程，通过世代相传，每代人获取的知识被传递下去。

由于我逐一讲述这些机制（进化、形态发育、学习），这个过程听起来似乎是离散的，但实际上，它们是协同交织在一起的。形态发育的力量与文化教育的过程之间没有明确的分界线。当一位母亲与新生婴儿咿咿呀呀交谈时，这既是一个教育的过程，也是促使婴儿大脑成熟的辅助手段。形态发育过程本身就是一个自适应的过程，生物体内的每个细胞都在与其他细胞持续的相互作用中生长，在复杂的反馈过程中纠正错误，并确保生物体的正常发育。

在创造物种的进化过程和创造个体的成长过程之间，也存在相互促进的交互作用，最能说明两者之间的交互作用的例子就是鲍德温效应。该效应由进化生物学家詹姆斯·鲍德温（James Baldwin）于 1896 年首次提出，大约一个世纪以后，计算机科学家杰弗里·欣顿（Geoffrey Hinton）重新发现了它。鲍德温效应的基本原理是，将进化与成长相结合时，可以加快进化速度；成长的自适应过程可以修复进化过程产生的缺陷。

为了理解鲍德温效应，首先我们必须认识到通过进化获得这种特性的难度，即需要多个突变共同出现才能获得。以鸟类筑巢本能的进化过程为例，我们先合理地假设，完成筑巢需要几十个独立的步骤，比如寻找树枝、用喙捡起树枝、将树枝带回巢穴等。在这个例子中，我们进一步假设，每个步骤都需要不同的突变类型，并且为了获得收益（形成一个完整的鸟巢），鸟类需要完成所有的突变。换句话说，即便只缺少一个步骤，鸟巢也无法筑成，那么这只鸟就会在与其他同伴的竞争中落败，无法获得进化优势。显然，通过进化获取这一特性的问题在于，只有当其他所有突变都存在时，进化才会选择其中一个突变，而在单个个体中同时出现所有这些突变的概率极低。由于任何单一步骤就其本身来说并无什么用处，因此我们很难想象诸如筑巢之类的行为如何进化形成。

鲍德温效应体现了进化与学习之间的相互促进的交互作用。这种交互作用有助于解决上述难题，其方法是当产生筑巢任务中的某一个步骤的突变时，这只鸟就会获得部分奖励。如果一只鸟儿天生就能完成其中的一些步骤，那么比起其他无法完成这些步骤的鸟儿，它就更具优势，因为它需要学习的步骤更少，所以更有可能学会筑巢。鸟儿每多一个与生俱来的步骤，学会筑巢的可能性就更高一些，因此每个步骤对它而言都很有价值。这样看来，每个突变都会受到单独的青睐。因此，随这些突变产生的各种筑巢步骤将会逐渐进入鸟类的本能库，并最终促使筑巢行为的出现，而且，比起侥幸等待单个个体中同时出现所有突变，这个过程用时更短。实际上，鸟类的学习能力加快了其进化速度。鲍德温效应不仅适用于学习过程，也适用于个体成长过程中所有的自适应机制。

我之所以对通过进化能获得思维机器的前景感到乐观，部分原因是我们不必从头开始，可以根据大脑结构的模式来「初始化」最初的机器种群。无论我们在自然系统中观察到何种成长和学习模式，都可以从中选择一种，即便我们没有完全理解它们。就算我们的猜测不太准确，这也会起到一定的帮助作用，因为在解决方案附近搜索比在随机区域搜索更有效。如果在这个过程中纳入某个成长模型，思维机器的进化便可以发挥鲍德温效应的优越性。

还有一种效应能显著减少复杂行为形成的时间，它被称为指导效应。至少从某种程度上来说，婴儿的智力之所以能开发是因为他能向其他人学习，部分知识可以通过纯粹的模仿获得，有些知识可以通过明确的指导获得。人类的语言是一种令人惊叹的机制，它能将思想从一个大脑转移到另一个大脑，能使人类以远超生物进化的速度将有用的知识和行为代代相传下来。人类智能的「秘诀」存在于人类基因中，也存在于人类文化中。

然而，即使从我们所熟知的一切开始，我也不会奢望仅通过一个步骤就能进化到高级人工智能阶段。接下来我们来探讨这一系列阶段可能经历的进化过程。首先，我们要设计一种具有昆虫智能的机器，然后创造一个有利于昆虫智能的简单环境。通过这种发育机制让某个初始种群开始逐渐形成我们在昆虫身上观察到的神经结构。接下来，通过一系列更为丰富的模拟环境，我们可以使昆虫智能进化为青蛙智能、老鼠智能等。仅仅实现这一步就需要数十年的时间，中途还可能陷入死胡同、出现很多错误，或者一遍遍从头做起。不过，这项研究最终可能会培育出与灵长类动物大脑的复杂性和灵活性相当的人工智能。

如果我们创造出了可以理解人类语言的机器，就能利用人类文化跳过上述步骤。我认为，教育这台智能机器的过程与我们教育人类孩子的过程是相似的，即需要教授知识，包括技能、事实、道德和故事等。由于我们将人类文化融入了机器智能的形成过程，因此最终得到的产物不是完全的人工智能，而是人工智能支撑的人类智能。出于这个原因，我希望我们能和谐相处。

当然，建造这样一种人工智能会引发一系列道德问题。例如，一旦这种智能机器被创造出来，将其关闭是否合乎道德？我认为关闭它们可能是不对的，但我无意以人工智能产物道德地位讨论者自居。幸运的是，我们还有很多年的时间来解决这些问题。

大多数人对所设想的未来场景中将会发生的道德问题并不感兴趣，他们更关心人工智能可能给人类自身带来的哲学难题。大多数人都不喜欢别人将自己与机器相提并论。这是可以理解的：将人类比作笨拙的机器，比如烤面包机和汽车，甚至计算机，确实听起来像是一种侮辱。认为人类的思维是现代计算机的近亲的观点就如同说人类是蜗牛的近亲，有损人类的尊严。然而，这两种说法都是正确的，且都对我们有所帮助。正如我们可以通过研究蜗牛的神经结构来了解自己，也可以通过研究当今计算机中简单的思维，学到关于自身的知识。我们也许是动物，但从某种意义上来说，我们的大脑是一种机器。

宗教界的许多朋友对我将人脑视为机器，将思维视为计算而感到震惊。我科学界的朋友也因我宣扬我们永远无法参透思维的奥秘而指责我是神秘主义者。然而，我依然坚信，无论宗教还是科学，都没有揭示出全部的真理。我认为，意识是一般物理定律作用的结果，也是复杂计算的某种表现形式。我认为，这些说法并没有使意识失去神秘感和神奇性。如果意识是其他事物的产物，只会使意识变得更加神秘。人类的神经元信号和思维感官之间存在一条巨大的鸿沟，凭借人类的理解力可能永远无法跨越。因此，当我说大脑是一台机器时，并不是对人类思维的贬低，而是对机器潜能的承认。我不是认为人类思维比我们想象的更加简单，而是相信机器能做的比我们想象的更多。