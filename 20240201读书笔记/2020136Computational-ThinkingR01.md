## 记忆时间

## 目录

0101 What Is Computational Thinking?

0201 Computational Methods

0301 Computing Machines

0401 Computer Science

## 0101. What Is Computational Thinking?

An algorithm is a set of rules for getting a specific output from a specific input. Each step must be so precisely defined that it can be translated into computer language and executed by machine.

—— Donald Knuth (1977)

What is a computer? Most people will answer it is an electronic black box that does amazing things by collecting, storing, retrieving, and transforming data. Almost all our devices and gadgets are computers: phones, tablets, desktops, web pages, watches, navigators, thermometers, medical devices, clocks, televisions, DVD players, WiFi networks. Our services are software — bookstores, retail stores, banks, transportation, Uber, hotel reservations, Airbnb, filmmaking, entertainment, Dropbox, online courses, Google searches — and almost all run by unseen computers across an unseen worldwide network called「the cloud.」Computers have brought enormous benefits — new jobs, access to information, economic development, national defense, improvements in health, and much more. They have brought, as well, worrying concerns — job losses, globalization, privacy, surveillance, and more. It looks like everything that can be digitized is being digitized and computers are everywhere storing and transforming that information. A computer revolution is truly upon us.

How shall we think about all this? What do we need to understand about computers? What must we do to put a computer to work for us? How do computers shape the way we see the world? What new do we see? What is the role of programming? What are computers not good for?

### 1.1 The Power and Value of Computation

Computational thinking (here abbreviated CT) offers some answers to these questions. Much of CT is specifically oriented on figuring out how to get a computer to do a job for us — how to control a complex electronic device to do a job reliably without causing damage or harm. Algorithms are the procedures that specify how the computer should do a job. Although humans can carry out algorithms, they cannot do so nearly as fast as a machine; modern computers can do a trillion steps in the time it takes a human to do one step. The magic is nothing more than a machine executing large numbers of very simple computations very fast. Programs are the bridge: algorithms encoded in special-purpose languages that translate to machine instructions that control a computer.

But CT reaches further than automation. Information and computational processes have become a way of understanding natural and social phenomena. Much of CT today is oriented toward learning how the world works. A growing number of biologists, physicists, chemists, and other scientists are looking at their subject matter through a computational lens; professionals in the arts, humanities, and social sciences are joining in. Computer simulation enables previously impossible virtual experiments. The「information interpretation」of the world offers conceptual and empirical tools that no other approach does.

CT also advises us about jobs that computers cannot do in any reasonable amount of time. Or at all — some jobs are impossible for computers. Many social, political, and economic problems are beyond the pale of computers. By understanding the limits of computing, we can avoid the trap of looking to computing technology to solve such problems.

Obviously, designing a program or a machine to do so much in such a short time is a daunting design task that demands its own way of thinking if we are to have any confidence that the machine does the job without error. Indeed, understanding users, and designing systems specifically for them, turns out to be one of the great challenges of modern computing. Design is one of the central concerns of CT.

### 1.2 Defining Computational Thinking

Computational thinking has become a buzzword with a multitude of definitions. We have distilled the spirit of the multitude into this definition used throughout this book:

Computational thinking is the mental skills and practices for:

1 designing computations that get computers to do jobs for us, and

2 explaining and interpreting the world as a complex of information processes.

1-2『这里作者对 CT 下了定义，做一张术语卡片。（2021-05-04）』—— 已完成

The design aspect reflects the engineering tradition of computing in which people build methods and machines to help other people. The explanation aspect reflects the science tradition of computing in which people seek to understand how computation works and how it shows up in the world. Design features immersion in the community being helped, explanation features being a dispassionate external observer. In principle, it is possible to design computations without explaining them, or explain computations without designing them. In practice, these two aspects go hand in hand.

Computations are complex series of numerical calculations and symbol manipulations. Examples of numerical calculations are the basic arithmetic operations (add, subtract, multiply, divide) and the basic trigonometric functions (sine, cosine, and tangent). Examples of symbolic manipulations are logical comparison of numbers or symbols, decisions of what instructions to do next, or substitutions of one string of letters and numbers for another. Amazing computations can be carried out when trillions of such simple operations are arranged in the proper order — for example, forecasting tomorrow's weather, deciding where to drill for oil, designing the wings of an aircraft with enough lift to fly, finding which physical places are most likely to be visited by a person, calling for a taxi, or figuring out which two people would make a great couple.

Computers are agents that carry out the operations of a computation. They follow programs of instructions for doing arithmetic and logic operations. Computers can be humans or machines. Humans can follow programs, but are nowhere near as fast or as error-free as machines. Machines can perform computational feats well beyond human capabilities.

We use the word「job」to refer to any task that someone considers valuable. Today many people look to computers (actually, computations performed by computers) to get jobs done. They seek automation of jobs that could not be done without the aid of a machine. Computers are now getting good enough at some routine jobs so that loss of employment to automation has become an important social concern.

We do not equate「doing a job」with automation. Well-defined, routine jobs can be automated, but ill-defined jobs such as「meeting a concern」cannot. CT can help with jobs that cannot be automated. In the design chapter we will discuss the kind of CT that does this.

There is clearly a special thinking skill required to successfully design programs and machines capable of enormous computations and to understand natural information processes through computation. This skill — computational thinking, or CT — is not a set of concepts for programming. Instead, CT comprises ways of thinking and practicing that are sharpened and honed through practice. CT is a very rich skill set: at the end of this chapter we outline the six dimensions of computational thinking that you will encounter in this book: machines, methods, computing education, software engineering, design, and computational science.

### 1.3 Wishful Thinking

In our enthusiasm for computational thinking, we need to be careful to avoid wishful thinking. Perhaps the first and most common wish is that we can get computers to do any job we can conceive of. This wish cannot be realized because there are many jobs that are impossible for computers. For example, there is no algorithm that will inspect another algorithm and tell us whether it terminates or loops forever. Every programming student longs for such an algorithm to help with debugging. It was logically impossible in 1936 when Alan Turing proved it so, and it is still impossible today.

1-2『这里的信息阐述了「计算机」不能做的事，列举了 4 中不能「计算」的场景，等同于吴军之前一直强调的「计算机边界」问题。做一张主题卡片。（2021-05-04）』—— 已完成

Even if we stick to logically possible jobs, there are many that cannot be done in a reasonable time — they are intractable. One famous example is the traveling salesman problem, which is to find the shortest tour on a map of a country that visits each city just once. An algorithm to compute this would be of great value in the package delivery industry. The simplest way to find the shortest tour is to enumerate all possible tours and select the shortest. For a small set of 100 cities, this would take 10^130 years on the world's fastest supercomputer. For comparison the age of the universe is on the order of 10^10 years. Even the「simplest way」can be impossible! Algorithms analysts have identified thousands of common problems that are intractable in this way.

1『又看到经典的 NP 问题。（2021-05-04）』

The picture gets even more confusing because in most cases there are fast algorithms to find an approximate answer. They are called heuristics. Take, for example, the problem of finding the shortest tour connecting all 24,978 cities in Sweden. The enumeration algorithm for the traveling salesman problem would take on the order of 10^100,000 years! But in 2004 a team at the University of Waterloo using heuristics for optimization found a shortest tour and proved it to be correct. Their solution used 85 years of processing time spread over a cluster of machines that took several months to complete the job.

Computational thinkers need to develop enough experience and skill to know when jobs are impossible or intractable, and look for good heuristics to solve them.

A second example of wishful thinking is to believe that learning how to program in a computer science course or a coding-intensive workshop will enable you to solve problems in any field that uses computation. No, you will need to learn something about the other field too. For example, even if you have studied search algorithms in a programming course, you are not likely to be able to be useful to a genomics project until you have learned genome biology and the significance of biological data.

A third example of wishful thinking is to believe that computers are not essential to CT — that we can think about how to solve problems with algorithms and not be concerned with the computers that run the algorithms. But this is not so. When a computer does not have sufficient memory to hold all your data, you will seek ways to divide your problem into subsets that will fit. When a single processor does not have sufficient processing power, you will seek a computer with multiple parallel processors and algorithms that divide the computation among them. When the computer is too slow, you will look inside to find a bottlenecked component and either upgrade it or find a new algorithm that does not use that component. Even if your computer has sufficient memory, adequate processing power, and no bottlenecks, other aspects can limit your problem-solving progress, notably the speed of the internal clock, which paces the machine to perform computational steps in an orderly and predictable way. But some new machines, notably quantum computers and neural nets, have no clocks: How shall we think about programming them?

A fourth example of wishful thinking is to believe the computer is smart. If you are imprecise in translating human steps into program steps, your computation will contain errors that could cause disasters. Computers are incredibly dumb. They perform mindless, mechanical steps extremely fast but they have no understanding of what the steps mean. The only errors they can correct are the ones you anticipate and provide with corrective algorithms. You are the source of the intelligence; the computer amplifies your intelligence but has none of its own.

We advise you to approach CT with humility. It is a learned skill. Our brains do not naturally think computationally. Keep your perspective on the capabilities of computers and algorithms to do jobs, on the need to learn something about the application domain you want to design for, on the dependency of computation on computers, and the abject lack of intelligence in the machine.

### 1.4 Emergence of CT over Millennia

It might seem that CT is a product of the electronic computer age that began in the 1940s. Well, not really. Before the modern computer age there was a profession of mathematically trained experts who performed complex calculations as teams. They were called「computers.」They were by no means the first: the term「computer,」meaning「one who computes,」dates back to the early 1600s. The first electronic computing machines were called automatic computers to distinguish them from the human variety. Human computers and, even more so, the leaders of human computing teams, obviously engaged in computational thinking. So, many aspects of CT existed before electronic computers. Well before.

Primitive forms of CT as methods of calculation were recorded from around 1800 to 1600 BCE among the Babylonians, who wrote down general procedures for solving mathematical problems. Their rule-following procedures have features that we, from today's perspective, would label as forms of CT. Similarly, the Egyptian engineers who built the pyramids beginning around 2700 BCE obviously knew a lot about geometry and were able to calculate the dimensions and angles of stones for each part of the pyramid and of the leverage of ropes, pulleys, and rollers to move the stones into position. Computing is an ancient human practice.

Over the ages since ancient times, mathematicians sought to spell out procedures for ever more advanced calculations, moving beyond calculating merchant transactions and the geometry of structures, to trigonometry, astronomic predictions, celestial navigation, solving algebraic equations, and eventually computing with the calculus of Newton and Leibniz. By formalizing computing procedures, mathematicians made their expertise available to non-experts who simply had to follow directions of carrying out simple arithmetic operations in the proper order. A special class of those directions is today called an algorithm — a key concept in modern computing. The term「algorithm」comes to us from the Persian mathematician Muhammad ibn Mūsā al-Khwārizmī who, around 800 CE, discussed how to formulate mathematical procedures and gave examples such as finding the greatest common divisor of a set of numbers.

We humans have a penchant for automating routine procedures. So it has been for computational procedures: inventors sought machines that would automate computation for the purposes of greater speed and fewer errors. Building machines to carry out these procedures turned out to be much harder than specifying the procedures. Pascal designed and built an arithmetic machine in the 1600s that was able to add and subtract. It could multiply only in the hands of a human operator who understood repeated addition. It could not do division. Napier invented the logarithm, which became the principle of the slide rule — an aid for calculation that continued well into the second half of the 1900s. It could not add or subtract. In 1819 Babbage designed a machine of gears, shafts, and wheels that could calculate tables of arithmetic numbers such as logarithms. In the 1890 US census, Hollerith's punched card machines tabulated large amounts of data and, after its founding in 1924, IBM became wealthy from selling tabulating machines. In the 1920s engineers designed analog computers to calculate continuous functions by simulating them with circuits and gears. Designers of mechanical analog and digital computers were obviously computational thinkers. But even the analog computer idea is ancient, dating back to the Greek orrery, a mechanical device used to calculate planetary positions in 100 BCE.

Computing throughout the ages required computational thinking to design computational procedures and machines to automate them. The long quest for computing machines was driven not only by the need to speed up computation, but to eliminate human errors, which were common when easily bored or distracted humans performed many repetitive calculations. The designers believed automated machines would overcome the sources of error in calculations. Today we know better: while machines have eliminated some kinds of error, a whole horizon of new errors has appeared. Computing machines have become so complex that we do not know how much trust we can place on them.

Seeds of computational thinking advanced in sophistication over those many centuries. But only when the electronic computer became an industry in the 1950s did computer designers and programmers find an impetus to develop CT as a professional concern. These professionals gravitated to software because they could easily alter a machine's function by reprogramming the software. The emerging computing industry sought programmers and engineers schooled in computational thinking and practice. Educators inquired into how to teach them. The computer science (CS) field that emerged by 1960 inherited the responsibility for defining and teaching CT.

Throughout this book, we present many examples from the history of computing to illustrate the needs to which CT responded, the new possibilities for actions CT enabled, and the dramatic mind shifts CT caused in how we see automation, science, and the world. Although CT as a way of thinking has existed for thousands of years, the term「computational thinking」is relatively new: the first occurrence we are aware of is from 1980.

### 1.5 Emergence of Education Movement for CT in K–12 Schools

The first computer science (CS) department was founded at Purdue in 1962. Academic computer science matured along a bumpy and challenging path. Many universities were initially skeptical whether the new field was really new or scholarly enough; it looked rather like a branch of electrical engineering or applied mathematics. Many computer science departments were formed only after pitched political battles in their universities. Despite the political difficulties, the growth was steady. By 1980 there were about 120 CS departments in the US alone. Today all major and many smaller universities have one.

During the first 40 years, most of the concerns of practitioners in computing were focused on getting the technology to work. Everything we today consider a core technology had to be invented, designed, tested, and retested — take, for example, programming languages, operating systems, networks, graphics, databases, robotics, and artificial intelligence. One of the central elements of CT, designing reliable computing technologies, became a standard during those times.

In the 1980s, computing experienced a dramatic opening as scientists in all fields brought computation and computational thinking into mainstream science. At first, they used computation to simulate existing theoretical models or numerically tabulate and analyze data from experiments. But they soon discovered that thinking in terms of computation opened a door to a whole new way of organizing scientific investigation — CT led to Nobel Prizes for discoveries that had previously eluded scientists. Many declared computation a third pillar of science, alongside theory and experiment. CT was extended to designing computations throughout science, especially in response to「grand challenge」questions posed by leaders in the different fields. Every field of science eventually declared it had a computational branch, such as computational physics, bioinformatics, computational chemistry, digital humanities, or computational sociology.

Computer scientists had mixed reactions to these developments. Remembering their battle scars from forming departments, some were quite sensitive to the public image of computer science and wanted to control it. Some regarded the computational science movement as a means to hijack computer science by scientists who had previously expressed skepticism about computer science. As a result, computer scientists had a strong motivation to help the public understand the technology and theory of computing. Computer science educators worked with K–12 educators to define computer literacy courses, but these were not very popular. Around the year 2000 some educators proposed a more sophisticated approach they called「fluency with information technology」; high school teachers adopted a popular textbook in that area. Even with the success of the fluency approach, few high schools adopted a computer course. Computer science educators continued to seek ways to penetrate K–12 school systems and expose every student to computing.

A turning point came in 2006 when Jeannette Wing, then starting as an assistant director at the US National Science Foundation (NSF), reformulated the quest from fluency to computational thinking. She proposed that CT was a style of thinking that everyone needed to learn in the computing age. At the NSF she mobilized significant resources to train teachers, upgrade the Advanced Placement test, design new「CS principles」first-courses for colleges, define CT for the K–12 education sector, and issue curriculum recommendations for K–12 schools. This「CS for all」movement has achieved much greater penetration of computing into K–12 schools than any of its predecessors.

The definitions of CT that have emerged from the post-2006 CT movement have moved conspicuously into the public view. But many public definitions, especially as interpreted to us by policymakers, are quite narrow compared to the notions of CT developed over the earlier centuries of computing. Mainstream media occasionally give a misinformed view of the scope and influence of computing. They have led people unfamiliar with computing to make inflated claims about the power of CT that will mislead students and others into making promises about computers they cannot deliver.

### 1.6 Our Objectives in This Book

Our objective in this book is to lay out the magnificent fullness of computational thinking and its precepts about computation and to dispel misunderstandings about the strengths and limits of computing.

Computational thinking evolved from ancient origins over 4,500 years ago to its present, highly developed, professional state. The long quest for computing machines throughout the ages was driven not only by the need to speed up computation, but also to eliminate human errors, which were common when easily bored or distracted humans performed many repetitive calculations. A special thinking skill evolved to accomplish this.

The development of computational thinking opened six important dimensions that are characteristic of CT today.

2『 CT 6 大核心元素。做一张主题卡片。（2021-05-04）』—— 已完成

1 Methods. Mathematicians and engineers developed methods for computing and reasoning that non-experts could put to work simply by following directions.

2 Machines. Inventors looked for machines to automate computational procedures for the purpose of greater speed of calculation and reduction of human errors in carrying out computations. This led eventually to the invention of the digital electronic computer that harnesses the movement of electrons in circuits to carry out computations.

3 Computing Education. University educators formed computer science to study and codify computation and its ways of thinking and practicing for institutions, businesses, science, and engineering.

4 Software Engineering. Software developers formed software engineering to overcome rampant problems with errors and unreliability in software, especially large software systems such as major applications and operating systems.

5 Design. Designers bring sensibilities and responsiveness to concerns, interests, practices, and history in user communities.

6 Computational Science. Scientists formed computational science to bring computing into science, not only to support the traditions of theory and experiment, but also to offer revolutionary new ways of interpreting natural processes and conducting scientific investigations.

These six dimensions are like different windows looking at CT. Each window offers a particular angle of looking. Some aspects of CT may be visible through two windows, but each in a different light. In the next six chapters we will examine CT in relation to each dimension above. We round out with a semifinal chapter on CT in modern general education and a concluding chapter about the future of CT.

Chapter 2: CT related to algorithmic procedures to automate processes

Chapter 3: CT related to computing machinery

Chapter 4: CT related to the theory of computing and academic discipline

Chapter 5: CT related to building large software systems

Chapter 6: CT related to designing for humans

Chapter 7: CT related to all the sciences

Chapter 8: Teaching CT for all

Chapter 9: The future of CT

We offer our stories of these dimensions to show you the power of CT and the ways in which it might help you in your work with computers and computation. Computational thinking evolved from ancient origins over 4,500 years ago to its present, highly developed, professional state. The long quest for computing machines was driven not only by the need for speed, but also to eliminate human errors.

## 0201. Computational Methods

If controversies were to arise, there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in their hands, to sit down to their slates, and to say to each other (with a friend as witness if they liked): Let us calculate.

—— Leibniz, in Russell's translation (1937)

When Peter was age 10, his twinkly-eyed math teacher told him he could read minds.「How could you do that?」Peter asked. The teacher said,「Here, I'll show you. Think of a number. Double it. Add 8. Divide by 2. Subtract your original number. Now concentrate hard on the answer. ... There, I see it. The answer is 4.」Peter was so astonished he insisted that the teacher show him how to do this.「Well,」said the teacher,「it's just math. Let's say X is your number. Then I got you to calculate the expression (2X+8)÷2–X = 4. Your initial number was subtracted out. The answer is always half the number I told you to add.」Peter had many fine moments reading the minds of his family and friends with this method. He also got hooked on mathematics and computing.

This is one of many mathematical methods handed down over many generations. Methods like this are behind a pack of「automatic tricks」used by magicians, where the trickster leads the subject through a series of steps to an answer known to the trickster and believed by the subject to be a secret. The sleights of mind to accomplish this are in the math steps known to the trickster but not the subject. They work for any trickster who follows the directions, even if the trickster has no idea why the directions work.

Many other methods with a more serious purpose were handed down through the ages. One of the earliest methods, taught to many schoolchildren today, comes from the Greek mathematician Euclid around 300 BCE. He gave a method to find the greatest common divisor (GCD) of two numbers, which is the largest integer that divides both numbers. Euclid found a clever reduction rule by noticing that the GCD of two numbers divides their difference. He repeatedly replaced the larger number with their difference, until both were the same. For example, GCD(48,18) = GCD(30,18) = GCD(12,18) = GCD(12,6) = GCD(6,6) = 6. This method was used to reduce fractions. Today it is among the basic methods underlying cryptography.

Another famous method dating back to the Greeks was the Sieve of Eratosthenes, used to find all the prime numbers up to a limit. This method begins with a list of all the integers from 2 to the limit. It then crosses out all the multiples of 2, then all multiples of 3, then of 5, and so on. After each round of elimination, a new prime will be revealed; the next round crosses out all its multiples. This is a very efficient method to find small primes and has been adapted to finding large primes for keys in modern cryptographic systems.

The Greeks were also interested in calculating the areas of shapes. They did so by finding ways to tile the shapes with simple forms such as squares or triangles, and then successively reducing the dimensions of the forms until the original shape is almost completely filled with them. This method, first recorded in the period 400–350 BCE, was a precursor to better methods introduced in the modern calculus two thousand years later.

Many mathematicians used such methods to construct infinite series of simple terms that converged to some limit. Math books are filled with tables of series; mathematicians used them to replace long series with closed forms. One such example is the series , which gives a way to calculate the value of π, with greater precision when more terms are included.

The calculus, proposed independently by Newton and Leibniz around 1680, perfected the idea of approximating objects and curves by calculations over infinite series. The idea was to represent geometric forms and continuous curves with very small components that interacted locally — for example, fill the form with small boxes or model the curve as a series of tiny segments bound to their immediate neighbors with attractive forces. Then find a larger quantity such as area or length by adding the components. When the size of the components was allowed to approach zero, the expressions from these infinite sums would be exact. The rule of local interaction was represented as a derivative and the summation as an integral. Motivated by calculus, mathematicians evaluated functions by dividing time and space into small increments enumerated as a「grid」and iteratively calculated the value of a function at each grid point. This approach was a boon to physics, in which many mathematical models of physical phenomena were expressed as differential equations computable on finite grids.

Another famous mathematical method was the Gaussian elimination for solving systems of linear equations. Gauss worked with this method in the mid-1800s, although Chinese mathematicians knew it in 179 BCE. Very efficient forms of this algorithm are used in modern graphics displays to render three-dimensional objects in real time.

These few examples illustrate the richness and utility of the treasure trove of methods bequeathed to us over many centuries.

We can conclude from examining these methods that many were quite sophisticated. Their purpose was to capture, as a set of steps, what needed to be done in a complex calculation. Initially those steps were carried out by mathematicians, but with enough refinements a method could actually be used by anyone who could follow the directions. An important but subtle point is that the steps of the method had to be unambiguous. The less the ambiguity, the more trustworthy the method was in the hands of non-experts. It was a practice to reduce ambiguity by replacing fuzzy steps with precise chains of arithmetic and logic operations.

Beginning around 1650, some mathematicians started to look for machines to carry out the basic operations of common methods. Some methods were too complex to be easily remembered. Some methods needed to be iterated many times, and it was difficult for easily distracted human beings to complete them without errors. The machines would allow for much faster computation and fewer errors.

To build machines, mathematicians and inventors had to devise methods, such as the positioning of wheels and gears, to represent numbers with physical quantities. They also had to devise representations for logic steps such as a conditional jump or a loop. Today, representations of data and logic steps are important core elements of computational thinking. In the rest of this chapter we describe these aspects in more depth.

### 2.1 The Quest to Eliminate Intuition

The computational methods that evolved in the history of mathematics were intended to help builders, engineers, merchants, and scientists to calculate numbers. Ancient merchants invented number systems, accounting systems, and tools like the abacus to keep track of their businesses. Ancient engineers invented ways to build weapons of war and civilian structures of peace. All sought reliable methods of dealing with calculations involving large quantities of numbers so that their artifacts worked as intended and were dependable.

Their methods were handed down through apprenticeships and were worked mainly by experts. The experts developed rules-of-thumb, heuristics, learned hunches, and other intuitions that enabled them to solve problems that the uninitiated could not solve at all. The modern term「intuition」describes the expert's action of rapidly articulating a solution, based on extensive experience with similar situations. Intuition enables experts to find the essential core of the problem, skip unnecessary steps in solving it, and switch between solution approaches. Intuition is a manifestation of expertise and an enabler of new findings.

It might seem paradoxical then that throughout the ages much work in mathematics and logic has aimed at eliminating intuition from routine calculation and logical inference. Routine computing tasks were required to be as simple and「mechanical」as possible in order to always yield the same results regardless of who did the calculations. Mathematicians throughout history have sought to capture expertise into step-by-step procedures someone could follow with little training. Eliminating intuition from routine jobs did not mean eliminating experts, but rather making their expertise available to a large number of non-expert people.

The modern ideas of symbolic information representation, symbol processing, unambiguous computational steps, basic arithmetic, algorithms, synchronization of computations, and systematic error checking are all inheritances from those many centuries. By showing how mechanization of calculations has been a key feature in numerous developments of computational methods, our aim is to reveal how many computational thinking skills have been an integral part of many other kinds of thinking long before modern computing. Many features and practices of computational thinking support the designing of computations in many fields, not just in today's computer science.

### 2.2 Numerical Representations and Numerical Methods

Computational thinking, like much of modern science, relies on a process of representing things in the world in numbers or other symbols. A representation is an arrangement of symbols in an agreed format; each representation stands for something. We frequently use numbers to represent quantities such as census data on populations, business accounting ledgers, or engineering measurements. But numbers are not the only kinds of representation. Scientists and engineers represent phenomena with equations, such as a linear algebraic matrix for rotating an object or a differential equation for planetary motion. They also represent objects with mechanical artifacts such as models of buildings, wind tunnels, or planetary orreries. Numbers, equations, and models underpin the scientific ideals of measurement, experimentation, and reproducibility of results. In the computing age, representations with non-numeric symbols have become ubiquitous — for example, a Java language program, a bitmap of an image, or a soundtrack of music. Today we use the term digitization for the process of encoding almost any information in the form of data representations that can be processed by computers.

It might seem paradoxical that throughout the ages much work in mathematics and logic has aimed at eliminating intuition from routine calculation and logical inference. Eliminating intuition from routine jobs did not mean eliminating experts, but rather making their expertise available to a large number of non-expert people.

Some skeptics did not trust these computational methods because numerical calculations were too susceptible to tiny errors in the precision of parameters and variables of the computation. This led the designers of methods to find constraints to keep accumulating errors within acceptable bounds. Today's computing machines have the same problems because the machines have limited precision (such as 32 or 64 bits) and round-off errors could accumulate in poorly designed algorithms. The calculus was a breakthrough because it allowed designers systematic ways to limit errors in their finite-difference calculations.

### 2.3 Decomposing Computing Tasks

During the time leading up to World War II, the US Army developed ever more sophisticated artillery that could fire shells over several miles. Gunners needed to know how to aim their artillery given the range, the difference of elevation, and the winds. The Army commissioned teams of human computers to work out firing tables for these guns. The gunners simply looked up the proper angle and direction to aim their guns, given their measurements of range, elevations, and winds.

One of the most well known of these teams comprised women working at Aberdeen Proving Grounds around 1940. They organized into assembly lines, each one doing a different stage of computation, until they compiled the firing tables. For tools they used mechanical calculators that do basic arithmetic (add, subtract, multiply, divide). They followed programs (i.e., sets of procedures) that managers established to divide the work and to govern which intermediate calculations moved from one human computer to the next. As trained mathematicians, the human computers were able to spot errors in their computations and thus keep the firing tables error free.

Today's computational thinking follows a similar pattern learned from those days:

1 Break the entire computation into pieces that could be done by separate, interacting computers.

2 Arrange the computers to optimize their communication and messaging patterns — for example into an assembly line or as a massive parallel fan-out and join.

3 Include error checks into their methods so that recipients could verify that their inputs and outputs were correct.

Modern software designers are familiar with these principles under the following names: distributed computing, parallelization, and error checking. But those practices were not originally developed for machines — they were developed for human computers.

The US Army wanted to perform these computations at much larger scales and much faster than human teams could at Aberdeen, so it commissioned the electronic computer project ENIAC at University of Pennsylvania to do this. The designers of ENIAC faced huge challenges, such as learning how to build reliable electronic circuits to carry out the same computations much faster, and learning how to design the control programs and algorithms to prevent errors from accumulating in the computations. The method of decomposition of the task into unambiguous steps that passed data between them moved from being a management principle at Aberdeen into a design principle for automatic computers.

Concern over errors grew as machines became larger and more complex. Today in computer science we still teach this old wisdom: errors can happen at any stage of the computing process, including describing the problem, preparing the formulas, setting the constants, communicating data, recording and retrieving the data, carrying out the prescribed steps, or displaying the results.

### 2.4 Rules for Reasoning

Designing computations around unambiguous steps is not enough to give confidence that computations are free from errors. The steps must be strung together to follow a plan. At any stage in the computation the plan must tell us unambiguously what the next step is. Deciding what the next step is should be an exercise in logic.

A long-established branch of mathematics and philosophy has been concerned with logic. Can we provide the means to develop long chains of reasoning to solve problems and to verify that a given chain of reasoning is valid? As their counterparts in calculation, logicians sought ways to formalize and automate reasoning. Philosophers such as René Descartes and Gottfried Leibniz in the 1600s sought a language that would completely formalize human inference and reduce misunderstandings. Their goal was to establish a standard way to express concepts and rules of inference to definitively establish the truth or falsity of statements. According to their vision, such a「language of thought」would bring an end to disagreements in all domains, because every debate could be resolved through pure logic.

Progress toward this dream was slow. A breakthrough came in the 1800s. George Boole (1815–1864) was fascinated by how well-formulated, mathematical symbol systems were able to provide results for problems nearly automatically once the correct values were set in the formula. [1] In his book, Laws of Thought (1854), he presented「an algebra of thought」paralleling the algebra of numbers. His logic included variables whose values could be either true or false. He could form logical expressions, which were formulas of variables connected by operators such as and, or, and not. Nearly nine decades later (1937), Claude Shannon showed how Boole's algebra could describe the function of relay circuits in telephone systems and other electrical circuits. Boolean algebra was perfected for electronic circuit design in the 1950s, where it provided a means to find the smallest circuit for a given logic function and a means to design the circuit to avoid race conditions, which are ambiguous outputs caused by signals changing at different speeds in different parts of the circuit. Boolean algebra became a fixture of computer circuit design.

1『这里看到了 2 个熟人。George Boole，布尔，布尔值。Claude Shannon，香农，信息论。（2021-05-24）』

Despite its merits, Boole's algebra of logic had some serious limitations. Sentences that refer to sets, such as「everybody with a gray hair,」while perfectly understandable in natural language, could not be expressed in Boolean logic. There was no way to generate a list of entities satisfying a formula; the concepts of「everybody」and「somebody」had no clear meaning. There were no rules for the important quantifiers all and some.

Gottlob Frege (1848–1925) presented a new system of logical inference,「language for pure thought」(1879), which today is called predicate logic. It included new quantifiers for all and some and closed gaps in Boolean logic. Frege's system also presented mechanical rules for symbol processing that could be followed without appealing to human intuition. Frege's predicate logic resembles a programming language in that it provides an artificial, formal language that presents unambiguous, deterministic, and mechanical rules for symbol processing.

3『戈特洛布·弗雷格 (Gottlob Frege)，著名德国数学家、逻辑学家和哲学家。是数理逻辑和分析哲学的奠基人。（2021-05-24）』

In the early 1900s it looked like the vision for a formal language of thought was about to be fulfilled. The merger of mathematics and logic gave rise to Russell and Whitehead's magnum opus Principia Mathematica (1910) and to logical empiricism in the sciences. But one element was still missing to achieve the dream: a method for definitively deciding whether a statement in predicate logic was true or false. The question for whether such a method exists became known as the「decision problem.」By the 1920s, it was taken as one of the major challenges in mathematics. Most mathematical logicians believed that a method existed, but no one could find it.

1 Davis (2012).

### 2.5 Mechanizing Computation

In 1935, a young Cambridge mathematics student was introduced to the decision problem. He became fascinated by the words a lecturer used to pose it: Was there a mechanical process for deciding, in a finite number of steps, whether a proposition in predicate logic is true or false? That student, Alan Turing (1912–1954), decided to develop a thoroughly mechanistic model of computing so that he could investigate the decision problem.

Turing started with the idea that, when calculating numbers, a human computer writes down a series of symbols on the paper. He represented the paper as a linear sequence of boxes each holding a single symbol. In calculating, the person moves attention from the current box to either of its nearest neighbors, possibly changing the symbol in the box. He assumed that the mind of the person doing the calculation was in one of a finite number of states, and that each of these basic moves on the paper was generated by a transition from the current state to a specified next state. This process continues until the calculation is complete. Turing took these basic actions — when in a given state, move left or right one box, read symbol at the current box, change the symbol in the current box, and move to the next state — as the basic mechanics of carrying out a computation. Clearly a machine could do these steps and keep track of the states. He noted that this machine modeled steps in calculating numbers or evaluating logic functions. After demonstrating how such a machine would work, Turing showed that there is one such machine that can simulate all others — implying that the machine model is a universal way to represent all calculations and proofs. He then proved that no machine could solve the decision problem because the very existence of a machine that could do so led to a logical paradox. This tour-de-force eventually made him famous for his「Turing machine」and its implications for computation.

A few years later, the electronic digital computer provided the means to automate calculation and proof — finally realizing, at least to some extent, the visions of mechanizing calculation and reasoning. Automation was the key to all these developments. To emphasize this, Turing called his machines a-machines, with「a」meaning「automatic.」Similarly, the engineers designing the first electronic computers in the 1940s, such as UNIVAC and BINAC, gave them names ending in「-AC」meaning「automatic computer.」Through the 1980s, computer science itself was often characterized as the science of automation. The key aspect of automation demands that a machine do the work without human intervention. The automatic computer is the ultimate realization of the old dream of making calculation available for the masses without requiring them to be experts in doing calculations.

Another key aspect of automation is recognizing that automatic computers cannot perform certain important tasks. Turing showed this when he proved no a-machine could exist to solve the decision problem. His same reasoning shows that problems of practical interest — such as determining whether a given computer program will halt or instead enter an infinite loop, or whether a given program contains a computer virus — cannot be solved by any machine. For this reason, a large segment of CT is concerned with how to provide partial solutions to problems that cannot be solved by automatic computers.

The automatic computer and the understandings of its limitations would not have been implemented without the merger of calculation and logic. It is no wonder many people consider logical thinking an essential element of computational thinking.

### 2.6 Computational Thinking Insights Come from Many Fields

It should be clear from this discussion of the origins of computational thinking that CT is not about how computer scientists think. Modern computer science is the last 1 percent of the historical timeline of computational thinking. Computer scientists inherited and then perfected computational thinking from a long line of mathematicians, natural philosophers, scientists, and engineers all interested in performing large calculations and complex inferences without error. CT is a feature of many fields, not only computing.

Logicians wished to create formal systems where one could start from the premises and, by following chains of substitutions within a formal system of rules, would always arrive at the same conclusions. The logical insights of Boole and Shannon — that a few logical operations can express the truth values of all propositional logic as well as the logical design of digital circuits — were driven by an old quest to banish all human emotion and judgment from logical inference. These insights are counted today as the first principles of computing. Frege's logical insight — predicate logic — presented a more powerful system of inference having many similarities with modern programming languages. Turing's insight into the essential features of automatic processing — that five actions and a finite number of states are enough for any computation — came from mathematical logic.

Other basic insights of computational thinking arose from science and engineering. Among the most important is the realization that most computations in science and technology require unimaginably long calculations that are well beyond the capabilities of a human team. The designers of computational methods to solve practical problems are obsessively concerned with controlling and limiting errors by making the computational steps simple and unambiguous and their connective logic unimpeachable.

The computer of today is the machine many sought throughout the ages to automate calculation and free it from the frailties of humans and the need for their intervention and judgment. Modern computing researchers and professionals embody this long history and excel at automating computations using the best methods available. However, as we will see in the next chapter, the wish of building real systems for very large, error-free computations has been exceedingly difficult to achieve.

The computer of today is the machine many sought throughout the ages to automate calculation and free it from the frailties of humans and the need for their intervention and judgment.

### References and Further Reading

Davis, Martin. (2012). The Universal Computer: The Road from Leibniz to Turing. CRC Press.

Grier, David A. (2005). When Computers Were Human. Princeton University Press.

Hodges, Andrew. (1983). Alan Turing: The Enigma. Vintage Books.

Priestley, Mark. (2011). A Science of Operations: Machines, Logic and the Invention of Programming. Springer-Verlag.

Rapaport, William J. (2018). Philosophy of Computer Science. An online book draft, https://cse.buffalo.edu/~rapaport/Papers/phics.pdf.

Williams, Michael R. (1997). A History of Computing Technology. 2nd edition. IEEE Computer Society Press.

## 0301. Computing Machines

The intolerable labour and fatiguing monotony of a continued repetition of similar arithmetical calculations, first excited the desire, and afterwards suggested the idea, of a machine, which, by the aid of gravity or any other moving power, should become a substitute for one of the lowest operations of human intellect.

—— Charles Babbage (July 3, 1822, letter to Humphry Davy)

Now we will look at the evolution of computing machines and the dimension of computational thinking needed to design and understand them. The primary practical motivation for building computing machines was always to speed up computation and eliminate the errors inherent in human computing.

People have always been fascinated by the idea of building devices that automated aspects of human behavior or human thinking. For millennia, craftsmen built automata for art and amusement, such as animated animal figures, music boxes, player pianos, and human-like figurines mimicking people's behavior. The Mechanical Turk, a chess-playing automaton, created a sensation in 1770 because it seemed to mechanize chess play, then considered a high-order mental skill. It was later revealed to be an elaborate hoax. But it titillated the curiosity of inventors who wondered if they could really build a chess-playing machine. Some philosophers believed that automata for calculation, another revered human mental skill, might be more feasible because the rules of basic arithmetic were much clearer and simpler than the rules and strategies of chess.

### 3.1 The Rise of Computing Machines

When experts can codify, as procedural steps, what they know about calculation and reasoning, their knowledge becomes useful to many non-experts, who can obtain the results without error simply by following directions. But no matter how precise the procedure, human operators are prone to making mistakes. They are forgetful, they do not fully understand every computational operation, they are easily distracted, and they are quickly bored by a long routine calculation. No matter how simple and unambiguous the steps, human computers make mistakes. A lot of them. One study of 40 volumes of old mathematical tables found 3,700 errors, and another found 40 errors on just one page.

For this reason, inventors through the ages sought computing machines and aids for calculation that would allow humans to complete longer computations with fewer errors. This was a slow process. The slide rule was invented around 1620. By sliding sticks marked with logarithmic scales past each other, it implemented the method of multiplication based on summing logarithms. But the slide rule could not add or subtract. Blaise Pascal designed a calculator in 1642; it could add and subtract, but could not multiply or divide. Attempts by others to extend Pascal's design to permit multiplication failed.

The slide rule found its home among engineers and the arithmetic calculator among mathematicians and accountants. Over the following centuries, these kinds of machines were gradually improved. By the 1930s, Keuffel and Esser Company was the primary supplier of log-trig slide rules and Marchant was the primary supplier of mechanical calculators that did all four arithmetic operations. Many slide-rule and mechanical calculator companies were swept away by the avalanche of change unleashed by the electronic computer revolution in the 1950s. New companies such as Hewlett-Packard and Texas Instruments started to produce all-electronic desktop calculators that could perform all slide-rule and arithmetic functions. The coup de grâce came in 1972 with the HP-35 programmable handheld calculator, which replaced the slide-rule on the engineer's belt.

Despite their popularity, the slide rule and calculating machine had two serious limitations. First, they could not perform long chains of calculations; human operators had to do that. Second, these tools could only be used for a single purpose. The electronic digital computer overcame these limitations with a radical idea: software stored internally in the machine's memory. Software could perform long calculations and could easily be adjusted to change the operation of the underlying machine.

Precursors to the idea of software originated well before the electronic computing age. In the early 1700s, French textile weavers experimented with machines that could weave complex patterns using an automatic loom. One of the more well known of these machines was the Jacquard loom, which was controlled by long chains of punched cards; a hole in a card let a hook through, lifting a thread that became part of a single line of the weave. Jacquard's automatic loom revolutionized textile weaving. Jacquard's cards were a form of external, changeable software that controlled the operation of the loom.

The idea of controlling machines with punched cards appealed to Herman Hollerith, who designed a machine to tabulate the data from the 1890 US Census. He recorded each citizen's data as a pattern of holes punched in a card, representing characteristics such as sex, address, and ethnic origin. The tabulating machine selected out cards meeting given characteristics and tallied statistics for the selected group of citizens. With Hollerith's machine, the Census Bureau completed its analysis of 63 million records in one year, far faster and cheaper than any previous census. In the following years, the same technology was adopted for myriad data processing tasks: keeping track of health of tens of thousands of soldiers, agricultural censuses, rail freight waybills, and so on.

Before seeing where tabulating machines led, we would like to back up 50 years to the significant development by Charles Babbage and Ada Lovelace: the general-purpose computer.

### 3.2 The Babbage Machines

Charles Babbage designed two significant computing machines in his long career. His Difference Engine (ca. 1820) automated the calculation of mathematical tables such as tables of logarithms or sines. His Analytical Engine (ca. 1840) was a general-purpose computer capable of any computable function.

In Babbage's day, experts prepared books of tables of important functions such as the logarithms of all six-digit numbers. These tables were commonly used for mathematical calculations; for example, one could multiply two numbers by looking up and adding their logarithms. These tables were computed by hand using difference formulas that calculated each line of the table from the previous line. Babbage knew that these hand-computed books contained many errors, and those errors sometimes led to serious consequences. For example, he argued that errors in the navigation tables used by the British Navy caused shipwrecks. He wished to eliminate the errors by replacing humans with machinery that does not get tired, bored, or distracted. He conceived of a machine that he called Difference Engine to calculate and print tables of numbers. Intrigued, the British government gave him funds to develop it.

Babbage spent the better part of the next 20 years trying to build his machine. It was a much bigger challenge than he thought: the mechanical engineering methods of the day were not able to produce thousands of gears and levers with the precision needed to avoid skipping or jamming. In the 1830s he conceived of a new design called the Analytical Engine, which would need fewer parts and would be more powerful — capable of calculating any mathematical function. But by that time, the government distrusted him over his failure to deliver a Difference Engine and refused to back his Analytical Engine project. He pursued that project with scraps of funding until his death in 1871, but never completed it. His visionary ideas lay dormant for the next 80 years.

The Analytical Engine took instructions from punched cards, an idea from Jacquard's loom. The punched cards contained a program that would instruct the machine to automatically compute a mathematical function. It was able to decide what to do based on earlier results (selection) and repeat parts of its program (looping). It had separate units for separate functions of the machine: input, processing, memory, and output. It composed machine instructions from microprograms.

Babbage collaborated with a gifted English mathematician, Ada Lovelace, who designed algorithms for the Analytical Engine. One of her example programs calculated a sequence of rational numbers called Bernoulli numbers. Babbage and Lovelace are often regarded as the first programmers. What is more, Lovelace saw Babbage's machine as more than a number calculator; for her it was a processor of any information that can be encoded in symbols. She called the study of such programs「the science of operations.」Her insight that computing machines can calculate not only over numbers, but over symbols that can stand for anything in the world, anticipated by a hundred years a key tenet of the modern computer age. Lovelace saw the computer as an information machine.

The vision of both Babbage and Lovelace was groundbreaking. Their designs introduced many ideas today considered as features that distinguish computational thinking from other kinds of thinking. Besides representing programs in a changeable external medium, the Analytical Engine embodied many aspects of modern computers: digital representation of data, programming, machine-executable algorithms, control structures for choosing cases and looping, arithmetic-logic unit, and microprogramming to break machine instructions into low-level logic gate operations. Ironically, some central insights of the computer age were born in the age of steam.

### 3.3 The Stored-Program Computer

Babbage's logical designs for his computer could not be realized on the era's technology, but many decades later, the dawning age of electronics opened up new possibilities. The period from the late 1930s was one of intense experimentation to build computing machines. Konrad Zuse built a computer in Germany in 1938, but the German government did not take it seriously and it made little impact. Howard Aiken, in partnership with IBM and sponsored by the US Navy, built the Mark I at Harvard in 1944. It was an electromechanical computer that straddled the mechanical world governed by Newton's laws of motion and the light-speed world governed by Maxwell's laws of electromagnetism. Its programs and input data were stored externally on punched paper tapes.

At the Moore School of Electrical Engineering at the University of Pennsylvania, John Mauchly and Presper Eckert — with support from the US Army — designed what is perhaps the most famous among the first electronic computers. Their ENIAC machine went into operation in 1945 and was used to calculate artillery-firing tables and explore the feasibility of the thermonuclear weapon. The ENIAC (Electronic Numerical Integrator and Computer) took its program from an external wire patch board; programming it was tedious. The ENIAC machine was very influential as a proof-of-concept of fully electronic computing: it worked, it was fast, and it inspired better machines soon after. Its engineers founded Univac, the first commercial company to offer an electronic computer.

In 1945, the ENIAC team, joined by John von Neumann, met to design a better machine based on their experience. Aside from the ENIAC being difficult to program, its memory was limited, and it used many thousands of vacuum tubes (18,000 of them) that gradually wore out. For their new design, the team separated the machine into three main subsystems: the central processing unit (CPU) for performing the arithmetic and logical operations, the memory for storage, and the input-output (I/O) unit for communicating with the external world. To speed up the computer, they designed a CPU that took its instructions from memory, not external punched cards or tapes, thus initiating the「stored program computer」idea. By a quirk of history, this way of organizing a machine became known as the「von Neumann architecture」because von Neumann took the notes on their meetings and distributed them. He claimed to be the note taker, not the designer. The von Neumann architecture emerged as a consensus, the plan for almost all commercial computers from that time to the present. The notion that a CPU traces out an instruction sequence among instructions stored in memory has become a central tenet of computational thinking.

### 3.4 Computational Thinking and Machines

Let us now examine the various precepts of computational thinking that these early machines and their operating systems gave us.

#### 3.4.1 Digital Representations with Signals and Binary Codes

To be processable, data must be represented as signals in the machine or as measurable disturbances in the structure of storage media. There is no information without representation. Arithmetic operations such as add and subtract must be represented as rules for transforming signals. One early way to represent a decimal digit was a ring of 10 dual-triode vacuum tubes simulating a 10-position wheel. This scheme was much more expensive than a 4-tube binary representation of the same digit. Proposals to represent decimal digits with 10 distinct voltages were dismissed because of the complexity of the circuits. Engineers quickly settled on using binary codes to represent numbers because binary-coded arithmetic used many fewer components than decimal-coded arithmetic, and because circuits to distinguish two voltage values were much more reliable than circuits to distinguish more than two values. Moreover, storage could easily be built from available two-state technology such as acoustic delay lines, magnetic cores, flip-flop circuits, or phosphor patches on a cathode-ray screen. The decision to abandon decimal arithmetic and use binary codes for everything in the computer led to very simple, much more reliable circuits and storage media. The term「bit」came into standard use as shorthand for「binary digit.」Today no one can think about contemporary computers without thinking about binary representations.

It is important to keep in mind that internally the computer does not process numbers and symbols. Computer circuits deal only with voltages, currents, switches, and malleable materials. The patterns of zeroes and ones are abstractions invented by the designers to describe what their circuits do. Because not every binary code is a valid description of a circuit, symbol, or number, the designers invented syntax rules that distinguished valid codes from invalid ones. Although the machine cannot understand what patterns mean, it can distinguish allowable patterns from others by applying the syntax rules.

We cannot overemphasize the importance of physical forms in computers — such as signals in circuits or magnetic patches on disks — for without these physical effects we could not build a computer. Although computer programs appear to be abstractions, they cannot work without the machines harnessing physical phenomena to represent and process binary numbers. For this reason, it is safe to say that every dataset, every program, and every logic circuit layout is a「strategic arrangement of stuff.」

#### 3.4.2 Boolean Algebra and Circuit Design

Because of Claude Shannon's insight that George Boole's logic precisely described electronic switching circuits, today we cannot think about computers without thinking about Boolean algebra. Boolean algebra helps us understand how the hardware implements the machine instructions generated by a compiler. But Boolean algebra is an abstraction. Sometimes it hides physical race conditions caused by multiple signals following different paths to the same output; race conditions can cause errors by causing the circuits to deviate from their Boolean formulas. This confounds programmers who are only aware of the abstractions and not the circuitry, and for that reason cannot find the errors by studying their programs.

#### 3.4.3 The Clocked CPU Cycle for Basic Computational Steps

The physical structure of computers consists of registers, which store bit patterns, and logic circuits, which compute functions of the data in the registers. It takes time for these logic circuits to propagate signals from their input registers to their output registers. If new inputs are provided before the circuits settle, the outputs are likely to be misinterpreted by subsequent circuits. Engineers solved this problem by adding clocks to computers. At each clock tick the output of a logic circuit is stored in its registers. The interval between ticks is long enough to guarantee that the circuit is completely settled before its output is stored. Computers of the von Neumann architecture cannot function without a clock. Today computers are rated by their clock speeds — for example, a「3.8 GHz processor」is one whose clock ticks 3.8 billion times a second.

The existence of clocks gives a precise physical interpretation to the「algorithmic steps」in the digital realm. Every algorithmic step must be completed before the next step is attempted. The machine supports this by guaranteeing each instruction will be correctly finished before the next instruction is attempted. (There are a few types of computers that do not use clocks, but they will not be discussed here.) Clocks are essential to support our notion of computational steps and guarantee that the computer performs them reliably.

#### 3.4.4 Control Flow

From the time of Babbage and Lovelace, programmers have realized that the machine must be able to decide which instructions are next. They do not always follow a linear sequence. In the von Neumann architecture, the address of the next instruction is stored in a CPU register called the program counter (PC), which is updated after each instruction. The default is to execute the next instruction in sequence (PC set to PC+1). One common deviation from linearity is to branch to another instruction at a different memory location, say X. The decision to branch is governed by a condition C (such as「is A equal to B?」) and the jump from one part of the program to another part is implemented by an instruction that says「if C then set PC to X.」This method of controlling the program counter so that the program execution jumps to a different part of the code is manifested in computational thinking as the if-then-else construct in programming languages.

#### 3.4.5 Loops: Small Programs Making Big Computations

If all our programs were nothing more than decision trees of instruction sequences each selected by if-then-else, they could never generate computations longer than the number of instructions in the program. The loop allows us to design computations that are much longer than the size of the program. A loop is a sequence of instructions that are repeated over and over until a stopping condition is satisfied. A loop can be implemented with an if-then-else that branches back to the loop's start when the stopping condition is false. A common programming error is a faulty stopping condition that does not exit the loop. That behavior is called an「infinite loop.」

Alan Turing proved that there is no algorithm for inspecting a program to determine if any of its loops is infinite. This makes debugging a challenging problem that cannot be automated. Programmers spend a great deal of time looking for mistakes in their programs.

Some programs are built on purpose to loop forever. This is very common in service processes on the Web. The service process waits at a homing position for an incoming request; it then executes code to fulfill the request and returns to its homing position. While this facilitates designing service processes, it does not remove the challenge of proving that the service process always returns to its homing position.

#### 3.4.6 The Address-Contents Distinction

Stored-program computing machines introduced a distinction between an address (a name) and a value (associated with the name). In a program, a variable X names a memory location holding a value. In classical algebra, X is an unknown value. In a program, the statement「X=3」means「store the value 3 in the memory location named X.」Contrast this with the meaning of「X=3」in classical algebra, which is「the unknown X has the value 3.」In a program,「X=3」is a command; in algebra it is a fact. This distinction is part of our computational thinking. Novice programmers who do not make this distinction often draft programs that do not work.

#### 3.4.7 Subprograms

By the late 1940s, designers of computers realized that a common practice of programmers would be to write code for standard functions that could be invoked from anywhere in their programs. For example, an expert programmer could write code for a SORT function that anybody else can use to arrange a list of numbers in ascending order. To enable the efficient invocation of such subprograms, the designers included a new kind of branch instruction in their machines. An instruction「CALL X」would remember the current value of the program counter (PC) and then set PC to X, thereby transferring control to the subprogram stored in memory at location X. On completion, the subprogram would execute a「RETURN」instruction that restored the remembered PC value, enabling the original program to resume operation from the point of call.

The idea of subprograms has become an important principle of computational thinking. Hardware designers have given us efficient implementations. Subprograms appear in programming languages as「subroutines,」「functions,」and「procedures.」It is taken for granted today that programs are divided into modules implemented as subprograms.

#### 3.4.8 Universal Machines

In 1936, Alan Turing introduced the idea of a universal machine — a computer that could simulate any other computer, given the program of the other computer. The universal machine itself was not very complicated. This idea was implicit in the designs of machines dating back to Babbage's Analytical Engine: designers build one base machine that can run many programs. The base machine is an example of a universal machine. Today this is taken for granted: software designers assume that compilers and operating systems will make their software work on a basic underlying universal machine.

Sometimes people equate the idea of a universal machine with a stored program computer. They are not the same. Babbage's Analytical Engine was a universal machine whose programs were external decks of punched cards. The ENIAC was a universal machine whose programs were external patch boards. After 1945, computers were universal machines that stored their programs in internal memory.

The stored program computer makes it possible to switch the interpretation of a set of bits in memory between data and instruction. The very same patterns in the computer memory can be bits that represent things (data), as well as bits that do things (instructions). A compiler, for example, generates machine code as output data; the CPU can immediately interpret those data as executable instructions. Some early machines allowed programs to modify their own code to achieve greater efficiency. But most operating systems prohibited this by making machine code read-only: that allows the sharing but not the changing of code. The older idea of self-modifying programs is far from dead: malware today constantly modifies its own code to escape detection by antivirus software.

#### 3.4.9 Fault Tolerance and Data Protection

Logic circuits regularly experience errors from physical causes. For example, the state of a component might be unpredictable if conflicting signals arrive at the same time, or if the clock is too fast to allow some components to settle into new states, or if components deteriorate and fail over time. Circuit engineers spend a lot of time on fault tolerance. They have generally done a good job because hardware is sufficiently reliable that users do not worry about errors in the hardware.

In the 1950s design engineers began to think about multiple-access computers that would be shared within a user community. Correspondingly, CT expanded from single-user computations to multi-user computations. Multi-user systems had to guarantee that no user could access another's data without explicit permission. This setup would provide the significant benefit of allowing users to share programs and data and would reduce the cost per user by spreading costs across many users. Designers of the first operating systems achieved this by isolating each executing program in a private region of memory defined by a base address and length. The base-length numbers were placed in a CPU register so that all memory accesses from the CPU were confined to the defined region of memory. This idea of partitioning memory and setting up the hardware so that it was impossible for a CPU to access outside its private memory was crucial for data protection. It not only protected user programs from each other; it could be used to protect users from untrusted software, which could be confined into its own memory region.

Users of machines and networks today are aware they are sharing their machines and networks with many others. They assume that the operating systems and networks are enforcing the isolation principle by keeping the executing programs in private memory regions. When they download new software they do not trust, they expect their operating system to isolate the new software in a memory region called a「sandbox.」

Although it has been in our computational thinking for a long time that operating systems isolate programs, many computer chips designed in the 1980s dropped out the memory bound checks in order to achieve greater speed. Many security specialists are now regretting this omission. New generations of hardware may once again enforce the security checks that CT experience leads users to believe are present.

### 3.5 Beyond the von Neumann Architecture

One of the popular modern definitions of computational thinking is「formulating problems so that their solutions can be expressed as computational steps carried out by a machine.」This definition is closely tied to the framework of the von Neumann architecture. In effect, the definition is a generalization of the operation of the CPU in a von Neumann machine.

After half a century, the von Neumann architecture has been approaching its limits. There are two main reasons. One is that the underlying chip technology, which has been doubling its component count every two years according to Moore's law, can no longer absorb the continuous reductions in component size. Soon components will be so small they cannot comprise enough atoms to allow them to function properly. The impending end of Moore's law has motivated extensive research into alternative architectures.

The other reason is that the separation of processor and memory in von Neumann architecture creates massive data traffic between processor and memory. One technology invented to lessen the processor-memory bottleneck is the cache, which retains data in the CPU rather than returning it to memory. Another technology intersperses processor and memory in a cellular array to spread the data load among many smaller processor-memory channels. A third technology is special purpose chips — ones that do a particular job exceptionally well but are not general-purpose computers themselves. An example is the graphics processing units (GPUs) now permeating every computer with a graphics display. Special purpose processors are themselves the subject of extensive research.

Two new categories of computer architecture have been getting special attention. Both are potential disruptors of today's computational thinking. One is the neural network, which has been the powerhouse behind recent advances in artificial intelligence. A neural network maps large bit patterns (for example, the bits of a photograph) into other bit patterns (for example, labeled faces in the photograph). The input signals travel through multiple layers where they are combined according to assigned weights. An external algorithm trains the network by presenting it with a large number of input-output pairs and assigning the internal weights so that the network properly maps each input to its corresponding output. Training a network is computationally intensive, taking anywhere from many hours to several days. A trained network is very fast, giving its output almost instantly after the input is presented. Graphics-processing chips have been successful in achieving fast response of a trained neural network. Although machines capable of only pattern matching and recognition are not general-purpose (universal) computers, they have produced amazing advances in automating some human cognitive tasks, such as recognizing faces. However, there is no mechanism for verifying that a neural network will give the proper output when presented with an input not in its training set. It is very jarring to our computational thinking to be unable to「explain」how a computational network generated its conclusion.

The invention of the fully-electronic stored program computer changed the very concept of computing and created a fresh world of computational concepts that had few counterparts or precursors. The concepts, practices, and skills for designing programs and computers quickly diverged from mathematics and logic. It was a profound change.

The other computer architecture getting special attention uses quantum mechanical effects to process data. These quantum machines represent bits with electron spins and connections with quantum effects such as entanglement. Quantum computers can perform some computations much faster than von Neumann computers. One such computation is factoring a large composite number into its two constituent primes. The intractability of factoring on von Neumann architectures has been the principle behind the security of the RSA cryptosystem, which is currently the most secure cryptosystem in wide use. Quantum computers threaten to break its security. Because their operation is nothing at all like that of the von Neumann computers, most people trained in computer science rather than physics find it very difficult to understand the operation of these machines or how to program them.

These two examples illustrate how each kind of machine has an associated style of computational thinking and is quite good at particular kinds of problems. A person with advanced knowledge in CT would be familiar with these architectures and, as part of the design process, would select the best architecture for solving the problem. At the same time, particular machine types can also induce a kind of「blindness」 — for example, designers schooled in the basic von Neumann architecture think in terms of instructions and have trouble understanding how a quantum computer works.

Until the 1940s, computing was seen largely as an intellectual task of humans and a branch of mathematics and logic. The invention of the fully electronic stored program computer changed the very concept of computing, and it created a fresh world of computational concepts that had few counterparts or precursors. The concepts, practices, and skills for designing programs and computers quickly diverged from mathematics and logic. It was a profound change.

And until the 1940s, computational thinking was embedded in the tacit knowledge and state-of-the-art practices of many different fields, including mathematics, logic, engineering, and natural sciences. After the 1940s, computational thinking started to become the centerpiece of the new profession that designed information machines to do jobs humans never thought were possible.

### References and Further Reading

Aspray, William, ed. (1990). Computing Before Computers. Iowa State University Press.

Campbell-Kelly, Martin, and William Aspray. (2004). Computer: A History of the Information Machine. 2nd edition. Westview Press.

Ceruzzi, Paul E. (2003). A History of Modern Computing. 2nd edition. MIT Press.

Cortada, J. W. (1993). Before the Computer: IBM, NCR, Burroughs, and Remington Rand and the Industry They Created, 1865–1956. Princeton University Press.

Williams, Michael R. (1997). A History of Computing Technology. 2nd edition. IEEE Computer Society Press.

## 0401. Computer Science

The question「What can be automated?」is one of the most inspiring philosophical and practical questions of contemporary civilization.

—— George Forsythe (1969)

In the 1950s academics started to advocate for the formation of computer science programs in universities to meet a rising hunger for learning the new technology. Many precepts of CT were refined and perfected in computer science departments since that time. We turn now to the story of how CT developed in the universities.

Before we begin, we would like to point out a few key aspects of the academic environment in which CT developed. First and foremost, computing is a technical field blending engineering, science, and mathematics. Most computing students come to university to learn a profession of software and hardware designers, not to obtain a general education. Employers also come to university to recruit graduates for jobs. Thus, the CT that evolved along with academic computing has always had a strong component around design and has been strongly influenced by what employers say they need.

But that is not all. Universities are organized into a set of departments by discipline and a scattering of cross-disciplinary institutes and centers. The departments fiercely protect their identities, budgets, and space. Because their budgets depend on students enrolled, they are protective of their enrollments. And because enrollments depend on reputation and reputation on research productivity, university departments are protective of their research domains.

Another important shaping aspect of academia is the practice of seeking consensus on all decisions. Everybody wants a say, whether it is hiring a new person, awarding tenure, deciding what courses will be offered, approving possibly overlapping courses proposed by other departments, or approving the formation of new programs or departments.

This is the atmosphere in which new CS departments and academic computational thinking were formed. The founders worried about curriculum and industry demand in the context of a set of consensus-seeking departments fiercely guarding their prerogatives, always concerned with public image and identities.

The new departments proposed by the founders were split off from their existing departments. Their home departments often did not support the split because they would lose students, budget, and identity. The founders encountered a lot of resistance from other departments that did not deem a new department focused on computer technology to be legitimately science or engineering, or see that it would provide a unique intellectual perspective. Forging a consensus favoring formation of a new department was a challenge. Thus, the founders spent a good deal of time debating about the substance of computing, why it was different and new, and how it would benefit the other fields. They built a good case and were successful. Slowly the number of computer science departments grew, from 1 in 1962 to around 120 in 1980 in the US alone. Eventually in the late 1990s computer science took off as people finally realized the computing revolution is real. Today nearly every university has a computer science department.

Computer science departments are found in schools of science, engineering, and even business. Why so many homes? The answer echoes those early political fights: the new departments were established in the schools that were most welcoming. Because most of the departments were in schools of science and engineering, by the 1980s, computer scientists were labeling their field「CS&E.」That mouthful was simplified in the 1990s as「computing」became a popular shorthand for CS&E and its European counterpart「informatics.」In the 1990s some universities went further and established separate schools of computing, a movement that continues to grow today. What a turnaround!

Two academic computer societies were formed in the early days: the IEEE-CS (Institute of Electrical and Electronics Engineers Computer Society) in 1946, and the ACM (Association for Computing Machinery) in 1947. Because of their diligence to develop and promote curriculum recommendations, there are a series of snapshots of the computing curriculum at regular intervals — 1968, 1978, 1989, 1991, 2001, and 2013. These snapshots show how the concerted efforts of computing pioneers to articulate a unique identity for computer science led them to recognize computational thinking as a distinguishing aspect from the beginning. In hindsight, we can discern four eras describing how universities thought about computing and how those views of computational thinking changed:

1 Phenomena surrounding computers (1950s–1970s)

2 Programming as art and science (1970s)

3 Computing as automation (1980s)

4 Computing as pervasive information processes (1990s to present)

We will discuss these eras in following sections.

These four stages of CT development in the universities were strongly shaped by the initial resistance to computer science from other fields: academic computer scientists spent a lot of effort clarifying and justifying their field. But computer science was not always the receiver of resistance. There were two important instances when computer science was the giver. One was the computational science movement in the 1980s, which was eschewed by many computer scientists. A common reaction to an announcement by the physics or biology department that they were setting up a computational science branch would be a howl of protest that those departments were impinging on the territory of computing. Some computer scientists believed that physics and biology, having now recognized the importance of computing, were trying to hijack the field they once vociferously opposed. Eventually computer scientists got over this and now work collaboratively with computational sciences. We will talk about computational science in chapter 7.

A similar process happened with software engineering. The computing departments that viewed themselves as science were not receptive to the practices of teaching and doing projects common in engineering. Software engineering had trouble gaining a foothold in those departments. There was an ongoing debate in computer science for a long time about whether software engineering is part of computer science or should be its own department. We will talk about that in chapter 5.

### 4.1 Phenomena Surrounding Computers

The developers of early automatic computers realized quickly that the new machines required a way of thinking and designing that differed from anything already existing in science or engineering. The ACM and IEEE started journals for the young field in the early 1950s. The Moore School, home of the ENIAC project, was an early starter of computing education in 1946 with a two-month intensive course on「theory and techniques for design of electronic digital computers.」In the 1950s the Moore School offered a multi-discipline degree in computing that included numerical analysis, programming, and programming language design. Other schools started their own programs.

These early efforts to establish computing as an academic discipline were slow to gain traction. The impediment was more than a cautionary hesitancy to see if computers were here to stay; it was a deep doubt about whether computing had academic substance beyond mathematics, electrical engineering, and physics. Outsiders typically saw the computing field of the 1950s as an impenetrable and anarchistic thicket of idiosyncratic technology tricks. What is more, the different perspectives to thinking about computing were disunited: those who designed computing machines were mostly unaware of important developments in the theory of computing such as Turing on computable numbers, Church on lambda calculus, Post on string manipulation, Kleene on regular expressions, Rabin and Scott on nondeterministic machines, and Chomsky on the relation between grammars and classes of automata. [1]

Academics who proposed full-fledged computer science departments or programs in research universities met stiff resistance. Many critics did not believe in the value of computing's new ways: common objections included lack of unique intellectual content and lack of adequate theoretical basis. Purists argued that computers were human-made artifacts and not natural occurrences, and thus their study could not be counted among the noble natural sciences. On top of all that, many doubted whether computing would last. Until there was a consensus among many departments, no one could found a computer science department.

This tide began to change in 1962, when Purdue established the first computer science department and Stanford followed soon thereafter. Over the next two decades the number of departments grew slowly but steadily to well over a hundred just in the US. Even so, many academics continued to question whether computer science was a legitimate field of science or engineering.

A major shift in the question about the legitimacy of computing happened in 1967, when three well-recognized computer scientists — Allen Newell, Alan Perlis, and Herbert Simon — published a famous letter in Science addressing the question. They wrote:「Wherever there are phenomena, there can be a science to describe and explain those phenomena. Thus, ... botany is the study of plants, ... zoology is the study of animals, astronomy the study of stars, and so on. Phenomena breed sciences. ... There are computers. Ergo, computer science is the study of computers. The phenomena surrounding computers are varied, complex, rich.」[2] From this basis they quickly dismissed six objections, including the one that computers are human-made and are therefore not legitimate objects of a science. Herb Simon, a Nobel laureate in economics, so objected to the notion that there could be no science surrounding human-made objects that he wrote a now-classic book titled Sciences of the Artificial refuting this idea. [3] He gave an example from time-sharing systems (computers that allow many simultaneous users): The early development of time-sharing systems could not have been guided by theory as there was none, and most predictions about how time-sharing systems would behave were astonishingly inaccurate. It was not possible to develop a theory of time-sharing systems without actually building those systems; after they were built, empirical research on their behavior led to a rich theoretical base about them. In other words, CT could not approach problems from one direction only — the engineering aspects and scientific-mathematical aspects of computing evolved in a synergistic way to yield a science that was not purely a natural science.

The notion of computing as the study of phenomena surrounding computers quickly gained traction, and by the end of the 1960s was taken as the definition of computing. A view of the field's uniqueness started to form around that notion. The term「algorithmic thinking」was used to describe the most obvious aspect of new kind of thinking. The field's unique aims, typical problems, methods of solving those problems, and kinds of solutions were the basis of CT.

The computing pioneers expanded computational thinking beyond what they inherited from the long history of computation. They focused on the construction principles of programs, computing machines, and operating systems. They worked out a large number of computing concepts that are today taken for granted, including named variables, control structures, data structures, data types, formal programming languages, subroutines, compilers, input-output protocols, instruction pipelines, interrupt systems, computing processes, memory hierarchies, caches, virtual memory, peripherals, and interfaces. Programming methodology and computer systems architecture were main drivers in the development of computational thinking. By 1970, most computer scientists said that computing's characteristic ways of thinking and practicing — which today are called computational thinking — embrace all the knowledge and skills relating to computers.

Computational thinking divided early into a hardware flavor and a software flavor. The hardware flavor was followed by computer engineers in the engineering school; the software flavor by software designers and computing theorists in the science school.

1 Mahoney (2011);

2 Newell, Perlis, and Simon (1967).

3 Simon (1969).

### 4.2 Programming as Art and Science

The 1960s were a maturing period for computing that produced considerable richness in the ways computer scientists thought about their work and their field. The subfield of operating systems was born in the early 1960s to bring cheap, interactive computing to large user communities — CT acquired a systems attitude. The subfield of software engineering was born in the late 1960s from a concern that existing models of programming were incapable of developing reliable and dependable production software — CT acquired an engineering attitude. The subfield of networking was born in 1967 when the ARPANET project was started — CT acquired a networking attitude.

With a solid, reliable technology base in place, the field's attention shifted to programs and programming. Many programming languages came into existence along with standard ways of programming. A huge interest in formal verification of programs welled up, seeking a theory-based way to demonstrate that programs were reliable and correct. A similar interest in computational complexity also welled up, seeking analytical ways to assess just how much computational work the different algorithms required.

Computer programs are expressions of algorithms in a formal language that, when compiled to machine-executable form, control the actions of a machine. Programs are central to nearly all of computing: Most professionals and researchers in computing work in some way or another with programs. On the first stored-program computers of the 1940s, programming was done in assembly languages that converted short abbreviated codes for instructions line-by-line to machine code that computers can run. For example, the instruction「ADD R1,R2,R3」would place the sum of registers R1 and R2 into register R3. That instruction was converted to machine code by substituting binary codes for ADD, R1, R2, and R3. Writing programs in assembly language was very tedious and error-prone.

Programming languages were invented to provide precise higher-level expressions of what the programmer wanted, which could then be unambiguously translated by a compiler to machine code. This greatly simplified the job of programming, making it much more productive and much less error-prone. The first widely adopted programming languages introduced a plethora of new CT concepts that had few or no counterparts in other intellectual traditions.

Most programming languages were aimed at helping automate important jobs such as analyzing scientific data and evaluating mathematical models (FORTRAN in 1957), making logical deductions (LISP in 1958), or tracking business inventories and maintaining customer databases (COBOL in 1959). A few languages aimed at allowing people to communicate precise specifications of algorithms that could be incorporated into other languages. The ALGOL language (1958) was developed from this perspective.

The idea that languages cater to particular ways of thinking about problems came to be called「programming paradigms.」For example, imperative programming saw programs as series of modules (called「procedures」) whose instructions commanded the machine. FORTRAN, COBOL, and ALGOL all fit this category. Object-oriented programming treated programs as collections of relatively self-sufficient units,「objects,」that interact with each other and with the outside world by exchanging messages. Later languages such as Smalltalk and Java fit this category. Functional programming treated programs as sets of mathematical functions that generate output data from input data. LISP is an example.

These programming paradigms were seen in the 1970s as different styles of algorithmic thinking. They all sought programs that are clear expressions for humans to read and perform correctly and efficiently when compiled and executed. Donald Knuth, in his major works The Art of Computer Programming and Literate Programming, and Edsger Dijkstra in his work on structured programming, epitomized the idea that computing is about algorithms in this sense. By 1980, most computer scientists said that computational thinking is a set of skills and knowledge related to algorithms and software development.

But things got tricky when the proponents of algorithmic thinking had to describe what algorithmic thinking was and how it differed from other kinds of thinking. Knuth compared the reasoning patterns in mathematics textbooks and computing textbooks, identifying typical patterns in both. [4] He concluded that algorithmic thinking differed from mathematical thinking in several aspects: by the ways in which it reduces complex problems to interconnected simple ones, emphasizes information structures, pays attention to how actions alter the states of data, and formulates symbolic representations of reality. In his own studies, Dijkstra differentiated computer scientists from mathematicians by their capacity for expressing algorithms in natural as well as formal languages, for devising notations that simplified the computations, for mastering complexity, for shifting between abstraction levels, and for inventing concepts, objects, notations, and theories when necessary. [5]

Today's descriptions of the mental tools of CT are typically much less mathematical in their orientation than were many early descriptions of algorithmic thinking. Over time, many have argued that programming and algorithmic thinking are as important as reading, writing, and arithmetic — the traditional three Rs of education — but the proposal to add them (as a new combined「R」) to that list has yet to be accepted. Computing's leaders have a long history of disagreement on this point. Some computing pioneers considered computing's ways of thinking to be a generic tool for everyone, on a par with mathematics and language. [6] Others considered algorithmic thinking to be a rather rare, innate ability — present with about one person in fifty. [7] The former view has more support among educators because it embraces the idea that everyone can learn computational thinking: CT is a skill to be learned and not an ability that one is born with. [8]

The programming and algorithms view of computing spawned new additions to the CT toolbox. The engineering-technology side provided compilers (for converting human-readable programs to executable machine codes), parsing methods (for breaking programming language statements into components), code optimization, operating systems, and empirical testing and debugging methods (for finding errors in programs). The math-science side provided a host of methods for algorithms analysis such as O-notation for estimating the efficiency of algorithms, different models of computation, and proofs of program correctness. By the late 1970s it was clear that computing moved on an intellectual trajectory with concepts, concerns, and skills very different from other academic disciplines.

4 Knuth (1974, 1985).

5 Dijkstra (1974).

6 Forsythe (1968).

7 Knuth (1985).

8 Guzdial (2014)

### 4.3 Computing as Automation

Despite all its richness, the view of computing as the study and design of algorithms was seen as too narrow. By the late 1970s, there were many other questions under investigation. How do you design a new programming language? How do you increase programmer productivity? How do you design a secure operating system? How do you design fault-tolerant software systems and machines? How do you transmit data reliably over a packet network? How do you protect systems against data theft by intruders or malware? How do you find the bottlenecks of a computer system or network? How do you find the response time of a system? How do you get a system to do work previously done by human operators? The study of algorithms focused on individual algorithms but rarely on their interactions with humans or the effects of their computations on other users of systems and networks. It could hardly provide complete answers to these questions.

The idea emerged that the common factor in all these questions, and the soul of computational thinking, was that computing enabled automation in many fields. Automation generally meant one of two things: the control of processes by mechanical means with minimal human intervention, or the carrying out of a process by a machine. Many wanted to return to the 1960s notion that automation was the ultimate purpose of computers and among the most intriguing questions of the modern age. Automation seemed to be the common factor among all of computer science, and CT seemed to be about making automation efficient.

In 1978 the US National Science Foundation launched a comprehensive project to map what is essential in computing. It was called the「Computer Science and Engineering Research Study」(COSERS). In 1980 they released What Can Be Automated?, a thousand-page tome that examined numerous aspects of computing and its applications from the standpoint of efficient automation. [9] That study answered many of the questions above, and for many years, the COSERS report offered the most complete picture of computing and the era's computational thinking. It is still a very relevant resource for anyone who wants an overview, written by famous computing pioneers, of many central themes, problems, and questions in computing.

Well into the 1990s, the computing-as-automation idea was adopted in books, research reports, and influential policy documents as the「fundamental question underlying computing.」This idea resonated well with the history of computational thinking: As we discussed in the previous chapters, automatic computing realized the dream of applied mathematicians and engineers to calculate rapidly and correctly without relying on human intuition and judgment. Theoreticians such Alan Turing were fascinated by the idea of mechanizing computing. Practitioners saw their programs as automations of tasks. By 1990,「What can be automated?」became a popular slogan in explanations of computing to outsiders and a carrying theme of computational thinking.

Ironically, the question of「what can be automated」led to the undoing of the automation interpretation because the boundary between what can and cannot be automated is ambiguous. What was previously impossible to automate might now be possible thanks to new algorithms or faster hardware. By the 1970s, computer scientists had developed a rich theory of computational complexity, which classified problems according to how many computational steps algorithms solving them needed. For example, searching an unordered list of N items for a specific item takes time proportional to N steps. Sorting a list of N items into ascending order is more complex: it takes time on order of N2 steps by some algorithms and on order of N log N steps by the best algorithms. Printing a list of all subsets of N items takes time proportional to 2N. The search problem is of「linear difficulty,」the sorting problem is of「quadratic difficulty,」and the printing problem is of「exponential difficulty.」Search is fast, enumeration is slow; computational complexity theorists call the former「easy」and the latter「hard.」

To see how vast the difference is between easy and hard problems, imagine that we have a computer that can do 1 billion (109) instructions per second. To search a list of 100 items would take 100 instructions or 0.1 microseconds. To enumerate and print all the subsets of 100 items would take 2100 instructions, a process that would take around 1014 years. That is 10,000 times longer than the age of the universe, which is very roughly around 1010 years old. Even though we can write an algorithm to do that, there is no computer that could complete the job in a reasonable amount of time. Translating this to automation, an algorithm to automate something might take an impossibly long time. Not everything for which we have an algorithm is automatable in practice. Over time, new generations of more powerful machines enable the automation of previously intractable tasks.

Heuristic algorithms make the question of computational hardness even more interesting. The famous knapsack problem asks us to pack a subset of items into a weight-limited knapsack to maximize the value of items packed. The algorithm for doing this is similar to the enumeration problem and would take an impossibly long time for most knapsacks. But we have a rule-of-thumb (a「heuristic」) that says「rate each item with its value-weight ratio, and then pack in order of decreasing ratios until the knapsack is full.」This rule of thumb packs very good knapsacks fast, but not necessarily the best. Many hard problems are like this. There are fast heuristic algorithms that do a good job but not necessarily the best. We can automate them only if we find a good heuristic algorithm.

The early findings about what things cannot be done in computing, either because they are impossible or just too long, led to pessimism about whether computing could help with most practical problems. [10] Today the mood is much more optimistic. A skilled computational thinker uses a sophisticated understanding of computational complexity, logic, and optimization methods to design good heuristic algorithms.

Although all parts of computing contribute to automation, the field of artificial intelligence (AI) has emerged as a focal point in computing for automating human cognitive tasks and other human work. The CT toolbox accumulated heuristic methods for searching solution spaces of games, for deducing conclusions from given information, and for machine-learning methods that find problem solutions by generalizing from examples.

9 Arden (1980).

10 In his talk A Logical Revolution, Moshe Vardi describes the changing role and perceptions of logic in the field of computing, including the 1980s gloominess over what computers cannot do.

### 4.4 Computing as Pervasive Information Processes

The spread of computing into many fields in the 1990s was another factor in the disintegration of the automation consensus of computational thinking in the academic world. Scientists who ran simulations or evaluated mathematical models were clearly thinking computationally but their interest was not about automating human tasks. A computational interpretation of the universe started to gain a foothold in sciences (see the next section,「The Universe as a Computer」). The nail went into the automation coffin when scientists from other fields started saying around 2000 that they worked with naturally occurring information processes. Biologists, for example, said that the natural process of DNA transcription was computational. There was nothing to automate; instead they wanted to understand and then modify the process.

Biology is not alone. Cognitive scientists see many brain processes as computational. Chemists see many chemical processes as computational and have designed new materials by computing the reactions that yield them. Drug companies use simulations and search, instead of tedious lab experiments, to find new compounds to treat diseases. Physicists see quantum mechanics as a way to explain all particles and forces as information processes. The list goes on. What is more, many new innovations like blogging, image recognition, encryption, machine learning, natural language processing, and blockchains are all innovations made possible by computing. But none of the above was an automation of any existing process — each created an altogether new process.

What a radical change from the days of Newell, Perlis, and Simon! Then the very idea of computer science was attacked because it did not study natural processes. Today much of computing is directly relevant to understanding natural processes.

### 4.5 The Universe as a Computer

Some researchers say there is another stage of evolution beyond this: the idea that the universe is itself a computer. Everything we think we see, and everything we think, is computed by a natural process. Instead of using computation to understand nature, they say, we will eventually accept that everything in nature is computation. In that case, CT is not just another skill to be learned, it is the natural behavior of the brain.

Computer science's self-story as the field that studies automation faded by the turn of the century. The nail went into the automation coffin when scientists from other fields started saying that they worked with naturally occurring information processes.

Hollywood screenwriters love this story line. They have taken it into popular science-fiction movies based on the notion that everything we think we see is produced for us by a computer simulation, and indeed every thought we think we have is an illusion given by a computation. It might be an engaging story, but there is little evidence to support it.

This claim is a generalization of a distinction familiar in artificial intelligence. Strong AI refers to the belief that suitably programmed machines can be literally intelligent. Weak AI refers to the belief that, through smart programming, machines can simulate mental activities so well they appear intelligent without being intelligent. For example, virtual assistants like Siri and Alexa are weak AI because they do a good job at recognizing common commands and acting on them without「understanding」them.

The pursuit for strong AI dominated the AI agenda from the founding of the AI field in 1950 until the late 1990s. It produced very little insight into intelligence and no machines came close to anything that could be considered intelligent in the same way humans are intelligent. The pursuit for specialized, weak AI applications rose to ascendance beginning in the 1990s and is responsible for the amazing innovations with neural networks and big data analysis.

Similar to the weak-strong distinction in AI, the「strong」computational view of the universe holds that the universe itself, along with every living being, is a digital computer. Every dimension of space and time is discrete and every movement of matter or energy is a computation. In contrast, the「weak」computational view of the universe does not claim that the world computes, but only that computational interpretations of the world are very useful for studying phenomena: we can model, simulate, and study the world using computation.

The strong computational view is highly speculative, and while it has some passionate proponents, it faces numerous problems both empirical and philosophical. Its rise is understandable as a continuation of the ongoing quest to understand the world through the latest available technology. For instance, in the Age of Enlightenment, the world was compared to the clockwork. The brain has successively been compared to the mill, the telegraph system, hydraulic systems, electromagnetic systems, and the computer. The newest stage in this progression is to interpret the world is not a classical computer but a quantum computer.

### References and Further Reading

Arden, Bruce W., ed. (1980). What Can Be Automated? Computer Science and Engineering Research Study. MIT Press.

Daylight, Edgar G. (2012). The Dawn of Software Engineering: From Turing to Dijkstra. Lonely Scholar.

Dijkstra, Edsger. W. (1974). Programming as a discipline of mathematical nature. American Mathematical Monthly 81 (6): 608–612.

Knuth, Donald E. (1974). Computer science and its relation to mathematics. American Mathematical Monthly 81 (April): 323–343.

Knuth, Donald E. (1985). Algorithmic thinking and mathematical thinking. American Mathematical Monthly 92 (March): 170–181.

Mahoney, Michael Sean. (2011). Histories of Computing. Harvard University Press.

Metropolis, N., J. Howlett, and Gian-Carlo Rota, eds. (1980). A History of Computing in the Twentieth Century: A Collection of Essays with Introductory Essay and Indexes. Academic Press.

Newell, Alan, Alan J. Perlis, and Herbert A. Simon. (1967). Computer science. Science 157 (3795): 1373–1374.

Simon, Herbert A. (1969). Sciences of the Artificial. MIT Press.

Smith, Brian C. (1998). On the Origin of Objects. MIT Press.