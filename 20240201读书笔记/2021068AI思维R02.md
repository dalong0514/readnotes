## 记忆时间

## 目录

0301 AI 炼金术：数据产生价值

0401 人人都能理解 AI

## 0301. AI 炼金术：数据产生价值

这正是历史知识的矛盾。知识如果不能改变行为，就没有用处。但知识一旦改变了行为，本身就立刻失去意义。我们拥有越多数据，对历史了解越深入，历史的轨迹就改变得越快，我们的知识也过时得越快。

—《未来简史》—

方法论是以解决问题为目标的体系或系统。在方法论的指导下我们会有一个纵观全局的广阔视野，有一套统筹全局的落地方法。缺乏方法论的指导，不论多么充满智慧的思维都难逃纸上空谈的结局。AI 思维就是从理论到实践的一套完整的思维，它既有理论层面关于人工智能的认识，也有实践层面指导人工智能落地的方法论。本章旨在讲清楚 AI 思维中的「术」，也就是从方法论的角度阐述 AI 思维是如何从数据中得出决策，创造出源源不断的价值的。数据量越发庞大，以术理事，AI 炼金术可理清纷繁复杂的数据；新问题层出不穷，以术驭事，AI 炼金术可将数据为我所用，优化业务结果；面临抉择犹豫不决，以术成事，AI 炼金术可通过深度学习等人工智能模型驱动决策。接下来就让我们了解一下，AI 炼金术到底是如何从数据中创造出价值的。

### 3.1 从数据到价值的历程

在人工智能的整个生态系统中，数据既是根本，也是重点。在人类社会中，数据无处不在，而且生生不息，越来越多。我们每天的所看所感都可以转化为数据，这些数据就是各种行为和现象的记录。特别是在互联网如此发达的当下，我们能够接触和获取到的数据更为多维、多样而且庞杂。当然，这对于人工智能的运用来说，完全是一件好事，因为人工智能的运行离不开数据的支持。对于人工智能来说，数据更多，就相当于炼油厂运来了更多的原油，人工智能就能够更好地运作，为人类的生产、生活增添价值。人工智能通过数据驱动决策而产生的变现是我们能够真实看到的，比如优化提升的用户体验，实打实的业绩、利润，或者是口袋里不断飙升的年终奖。

关于数据的重要性，我们先从奈飞（Netflix）的案例说起。奈飞最出名的身份是世界最大的在线影片租赁服务商，但它同时也是美国的流媒体巨头，业务广泛，也涉足影视剧拍摄。奈飞想在好剧频出的美剧圈拍摄出一部像《权力的游戏》《生活大爆炸》《破产姐妹》这样的高人气剧集，于是它开始探索 —— 究竟该怎么做呢？据统计，奈飞在全世界的订阅用户总量达 3 300 万；每天，用户都会在奈飞的网站上产生 3 000 多万个行为，订阅用户会在网站上给出 400 多万个评分和 300 多万次搜索请求，询问剧集播放时间等。奈飞借助这些大数据进一步分析 —— 通过分析几千万用户的观影历史、观影题材内容、故事情节、演员选择以及对不同影视剧的评分等数据，决定拍摄一部有关一位参议员的连续剧，这部剧就是政治题材的人气美剧《纸牌屋》。该剧一经播出就获得了巨大成功，在美国及 40 多个国家和地区成为最热门的在线剧集之一。因为这次新鲜的尝试，奈飞前所未有地获得了 112 项艾美奖提名，也让美剧巨头 HBO（Home Box Office）电视网 18 年来第一次失去了剧集霸主的地位。

《纸牌屋》的惊人成就离不开奈飞数以亿万计的数据的支持。由此可见，数据是新时代商业取胜的重要因素。就像上面说的，奈飞通过对大量受众数据的分析，采取合理决策方案，最终获取了极大的效益和声誉。所以数据是 AI 思维的基础，要从数据产生价值，就要利用人工智能通过对数据的深度分析，引导决策者做出最优决策，从而落地价值。

如何把数据变为价值，主要涉及两个问题。第一个问题，什么样的数据在场景中有价值？无论是在电商行业、广告行业、教育行业，还是生产制造行业，企业的负责人都会不约而同地考虑他们要在网站或者设备、机器上收集什么样的数据。其实，只要是有效数据，都能为之所用，只不过要如何用或者用来做什么各不相同罢了。第二个问题是，如何通过人工智能让这些数据产生价值？这个问题已经得到了初步的解答：人工智能从诞生到现在已经经历了几十个年头，所以行业内已经有比较通用的模式应用人工智能，从而可以从数据中提取价值。

人工智能的应用并非只对行业的格局产生影响，实际上，它与我们的日常生活也息息相关。现代人每天奔波于公司与家的两点一线中，一旦有了节假日这样属于自己的时间，大部分人都会选择出门逛街看风景、到商场购物，或是看一场热映的电影，抛掉平日工作的紧张，让身心彻底放松。所以，我们都希望自己能够得到全方位的引导和服务，而不想再在这方面花费大量时间和精力了。但是，一般情况下，商家并不能很清楚地知道哪些顾客需要什么样的商品，所以尽管每天都有很多顾客路过店家门口，却并不能实现有效转化。另一方面，消费者难得有了一个放松休闲的机会，却因为没能够快速、准确地找到相关商品或者服务而影响了大好心情。为解决这些问题，人工智能就派上了用场：商家可以通过人工智能对顾客数据进行分析，预测顾客需求，根据需求定期向顾客推送商品或者服务的信息；用户接收到信息，也不必为自己在休息时去哪里、做什么这样的问题而绞尽脑汁，只需要放假时直接去商家店里选择自己心仪的商品和服务就可以了。最终的结果很大概率是产生了交易。换言之，人工智能为商业活动带来了实际价值。

从 AI 思维角度来说，人工智能可以从数据中提取价值，可只有数据是不够的，数据量还必须充足。如果没有对足够的数据进行分析，或者对数据的利用不够恰当，都可能导致决策失误。决策一旦失误，自然会影响到整个行为最终诉诸的回报，也就是我们所说的价值。

#### 3.1.1 数据产生决策：为了懂你，我学了很多知识

有了充足的数据，就等于是为人工智能这个系统备足了原料，接下来我们就来看一下，AI 炼金术是如何从数据中挖掘出价值的。如图 3-1 所示，这整个过程包含了四个步骤。我们将通过全球知名的高端化妆品品牌兰蔻使用人工智能机器人促进销量的案例来进行仔细分析。

图 3-1 从数据产生价值的过程示意图

第一步，从数据产生决策。消费者千千万，每个人都有各自的喜好和适用款，所以他们的行为是多样的；而从人的单方面观感上又很难判断哪类人、多少人喜欢怎样的商品、怎样的服务，适合怎样的款式。为此，兰蔻引入人工智能机器人，并使其化身为兰蔻专柜导购中的一员。要使人工智能机器人胜任这个岗位需要做很多前期准备，其中最重要的两点是 —— 第一，熟记并灵活运用所有产品信息、卖点和美妆知识；第二，快速明确顾客需求，推荐给顾客真正需要的东西。

从具体场景来说，兰蔻专柜的大部分顾客是女性，但不同女性对化妆品、护肤品的需求千差万别。有的人注重爆款，有的人注重功效，有的人注重成分，甚至有人买化妆品只是为了试色。对于过往的真人导购来说，首先，他们必须了解每一种产品的功效、成分、适用人群等，才能很好地应对顾客的层层「拷问」。其次，导购还需要熟悉不同肤质、不同肤色、不同脸型的顾客适合的美妆产品，才能留住顾客的心。人工智能机器人导购员如何掌握这样的「读心术」呢？一方面，人工智能机器人通过与顾客进行对话交流，理解和预测出顾客的需求；另一方面，人工智能机器人还需要从多种来源的数据中挖掘获取美妆行业的知识，并通过分析整合，为用户进行精准的推荐与引导，让顾客享受到愉悦的购物体验。

#### 3.1.2 决策到行动：为了提高顾客满意度，我使尽浑身解数

第二步，从决策到具体行动。从数据到决策的过程让我们知道了不同用户的需求，为具体行动提供了方向。具体行动是要深入到个体，根据目标用户的具体喜好来分析的。为了让用户的体验最优化，人工智能机器人做足了功课，不遗余力地提高顾客的满意度。在给顾客进行推介交流时，人工智能机器人可以像真人导购一样为大家进行对话式个性化推荐。

在对话的场景下，人工智能机器人可以通过三种方式向顾客进行推介。首先是要理解顾客的语言来分析对方需求，例如，对方说：「我这几天脸上皱纹有点多，皮肤也很干，有什么产品推荐吗？」人工智能机器人能够根据对这些语句的理解和分析，进行智能推荐：「试试具有滋润功效的小黑瓶！」其次，人工智能机器人可以通过人脸属性识别来了解用户需求。例如，有顾客问：「我这个年纪适合用什么呀？」人工智能机器人能够通过对该顾客面部的扫描识别，发现对方有皱纹、年龄偏大，并非常情感化地回复：「皱纹不可怕，推荐塑颜紧致系列哦！」最后，基于前期大量的知识学习和数据输入，人工智能机器人能够进行多元数据挖掘与推荐。比如，当有顾客表露：「我要一只鬼怪色口红送人。」人工智能机器人能够将这些信息与内置的数据库匹配，给出精准引导：「鬼怪色的色号是 290，超美，在口红区！」或者当顾客问道：「最近换季，有什么推荐呢？」人工智能机器人也能根据所学知识为顾客解忧：「试试水光润系列，深入补水还能抗氧化，非常火，广受好评哦！」有些顾客拿不准自己的偏好，可能会问：「我想买个眼霜，最近流行的哪种比较好呀？」此时，人工智能机器人会根据以往的消费者数据，提供合理的推荐：「今天已经有 19 个顾客小姐姐询问过小黑瓶大眼精华眼霜呢，要不要试试？」

无论是哪一类顾客、哪一种需求，人工智能机器人都能根据大量的数据基础，依靠人工智能进行分析，为顾客提供高质量的推荐。这样，在精准给到高质量信息的情况下，人工智能机器人提高了顾客的满意度。

#### 3.1.3 个体产生结果：真是太能干了！

第三步，个体产生结果。在得到了这一系列的推荐后，顾客开始做出相应的行动。在此之前，兰蔻专柜的负责人曾表示：「有时候顾客太多，难免有照顾不周的地方，如何提升服务质量、给顾客最佳的体验，是我们一直思考的事情。」而当人工智能机器人加入后，迎送宾客、为顾客解答疑问、推荐化妆品、漫游行走寻找指定产品等工作都可以全权交给人工智能机器人去做，并且这样能给顾客带来趣味性、多样化的购物体验。在场受到人工智能机器人帮助的顾客都惊叹：「真是太能干了！」

更准确地来说，人工智能机器人在为导购人员提供更好的帮助的同时，也为顾客带来了更智能的服务，让他们拥有「上帝之感」，开启了顾客愉快的购物体验。在现场，有位女士问：「能帮我找一下大粉水吗？」「人工智能机器人带你去看看吧。」这位女士感叹：「万万没想到，接待自己的居然是一个机器人！」

以往人们去化妆品专卖店购物，进门不到三分钟就会有好几个美容导购员寸步不离地进行推销，即使有些人已经有了选购的目标，导购员依然会跟在身后不厌其烦地介绍。这些导购员的过度热情令人十分尴尬，也使得导购员这一角色备受争议。但是如今，导购机器人的出现解决了这类问题，同时也满足了顾客对咨询、引导的需要。

#### 3.1.4 实现价值：人工智能服务于人类的一次探索

第四步，实现价值。说到价值，人工智能机器人的出现，满足了顾客的购物需求，给顾客带来了美好的休闲体验。这对于商家来说又意味着什么呢？或者说，商家从中获得了什么呢？这次兰蔻与人工智能机器人的跨界合作，是美妆与人工智能的一次探索。目前，人工智能已经落地多个行业，有效地辅助着人们的工作。有了「美妆知识 + 机器人」这样的人工智能解决方案，导购能够从基础的重复性劳动中解放出来，学习更多的专业知识和技巧，提升个人服务能力。这也就意味着，商家能够在减少许多人力成本的同时，更好地在导购层面提升质量。除此之外，人工智能机器人还可以基于人脸属性识别和顾客历史消费数据等，进行精准营销。比如，针对陌生的女性顾客优先推荐促销信息，或者为有商品咨询记录的顾客提供定制化的商品介绍、引导等购物服务。这样，给商家带来的最终的价值是，他们能够获取极高的顾客购买率。

人的认知有时候是一个很主观的过程，在化妆品领域更是如此。可能这几天周围的人都在讨论豆沙色的口红或者玫瑰香的粉底液，你就直观地认为他们一定喜欢这类化妆品，或者说全国、全世界范围内的顾客都会喜欢这个款式。但有时候人们的讨论焦点的触发动机并不是单一的，这些动机在日常的对话中并不能准确体现出来，那么这样的主观揣测就未免过于武断。有些顾客之所以买豆沙色的口红，是因为这款口红刚好是她喜欢的演员或者偶像代言的；有些顾客之所以买豆沙色的口红，是因为刚好有套衣服与之相配。这种阶段性的需求可能过段时间就不灵了。如果始终按照之前的需求为顾客推荐，就可能会出现顾客需要粉底液而为她推荐防晒霜的「错配」现象。在这一方面，人工智能可以精准地抓取顾客在方方面面留下的迹象进行组合判断，更全面、更精准、更客观地判断出你究竟对哪种化妆品里的哪种因素更「来电」。所以，我们要摒弃下意识的主观感受，恰如其分地挖掘、利用好数据。

这四步完成，实际上我们就走完了从数据到价值的一次旅程：人工智能机器人通过对大量数据进行学习，了解顾客的需求，定向合理地推荐一些符合受众品位的化妆产品。对于顾客来说，他们能够及时获取最符合自己兴趣和心意的产品，满足了自身愉悦购物的需求；对于商家而言，顾客对感兴趣产品的咨询和光顾次数，就直接影响了顾客的购买率。

这是一个从数据出发，到价值为止的过程。但一次业绩的提升并不是终点，人工智能还会把这一次的数据反馈到模型里，从而提升下一轮的决策质量，也就是说，数据和人工智能会持续地为我们带来价值。所以对于像兰蔻这样的商家和企业来说，运用人工智能其实就是了解顾客需求的过程，并能够有效地提升服务。

到这儿我们总结一下，AI 思维的核心竞争力是去平均化的能力。所谓的平均化思维，指的是无差别化，每个人、每个产品都同质化；而去平均化，则是肯定人与人、环境与环境的差异性，不同的人有不同的想法和喜好，根据这些差异化需求产出差异化体验。我们可以通过数据和人工智能准确理解每个人的潜在动机，从而最优化决策，达到各自理想的目标。如能合理使用，无疑将带来更高的生产力和更合理的资源利用方式，产生新的价值。AI 炼金术便能从数据中产生价值，同时它也是 AI 思维的具象表现形式。

通过人工智能决策来实现价值的源头是数据，所以，数据的广度和精确度决定了人工智能能够为人类做出多大的贡献。数据越多，越能更全面地反映个体的实际行为与想法，人工智能的预测就会越精准。如果赋予人类预测的能力的话，我想大多数人都会选择先预测一下自己的命运。所谓「知生死安天命」，随着人类身体数据的累积，人工智能对病人的死亡时间预测的准确率也有望更加精准。相关资料显示，科学家通过人工智能解读数据，利用 200 万份电子病历来进行人工智能深度神经网络的训练，全方位审查患者家族史、用药情况、各项体质指数、食物摄入等因素，进而能够预测机体的未来。目前，这一人工智能模型已经能够依据大量的数据提前 12 个月预测到患者的死亡时间，准确率达 90%。研究团队还将这一预测结果与英国国家统计署的死亡记录、英国癌症注册记录等数据库的死亡数据进行了比对，之后又将这套人工智能模型与两项标准的预测方法进行比较，结果显示，这一新模型的准确率比之前的预测方法高了 10.1%。

根据上面这些数据，科学家们不仅能够预测到很多原本无法定论的结果，而且还能了解到影响人生命期限的多种因素，从而使人们根据这些结果对自身生活习惯进行调整，达到延长生命的目的。例如，科学家在实验中对受试者的水果、蔬菜、肉类、奶酪、谷物、鱼类、酒精等食品摄入的量做了考查，找到最利于生命体健康的饮食摄入比例，在病人治疗期、康复期对其饮食结构进行调整，使其达到促进机体代谢健康的目的。这也是人工智能通过数据进行预测的价值所在。

新制度经济学的鼻祖、诺贝尔经济学奖获得者罗纳德·科斯曾说过：「如果你拷问数据到一定程度，它会坦白一切。」大数据包罗万象，能够将许多看似并不相关的事件联系在一起，使我们能够更加清晰明了地把握事物发展的趋势，跟上瞬息万变的社会潮流。谁掌握了大数据，谁就掌握了主动权，不仅能够决策世事，更能够成全自己。但数据就像一片汪洋大海，望不到边际，我们自己很难游上岸；这时就需要人工智能做舟 —— 长风破浪会有时，直挂云帆济沧海。

### 3.2 AI 如何做出决策

前面我们讲了从「数据」到「价值」的历程，大数据的出现使人工智能的决策更高效更聪明。数据加人工智能，为我们在决策时提供了无数个在以前无法实现甚至是无法想象的可能性。那么人工智能是如何一步步地将「数据」转化为「决策」的呢？如图 3-2 所示，从「数据」产生「决策」的具体过程是这样的：首先要通过对历史数据的特征处理产生特征数据，然后通过机器学习产生预测模型，之后需要通过模型评估来产生回测效果。在回测中检验合格的模型就可以投入生产，生产过程中如果有新的个体数据输入，就会通过生产模型得到预测分数，根据预测分数的高低和其他因素进行业务优化，产生针对个体数据的业务决策。模型本身也要受到监控，监控的目的是产生质量报告，来确保模型正常运行。这一整套框架被称为「人工智能决策引擎」，接下来我们将对它进行详细了解。

图 3-2 人工智能决策引擎示意图

#### 3.2.1 数据和特征处理

毋庸置疑，数据是当下这个时代很重要的一种资产。我们经常提到的大数据有「3V」：1）Volume，即数据量，以往数据量少，以 MB、GB 为单位，但是随着互联网的发展，数据不断积累，现在数据量已经发展到了以 PB、EB、ZB 为单位表示；2）Variety，即多样性，根据数据的结构是否统一，将数据分为结构化数据、半结构化数据以及非结构化数据；3）Velocity，即速度，指的是数据的产生速度越来越快。

那么数据是如何产生的呢？首先，社会在发展、人类在生活，就会有源源不断的新数据涌现出来。无论是机器生产还是市场营销活动，都沉淀了越来越多的数据。其次，随着互联网的发展，知识的权威性不断下降，任何人都可以成为信息的发布者，数据就这样不断地积累，数量越来越多。最后，随着学科的不断发展，人们描述同一类型事物的角度越来越多，例如随着基因科学的发展，现在用来描述基因的指标维度非常多，比如说，每个人的 DNA（脱氧核糖核酸）分子是由约 30 亿个碱基对按精确的顺序排列而成的，一个碱基对就对应了一个维度，那么一个人的 DNA 分子就有几十亿个维度。像这样的例子其实还有很多，数据就这样不断积累到了一个非常庞大的数量。

通常，在各种各样的业务过程中形成的历史数据很杂乱，并且分散在不同的数据库和数据表里，不够系统化。为了更好地利用它们，我们需要把它们整合在一起，得到一套结构明确、条理清晰、便于理解的系统化数据。比如说现在需要处理一份在过去两年时间里形成的顾客在不同商家的消费行为数据，由于顾客消费行为的多变性以及商家的多样性，这份数据的数据量庞大且类型多样。而人工智能会对这些复杂的数据做特征处理，特征处理是通过对原始数据的处理和加工，将原始数据维度转换为特征数据的过程，特征是数据中所呈现出来的重要特性，通常是通过数据维度的计算、组合或转换得到的。经过特征处理，数据就被加工成了机器学习能够理解的数据形式。

过去，特征处理都是通过单台计算机进行，处理效率比较低。2005 年，由阿帕奇（Apache）基金会所开发的哈杜普（Hadoop）可以将多台计算机连接在一起形成计算机集群，提供分布式并行的特征处理和分析功能，实现对复杂数据的快速、可靠的计算。由于其技术的高效性，对特征处理能力的提高，特征处理变得越来越容易，所以特征处理的门槛越来越低。到 2008 年左右，越来越多的公司意识到大数据时代分布式并行计算的重要性，同样地，这也为人工智能处理大数据提供了完善的基础设施。但特征处理的过程通常也是枯燥而繁重的，看不到直接有用的结果。如果卡在这步，对企业来说，人工智能就无法落地。

#### 3.2.2 机器学习和优化

做好了特征处理，下一步就是机器学习。机器学习，就是在特征上建立模型的过程，这里需要选择合适的预测模型，运用历史数据学习出模型。机器学习在各行各业都开始应用，例如在零售领域，通过发放优惠券来刺激老顾客、吸引新顾客是常用的营销手段，但是随机投放优惠券往往效果不明显，在某种程度上还会打扰到顾客，这样不仅造成了营销成本的浪费，还会降低品牌的声誉。为改善营销效果，一些零售商采用机器学习来进行营销。机器学习根据大量消费数据学习出预测模型，能够预测顾客是否需要某种类型的优惠券，以此来决定是否向该顾客投放优惠券。这样不但使真正需要的顾客享受到了优惠，还改善了商家自身的营销效果。

机器学习的过程，其实和人类通过经验总结生活规律再来指导我们之后的生活是相类似的。我们通过生活规律来指导的是以后遇到的事情，同样地，机器学习需要预测的也是将来。未雨绸缪，早为之所，商家可以根据机器预测提前做很多事情来促进顾客的消费，在最短的时间内抢占市场份额，扩大利润，也为自己争取更多时间，使自己更加灵活地应对一些突如其来的市场变化。如果一个人平时有吃零食的习惯，那他一定听说过良品铺子。从竞争激烈的零食行业中发展到现在会员数量达 7 400 万、2 000 多家线下门店以及覆盖多个外卖渠道，良品铺子的崛起离不开人工智能的帮助。零食品类所面向的消费群体是非常大的，但零食又是一个随意购买性的商品，也就是说平时你可能想不起来买零食，但是如果身边刚好有一个零食货架，你可能会很随意地拿几样薯片和可乐去结账。

为了抢占如此大的消费市场，推动自己的产品覆盖更多的场景，良品铺子开始借助人工智能来帮助提高自身的业务预测能力。人工智能的任务就是对良品铺子的各种历史数据进行学习，然后将其多个渠道的会员数据打通，在这些数据的基础上了解用户需求，也就是通过机器学习建立预测模型。这样的模型可以提前预测哪些用户购买零食的可能性大、哪些用户最喜欢购买哪几种零食，在此基础上为用户提供更加符合个人喜好的个性化推送；推送渠道也会根据用户最常使用的渠道来确定，真正实现以用户为中心的精准营销。除此之外，人工智能还可以为前端业务创新提供支持。譬如说，如果跟踪用户购买情况的数据，显示某段时间内用户的购买热情有所下降，商家就可以制定秒杀、拼团或者发放优惠券等活动来刺激用户购买力。

机器学习产生模型还能用来检测接吻场景。这个功能的发展源自斯坦福大学数据科学家阿米尔·齐亚（Amir Ziai）的奇思妙想，他想让人工智能学习什么是接吻，所以他从好莱坞电影数据库中挑选了 100 部经典电影中的代表性片段，按内容将不同的电影片段标记为接吻或者非接吻片段，并通过机器学习分析了这些片段，特别是观察电影片段中人物的嘴唇动作，然后配合场景和电影背景音乐，学习什么是接吻，形成了专门的接吻识别模型。一般而言，接吻是影片中的高潮节点，推动了整个故事的发展，这个模型可以将影片中的接吻片段标记出来，为做视频剪辑工作的人员提供精彩片段的筛选过程。与接吻模型属于同类型的微表情识别模型和动作识别模型现在也展开了应用，比如在 2019 年，日本开发出一种基于人工智能的偷窃预警系统 VaakEye，它经过 10 万小时的训练，能够识别出顾客是否将商品装入包中，或者是否出现可疑行为，并提醒超市工作人员注意防范。日本 50 多家便利店应用了 VaakEye，其盗窃行为识别的准确率高达 81%，将便利店的入店行窃损失减少了 77%。

模型建好之后，仍然需要详细的评估和验证。模型评估，也就是回测模型效果的过程。回测指的是用历史数据来验证建立的预测模型是否有效。预测模型，顾名思义，是用来预测未来的、目前尚未发生的事情。经过回测，评估合格的模型才有资格进入生产环节。这样做是因为只把模型构建完成是不够的，我们并不知道这个模型是否具有实践中的普适性，是否存在差错和不合理的地方，如果匆忙将模型投产，可能会为企业带来损失；片面地忽视回测结果，缺少后续推动，也会导致人工智能落地艰难。科学的回测既能保证人工智能为我们创造价值，又能为企业的每一个业务决策提供保障，避免损失。很多企业的领导不了解回测这种手段，也不重视过程，只关注最终的收益。事实上，他们忽略了科学的回测结果跟最终的经济结果之间本身就有着极强的相关性，是实践中非常重要的信号。通常，只要回测结果证明模型是合格的，最终的经济结果就很有可能令人满意。

只有模型评估合格，达到预期的效果以后，我们才能将它投入例行化生产，开始预测分数。比如说在一个借贷项目中，对许多参与者的信用程度进行预测，然后将预测的分数做个排序，通常来说，信用分数越高的用户获得贷款的可能性就越大。模型生产涉及大量工程化的工作，需要将软件开发工作系统化、模块化、规范化，从而保证高稳定性和可靠性，这个过程要在复杂的计算机和数据系统上进行，并与其他软件进行耦合，所以需要人工智能科学家和工程师紧密的配合。我见过一些大公司，人工智能科学家和工程师之间不能紧密配合，团队之间割裂，即便他们拥有一流的人工智能团队，人工智能也无法发挥价值。

下一步就是业务优化。业务优化，也就是从预测分数做出决策的过程。对个体行为准确的预测和把握，可以帮助行业决策者做出优秀的商业决策。在这个过程里，我们会通过优化整体的业务目标，比如说收益，把预测结果转化为具体的、可执行的业务决策。例如，Caliber Collison 是一家美国的汽车修理公司，离职率曾高达 40%。公司研究后发现，这一问题的部分原因在于其门店有时没有足够的车供员工修理，导致员工之间的工资差距较大。于是它开始从人力资源软件中提取出员工数据，建立定制的机器学习模型，预测员工是否会考虑离职。然后，Caliber 通过该模型对员工离职率的预测分数进行业务决策。例如，如果模型检测到一名员工的工资在几周内一直下降，那么他的预测分数就会相对高一些。为了尽量让员工留下来，Caliber 的地区经理就会多分配给他一些汽车去修理，让他的工资有所上升。相反，如果模型检测到一名员工很忙，虽然他的工资很高，但是他感到很疲惫，那么他离职的预测分数仍然不会低，这时公司就可以把一些工作重新分配给他的同事。如此一来，公司的工作结构就更加均衡，Caliber 的离职率也直线下降，每年在招聘上节省了百万美元。

除了帮企业做出优秀决策之外，业务优化还能引导用户做出选择。当你准备在购物网站搜索想要的商品时，人工智能推荐给每个用户的商品列表就是优化的结果，它们能够帮助用户更快地找到自己感兴趣的商家，也能为商家做更好的市场推广，这其中的经济价值是巨大的。eHub 就是这样一个人工智能营销平台，它收集了来自营销、销售和客服的多渠道数据，通过这些数据建立预测模型。这些模型可以根据数据中反映出的用户属性特征和行为特征分析用户的喜好、需求，并进行分数预测。比如一个用户经常购买糖果、巧克力、奶油蛋糕，那么模型就会预测该用户购买甜食的分数高，然后给该用户推荐其他品牌的糖果、巧克力或者其他甜食，甚至是其他喜欢吃甜食的用户经常购买的商品，以此来实现一对一的个性化营销、精准营销。用户看到每天推送的都是自己喜欢的商品，购买概率也会大大上升。由于该平台十分符合自己的需求，用户在平时也会多浏览该平台的商品。

最后一个环节就是模型监控。模型监控是监控模型效果和质量的过程。由于数据源会变化，各种外部因素会干扰数据，我们很难确保一开始正确的模型一定能持续工作并有产出。因此，我们要对每一个环节都进行严格的质量把控。质量把控需要通过模型监控，持续地跟踪模型效果来实现，输出实时的质量报告，一旦出现问题，及时向相关人员报警，进行排查。很多人工智能项目在开始时没有设置模型监控，导致了机器学习结果出现错误。例如，巴西一家金融服务公司开发了一个聊天机器人模型，这个模型可以和求职者聊天并传达应聘信息。聊天机器人模型会让求职者通过实时聊天和电话回答一系列问题，但是没有设置模型监控，所以在使用该聊天机器人模型时，出现了两个关键问题：一是发错了给求职者的个人资料和职业表格，二是安排的面试时间错误造成人力资源部门工作延误。由于没有对聊天机器人模型进行精确的调整，该公司判定所收集的数据中有大约 10% 是不正确的。该公司人员坦言：「我们犯了一个错误，以为一切都解决了，而没有去监督聊天机器人模型。教训是，一定至少要有几个月的时间全时监督聊天机器人模型。我们不能停止关注可能出现的偏差，以及新出现的情况 —— 当我们开始这个项目时，这些都是出乎意料的。」正是因为这家金融服务公司在使用人工智能时忽略了模型监控环节，才导致后续出现问题。这也正验证了模型监控环节的重要性。

这就是从「数据」产生「决策」的全过程：第一，将历史数据经过特征化处理为机器理解的数据形式。第二，通过机器学习产生能够准确预测新样本的预测模型。第三，通过模型评估回测模型是否合格。第四，评估合格的模型就可以投入生产，生产过程中如果有新样本输入，就可以通过模型产生预测分数，再根据预测分数的高低和其他因素进行业务优化，产生针对个体数据的业务决策。第五，对模型本身进行监控，产生质量报告，时刻保证模型的正常运行。

#### 3.2.3 人工智能助力精准推荐

我们都知道人工智能是能够大大解放人力、产生高效价值的新生事物，但是我们也经常听到有人说人工智能在企业落不了地。他们认为人工智能还未发展到能够真正为企业产出价值的阶段，又或者，人工智能只是一场众人追捧的闹剧，并不真的适合在企业中应用。但这正显示出他们对于人工智能不够了解。如果上述过程都做到了，如果数据充分，每个步骤都能正确地实现，是不可能没有效果的。例如在接下来的这个案例中，人工智能不仅成功落地，还帮助企业提升了效益，帮助其在短短四年时间内，市值增长为原来的 3 倍多。

这个企业就是我之前任职的 PayPal 公司。人工智能在 PayPal 落地的整个过程是从历史数据开始的，比如说 PayPal 的历史交易数据，也就是记录哪个用户在哪个商家消费了多少金额的一套数据。这些数据分布在不同的系统中，我们首先把历史数据全部整理了出来，并且做了特征化处理，加工成机器学习能够理解的特征数据。

下一步就是机器学习，用来建立预测模型。机器学习需要预测的是将来，简单点来说，我们的人工智能需要预测的就是将来某个顾客最有可能在哪个商家购物。如果该顾客是体育爱好者，那么他将来去购物的店可能是耐克、阿迪达斯、安德玛，即使之前他没去过，这样的推荐仍然会对他有很大的吸引力。相反，如果给这个顾客推荐花店、玩具店之类的商店，那么可能就是缘木求鱼、南辕北辙了。通过机器学习，人工智能可以在多次的推荐后，通过用户的浏览时间以及购买情况得知推送的内容准不准确，符不符合用户的需求，通过用户在不同渠道的响应率得知用户最经常使用的渠道是哪个。经过多次这样的推荐和用户反馈之后，能够实现模型的持续学习，逐渐形成精准的推荐模型。

模型建好之后，需要对其进行回测，评估这个模型是否合格。那我们现在如何确定它是有效的呢？方法很简单，就是用过去的数据来模拟和验证。具体来说，如果我们用 2018 年一整年的数据来建立这个推荐模型，就可以用 2019 年的数据来「回测」，因为对于从 2018 年的数据学习出的模型来说，2019 年的数据代表了未来。所以，如果模型在 2019 年的数据上效果良好，它就通过了回测。通过回测，我们就可以在模型投产落地之前知道构建的人工智能决策引擎的预测是否准确、准确率有多高，是否真的能为我们的企业创造出价值，防止引擎开始落地时才发现没有效果、成本全部打水漂的现象发生，为人工智能的顺利落地提供了保障。

预测模型建立出来并评估合格之后，就要把它投入生产，例行化地通过模型产生预测分数。PayPal 这种类型的公司对生产的要求很高，对模型生产过程中的检验、部署、监控，都有相当严格的要求。完成了模型生产和监控后，我们就有了源源不断的预测分数，对应的是每一个用户将来在各个商家购物的概率。

当我们得知用户是否在各个商家购物的预测分数后，就要进行业务优化。针对一个用户，我们将这个用户可能会在各个商家购买的预测分数按照高低排序，形成这个用户的商家推荐列表，这样就能帮助用户更快地找到自己感兴趣的商家和商品。针对多个用户，我们可以将这些用户按照他们在某个商家购物的预测分数的高低排序，形成这个商家的用户推荐列表，帮助商家更快地发现哪些用户是营销转化率最高的用户，使得商家更好地打开市场。

这一整套人工智能决策引擎执行下来，并通过运营数据的反馈多次进行优化，最终可以帮助 PayPal 更好地进行业务决策。当然这不是终点，当人工智能决策引擎开始正式助力 PayPal 的业务运营之后，我们仍然需要对该模型进行监控，实时更新质量报告，这样才能及时发现可能出现的问题，及时抢修，避免出现更大的损失，同时保证人工智能为企业和用户提供持续而且优质的服务。

以上就是这套人工智能决策引擎在 PayPal 落地的过程。这套决策引擎落地后，PayPal 用户开始积极响应 PayPal 的营销活动，其所带来的收益远远高于活动成本，而且产生了稳定的增益。几年来，这套人工智能决策引擎的顺利运作，为 PayPal 经营发展提供了有力的支持，直至 2018 年，PayPal 市值突破千亿美元，人工智能决策引擎可以说是功不可没。

人工智能决策引擎是一套完备的企业决策引擎，它可以应用到企业决策的每一个环节，就像代表智慧的九连环，每一个环单独看都不起眼，但是所有环连在一起就成了智慧的象征，能够为企业创造出源源不断的价值。而且人工智能决策引擎环环相扣，每一个环节都必不可少，不论哪一个环节出了问题，都会直接影响甚至是威胁到接下来的环节，之前做好的准备也很有可能功亏一篑。人工智能决策引擎是 AI 思维的重要框架，也显示出 AI 思维的缜密智慧：只有事无巨细地做好每一件事情，才能真正从数据中得出决策，人工智能才能成功落地。

### 3.3 业务优化方法

水往低处流，但人要往高处走。在工作中，我们兢兢业业，精益求精，希望每一次的绩效考核都可以拿第一；在学习中，我们尽心竭力，一丝不苟，希望经历过的每一场考试都可以名列前茅；在生活中，我们脚踏实地，善始善终，希望活过的日子都不会有遗憾。浑浑噩噩也是一生，但大多数人都不会放弃自己，所有不甘于现状的努力都出于对一个「最」字的追求。「最」字体现在人身上是追求极致人生的动力，体现在 AI 思维上就是通过人工智能找到一个让系统发挥最大效能的方案，找到解决问题的最佳决策。

#### 3.3.1 何为最优化？

人生大方向的最优解是建立在日常小事情最优解的积累上的。选择哪一种材料的衣服，才能舒适又美观；选择哪一种食物，才能美味又营养；选择在哪里买房子，才能交通便利又价格适中；选择哪一条路线上班，才能快速地到达公司；选择哪一家公司，才能工资多又同事和睦。找到这些最优决策的方法叫作「最优化方法」。最优化，就是在一定的约束条件下，从众多可能的选择中找出最优选择，使系统的目标函数找到最大值或最小值。形象点来理解，就是你面前有无数座连绵不断的高山，你需要在这些高山中找到海拔最高的那个山峰或者海拔最低的那个山谷。

不但个人需要找到人生的最优解，各行各业也都希望找到这个最优点，在不多浪费任何一种资源的条件下达到最佳的效果。在商业中，通过投资、收购等一系列操作，追求利润最大化；在体育运动中，通过完整而系统的训练，让运动员更高、更快、更强；在医疗行业中，通过各种治疗，让病人在最短的时间内摆脱病痛。那你肯定会有疑问：人工智能与最优化有什么关系呢？

首先，人工智能是为服务人类而生的，所以人类需要追求最优解，人工智能也需要追求最优解。其次，从本质上来看，人工智能的最终目标其实就是最优化：在数量庞大的数据以及瞬息万变的环境中，做出最优的决策。最优化贯穿了人工智能产生、发展和运用的全过程，从预测结果到业务决策，用的都是最优化方法，因而，最优化方法是人工智能最基础的知识。

人工智能的全过程其实都是在寻找最优解，通过机器学习，产生了对个体的预测分数，我们对未来的事件就有了基于数据的预判。就像我们在路上行车，有了清晰向前的视野，这对我们接下来产生的决策非常有帮助。在人工智能决策引擎中，首先要通过对历史数据的特征处理产生特征数据，然后通过机器学习产生预测模型，最后进行决策。那么，AI 思维如何帮助我们产生最优化的业务决策呢？

如图 3-3 所示，从整体来看，优化过程共分为以下三个步骤：定义问题、优化模型、输出应用。人工智能是不能直接理解我们的要求的，所以我们要把业务的需求转化成人工智能可以理解的数学语言，这就是定义问题。

图 3-3 优化过程示意图

定义问题包括以下三个要素：第一个要素是可用资源，就是我们现有的各种资源，比如要生产一种产品，我们有多少原材料，生产工人技术的熟练程度，还有生产机器的数量等。第二个要素是约束条件，比如说要想任务保质保量地完成，工人和机器都需要休息，所以约束条件就是工人每天只能工作 8 小时，机器连续运转 16 小时就需要散热和检修。第三个要素是业务目标：任何业务决策都需要一个目标，比如要生产多少产品，这个项目才算完成；这个营销计划执行完，要增长多少用户量；或者这个季度结束，要完成多少订单。在数学上这个目标可以通过目标函数来定义，也就是 f（项目）= 产量，f（营销）= 用户量。在制定这个目标函数的过程中，实际上作为具有 AI 思维的你，已经开始在宏观的层面思考业务问题了。

目标函数由预测分数和决策变量构成，决策变量是我们对个体的决策。比如说，我们花了大量的资金在网站上投放广告，但是广告效果并不是很理想，我们就会想去提高广告的点击量。这时，我们的目标就是最优化广告投放的总点击量，预测分数是针对每一个用户点击率的预估，决策变量则是是否对某个用户投放，从而做出对哪些用户投放广告的决策。显而易见，我们把所有的分数做个排序，选择高点击率的用户投放即可。这是个简单的目标优化问题。

优化模型决定了我们需要选用什么样的优化方法，而我们一般选用数学优化方法。总的来说，优化问题分为两种：有约束优化和无约束优化。顾名思义，有约束优化是在一系列约束条件下，寻找一组决策变量，使目标函数值达到最优；无约束优化与有约束优化相对应，指的是没有任何约束条件的优化。现实中的优化问题通常是有约束的，因为我们做一件事的资源总是有限的，比如一个企业要在一个周之内做完一个项目，一周就是时间约束，也就是我们的时间资源是有限的。或者是这个企业要用 3 000 元来办一场活动，这 3 000 元就是预算约束，也就是我们的成本和预算是有限的。

输出应用就是输出我们的最优解，是优化方法的最终目标。比如我们花最少的时间到达了目的地，用最低价格购买了一幢房子，用最高的效率完成了生产任务或者以最大的产量完成了绩效目标。

其实我们在很久之前就接触过这种最优化的思维。比如语文课本上「田忌赛马」的故事讲的就是最优化的思维，四大名著之一的《三国演义》就是军事最优化理论的真实写照；我们从学生时期就开始做求最大利润、最快速度、最少成本这样的极值数学题，其实都是在训练我们的最优化思维。在 AI 思维中，最优化思维十分重要，然后才是对问题的求解。我们做过很多这种求极值的数学题，不知不觉中就积累了最优化解决问题的能力，不过我们在现实生活中遇到的问题要比数学题复杂得多。情况是错综复杂的，但我们只要有了这种最优化的思维，能够把在生活中遇到的各种问题和现象翻译成人工智能可以理解的数学语言，建立适当的数学模型，这些问题就可以迎刃而解了。下面我们就来看一下在现实生活中，优化思维是如何解决业务难题，为企业带来商业价值的。

#### 3.3.2 优化的业务价值

我们在讲优化的第三步输出应用时讲到，优化方法的最终目标有：最高效率、最大产量、最少时间和最低成本等。这其实就是优化所带来的业务价值的一个缩影。试问哪一个公司不想用最低的成本、最少的时间来获得最高的产量呢？但是这在现实生活中基本上是不能兼得的，用最少的时间获得最高的产量，就意味着可能要付出更高的成本。优化其实就是在这些冲突之间帮助企业找到一个平衡点，在现有资源的范围内，保证企业最大限度地赢利，还能为用户提供最优质的服务，提高用户的满意程度。

更详细地讲，优化带来的业务价值有如下几条：第一，优化可以帮助企业降低生产成本，提高企业运作效率，缩短生产和服务的交付周期。德国一家汽车生产公司有两条汽车生产线，各负责生产一种型号的汽车，这两条生产线每年能够生产 20 万辆汽车，这已经是一个很高的产量了。这家公司仍然不满足，想在不开通第三条生产线的情况下生产第三种型号的汽车。为了实现这个目标，它自主搭建了一个流程优化系统，在没有花费大量资金去建立新生产线的条件下，将生产力提高了 30%，将生产周期从 7 天缩减到了 4 天。由此，该公司节约了 5 亿美元的成本。这种能够看见的成本优化，提高了工作效率，增加了产量，在日后能够带来的经济效益是不可估量的。

第二，优化可以帮助企业提高资源利用率，向客户提供更加优质、灵活且准确的服务。法国一家快递公司之前送货速度没有比别的快递公司快，快递费也因为成本问题不能降低，相比之下没有任何竞争优势，迟迟不见盈利。不过，这家快递公司有一个特点 —— 它依附于一家航空公司，因此，综合考虑公司可以调动的资源，该快递公司决定利用航空资源来优化自己的快递业务，这样快递的送货速度就有了保证。但同时会出现一些其他优化问题，比如飞机搭载乘客之外，运输快递的数量是有限制的，飞机的路线和快递路线不能完全匹配，飞机加入后每个快递集散中心工作量上升等。但该公司的优化模型通过充分利用现有航空资源，合理安排飞机路线等方法，不但节约了增添航空设备的成本，还提高了送快递的速度，在提高公司本身竞争优势的同时，为用户提供了快速优质的物流服务。

第三，优化可以帮助企业实现精准营销，提升销量，扩大产品和企业的知名度和影响力。在广告营销领域，需要解决的问题有很多。首先，这个广告要投放给谁的问题。通过最优化思维，我们可以找到投放成本最低、需求最高、转化率最高的那一批用户，优化了广告的响应率。其次，通过什么渠道去进行推广营销的问题。通过最优化思维，我们可以找到用户打开频率最高、响应率最高的渠道，比如某用户平时使用淘宝的频率比京东高，那广告当然要投放在淘宝平台，这样可以优化广告的曝光率。最后，问题在于，要向用户推送什么样的广告。通过最优化思维，我们可以发现每个用户不同的需求，商家的广告也需要根据需求来定制，这样用户收到的广告推送就会千人千面，更符合他们个性化的需求，用户的购买欲望也会相应地提升，在一定程度上优化了产品的销量。

第四，优化可以帮助企业减少库存，降低管理风险。日本一家家具企业经常大量购进优质木材，因而经常会出现所有订单都完成了还剩下很多木材的情况。这些木材存放的时间久了会出现虫蛀、受潮、色泽不均等问题，无法再用于做新家具，这样不但产生了大量的仓储成本，还浪费了大量的资源。为解决这个问题，该公司引进了一个优化系统，对从木材采购到木材保管的全过程都进行了优化。首先，在采购之前，系统会根据优化模型算出一个最佳的木材采购量，这样既避免了过度购买木材，也减少了采买过程中的贪腐行为；其次，该企业还会对剩下的木材进行防蛀防潮湿处理，避免了资源浪费。仓储量的下降减少了工作人员的工作量，降低了管理风险。

#### 3.3.3 业务优化应用案例

优化能够带来如此大的业务价值，那它在实际生活中是如何实践的？在实际业务运作中是如何发挥作用的？现在我们就通过具体案例来说明一下优化是如何执行的。比如说，现在有一个电商网站要进行一次营销活动，该网站的用户量巨大，如果这次营销活动要针对每一个用户，营销成本就会非常大，所以这个网站打算只选择部分顾客进行营销。对这个网站而言，每个用户的营销成本是不一样的，我们需要根据已有顾客的预测转化率，最大化选择的顾客的总分数。这其中的约束条件是：营销成本不能超过总预算。

假设我们有 5 个顾客，每个顾客的营销成本分别是 c1, c2 … c5（已知），转化率分别是 p1, p2 … p5（已算出），x1, x2 … x5 表示我们是否对一个顾客营销。那么优化问题可以这么写：

最大转化率： 

$$p_1x_1 + p_2x_2 ...... + p_{10}x_{10} $$

约束条件： 

$$c_1x_1 + c_2x_2 ...... + c_{10}x_{10} ≤ 预算$$

x1, x2 … x10 取值 0 或 1（0 代表不进行营销，1 代表进行营销）

若 5 个顾客的营销预算为 1 500 元，营销成本和转化率如表 3-1 所示：

表 3–1 营销成本和转化率

| 用户 | 1 | 2 | 3 |
| --- | --- | --- | --- |
| 营销成本（c） | 100 | 300 | 500 |
| 营销转化率（p）| 27% | 68% | 76% |

则优化问题应该这样写：

最大转化率：

$$0.27x_1 + 0.68x_2 + 0.76x_3 + 0.83x_4 + 0.95x_5 $$

约束条件：

$$100x_1 + 300x_2 + 500x_3 + 700x_4 + 1000x_5 ≤ 1500 $$

在进行顾客营销时，我们当然会选择那些转化率高的用户来进行营销，但是我们同样也要考虑到营销的成本。在这 5 个用户当中，第五个的预测转化率最高，但是要对他进行营销的成本是 1 000 元，成本太高，如果都是这样的用户，那我们很有可能会超出预算，而且还没有达到预期的营销效果，这样就得不偿失了。第一个的营销成本最低，但是转化率只有 27%，仍然达不到想要的营销效果。相比之下，中间 3 个用户的营销成本适中，营销效果也很好，性价比最高，而且成本在预算之内，所以最后我们选择中间 3 个进行营销，达到效果最优化。

通过上述预测分析和优化模型，我们就可以看出哪些是营销成本低但营销效果不好的用户，哪些是营销效果好但营销成本高的用户，而我们最终选择的肯定是那些转化率高且成本适中的用户，对这些用户进行营销可以控制成本在营销预算之内，又能达到很高的转化率。这是最优化思维最初的目标：以最低的成本达到最佳的效果。同时，这也是最优化思维最终的结果：帮助企业实现了最佳的营销转化率。

最优化思维在诸如航空、物流、生产制造、金融、资源管理、环境保护、电力管理等领域中得到了广泛应用，在相关领域都已经有成功的优化实施案例。以物流行业为例，我们来看一下最优化思维如何在实际场景中展开应用。

近几年电子商务迅速崛起，在网上购物已经成了大众生活的常态，随之而来的是物流行业的蓬勃发展。国家发改委统计数据显示，2018 年中国物流成本占国内生产总值的比重高达 14.5%，而相比之下，欧美国家的物流成本占其国民生产总值的 10% 都不到。为切实有效地解决国内物流成本的问题，最优化思维势在必行。

在一切物流问题中，最重要的莫过于车辆路线问题。在面对数量众多且需求不同的客户时，组织多少辆货车最合适，走哪一条路线路程最短，选择哪一种物流方式成本最低、耗时最短等一系列问题，都属于车辆路线问题，也都属于最优化思维应当解决的问题范围。

现在许多地图服务商都提议提供最短路线或者最短时间方案，但是在实际运输过程中，要真正实现最短路线、最快时间以及最低成本，还需要解决许多复杂问题，例如不同货物配送的先后顺序，水、陆、空三种运输方式和多家快递公司的不同组合搭配，等等。此外，考虑到诸如限行、修路、恶劣天气或者交通事故等突发状况，物流所要优化的问题，复杂度直线上升。

为了优化解决这些问题，市场上出现了一些优化模型，例如针对物流优化的小马驾驾。首先，面对复杂且数量巨大的物流信息，优化模型会先对其进行学习，并在训练和学习中不断优化自身，让自己的判断越来越准确；其次，为了应对突发状况，优化模型可进行实时判断，做出路线调整，找到物流的最优路线；为了优化目前物流领域存在的业务模式多变、效率低下、资源利用不合理等问题，优化模型提出了多种提货送货模式，比如多地点提货单地点送货、单地点提货多地点送货、多地点提货多地点送货等，大大提高了提货送货的效率，节约了时间成本。

在选择最短路线方面，优化模型将所有订单综合考虑，整合所有路线，将所有目的地都串联在一起，使得同一个目的地的商品能够一起派送。除此之外，优化模型还会考虑到回程问题，避免出现货车空仓回程的情况。这样既解决了回程问题，又最大可能地降低了运输成本，提高了物流效率。在配送服务方面，模型也可以根据用户需求灵活调整车辆、车型、仓库等，达到优化的效果。在控制成本方面，除了缩短物流时间可以降低成本之外，选择不同的车型或者运送方式也可以很好地降低成本，比如短途运送小型货物可以选择小型货车，成本相对较低；但是长途运送生鲜货物，还是应该选择航空运输，航空运输虽然成本高，但是生鲜商品的配送费本身也很贵，航空运输速度快，能够保证商品的新鲜程度，提高用户对物流的满意程度，这样才能留住客源。据统计，使用该优化物流模型制定的优化物流方案，可以将每日全城的调度时间从 3 小时减少到 15 分钟，企业的运送成本可以降低 10% 到 20%。

面对海量的信息数据、复杂的环境、多变的市场和需求，我们在日常工作中，必然会求助于人工智能。如果企业不拥抱人工智能，就很难跟上时代快速发展的步伐，甚至会面临被淘汰的危险。我们需要人工智能为我们提供业务决策，其实是需要它为我们提供最优的那个决策。可以说，最优化是 AI 思维解决问题的出发点。有了 AI 思维的帮助，我们就可以更加轻松准确地在生活、工作和业务运作过程中做出最佳的决策。

### 3.4 会思考的人工智能

《终结者》里的天网是一套人工智能防御系统，但它却拥有了自我意识，想要毁灭人类；《我，机器人》中具有自我思考能力的人工智能机器人 VIKI 在负责生产机器人的同时，还在默默计划让机器人成为人类世界的主宰；《魔种》中的人工智能系统 Proteus IV 不但具有了自我意识，还将人类囚禁起来。现在越来越多的科幻电影里出现了人工智能的身影，而且它们都不约而同地拥有了独立思考的能力。其实从人工智能出现以来，人们一直都有这样的疑问：人工智能是否能够和人类一样拥有独立思考的能力？

关于这个疑问，被称为计算机科学之父的艾伦·图灵在 1952 年曾经进行过一个测试：将一个人和一台计算机分别关在两个房间内，再找一个人对房间里的人和计算机分别进行提问。提问者事先并不知道哪个房间里的是人，哪个房间里的是计算机，他要通过二者对问题的回答进行判断。如果有超过 30% 的人不能准确判断出哪一个是人，哪一个是计算机，那么这台计算机就通过了此次测试，而且可以被认为是具有了人的智能。这一用来判断机器是否具有人类智能的测试后来被称为图灵测试。一直没有计算机能通过图灵测试，直到出现了深度学习，人们才看到了一丝曙光。

#### 3.4.1 深度学习的前世今生

我们知道，机器学习是人工智能的分支，它专门研究计算机如何模拟和实现人类的学习行为。在人工智能发展过程中，机器学习占据核心地位。通过各种模型，机器学习可以从海量的数据中学习出规律，从而对新的数据做出智能识别或者预测，并且为决策提供支持。深度学习是机器学习的一种。如图 3-4 所示，人工智能是一个范围很大的概念，其中包括了机器学习。机器学习是人工智能提升性能的重要途径，而深度学习又是机器学习的重要组成部分。深度学习解决了许多复杂的识别和预测难题，使机器学习向前迈进了一大步，推动了人工智能的蓬勃发展。那么深度学习又是如何发展起来的呢？

1-2『看到这里的信息才算明确了：AI、机器学习和深度学习的关系，做一张任意卡片。（2021-02-26）』

图 3-4 深度学习关系示意图

深度学习的概念最初起源于人工神经网络（Artifi cial Neural Networks）。科学家发现人的大脑中含有大约 1 000 亿个神经元，大脑平时所进行的思考、记忆等工作，都是依靠神经元彼此之间的连接而形成的神经网络来进行的。人工神经网络是一种模仿人类神经网络来进行信息处理的模型，它具有自主学习和自适应的能力。

2『深度学习，做一张术语卡片。（2021-02-26）』

1943 年，数学家皮茨（Pitts）和麦卡洛克（McCulloch）建立了第一个神经网络模型 M-P 模型，能够进行逻辑运算，为神经网络的发展奠定了基础。生物神经元一共由四个部分组成：细胞体、树突、轴突和突触，M-P 模型其实是对生物神经元结构的一个模仿，如图 3-5 所示，左边是生物神经元的示意图，右边是 M-P 模型的示意图，为了建模更加方便简单，M-P 模型将神经元中的树突、细胞体等接收到的信号都看作输入值 x ，突触发出的信号视作输出值 y 。1958 年，计算机科学家罗森布拉特（Rosenblatt）发明了感知机，分为三个部分：输入层、输出层和隐含层。感知机能够进行一些简单的模式识别和联想记忆，是人工神经网络的一大突破，但这个感知机存在一个问题，就是它无法对复杂的函数进行预测。20 世纪 80 年代，人工智能科学家鲁姆尔哈特（Rumelhart）、威廉姆斯（Williams）、辛顿（Hinton）、莱库（LeCun）等人发明的多层感知机解决了这个问题，推动了人工神经网络的进一步发展。20 世纪 90 年代，诺贝尔奖获得者埃德尔曼（Edelman）提出 Darwinism 模型，并建立了一种神经网络系统理论，对 90 年代神经网络的发展具有重大意义。

图 3-5 神经元及 M-P 模型示意图

从这之后，神经网络技术再也没有出现过突破性的发展。直到 2006 年，被称为人工智能教父的辛顿正式提出了深度学习的概念，认为通过无监督学习和有监督学习相结合的方式，可以对现有的模型进行优化。这一观点的提出在人工智能领域引起了很大反响，许多像斯坦福大学这样的著名高校的学者们纷纷开始研究深度学习。2006 年被称为「深度学习元年」，深度学习从这一年开始迎来了一个爆发式的发展。2009 年，深度学习应用于语音识别领域。2012 年，深度学习模型 AlexNet 在 ImageNet 图像识别大赛中拔得头筹，深度学习开始被视为神经网络的代名词。同样是在这一年，人工智能领域权威学者吴恩达教授开发的深度神经网络将图像识别的错误率从 26% 降低到了 15%，这是人工智能在图像识别领域的一大进步。2014 年，脸书开发的深度学习项目 DeepFace 在识别人脸方面的准确率达到了 97% 以上。2016 年，基于深度学习的 AlphaGo 在围棋比赛中战胜了韩国顶尖棋手李世石，在世界范围内引起轰动，这一事件不但使深度学习得到了认可，人工智能也因此被社会大众所熟知。2017 年，深度学习开始在各个领域展开应用，如医疗影像、金融风控、课堂教学等，在不知不觉中已经渗透到我们的生活中。

#### 3.4.2 深度学习的经典模型

那么深度学习到底是什么呢？深度学习是建立在计算机神经网络理论和机器学习理论上的科学，它使用建立在复杂的网络结构上的多处理层，结合非线性转换方法，对复杂数据模型进行抽象，能够很好地识别图像、声音和文本。深度学习有两种经典模型：CNN 和 RNN。

CNN 全称是 Convolutional Neural Network，也就是卷积神经网络。对于卷积神经网络的研究最早出现于 20 世纪 80 至 90 年代，到了 21 世纪，随着科学家们对深度学习的深入研究，卷积神经网络也得到了飞速的发展，该网络经常用于图像识别领域。如图 3-6 所示，卷积神经网络共分为以下几个层级部分，输入层（input layer）、卷积层（convolution layer）、池化层（pooling layer）、全连接层（fully connected layer）。

图 3-6 卷积神经网络工作过程示意图

当图像进入输入层，模型会对这个图像进行一些简单的预处理，比如降低图像维度，便于图像识别。卷积层里的神经元会对图像进行各个维度的特征提取。这一提取动作不是针对原图像进行的，而是仅对图像的局部进行特征提取，比如需要识别的是一张包含小狗的照片，但是神经元只负责处理这张照片中的一小部分，比如狗的耳朵、眼睛。卷积层对图像不同尺度进行特征提取，大大丰富了获取特征的维度，有助于提升最终识别的准确度。池化层对图像进行压缩降维，降低图像识别需要处理的数据量。全连接层需要做的就是将前面所提取出来的所有图像特征连接组合起来，如图 3-7 中，将提取到的小狗的头、身体、腿等局部特征组合起来，形成一个完整的包含小狗的特征向量，然后识别出类别，这就是卷积神经网络进行图像识别的全过程。

图 3-7 卷积神经网络图像识别过程示意图

通过对卷积神经网络工作过程的梳理，我们总结出卷积神经网络的三个特性：第一，图像识别不需要识别图像的全部，每个神经元只需要聚焦到图像的一小部分，识别的难度降低；第二，卷积层对应的神经元可以应用于不同的图像识别任务，比如图 3-7 中的神经元，经过训练，已经能够识别出小狗，那这些神经元也可以应用于识别其他任何图像中的相似物体；第三，虽然图像特征的维度降低了，但是由于保留了图像的主要特征，所以并不影响图像识别，反而降低了识别图像需要处理的数据量。这三个特性决定了卷积神经网络非常适合用于图像识别。例如由牛津大学开发的 VGG 模型就是基于卷积神经网络模型建立的，它在识别物体的候选框生成、图像的定位与检索等方面十分准确，这使得它在 2014 年 ImageNet 竞赛定位任务中获得了第一名。

人工神经网络和卷积神经网络在深度学习领域都占有一席之地，但它们识别的都是独立的事件，比如卷积神经网络非常擅长识别独立的图像，如果让它识别 100 张照片，输出的结果互相不受任何影响，但是让它识别或者预测一句连续的话，比如一个寓言故事或者翻译一段英文，可能就没有这么好的效果了。可是在现实生活中，我们会遇到很多连续的事件，比如「小明每次去超市都会买很多苹果，因为他最喜欢吃（ ）」，联系上下文，我们可以很容易推测出括号里应该是「苹果」这个词，因为括号前的「吃」字是一个动词，动词后面经常跟着的是名词，而这个句子中的名词只有「苹果」最合适。为了能够识别这些连续性很强的事件，弥补人工神经网络和卷积神经网络的不足，RNN 模型诞生了。

RNN 全称是 Recurrent Neural Network，也就是循环神经网络。对于循环神经网络的研究最早出现于 20 世纪 80 年代，由约翰·霍普菲尔德（John Hopfi eld）、迈克尔·乔丹（Michael Jordan）以及杰弗里·埃尔曼（Jeffrey Elman）等人提出，该模型经常用于时序信号（如语音）的识别和理解。

循环就是重复的意思，循环神经网络模型在运行时会对同一个序列进行循环操作。序列是被排成一列的对象，序列中的元素相互依赖，排列顺序非常重要，比如时序数据、对话等，一旦顺序错乱，含义和作用都会发生巨大改变。循环神经网络解决了卷积神经网络不能很好地识别连续性事件的问题，在深度学习领域发挥着不可替代的作用。

循环神经网络之所以能对连续性事件进行识别，是因为它不仅将当前的输入数据作为网络输入，还将之前感知到的一并作为输入。根据记忆的长短，从第一层开始，将激活传递到下一层，以此类推，最后得到输出。图 3-8 就是一个循环神经网络的示意图，它由输入层、隐藏层和输出层三部分组成。循环就发生在隐藏层。隐藏层里一般会设置一个特定的预测函数，当我们向循环神经网络模型输入一个连续性事件后，在隐藏层的这个函数就会进行运算，这个运算结果又可以作为输入进入隐藏层再一次进行运算。如此这般，就形成了一个不断循环的预测，这个预测与新输入的数据有关，也取决于每一次循环的输入。

图 3-8 循环神经网络原理示意图

连续性数据在日常生活中出现的频率之高决定了循环神经网络有着广泛的应用空间。例如，我们可以依靠循环神经网络预测一句话中的下一个词语或一篇文章中下一句话是什么，以此来生成文本，现在很多写稿机器人就是基于循环神经网络来进行运作的。循环神经网络模型还可以将文本翻译成其他的语言，所以它也广泛用于机器翻译。循环神经网络另一个常见的应用是语音识别，我们现在使用的很多智能语音助手都应用了循环神经网络。

股票的价格波动也存在一定的规律，根据前一段时间的股票波动情况可以大致预测出之后股票的走势。因此，循环神经网络在股票预测方面有先天的优势。随着经济的发展，股票市场的规模不断扩大，大量股票历史数据的积累使得循环神经网络可以学习股票的走势规律。比如，当循环神经网络发现，某只股票价格不断下跌超过七天，之后就会缓慢上升，并且在很长一段时间内这只股票都呈现出这个规律时，如果这只股票价格再一次持续下跌，那么下跌的第七天就是股民买入的最好时机。实践证明，循环神经网络对股票的预测能够较好地拟合真实数据，具有很高的应用价值。

循环神经网络可以有效地进行文本识别，而在商业世界中，最重要的一种文本数据就是商品评论。随着生活节奏的加快、电子商务的兴起，人们越来越倾向于网络购物。网购时，用户没有见过真实的商品，只能通过商家的描述进行了解，而商家的描述又不能保证用户了解到的信息完全属实，此时，用户的商品评论就成为反映商品质量和商家信用的一个重要参考标准。但是，用户的评论也有很大的主观性，如何结合用户的主观评价正确评估商品质量及商家信用成为一个亟须解决的问题。在循环神经网络强大的文本识别功能的帮助下，我们可以很好地解决这个问题。循环神经网络在分析评论的过程中，最重要的一个步骤是对用户的主观评价进行处理，即通过循环神经网络分析用户的商品评论，再将其转化为对商家的等级评价。比如，循环神经网络识别出不同的商家同时在售卖同一种商品，但在商品质量方面，商家甲好评数远远高于商家乙，那么在这一方面，商家甲的等级评价就会高于商家乙的等级评价。影响商家等级评价的因素还有很多，比如服务态度、发货速度，以及商品与描述相符度等，将这些因素全部考虑在内，就会形成一个全面的商家等级评价。循环神经网络在商家评价方面的应用使用户不会被海量商品信息以及其他用户的主观评价所混淆，直接找到符合自身需求并且质量上乘的商品，提升了用户的网上购物体验。

2『深度学习的两大经典模型：卷积神经网络和循环神经网络，做一张术语卡片。（2021-02-26）』

#### 3.4.3 深度学习商业网络

数据是商业发展的基础，也是商业竞争的关键。现在越来越多的公司都在通过数据的深度分析来了解用户的需求，把握市场的发展方向，提高自己的竞争力，以此达到在激烈的商业竞争中获胜的最终目标。深度学习作为人工智能的关键能力，其对数据的理解能力也超过了之前的相关模型。对数据的深度理解能够挖掘出数据背后隐含的知识，能更加快速准确地掌握用户需求，所以深度学习在商业场景中具有非常重要的应用价值。下面我们来介绍两个在商业领域应用的深度学习网络：潜在客户转化率深度学习模型和客户购买力深度学习模型。

如图 3-9 所示，这是专门用于房地产行业的潜在客户转化率的预测模型。这个模型可以通过线上、线下数据以及在 App 上对不同房产的浏览数据等，推测出一个潜在的购房客户转化为真正的购房客户的概率有多大。

图 3-9 潜在客户转化率深度学习模型示意图

在这个模型中，输入的数据包括：类别特征数据，如客户的房产数量、从事行业、家庭结构、渠道来源等；连续特征数据，如客户的年龄、预算金额、月供金额等；序列特征数据，如客户的关注楼盘、浏览记录、分享记录等。我们对不同分类的数据需要进行不同的操作，例如类别特征数据需要经过嵌入、池化以及全连接等操作，序列特征数据需要经过嵌入、卷积后才能进行池化以及全连接等操作。当这些操作都进行完之后，数据就可以进入拼接层，然后输出每个客户在接下来的一段时间内买房的概率。

如图 3-10 所示，这是一个专门用于预测房地产公司客户购买力的深度学习模型。这个模型可以通过一系列复杂的深度学习，最终预测出一个客户是高价值的买房客户还是相对低价值的买房客户。

在这个模型中，输入的数据包括客户的基本信息，如职业、收入、偿还能力、来源、意向、家庭结构、购房用途等；客户行为数据，如关注、点击、浏览、分享等；房屋描述信息，如建造年份、户型、位置、交通、物业等；还需要一些房屋、小区等周边环境的图像信息。不同类别的数据也需要进行不同的操作，例如客户基本信息中的类别特征数据需要进行嵌入操作，将稀疏的数据变成连续向量；客户基本信息中的连续特征数据就可以直接进行处理；客户行为数据要经过嵌入、卷积、池化以及全连接等一系列的操作；房屋描述信息数据要经过词向量化、嵌入、LSTM（长短期记忆网络）、堆叠特征处理。其中，LSTM 适用于处理和预测时间序列中，间隔和延迟相对较长的重要事件，堆叠特征处理可以将不同的特征拼接在一起。图像数据要经过图像预处理、基于预训练的 VGG 模型以及全连接操作。当这些操作都完成之后，所有的数据要一起进行决策树处理。最后，我们就可以看出这个客户到底是高购买力客户还是低购买力客户了。

图 3-10 客户购买力深度学习模型示意图

深度学习的出现为人工智能带来的是一种全新的思维方式和思考方式，它赋予了人工智能新的活力，为人工智能带来了无限的可能性。万物皆可智能正是因为人工智能有了深度学习的支持，是深度学习为人工智能插上了飞翔的翅膀。更有人提出，深度学习甚至决定了人工智能的未来，所以现在深度学习越来越成为机器学习和人工智能不可或缺的一部分。以小博大，见微知著，AI 思维强调要抓住事物的本质特征才能做出最好的决策。掌握了深度学习，就算是掌握了人工智能的核心，所以，深度学习是我们构建 AI 思维不可或缺的重要一环。

### 3.5 AI 炼金术的应用生态

中世纪的时候，人们十分相信占星术，认为天上的星体可以带来幸运；并且将地上的一些贵重金属视作天体的代表，比如黄金代表金黄色的太阳、白银代表银白色的月亮等，这些贵金属因为稀少并且被赋予了幸运的含义，价格昂贵。地中海附近的国家希望能有一种将贱金属转变为贵金属的法术，这种法术被称为炼金术。人工智能拥有的就是这种将看似没有价值的数据转变为价值难以估量的决策的「法术」。

我们先来回顾一下 AI 炼金术全过程：首先，我们要通过对历史数据的特征处理产生特征数据，然后通过机器学习产生预测模型。模型产生之后，需要通过回测对其进行模型评估，在评估中合格的模型才可以投入生产。生产过程中如果有新的样本数据输入，就可以通过模型来产生预测分数，根据分数的高低和其他因素来进行业务优化，产生针对个体的业务决策。决策指导产生针对个体的具体行动，行动作用在个体上产生结果，虽然各自的结果可能会有差异，但人工智能总体上能够优化业务，从而实现价值。AI 炼金术就是这样一步一步，对数据千锤万凿，百炼成金。

#### 3.5.1 人工智能赋能架构

AI 炼金术所创造的价值是不可估量的，对于企业，它是大赚 1 000 亿美元的秘密武器，对于个人，它是安居乐业人生美满的秘诀。但人工智能炼金术不是一座飘浮起来的空中花园，它需要建立在真正落地的实际系统上，需要系统给各个参与者赋能，否则再神奇的炼金术也只能是一场美好的空谈。那么人工智能炼金术最终要落地在一个什么样的系统上呢？

基于人工智能赋能架构的系统，能够为企业和消费者赋能。如图 3-11 所示，它分为左右两边，左边是 C 端，也就是用户端，通常涉及企业的用户或消费者，他可能在企业经营的门店里购买过商品，在企业运营的网站上注册过会员，或者是使用过企业提供的手机 App，总之他和企业有过一些交互和接触。右边是 B 端，也就是服务端，服务端就是要在企业的后台为用户提供优质的服务，确保用户端得到最佳的体验。

图 3-11 人工智能赋能架构示意图

服务端的系统分为两部分，一部分是数据中台 ，另一部分是 AI 中台。数据中台会对海量数据进行采集、计算、存储、加工，形成统一的标准数据，形成大数据集合，进而通过 AI 中台为用户提供个性化服务。从数据到价值的 AI 炼金术，是 AI 思维的具象表现形式，AI 中台则是 AI 思维的实现载体。在服务端，既需要数据、需要人工智能，也需要数据中台和 AI 中台的连通。只有数据中台和 AI 中台融为一体，人工智能才能知道瞬息万变的市场变化、用户的需求，才能给用户提供优质的服务。另外，数据中台和 AI 中台打通了，就会有源源不断的数据输入 AI 中台，人工智能训练的数据越多，训练的次数越多，它的智能程度就会越高，做出来的决策就会越准确，其所创造出来的价值就越高。

然后我们再来看用户端，用户端主要有两个模块，一个是设备端，另一个是应用层。现在的用户无论是线上还是线下跟企业交互，都要通过一些设备来实现，比如说在 PC（个人电脑）端看视频、在手机端玩游戏、通过 VR 头显（虚拟现实头戴式显示设备）感受虚拟现实，或者说与在实景中摆放的机器人交流互动。所以说 PC 端、手机端、VR 头显端或者是机器人端都是一个用户的触达点，都是一个企业的触角。通过这些触角，企业可以实时了解用户的需求并为其提供服务，比如说现在很多酒店都配备了智能机器人，如果酒店的房客半夜想要喝水，发现房间里的纯净水都喝光了，而酒店的工作人员都已经休息了，就可以让机器人为房客送纯净水。

在这些设备端里面可能会有 AI 芯片。在人工智能中，AI 芯片是专门用于模型运算任务的一个模块，它可以在手机里做大量快速的计算，比如说现在很多年轻人喜欢通过各种美图 App 给自己上妆或者为自己的照片换背景，这些特效的实现都是实时的，呈现效果也非常好，所以需要很大的人工智能运算量。面对越来越高的用户的体验要求和越来越高的运算量，传统的硬件已经不足以解决问题了，因此现在手机上都安装了专门用来处理这些问题的 AI 芯片。手机 AI 芯片发展迅速，比如说三星最新研发的手机 AI 芯片有了独立的神经网络处理单元，运算速度提升了 7 倍；华为研发的 AI 芯片 NPU（Neural Network Processing Unit，嵌入式神经网络处理器）使图形处理性能提升了 20%，能效提升了 50%，图像识别速度可达到每分钟 2 000 张；OPPO、vivo 使用的是科技公司联发科研发的手机 AI 芯片，有低能耗、高性能的特点。

虽然 AI 芯片只是一块小小的芯片，但是 AI 芯片产业却是一个大的产业，因为它既能在用户端上进行人工智能的运算，也能在服务端进行人工智能的运算。虽然服务端跟用户端是连通的，但是网络存在延迟，直接在用户端进行计算可以缓解这个延迟。从整体架构上来说，设备端和服务端中间是应用层，其中包括像微信、QQ（腾讯公司的即时通信软件）这样的应用 App，通常企业需要花大量的时间和精力去改良这些应用，提高用户体验和服务质量。例如，谷歌开发的语音助手 Google Assistant 的语音模型一开始是存储在谷歌服务端的，它能够准确识别出用户所说的指令并做出相应的回应。但是在使用过程中，用户对着 Google Assistant 说一句话，这句话需要上传到服务端才能被语音模型识别，然后再传回用户的手机，这就导致了语音识别速度很慢。为了改善用户在使用语音助手时的体验，谷歌对语音模型做了处理，将其大小从 100G 压缩到了 0.5G，使其可以直接缓存到用户的手机上。如此一来，语音在手机和服务端之间的传输时间大大减少，Google Assistant 的语音识别速度提高了将近 10 倍，极大地提高了用户体验。

所以前文提到的 AI 炼金术，其实是构建在这样一套赋能架构上的。从图 3-11 最左边的用户到最右边的企业，环环相扣，人工智能不仅可以由企业的服务端提供算力，也可以在用户的设备上进行运算。这块根据响应时间会有一个切分，对响应时间比较敏感的会在用户端进行，而对时间要求没有那么紧急的任务，可以放在服务端进行。另外，服务端有更多更全的数据，适合更为宏观的人工智能运算。这套连接服务端和用户端的赋能架构，是人工智能炼金术所依赖的系统基础。理解整个赋能架构，能够帮助我们更好地应用人工智能，通过人工智能更多的创造价值。

中台即在一些系统中，被共用的中间件的集合。—— 编者注

#### 3.5.2 人工智能应用矩阵

人工智能的发展、传播和被接受是经过了一段漫长的寒冬的。十多年前，它还只是一个不被人看好的小众领域，但是现在，它却已经成为街头巷尾的热点谈资，几乎任何事情都可以和人工智能联系在一起。

短短十年间，世界发生了天翻地覆的变化，新数据不断涌现，各种问题层出不穷，直到现在，人工智能的春天才算是真的到来了，各个领域都亟须人工智能的帮助。这也是为什么人工智能的商业化应用范围如此广阔，人工智能市场更是如一块等待开垦的处女地一般。如图 3-12，这是一份人工智能的商业版图，横轴代表不同的行业领域，例如金融、零售、医疗和教育等，纵轴代表不同的职能方向，例如营销、风控和安全等。横轴和纵轴共同构成了一个二维的商业矩阵，对于每个行业中的相关职能，人工智能都可以探索相关应用场景，例如在金融行业的营销、CRM（客户关系管理）、客服、风控和安全等方面以及零售行业的营销、CRM 和客服等方面都已经有人工智能实践落地。但其实，现在的人工智能只填充了广阔的商业领域中非常小的一部分，还有更多没尝试和拓展的人工智能应用场景。

在金融行业，人工智能应用十分广泛。以金融服务集团蚂蚁金服为例，他们推出了智能理赔服务，用户可以直接在手机上上传票据，通过智能审核就能迅速获得赔付款；支付宝将人脸转变为数字密码的刷脸支付功能也是人工智能在金融行业的应用。除此之外，人工智能还可以通过模型和投资组合优化等，为用户提供投资参考的智能投顾服务；通过自然语言理解、语音识别等，实时为用户咨询提供回答的智能客服服务；通过推荐模型，根据大数据为用户推荐个性化产品的智能营销服务；以及通过人脸识别和指纹识别，在支付时检测用户身份的智能风控服务。这样一系列服务使人工智能在金融领域扎实落地。

图 3-12 人工智能的商业版图

随着社会经济的发展，居民收入水平的提高，人们的消费能力逐渐上升，零售行业规模随之扩大。根据预测，2020 年全球零售销售总额将超过 26 万亿美元。由此可见，人工智能在零售业的落地也有很大的应用前景。例如，人工智能可以依靠基于大数据的机器学习模型，为用户生成精准化、定制化的推荐，提高零售业销量，Stitch Fix 的成功就是很好的证明。Stitch Fix 是美国的一家时尚电商，活跃用户达数百万人。它之所以在电商行业竞争如此激烈的当下能够留住用户，就是因为正确运用了人工智能，向用户推荐个性化商品。Stitch Fix 专门请数据科学家建立了人工智能推荐模型，根据用户的身高、体重、职业等信息以及穿衣风格和购买价位等偏好，为用户推荐定制化、个性化的商品。Stitch Fix 在 2016 年和 2017 年分别创造出了 7.3 亿美元和 9.8 亿美元的销售额，而且这些销售额来自智能推荐。由此可见，人工智能可以有效地赋能零售行业，为其带来效益的增长。

在工业、医疗行业等领域，人工智能虽然也有应用，但并不及在金融和零售行业普遍。从整体来看，由于人工智能是近几年才发展起来的新技术，所以它在很多行业和领域的应用并不广泛深入。但是人工智能发展势头很好，在每个行业都有落地的可能性，而且随着机器学习、计算机视觉、语音识别、自然语言处理等人工智能领域的发展，我们有理由相信，人工智能在各个领域都能扎下根去，遍地开花。

以前，行业追求的都是芯片驱动（Intel inside），用芯片为企业提供算力和速度，让企业能够跑赢市场，找到自己的阿尔法。但这都已经成为过去，以后行业追求的人工智能驱动（AI inside）才是真正的阳关大道。人工智能会成为行业的决策引擎，取代在战术层面上需要人工干预的场景，持续有效地让企业在最佳状态下运行；而且人工智能带来的效果不仅仅是「优化」这么简单，而是革命性的、成倍数的增长。如此，人工智能才能真正展现它的能力，真正为行业赋能。

#### 3.5.3 AI 炼金术的场景化应用

既然 AI 炼金术在各个领域都有落地应用的可能性，并且在多个领域的应用已经产生了不错的效果，那么，它在具体的场景下是如何展开应用的呢？为了便于理解，我们先从一个大多数人都比较熟悉的例子讲起。比如说，你在一个电商网站上买东西时，遇到了很多广告。这些广告位都是商家花钱购买的，即使你可能并不会购买广告位的商品，这个商家在此投入的广告费是无法撤回的，所以商家要尽可能地提高你购买广告位商品的概率。此时，人工智能精准推荐就成为商家提高购买率的法宝，因为显示一个顾客感兴趣的商品能大大提高他的购买率。事实证明，经过人工智能精准推荐出现在广告位的商品，让无数剁手党们欲罢不能。

百年大计，教育为本。社会各界对教育越来越重视，国家更是提出了科教兴国的教育发展战略。教育是不可忽视的，但与此同时，教育业也存在诸如教育水平发展不均衡，学生学习负担重、学习兴趣不高等问题。而人工智能的出现为这些问题提供了解决方案。在教学环节，教师可利用人脸识别模型实时检测学生随堂表现，辅助教师教学；在练习环节，文字识别模型可批改文科作业，图像识别模型可批改数学作业，这些智慧作业平台大大减轻了教师批改作业的负担；在考试环节，语音识别模型可对语言语音进行打分，也可利用人工智能进行试卷批改，统计、分析分数；课堂之外，学生可以利用人工智能搜集资料、完成作业等，比如遇到不会做的题目，学生可利用各种人工智能拍照搜题找到答案。除此之外，人工智能还可以根据学生的成绩、课堂表现、作业反馈等信息，判断该学生的学习习惯、学习能力以及学习程度，并为其制订个性化学习方案。例如，一个学生的英语听说能力很好，读写能力不足，那么在日常英语学习中，可以相对减少一点对于听说能力的训练，节省下来的时间用于加大读写能力的训练。通过人工智能制定的个性化学习方案，老师和家长能更好地帮助学生找到学习的乐趣，提高学习的能力，获得更多的知识。

人工智能在法律领域的应用也非常广泛。当事人如果遇到法律问题，人工智能可以根据当事人的信息、相似案情的判决等数据来进行决策，为当事人提供法律咨询，或者为当事人推荐最合适的律师。对于律师来说，人工智能的文本处理能力可以帮助律师搜集资料，例如相关法律条文、相关案情的判决书等。人工智能现在已经可以根据已有信息判断案件能否胜诉。比如说，CaseCrunch 公司的人工智能就可以根据案件信息进行判决预测，并且预测的成功率高达 86.6%，打败了人类律师 62.3% 的成功率。

2018 年斯坦福等高校联合开发的律政界人工智能软件，和 20 名有经验的律师做了一个比赛。该比赛的内容是在 4 小时内审查 5 项协议，并回答 30 个法律问题，涉及领域包括仲裁、赔偿等。最终，人工智能软件以 94% 的准确率完胜律师的 85%。人工智能为法律领域带来的价值是为不懂法律的大众提供专业的法律帮助，为律师减轻工作负担，提高法律案件处理的效率，协助法律系统创造一个公平清明的世界。

人工智能在维护社会公平自由的同时，还能为医疗行业贡献一份力量，维护人类的健康。根据《2018 世界人工智能产业发展蓝皮书》显示，2013 年至 2018 年第一季度，全球人工智能的投资在医疗健康领域的热度最高。据老牌咨询公司麦肯锡预测，到 2025 年，全球智能医疗行业规模将达到总计 254 亿美元，约占全球人工智能市场总值的 1/5。由此可见，人工智能在医疗领域有广阔的应用前景。

医疗行业长期存在优质医生资源分配不均的问题，与此同时，在诊断过程频频出现误诊漏诊现象，而且医疗费用一直居高不下。对于普通民众来说，也一直存在看病难、看病贵的问题。人工智能通过对大量医疗数据、病人信息的分析，可以了解病人的病情，甚至能够预测一些疾病的发病时间。根据这些信息，人工智能可以生成针对每个病人的个性化医疗方案，帮助病人尽快康复并预防其他疾病。而那些严重的疾病，比如基因变异引起的癌症，虽然千差万别，情况复杂，但是人工智能可以针对每个人独特的基因变异数据，协助制订最佳的治疗方案，最大限度地挽救生命。病人康复出院之后，人工智能还能提供长期的健康监测服务，防止旧病复发、新病萌芽等现象。同时，人工智能医疗流程系统还能为医护人员安排最合理的工作流程，避免医生在简单的咨询工作中耗时过长，从而提高医生的诊疗效率。

人工智能在各行各业的广泛应用空间可能会使你有些恐慌，觉得人工智能再发展下去将取代人类的工作，甚至给这个世界带来灾难。这不只是一个人的忧虑。早在 2016 年，英国著名物理学家霍金就说过人工智能可能是人类的灾难：「对于人类来说，强大人工智能的出现可能是最美妙的事，也可能是最糟糕的事。」但我们现在的人工智能大部分都是弱人工智能，是那些不能真正地独立推理和解决问题的智能机器。有人又要问，AlphaGo 不是具有自我学习独立思考的能力吗？其实不然，围棋有固定的规则，对手下棋的习惯可以被模型捕捉到，但真实世界里的情况千变万化，不用说庞大而复杂的基因数据了，就是普通人的行为都千差万别，没有人类参与的人工智能是很难成气候的。而那些能够独立推理和解决问题的强人工智能的出现目前来看还很遥远，所以我觉得，认为人工智能会取代人类这样的焦虑，有些杞人忧天。

## 0401. 人人都能理解 AI

在制造分秒的时候，钟表把时间从人类的活动中分离开来，并且使人们相信时间是可以以精确而可计量的单位独立存在的。分分秒秒的存在不是上帝的意图，也不是大自然的产物，而是人类运用自己创造出来的机械和自己对话的结果。

—《娱乐至死》—

在前文中，我们首先从「道」的层面理解了 AI 是时代的机遇，之后又从「法」的层面讲了 AI 思维的底层逻辑，最后从「术」的层面讲了 AI 炼金术。但是学习是一个螺旋式上升的过程，需要不断深入，不断提高；知识进一步提升，思维能力也会更强，理解和运用人工智能也是这个道理。本章将会更加深入下去，从「器」的层面讲解 AI 思维。「器」可以帮我们理解 AI 思维的奥秘，也是人工智能越战越勇的法宝，对人工智能的「器」有了深刻的理解，就像孙悟空有了金箍棒，可上九天揽月，就像哪吒有了混天绫和乾坤圈，可下五洋捉鳖。「器」是工具，而人工智能领域的「器」就是模型，模型是 AI 思维的核心，本章将挑选人工智能领域的经典模型来帮助你理解 AI 思维，其中包括分类和聚类的模型，图像、语音以及文本的识别模型，还有消费和社交数据的处理模型。首先让我们从 AI 的分类和聚类开始理解 AI 思维的「器」。

### 4.1 AI 的分类和聚类

只会纸上谈兵的赵括由于缺乏战场经验，在长平之战中输给了秦将白起；熟读兵书的马谡不听从有实战经验的副将的建议，将街亭输给了司马懿，逼得诸葛亮只能挥泪斩马谡。历史上许多故事都告诉我们：缺乏实践经验，只会空谈，必然会导致失败。

过了那个兵荒马乱的年代，虽然我们不会再经历千军万马丢城失地的覆灭，但在日常生活中，若缺少经验，我们常常会遭遇挫折。从没有学过游泳的人，为了冒险刺激就往深水区游，难免会多呛几口水，甚至失去生命；从没有投资过的人，看到别人获利就盲目进入风云莫测的股市，便是注定了赔钱的命运。

然而我们都是第一次生而为人，只要对这个世界仍然保持着好奇心，总会去尝试无数个缺乏经验的第一次。第一次失败了并不可怕，重要的是你从这次的失败中学到了什么。及时总结，在一次次的试错中，也就是从经验中，我们学习了万事万物的规律。操千曲而后晓声，观千剑而后识器。以后再遇到相似的问题，我们就能妥善地处理了；碰到新的问题，我们也能根据习得的规律从容地做出比较准确的判断了。

人工智能各种能力的形成也是这个过程。我们从经验出发，得到处理事情的方法论；人工智能从数据出发，得到预知未来的模型。经过对数据的多次学习，人工智能就像一个身经百战的智能人，如果再有新的数据输入，人工智能也可以独立做出准确的判断，以及合理的预测。对于人工智能来说，产生模型的方法论就是机器学习，下面我们来仔细讲解一下。

#### 4.1.1 机器学习的分类

我们获取知识的途径有两种，一种是通过他人的传递，例如我们遇到不会的问题时去请教老师或同学；另一种是在没有接受指导的情况下自学获得的。类似地，机器学习也有两种：监督学习和无监督学习。

监督学习是从标记的数据来学习模型的任务，每个标记的数据都是由一个输入数据（也就是通常所说的 x ）和一个期望的输出数据（也就是通常所说的标记 y ）组成。无监督学习处理的则是没有标记的数据。监督学习适合有明确预测目标的应用场景，即 y 可以被明确地定义和量化。而无监督学习通常适用于无法预先知道输出值的场景，只能对现有输入向量进行观察分析，它可以从数据中发现具有相似输入向量的结构，这些结构统称为簇（cluster）。

机器学习问题根据输出的数据来分，也有两类。一类是离散输出，通常用来表示个体的类别，比如把人分为两类，男人和女人；把客户分两类，高价值客户、低价值客户。数学中的离散是和连续相对的，所以另一类就是连续输出。连续输出的数据是指那些在一定区间内可以任意取值的数据，其数值是连续不断的，相邻的两个数据之间可以做无限的分割。比如说两趟地铁之间的间隔是 10 分钟，一个乘客在地铁站等地铁的时间是一个随机变量，取值范围可以是从 0 到 10 这个区间内的任意实数。所以连续输出有很大的应用空间，比如要预测公司的收入、客户的价值、生产流程的能耗、生产系统输出的气体浓度等，这些都需要用连续数字来表示，就都需要连续的输出。

监督类型分为通过监督学习和无监督学习，是机器学习问题分类的一种依据；输出分为离散型和连续型，是机器学习问题分类的另一种依据。将以上四类排列组合，就形成了四种不同类型的机器学习问题，如表 4-1，分别是：

如果通过监督学习，得到离散输出，就是分类问题。

如果通过无监督学习，得到离散输出，就是聚类问题。

如果通过监督学习，得到连续输出，就是回归问题。

如果通过无监督学习，得到连续输出，就是降维问题。

1-2『丁磊从「有无监督」「输出为离散或连续」这两个维度将机器学习问题划分为 4 大类，这个角度之前从未看到过，有一种醍醐灌顶的感觉，而且两维度划分四个区域这种方法论在其他 N 多领域看到过，很经典很经典。机器学习 4 个典型问题，做一张主题卡片。（2021-02-27）』

表 4–1 机器学习问题的类型

| - | 监督学习 | 无监督学习 |
| --- | --- | --- |
| 离散输出 | 分类问题 | 聚类问题 |
| 连续输出 | 回归问题 | 降维问题 |

这四类问题是机器学习的基本问题，深入理解了这四类问题，我们就能对机器学习有一个完整的把握。无论在哪一类问题中，机器学习的目标都是根据新的输入，做出预测。理解清楚了这四类问题的框架，我们就能深入体会到人工智能解决实际问题的思路。下面我会深入分析两类问题 —— 聚类问题和分类问题，它们也是机器学习中最常使用的两类。

#### 4.1.2 聚类问题

《战国策》有云：「物以类聚，人以群分。」在日常生活中，有各种事物需要聚类，连人也在无形之中有聚类。而在人工智能领域，聚类问题对应的是无监督学习、离散输出的机器学习，也就是要通过机器学习将一些没有标记过的数据归为一类。数据的维度越来越多，聚类的目标是找到这些数据中具有某些相似特征的簇，找到每个个体的输入数据和簇之间的归属关系。也就是说，聚类中，我们需要按照一定的模型，把所关心的个体归并到一些簇里。

这个世界日新月异，每时每刻都有新的事情发生，新闻记者在事件发生的第一现场为我们传递新闻。但是很多时候我们并不能了解到新闻的全貌，因为它散落在各大新闻报刊以及新闻网站的各个角落。聚类的出现就能够让零散的新闻聚集起来，帮助我们了解新闻的来龙去脉。通过聚类，我们可以有效地将那些相同主题的新闻聚集在一起，形成簇状的结构，我们再进行浏览时就会更加全面、准确。

在商业实践中，聚类对于发现一些特定的人群结构非常有用。就比如 PayPal 面对的是全球范围内的几亿用户，它可以通过聚类对这些用户成百上千的维度进行详细的分析，从中得到百余个簇。每个簇对应属性和行为比较接近的一类用户，不同簇之间的差异化程度比较大。例如，一个簇对应了年轻的母亲，一个簇对应了中年艺术家，一个簇对应了软件专业人士，等等。理解了人群的不同特点和诉求后，公司才可以针对性地定位、开发、推广产品。另外，聚类中有时会发现一些未曾关注却占比显著的簇，那么整个品牌的定位都会被聚类的结果和发现影响。PayPal 建立的这一套聚类体系帮助公司更好地了解用户和市场的需求，为他们提供了更加贴心的服务，由此可见，聚类体系是可以为商业带来价值的。

常见的聚类方法有：K - 均值聚类、EM 聚类、合成聚类、Mean-shift 聚类、频谱聚类。这些方法根据不同的数学原理来做聚类。方法不同，结果不同，但大的目标都是一致的，都是为了发现并输出数据中的簇。簇的标志性特征是内部差异化较小，而簇与簇之间的差异比较大，这也是聚类方法的基本原理。

下面我们来说明一下聚类。如图 4-1 所示，左边是原始的数据，用三种深浅不同的点来表示有三个簇。K - 均值聚类和 EM 聚类的结果分别在中间和右边的图上。因为聚类是无监督学习，没有方法能够完全还原原始的簇，但是合适的聚类方法还是能够发现数据内在的结构。各行各业都会产生数据，所以不论是在商业领域，还是在生物学、医学、天文学等科学领域里，聚类都有它的用武之地，都能够帮助各领域的专业人员发现新的知识。聚类就像一个数据显微镜，能够拉近我们和数据的距离，把微小的每个样本聚合成可以解读、可视化的簇，增加我们对数据的理解。

图 4-1 聚类示意图

#### 4.1.3 分类问题

下面我们来看一下分类，也就是监督学习、离散输出的机器学习。分类要做的是将一个预测函数 f ，作用在个体的特征 x 上，得到想要的输出 f(x)。这个看似简单的识别过程其实是非常强大和灵活的，基本上所有监督学习的过程都可以抽象成这样的形式。预测的目标可以是任何跟业务相关的量，包括顾客的价值、消费者的喜好、借款人的风险、病人康复的概率等。深入理解了人工智能后，你就可以针对你所在的领域，构造出相应的分类问题，从而有效地提升业务。

提到分类，就不得不提 2019 年全国都在热议的话题 —— 垃圾分类。2019 年 7 月，上海正式开始强制实施垃圾分类，个人投放垃圾错误最高可罚 200 元，单位投放垃圾错误最高可罚 5 万元。一时间，垃圾分类成为上海人民生活的主旋律。即便是将垃圾分为了可回收物、有害垃圾、干垃圾、湿垃圾四类，但是由于真实生活中垃圾种类繁多，混合垃圾难以分辨种类，很多人还是会分辨错误。其实国外许多国家实行垃圾分类很久了，分辨多种垃圾的问题也困扰国外人民很久了，所以有很多人工智能公司就开发了基于监督学习的垃圾分类系统。例如日本发那科（Fanuc）的分拣机器人 Waste Robot 就是这样一个垃圾分类系统，该系统内部拥有经过数据集训练的模型，可以通过实时检测来分辨垃圾的类型。

监督学习的模型不是想当然地形成的，它要经过严格的训练和预测环节，这两个环节也是监督学习最基本的环节。

2『监督式学习中，训练和预测，是两个核心环节。此处的信息补充进术语卡片「模型的训练和预测」。（2021-02-27）』

训练环节中：给定一个训练集，由标记样本构成 {(x1,y1), …, (xn,yn)}。所谓标记样本就是既有特征向量 x ，又有对应的标记 y 的一组数据。通过最小化训练集上的预测误差，估计一个预测函数 f，也就是我们的预测模型。需要特别注意的是，训练集都是由样本和对应的标记构成的。例如我手中有大象、猴子、长颈鹿等各种动物的图像，然后给这些图像打上相对应的名称标记（图像和名称标记一一对应），打过标记并且一一对应的图像称为训练集。

预测环节中：在新预测样本 x 上，使用 f 输出预测值 y = f(x)。就比如人工智能经过了刚才关于动物图像的训练，我们再把新的图像输入给它，此时人工智能就可以输出图像上的动物是什么了，这就是预测环节。

在训练阶段，样本的标记是已知的，但是在预测阶段，样本的标记是未知的。就像读书的时候，平时死记硬背的同学不一定考试考得好，考试考得好的同学，也未必在社会上发展出色。机器学习也是这个道理，在新的、未曾见到过的预测集上的预测效果，才是做好机器学习的关键。那么要做好机器学习，也就是训练出一个预测效果好的模型，需要多少数据样本呢？这个问题是无法一概而论的。如果你的数据样本大，行为或模式复杂，要预测的标记本身也会发生变化，那么这个模型需要的训练数据就多。通常，跟训练简单模型（好比后面我们要说的线性模型）比较，训练复杂模型需要的样本也会比较多。

需要注意的是，用来训练的样本一定要代表实际的业务场景，这样机器学习产生的模型才能在实际业务中产出良好的预测效果。如果在实际业务中遇到的预测样本和训练样本的特性相差甚远，那么模型是很难产出良好的预测效果的。比如你想开一家饭店，但却只在上海地区展开饮食喜好的调查，那么你大概会得出「消费者都比较喜欢吃清淡一点的食物」的结论。根据这个结论，你精心研发了一份清淡可口的食谱，但是这家饭店最终选址在川渝地区。可想而知，饭店开业后并不会有太好的生意，因为两地的人群样本对食物的喜好相差很大，上海样本上得出的结论很难在川渝样本上产生同样的效果。

通过前面的阐释，我们已经知道了分类的概念，下面再通过一个具体的图像分类的例子来更加深入地了解分类。如图 4-2 所示，训练阶段我们提供了一组有类别标记的图像，预测阶段，给定一个图像，我们的模型可以输出图像上的物体类别。

图 4-2 图像分类示意图

训练的时候，我们给了 6 类训练图像，分别是 3 种水果和 3 种动物。每张图像先经过特征化处理，产生图像特征。根据这些图像特征和标记，机器学习能够学习出一个模型。预测的时候，我们给定一张图，把它特征化后，模型就可以针对这个图像特征，输出一个类别预测。

之前提到过，机器学习中，数据以向量形式表示。对图像来说，有很多种向量表示的方法。我们挑选 3 种基本的向量表示方法展开讲解：原始像素值特征、直方图特征、GIST 特征。如图 4-3 所示，原始像素值特征，用的就是原图的彩色或者灰度像素值，把它们拉成一个向量作为图像的表示，这种简单的方法只能表示简单的图像。图像的像素值很容易受到环境中各种因素的干扰，用它们表示复杂图像的内容，可用性不强。直方图特征，是指加工原图像素值，用加工后的像素值分布，而不是原始值来表示图像。GIST 特征，用更复杂的方法来全面表示图像中的场景，能够更好地刻画图像的视觉特点，比如说是自然场景还是人工场景，场景是封闭的还是开阔的。

1『此时此刻看到直方图（不同亮度像素点的部分），直接想到了正态分布，是不是大部分比较常规的图片，其曝光图部分（直方图）基本是符合正态分布的，最和谐的。（2021-02-27）』

图 4-3 图像的特征向量表示方法

对于图像来说，设计的特征越精巧，在机器学习中，图像识别的效果就能够做得越好，但是人的肉眼却很难看明白这些复杂的特征。人工智能和人的思维方式不一样，对于人来说很直接的原始图像，计算机却很难处理；而构造精妙的图像特征向量，人则不一定能看明白。

#### 4.1.3 最近邻分类

前面以图像为例说明了机器学习分类的过程，下面我们来了解一种基本的分类方法：最近邻分类。这种方法不需要训练环节，只需要针对输入数据的距离定义。

2『分类问题，做一张主题卡片。（2021-02-27）』——已完成

如图 4-4 所示，我们用方形和圆形分别表示两种不同的训练样本，一个新的预测样本会根据离两类训练样本距离最近的数据点，来确定自己的分类。也就是说，如果和预测样本最近的样本是方形样本，我们就把它分类为方形，相反，如果和预测样本最近的样本是圆形样本，我们就把它分类为圆形。

图 4-4 最近邻分类示意图

这个分类过程类似人的思考过程，好比我们对一个不熟悉的物品，一个没去过的地方，一个不认识的人，我们会下意识地把它跟我们接触过的物品、地方、人做对比，用一个我们熟悉的事物的类别作为对这个新事物类别的「预测」。人的思维如此，机器学习也应用了这个原理。

虽然最近邻分类比较简单，但是它在很多实际场景里的应用效果尚能差强人意。有的时候，你可能并不知道要用什么样的模型来分类数据，那我建议你试试最近邻分类，先看看它能产出什么效果。对最近邻分类有了一些了解后，下面我们来介绍更多的分类模型。

#### 4.1.4 线性分类和 K - 近邻分类

在机器学习领域，线性分类是一种经典的分类方法，它的目标是寻找一个线性函数，直观上也就是一条线，来分割两类数据，如图 4-5 所示，在这条线的一边是方块的一类，另一边是圆形的一类，而且这条线与两边的距离要适中。

图 4-5 线性分类示意图

当然，分割线未必只能在纸上，也就是在二维空间里画的线，它也可能是三维空间里的一个平面，又或者是高维空间里的超平面。这个线性函数可以在任意维度的空间里，它取决于机器学习任务中特征向量的维度。例如，我们有 2 000 张花花草草的照片，如图 4-6 所示，每张照片由这个高维空间中的一个点表示。如果要把这两千张照片按花和草来分类，那这个线性函数就是图 4-6 所示的超平面。（因为我们无法看到高维空间里的超平面，所以这里的超平面用平面来表示。）

图 4-6 高维空间线性分类示意图

我们实现线性分类的方法叫作最小二乘法，又称最小平方法。通过最小二乘法可以计算出进行分类的这个线性函数的参数，也就是说可以计算出一条分类的最佳分割线。原理上，这个划分两类的分割线，要能充分「拟合」两边的训练数据，不能相差太远。但既然是拟合，就难免会有误差，在数学上，最小二乘法就是通过最小化误差的平方和来寻找最佳拟合的线性函数的。不论你的特征向量维度是多少，最小二乘的方法论都是通用的。理解了线性分类，我们就能理解更多的机器学习模型。机器学习最终学习出的函数形式可能会很复杂，不一定是线性的，但原理上都是通过最小化一个目标值来算出最佳的分类函数。

在实际应用中，线性分类最重要的优势是它的结果易于解读。线性分类会针对每个特征变量，输出一个权重，这个权重决定了变量在分类中的重要性。权重的绝对值越大，变量越重要。如果权重是正数，就意味着变量和预测目标有一定的正相关性；如果权重是负数，就意味着变量和预测目标有一定的负相关性。

举一个简单的例子来说明一下权重。比如你今天到商场逛街，逛累了之后要决定午饭吃些什么。那你就需要收集商场里所有能吃饭的地方的信息 —— 当然可能不需要复杂到建立一个模型，但是肯定也会在大脑里经过思考。假如你十分喜欢吃辣的食物，可以接受面食，不喜欢吃甜食，那么在这次决策里，吃辣的食物以及吃面食这两种变量所对应的权重就都是正数，只不过吃辣的食物权重更高，而吃甜食的权重就是负数。最终你的选择是这些变量的加权线性组合决定的：权重是正数的变量，权重数值越大，相应方案被选择的可能性就越大；权重是负数的变量，权重的绝对值越大，相应方案被选择的可能性就越小。所以你最后很有可能选择比较辣的食物，比如说川菜。

在人工智能里，除了最近邻分类、线性分类外，还有很多种分类方法，比如下面要介绍的 K - 近邻分类。K - 近邻分类的一般原理是给定一个训练集，输入新的样本，在训练数据集中找与该向量最邻近的 K 个样本。这些样本多数属于哪个类别，就把该新样本分到这个类别中。最近邻分类可以看作 K＝1 的情况。

我们举个例子来更好地理解 K - 近邻分类。如图 4-7 所示，下面是我们收集的几部电影的数据：主要统计的是电影中的战斗镜头和拥抱镜头的数量，还有电影的类型（未知电影除外），比如电影 A 中有战斗镜头 4 个，拥抱镜头 109 个，电影 A 为爱情片。

图 4-7 电影镜头数据统计图

根据上述收集的电影数据，我们可以计算出已知电影和未知电影的距离，如图 4-8 所示。

图 4-8 电影距离数据统计图

按照距离的递增排序，可以找到 K 个距离最近的电影，假定 K=3，则三个最靠近的电影依次是：电影 B、电影 C 和电影 A。所以未知类型电影的类型应该和离它距离最近的电影是一样的，所以它应该是爱情片。

除了 K - 近邻外，还有逻辑回归、支持向量机、决策树、随机森林和深度学习等模型。这些模型各有各的特色，就像是十八般武艺，了解之后你也能在实践中使用它们，做到游刃有余。最后，让我们再回到机器学习问题的本源，我们需要建立模型做出准确的预测，不同结构的模型都是通过数据学习出来的。模型在实践中的预测效果以及对业务的提升，都需要相应数据的支撑。一旦模型学习好了，人工智能就会像人在经验中总结出规律一样，在面对新问题时，帮助我们做出正确决策。

这个世界对于我们来说充满了太多的未知，需要我们不断地去探索和发现。人工智能的出现让我们在面对新的事物、新的问题时不再不知所措。不论是针对有标记的事物还是未标记的事物，机器学习都可以有效地进行预测，最终形成决策并为我们提供参考，帮助我们有章可循地认识这个世界，改造这个世界。

### 4.2 AI 感知图像和语音

在实际生活中，我们会遇到各种各样的数据，为了更好地对这些数据进行分析掌握，我们将其分为两类：结构化数据和非结构化数据。结构化数据就是能够用统一的结构加以表示的数据，例如数字、符号等。反之，非结构化数据是指那些字段长度可变，不能够用统一的结构加以表示的数据，如图像、视频、语音、文本等信息。非结构化数据与我们的生活息息相关，我们接下来就来了解一下人工智能是如何处理这些非结构化数据的。

前面我们已经提到过像佳丽的特征脸这种简单图像处理的例子，相信你已经有了一定的图像理解的基础。我们现在要从较为直观的人工智能理解图像开始，层层递进，更加全面深入地讲解人工智能理解非结构化数据的方法。

提到人工智能理解图像，就不得不说计算机视觉。计算机视觉是人工智能在视觉领域的应用，也就是说人工智能理解图像要通过计算机视觉来实现。1966 年，人工智能先驱马文·明斯基让他的一个学生「花一个夏天把相机接在计算机上，让计算机描述它看到的东西」。但明斯基低估了这个项目的难度，他认为只要一个夏天就能让计算机理解图像，但是之后又过了几十年，计算机还是不能很好地理解图像，直到现在，计算机也没有达到能够完全理解图像的高度。要想真正进入千家万户，计算机视觉仍然需要不断改进和提升。

回到人工智能理解图像的正题。图像就是像素值在二维空间的排列，像素可以用 RGB（红绿蓝）三色值来表示，或者灰度值来表示。我们看到的图像无一不是这样类型的数据。虽然人眼能够很简单地理解图像内容，但这对人工智能模型来说却非常复杂。所以我们从以下几个方向对图像理解展开讲解：图像特征、图像分割、图像识别和图像中的目标检测等。

#### 4.2.1 图像特征

图像特征主要包括图像的颜色特征、纹理特征、形状特征和空间关系特征。对图像特征的描述分为全局描述和局部描述。全局描述可以通过原始像素值特征、直方图特征、GIST 特征这三种方法来实现，但是因为局部描述比全局描述更细致，所以下面我们主要来介绍局部描述。

局部描述是计算机视觉研究的一个基本问题，它主要用来寻找图像中的关键点，进行物体特征的数字化描述。提到局部特征，首先要想到的是 SIFT 特征。SIFT 全名是尺度不变特征转换（Scale-Invariant Feature Transform），是一种用来检测与描述图像中的局部特征的计算机视觉模型，能够在空间尺度中寻找极值点，并提取出相应的位置、尺度、旋转不变量。尺度可以理解为图像的模糊程度，类似于眼睛近视的度数，尺度越大细节越少。不变量是指计算机视觉识别的图像中物体的本质特征。比如你拍同一个杯子，在不同的角度和位置拍出来的杯子大小以及杯子上花纹的角度位置都是不一样的，但是计算机视觉识别的是杯子的本质特征，虽然位置、大小、角度都发生了变化，但是杯子本身没有发生变化。这个由加拿大教授戴维·洛（David Lowe）总结完善的特征计算方法，在计算机视觉的发展史上非常有影响，在很多涉及图像的人工智能应用里都可以找到它的身影。

局部图像特征的检测与描述可以帮助识别物体，我们在观察一个物体时，如果角度、距离甚至光线不同，我们观察的结果也会有些许误差。但 SIFT 特征是基于物体上的一些局部外观的关键点，与图像的大小和旋转无关，对于光线、噪声、轻微视角改变的容忍度也比较高。例如图 4-9 中的两张照片，它们是同一个物体在不同的视角下拍摄而成的，在这两张照片上存在 SIFT 特征的对应关系，对应的 SIFT 特征用线条连接起来。可以看出，SIFT 特征能较好地描述图像内容，较少受干扰因素的影响。

图 4-9 SIFT 特征示意图

SIFT 特征高度显著且相对容易提取，因而用它很容易辨识物体，而且少有误识。SIFT 特征的本质是在不同的尺度空间上查找关键点，并计算出关键点的方向。SIFT 所查找到的关键点是一些比较突出的点。这些点包括角点、边缘点、暗区的亮点以及亮区的暗点等，它们不易因光照、线性变换和噪声等因素的干扰而发生变化。在人工智能里，这些点对物体的识别和检测非常重要。除此之外，使用 SIFT 特征描述部分遮蔽物体的检测率也比较高。

下面我们来具体讲解一下生成 SIFT 特征的过程。这个过程可以分解为四步。

第一步，尺度空间极值检测：搜索所有尺度上的图像位置，识别潜在的对于尺度和旋转不变的兴趣点。

第二步，关键点定位：在每个候选的位置上，通过模型来确定位置和尺度，根据每个候选位置的稳定程度来选择关键点。

第三步，方向确定：根据图像局部基于像素值变化的梯度方向（像素值增加最快的方向），给每个关键点分配方向。

第四步，关键点描述：在每个关键点周围的邻域内，在选定的尺度上计算图像局部的梯度，从而产生特征表示。

图 4-10 是 SIFT 特征产生的过程。左边显示了图像上一个关键点周边各像素点上的梯度，右边是根据梯度的方向分布生成的直方图，作为该关键点的 SIFT 特征向量。

图 4-10 SIFT 特征产生示意图

人工智能里常用的图像的特征向量表示方法还有很多。但是，不论是哪一种表示方法，不论它表示的是全局特征还是局部特征，它都有自己的特点和适用范围。通过特征向量表示图像，从而捕捉全部内容相关的信息，是十分具有挑战性的。

#### 4.2.2 图像分割

人工智能感知图像的一个重要应用是图像分割。图像分割就是机器自动从图像中分割出对象区域的过程，这样可以更加容易地获取图像中的内容和特征。图像分割在很多应用场景中发挥了重要作用。例如，在医学影像领域，医院在做肿瘤和其他病理的定位、组织体积的测量、诊断和治疗方案的定制时，都会用到图像分割。图像分割也被广泛应用于卫星图像分析领域，用来定位道路、森林等。此外，我们熟知的一些图像处理软件，比如 Photoshop、美图秀秀，它们的「抠图」功能，本质上也是通过图像分割实现的。

图像分割的一种基本方法是 K - 均值聚类，这种方法在前一节介绍聚类的时候提到过。其具体的过程是这样的：

第一步，从图像上任意选择 K 个像素作为初始簇中心。

第二步，对于剩下的像素，则需要根据它们与这些簇中心的相似度，将它们分配给与其最相似的簇。

第三步，计算每个所获新簇的中心，也就是簇中所有像素特征的均值。

重复第二步和第三步，直至收敛，也就是聚类结果不再发生变化。

如图 4-11 所示，右图是左边原图执行 K - 均值聚类（K=20）后的结果。实际应用中，为了提高速度，人们通常可以先对较大图像进行下采样，也就是均匀地减少像素，然后再进行聚类计算。

在图像分析中，直方图也是十分重要的。图像的直方图能够反映图像像素点灰度值的分布情况。基于直方图的图像分割方法通常会比其他类型的图像更快，因为只需要过一遍所有的像素。在这一过程中，需要先计算所有像素值的直方图，再通过直方图的峰和谷来找图像中的簇。如有必要，可以循环使用直方图来分割图像，找到更小的簇，直到没有新的簇为止。如图 4-12 所示，右图就是左图的直方图图像分割的最终结果。

图 4-11 基于 K - 均值的图像分割结果

图 4-12 基于直方图的图像分割结果

#### 4.2.3 图像和视频理解

前面我们介绍了图像特征和图像分割，这些都是理解图像语义的基础，接下来我们就要讲解人工智能是如何识别图像的。前文我们说过人工智能要通过机器学习来产生模型，模拟人脑的深度学习就是机器学习的一种，它允许我们在给定一组训练数据的情况下来训练神经网络模型预测输出。近期的深度学习有了比较大的提升，相对较少地依赖于针对特定数据类型的特征提取，可以实现端到端的处理和学习。端到端处理就是在输入端输入数据后，输出端会直接输出结果，而不用经过复杂的特征提取过程。基于深度学习的人工智能现在已经能够较好地识别图像、视频、文本等非结构化数据，并且识别准确率很高，达到了可以在实际场景中进行应用的水平。尽管如此，理解不同数据类型对应的特征表示，仍能帮助我们更好地理解数据本身的规律和运用 AI 思维。

在人工智能对于图像的识别这一领域，近几年的趋势是通过深度学习来实现图像识别，例如可以用 VGG、Inception 和 ResNet 等深度学习模型来识别。图 4-13 所展示的就是几个通过这几种模型识别图像内容的例子。例如，最左边一列是使用 VGG 识别的图像，对于第一张图像，VGG 认为有 98.14% 概率是足球，有 1.16% 的概率是橄榄球，有 0.42% 的概率是高尔夫球。由此可见，这些模型的正确率还是很高的。

图 4-13 VGG、Inception 和 ResNet 模型图像识别结果

图像中不可能只有我们所要识别的事物。当我们要识别一张照片中某个特定的人物时，这张照片中往往还有一些其他的东西，所以我们要进行目标检测。目标检测的任务是在图像上找到、定位并识别不确定数量的一类物体 —— 需要强调的是，这里的「不确定数量」很重要，对人工智能来说，是有相当的挑战性的。跟图像识别一样，目标检测常用专门的深度学习框架，比如 SSD（Single Shot Detector）、YOLO（You Only Look Once），但检测和识别是不一样的任务。通常，识别的结果是关于一幅图像的类别，而检测要回答的问题难度相对比较高。目标检测的应用场景包括人脸检测、人体检测、车辆检测、商品检测、遥感图像中建筑物检测等。如图 4-14 所示，目标检测模型能够检测出图像中的建筑物、行人和汽车。

图 4-14 目标检测示意图

几百年前，人们热衷于读书，甚至达到了洛阳纸贵的地步；十几年前，人们进入读图时代，读图成为风尚；而现在，视频成为人们的宠儿，拿起手机看短视频成为越来越多的人的习惯。打开手机、电视看剧，去电影院看电影，我们每个人都是视频内容的消费者，而短视频的兴起又让我们都可以成为视频内容的创造者。视频在我们的生活中的地位慢慢上升，人工智能对于视频内容理解的意义和重要性也相应地不断提升。

从理论上来讲，视频能够将一系列静态影像以信号的方式加以捕捉、记录、处理、存储、传送与重现。简单来理解，视频其实就是很多幅图像在时间上的组合，构成视频的每一幅图像叫作帧。用于测量显示帧数的指标叫作帧率，其单位是每秒显示帧数（Frames Per Second），简称 FPS。由于人眼特殊的生理结构，当所看画面的帧率高于 16 的时候，人们就会认为是连贯的，所以虽然我们看到的是一张张静止的图像，但是却有了动起来的体验。常见的视频编码格式有 AVI、MPG 等，在这些格式的视频形成过程中，其实都将视频的信息做了压缩。压缩后的文件变小，就更有利于处理、存储和传送了。

视频有哪些特征呢？首先，视频是由图像组成的，图像的特征也是视频特征的一部分。除了图像特征之外，视频中物体的运动也给我们的分析带来了新的特征。视频中常用的表示运动的特征叫作光流（optical flow）。光流反映了物体的运动模式，即反映了物体在某视角下由摄像头和背景之间形成的明显移动。通过视频中的图像序列模型，我们可以估计出运动物体在观察成像平面上像素运动的瞬时速度。

具体来说，光流模型可以计算两幅图像之间的变形，它假设一个物体的颜色在前后两帧没有巨大而明显的变化。基于这个思路，我们可以得到相应的图像约束方程。假定不同附加条件的光流问题对应不同的光流模型，常见的模型是 Lucas-Kanade 模型，如图 4-15 所示，我们在图片上显示了计算中的光流向量，向量的箭头表示的是像素运动的方向。

当我们检测出光流向量之后，就可以进一步生成 HOF 特征。HOF 的全称是 Histograms of Oriented Optical Flow，它可以对光流方向进行统计，得到关于光流方向分布的直方图。这个直方图表示了在不同的二维方向上，各分布了多少光流向量。由于 HOF 能够表示出视频中物体运动的本质特点，所以经常用于视频中的动作和事件识别。

图 4-15 光流示意图

通过光流向量和 HOF 特征对视频处理后，我们就可以通过人工智能识别出视频中的物体的运动了。但是在视频中，我们要识别的目标一般是运动的，如果要对这个目标进行识别，就需要对其进行跟踪。目标跟踪是指给出目标在跟踪视频第一帧中的初始状态，比如目标的位置和尺寸，自动估计目标物体在后续帧中的状态。如图 4-16 所示，视频中跟踪的目标通常会用一个矩形的框来标注。

如果视频中有我们感兴趣的人或物体，可以用目标检测的方法先检测到它，然后再跟踪。这样，我们就不止在一帧中获得了这个人或物体，还能一直跟踪他在哪儿。这在商业场景上大有用处，比如超市想知道消费者的行为轨迹，就可以采用检测和跟踪相结合的方法。商场公共区域都设有摄像头，通过摄像头记录下来的视频，人工智能可以识别出消费者在某家店停留的时间长短和相应的行为，然后估算消费者的消费概率。商家通过人工智能知道了消费者在商场里的完整行为轨迹后，可以做出许多帮助提升体验、促进消费的优化，比如在合理的位置设置休息区，以供消费者休息，这样既可以消除消费者的疲劳，又可以延长其在商场里停留的时间，购买商品的概率也会有所上升。

图 4-16 目标跟踪示意图

很多人会通过看电视剧或者电影来打发时间。在看电视剧或电影的过程中，我们能够轻而易举地分辨出电影电视剧中的主角和反派。近几年来，随着人工智能的视频识别不断发展，人工智能也变得和人类一样，在看电影和电视剧的时候，能够分辨出哪些是「好人」，哪些是「坏人」。并且通过这个功能，人工智能还可以对视频中的人物进行社交分析。

那么，人工智能是怎样做到从视频中分析人物之间的社交关系的呢？首先，通过观察可以发现：在与不同的人的交往过程中，人们选择的交互方式也是不同的。比如说，在日常生活中，一个人与比较亲密的同事、恋人之间的交互方式，和与疏远、有敌意的人之间的交互方式，往往是不同的。所以我们就定义了一个指标，叫作结合信号（grouping cue），它能够决定不同人物之间更可能属于何种关系（如亲密或疏远，友善或敌意等）。

然后，在视频中的每一个场景中提取相应的特征向量，通过有标记的训练数据和机器学习模型，学出人物间的结合信号，然后通过模型把这些结合信号整合起来，形成一个人物间的社交网络。有了社交网络，就可以分析人物中有几个社区和每个社区的领袖是谁。人工智能对电影和网络视频中的人物进行社交分析后，能够分辨出视频中有几组人，每组人的领袖是谁。如图 4-17，在电影《加勒比海盗：黑珍珠号的诅咒》（左上、右上）中，左边是善良的一派，领袖是杰克·斯伯洛；右边是阴险的一派，领袖是巴伯萨。在电影《哈利·波特》（左下、右下）中，左边的是正义的一派，领袖是哈利·波特；右边是邪恶的一派，领袖是伏地魔。

图 4-17 视频中人物的社交分析示意图

#### 4.2.4 语音理解

每天我们都听到的声音，在计算机中是以时间序列存储的。也就是说，在计算机中，声音是一串随着时间变化的数字，这串数字代表了在一个时刻声音信号的强弱。就像理解图像和视频一样，人工智能理解音频首先要提取特征。音频的特征主要包括：时域特征和频域特征。时域特征比较好理解，就是在信号时间序列上加工得到的对其的直接描述，包括信号幅度和信号能量等。其中，信号幅度是指信号上下波动的范围，信号能量一般与信号的振幅和持续时间有关。

要想理解频域特征，就不得不提傅立叶变换。傅立叶变换是一种分析信号的方法，它可以分析信号的成分，也可用这些成分合成信号。傅立叶变换，能将满足一定条件的函数表示成不同频率正弦或余弦函数的线性组合。在不同的领域，傅立叶变换具有多种不同的变体，例如连续傅立叶变换和离散傅立叶变换。音频信号的傅立叶变换为我们提供了一种极佳的表示形式。

计算频域特征需要先把信号进行傅立叶变换，然后才能生成频域特征。频域特征主要包括基础频率、频率组成、频谱中心、频谱流量、频谱密度等。因为不同的正弦或余弦函数对应了不同的频率，而频域特征是对音频的傅立叶变换的描述，所以被称作「频域」。频谱是频率的分布曲线，可以表示一个信号是由哪些频率的弦波组成。

跟图像一样，音频数据也可以切分，上述音频特征就可以用在音频分割中。与直观的图像分割不同，音频分割有两种类型：第一种是在时间上将音频切分成几段，每段包括了特定的声音信息，例如对话、音乐、动作声响、环境声响等，这是时间上的分割；第二种是把音频中同一时间叠加的声音分割开，产生若干个并行的音频流，分别对应各自的声源。这种分割对应的是音频信号在各个时间和频率上的分割。

音频理解的一个广泛应用是语音识别：亚马逊的 Alexa、小米的人工智能音箱「小爱同学」、机器人玩具、自动客服等场景中都在使用。那么，语音是如何转成文字的呢？图 4-18 显示了语音识别的全过程，这一过程通常有以下几个部分。

图 4-18 语音识别过程示意图

首先，是声学模型，声学模型是语音识别系统的重要组成部分。声学模型需要从语音数据中提取 MFCC（梅尔倒谱系数）特征。对人耳听觉机理的研究发现，人耳对不同频率的信号有着不同的听觉敏感度，其中，从 200Hz 到 5 000Hz 的信号对语音的清晰度影响最大。并且，对于人耳而言，低频音容易掩蔽高频音，高频音不易掩蔽低频音。基于这样观察得到的特征，MFCC 特征表示，在噪音较高的环境里，MFCC 特征仍然具有较好的语音识别性能。根据 MFCC 特征建立的 GMM-HMM 模型（高斯混合 — 隐马尔可夫模型），可以作为语音识别所需的声学模型。其中 GMM 用于对语音声学特征的分布进行建模，HMM 则用于对语音信号的时序性进行建模。这组 GMM 和 HMM 结合的模型，不但能捕捉单个 MFCC 特征，并且能够体现 MFCC 在时间上的变化，从而模拟出人耳对声音的感知。

其次，是语言模型，可以通过训练文本数据学习 N-gram 模型（N 元模型）。不论在哪一种语言中，词的出现都是有统计规律的，比如当我说「柳暗花明」这个词时，你想到的下一个词是什么呢？我想大家很有可能会想到「又一村」，基本上没有人会想到「又一花」或者「又一树」吧。N-gram 模型的主要思想就是这样的，它能够根据统计规律预测出一个词的前后内容。常用的是建模两个连续词的 Bi-gram 模型和三个连续词的 Tri-gram 模型，分别对应二元和三元的 N-gram 模型。有了这些模型之后，给定一个语音的输入，在生成特征后，通过声学模型、语言模型和已知的字典（也就是我们有哪些候选词作为输出），进行计算和搜索，语音识别系统就可以产生相应的文本输出。

目前，在环境安静、发音标准的情况下，语音识别模型已经完全可以对语音进行识别了，而且识别的准确率很高。但是这种准确率是有条件的。不论哪一款语音识别软件，如果将其放置在一个嘈杂环境下，或者当使用者的普通话不标准、带有口音时，语音识别的准确率都会大打折扣。提高在不同环境下通用语音识别的准确性，是语音识别一直以来的发展方向。

我们赞美天的湛蓝，海的宽广；我们歌颂诗的美妙，曲的动听。何其有幸，我们一出生就拥有看见、听见这个世界的机会。我们拥有感知这个世界的能力，人工智能因我们而生，我们帮助它更好地去体会这个世界。现在，海量数据驱动下的图像识别、视频识别、音频识别给了人工智能这个机会。也许人工智能并不理解在一层层模型下产生的技能到底代表了什么，但却能够帮助我们真正地实现「眼观六路，耳听八方」。

### 4.3 AI 理解自然语言

自然语言通常是指一种自然地随文化演化的语言，例如汉语、英语及其文字。自然语言可以表情达意，是人类沟通交流的工具，同时也能促进人类思维的发展。对自然语言的理解是人工智能的一门重要功课。同时由于自然语言的复杂性和多样性，对于自然语言的理解和处理也充满了挑战。但是我们在日常生活中并不常用「自然语言」这个词，自然语言是一个抽象的概念，我们一般从它的具体形式 —— 文本，展开研究。

要想理解文本，首先就要理解文本的特征。BOW 特征可以帮助我们更好地理解文本特征。BOW 全称是 Bag of Words，也就是词袋模型，它是自然语言处理领域常用的一种文本表示方法。在使用 BOW 特征的过程中，需要将每篇文章都看成一个「袋子」，这个「袋子」里面装的都是词汇，所以称之为词袋。BOW 最初是用于文本分类的，它通过将文本表示成特征向量来实现文本的分类。它的基本思想是忽略文本中的词序、语法和句法，只将文本看成是词汇的集合，而且文本中的每个词都是独立的。举个例子，有如下两个文本：

文本一：

Alice likes to play basketball, Bob likes too.

（艾丽斯喜欢打篮球，鲍勃也喜欢。）

文本二：

Charlie likes to play tennis.

（查利喜欢打网球。）

基于这两个文本，构造一个词典：

```
词典 = {1."Alice", 2."like", 3."to", 4."play", 5."basketball", 6."Bob", 7."too", 8."Charlie", 9."tennis"}
```

这个词典由 9 个不同的单词组成，利用词典的索引号，上面两个文本分别可以用一个 9 维向量表示，下列整数表示的是某个单词在文本中出现的次数，见表 4-2：

表 4-2 单词在文本中出现的次数

| 词典 | Alice | like | to | paly | basketball | Bob | too | Charlie | tennis |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 文本一 | 1 | 2 | 1 | 1 | 1 | 1 | 1 | 0 | 0 |
| 文本二 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 1 | 1 |

在上述例子中，向量中每个元素表示词典中相关元素在文本中出现的次数。但是通过上表我们可以发现，在构造文本向量时，并没有体现单词在原来句子中出现的次序。这是 BOW 表示的缺点之一，不过这并不影响人工智能对于这一文本的理解。试想，如果一篇文本中人群、大厦、银行、商场这样的词汇多些，而牛、羊、山谷、稻田这样的词汇很少，即使没有按照顺序排列，我们也能做出一个大致判断，得出它是一篇描绘城市的，而不是描述乡村的文本。

假定现在有一个很大的文本集合，里面有许多的文本，我们将文本里面的所有单词提取出来，构成一个包含 N 个单词的词典。通过 BOW 特征，每个文本都可以被表示成为一个 N 维向量，而人工智能非常擅长于处理数值向量。这样，我们就可以通过 BOW 特征来完成海量文本的分类以及其他任务了。

但是，BOW 特征有一个缺点，就是它无法反映出一个句子的关键词。比如上面的文本一中，「like」出现次数最多，若根据 BOW 特征提取这个句子的关键词，则为「like」，但是这个句子中的「basketball」也很重要，这是 BOW 无法准确提取出来的。我们用 TF-IDF 可以很好地解决这个问题。

TF-IDF 也是一种常用的文本特征的表示方法，它的主要思想是：如果某个词在一篇文章中出现的次数高，并且在其他文章中很少出现，则认为这个词是该文章的关键词，通过关键词就可以很好地对现有文本和其他文本进行区分。在 TF-IDF 中，TF 是词频，term frequency；IDF 是逆文档频率，inverse document frequency。在这个特征中，需要将词频和逆文档频率相乘得到对应词的特征值。

TF 表示词条在一个文本中出现的频率，在一个文本中出现次数越多，这个词的 TF 值就越高，就说明它对该文本越重要。IDF 反映了词条在所有文本中出现频率的逆向值，如果一个词在很多的文本中都出现了，那么它的 IDF 值就会低。相反，如果一个词很少在文本中出现，那它的 IDF 值就较高，说明这个词具有较强的代表性。两者的乘积可以用在特征向量中作为数值来表示一个词，从而改进 BOW 形式的表示。由于 TF-IDF 能够准确找出文本中的关键词，所以特别适合用来进行文本相似度对比、文本分类、信息检索、关键词提取以及文本摘要。现在很多搜索引擎、论文网站、新闻网站都应用了 TF-IDF 特征。比如，当你想要了解某些信息时，你会在搜索引擎上输入关键词，而搜索引擎之所以可以帮你找到答案，是因为它应用了 TF-IDF 特征来计算它检索出来的文本和关键词之间的匹配度。如果它发现这个关键词在某个文本中出现的次数最多，而在其他文本中很少出现，那这个文本就是你需要的答案。

#### 4.3.1 词嵌入

词嵌入，也就是 word embedding，它也是表示文本特征的一种方式，可以将自然语言表示的单词转换为计算机能够理解的向量形式。简单点来理解，我们都知道文本有千千万万，文本构成的维度也不计其数，所以 BOW 会产生很高维度的特征向量，而词嵌入将词映射到一定维度的连续向量，就能使得这个连续向量的维度比较低，例如几十到几百。产生词的向量之后，我们就可以进行这些向量间的运算，比如通过向量之间的相似度来度量不同词语之间的语义相关性。

词嵌入基于的假设是：如果两个词经常在相同的上下文中出现，那么这两个词的意思应该相近。所有学习词嵌入的方法都是在建模词和上下文之间的关系。如图 4-19，我们举了些词嵌入的例子。相似的词被映射成相近的向量，差别大的词映射以后对应的向量差异也比较大。所以「足球」与同样是球类的「篮球」距离很近，而与形容味道的「好吃的」距离很远。由此可见，通过词嵌入，词之间语义相似性可以被比较恰当地编码成向量了。

图 4-19 词嵌入示意图

近年来，智能手机的普及，使得网络信息迅速增长而且更加碎片化，微信消息、微博、商品评论等短文本大量出现，并逐渐成为网络文本信息的主流形式。短文本的稀疏性、不标准化、数据规模大且标注少等特征，给传统的基于长文本的数据处理带来了新的挑战。而词嵌入具有两个作用：维度缩减和语义扩充。词嵌入的维度缩减消解了短文本不标准、规模大的难题。词嵌入将语义相似的短文本投影到向量空间中的同一区域，这样产生的语义特征又可以作为补充信息，解决短文本语义稀疏、难以理解的局限性。词嵌入的应用提高了短文本分类的精度，为人工智能理解现在大量泛滥的短文本信息提供了帮助。

#### 4.3.2 文本分类

文本分类是对文本内容的识别。通过我们上文提到的文本特征化方法，再结合机器学习方法，就可以实现对文本数据的分门别类，例如每天发生的新闻铺天盖地，但是利用文本分类我们就能迅速地将其归为娱乐类、体育类、经济类新闻等。我们每天所接收到的信息也十分泛滥，比如每天都会收到来自四面八方的邮件，里面不乏垃圾邮件。这时，文本分类就派上了用场，一般的电子邮箱都会有文本分类的功能，将收到的邮件分为正常邮件和疑似垃圾邮件两类。

作为一种特殊的文本分类，情感分析是对带有感情色彩的主观性文本进行处理和分析的过程。情感分析可对不同文本进行处理，其中包括基于新闻评论的情感分析和基于产品评论的情感分析。新闻评论是指在关于新闻热点事件的评论中，发表观点的人包括社会中各个层次的人，不同层次的人表达的情感态度或观点不同，这能够反映出很多信息。对于新闻评论的情感分析，可以掌握大众对于新闻事件的思想动向，有利于政府和新闻媒体机构舆情监控工作的展开。

产品评论表达了消费者对一件商品各种功能的满意程度，是一种重要的市场反馈信息。通过基于产品评论的情感分析可以了解某一产品在用户心目中的口碑和用户的关注点等，为产品销售提供科学、可供决策的信息，而且有利于指导商家改善商品，提高用户体验。

例如现在要对某手机的产品评论做出情感分析。这个分析是从用户评论数据出发的，但是这些产品评论都是用户自由发表的，不能保证每一条都有意义，所以我们要对数据进行清洗，以此来保证获取的商品信息的质量。数据清洗后我们就要对评价对象和评价观点进行分析。对于手机来说，评价对象就是价格、系统、处理器、内存这些内容，当然也包括快递、售后等其他服务。评价观点一般分为积极、消极、中性这三类。例如「人脸识别和指纹识别速度非常快」可以标记为积极评论，「不好用，手机烫得要命」可以标记为消极评论，「也没有说的那么好，没啥特别的，也就还行吧」就可以归入中性评论。然后统计每一个评价对象所对应的三类评价观点各有多少，然后我们就可以对这些评论进行情感分析。比如该手机总评论数最多的对象是「外观」，这说明用户在购买手机时十分注意手机的外观，商家在开发新的手机产品时可以重点提高手机的美观度。假如用户对该手机外观的评论有 80% 是积极的，那说明用户对于该手机的外观比较满意，这是该手机的一个优势，商家在开展营销时可以以此为重点。

开发新产品时，也可以借鉴该手机外观上的一些特质，保持用户对手机品牌的满意度，相反，如果对该手机外观的评论有 80% 是消极的，就说明用户对该手机的外观不满意，这是该手机的劣势，那商家在开展营销时就要学会避重就轻，重点突出该手机的其他优势，开发新产品时还要及时改进。同理，对于其他评价对象的情感分析也是这样进行的，这样就可以全面地掌握这款手机的市场反馈信息，为商家不论是开展营销活动，提升用户体验，还是后续研发新产品提供参考。

文本情感分析还可以用在哪里呢？文本情感分析还可以预测股票价格的变动。这其中的奥秘是，人工智能可以从社交媒体中获取大众情绪变化的特征，利用这些特征，可以通过情感分析大致预测出大众情绪的变化。而事实上，公众情绪的变化又能在一定程度上影响股票价格的变动。就是说，文本情感分析可以根据大众情绪的波动预测股票价格的变动。

论文《推特情绪预测股票市场》（Twitter Mood Predicts the Stock Market），讲的就是公众情绪对股票价格变动的影响，由于其思维的先进性和研究的科学性，被大量引用。推特在美国是类似于微博这样的平台，用户可以在这个平台上记录分享自己每天的故事和情绪。这篇论文的主要研究思路是利用推特用户发表的内容数据，通过两种情绪分析方法 OpinionFinder 和 GPOMS（谷歌情绪状态简况），来抓取和分析公众的情绪变化。其中 OpinionFinder 是将人的情绪区分为正面和负面两类，而 GPOMS 将情绪分成六类，分别是平静（calm），警觉（alert），有信心（sure），活力十足（vital），友好（kind）和高兴（happy）。通过分析，文章发现公众情绪与股票价格指数有一定相关性。因此，该文作者大胆假设：公众情绪的某些指标可以有效地预测股价的变动。他们对此展开了一系列的研究，在一个复杂神经网络模型的基础上，将公众情绪的时间序列输入到该模型中，得到的预测效果比较理想，其预测股票价格指数收盘价涨跌方向的准确率高达 86.7%。

随着人工智能的普及，基于文本分析的应用涉及各个领域，而文本数据在日常生活中又随处可见，比如你随手拿起的一本书，你无意中瞥到的广告词，你无聊时刷到的朋友圈，都可以是人工智能理解的文本数据。机会都是留给会思考有准备的有心人的，所以你在平时也可以想一想，你所了解和接触到的这些文本数据，能不能用起来，可以产生怎样的实际价值，说不定就可以成为人工智能的下一个风口。

#### 4.3.3 实体识别

随着互联网的高速发展，人们获得信息的渠道越来越多，获取信息的方式日益简单。但是，随之而来的是信息的数量呈指数级增加，而且混乱无序，真假难辨。要想仅仅使用人力从海量的文本数据中获取所需要的信息，也变得难如登天。人工智能的出现解决了这一难题。人工智能通过一系列自然语言处理模型能够提取出复杂文本数据中的关键信息，而自然语言理解的关键步骤就是实体识别。

实体识别又叫命名实体识别（Named Entity Recognition，NER），是指识别文本中具有特定意义的实体，比如说人名、地名、产品名、属性名、日期、数字等。实体识别要做的就是从非结构化的输入文本中抽取出上述实体，并且可以按照业务需求识别出更多类别的实体。在应用过程中，实体识别的确切含义需要根据实际情况来确定，比如，在面向生物的实体信息抽取时，还包括细胞、蛋白质、基因、核糖核酸、脱氧核糖核酸等特殊生物实体。

实体识别在信息提取、问答系统、句法分析、机器翻译等领域应用广泛，在人工智能自然语言处理模型的应用过程中具有重要意义。例如在机器翻译中，有些人的名字在翻译成英文时把名放在前、姓放在后，而有些人的名字就直接用中文的汉语拼音来表示。实体识别可以准确识别出文本中是哪一种形式，大大提高了机器翻译的准确性。作为信息检索系统的一种高级形式，问答系统能用恰当的自然语言回答用户用自然语言提出的问题。当用户提出问题时，问答系统能够对可能的答案进行实体识别和深度分析，然后将最适合的答案呈现给用户，为用户营造一个信息丰富、答案准确、反应迅速的信息获取环境。

由于数量、时间、日期、货币等实体具有统一形式，我们通常可以对它们用模式匹配的方式取得较好的识别效果。但相比之下，人名、地名、机构名较复杂，识别难度很大，普遍存在嵌套和缩写的识别问题。例如「清华大学第一附属医院」这个组织机构名称里面还嵌套着同样可以作为组织机构名称的「清华大学」，再比如，有时人们会把「河北大学」缩写为「河大」，但是「河南大学」的缩写也是「河大」，这就对进行实体识别造成了很大的困难。而且新词汇也在不断涌现。例如随着互联网的发展，一些没有任何规则的网名昵称也为实体识别的进行带来了新的挑战。因此，近年来的实体识别主要以人名、地名、机构名这几种比较复杂的实体识别为主要的突破方向，由原来的单独针对人名、地名等进行识别的方式，发展到用更优化的方法同时进行各类实体的识别。近几年来，实体识别的效果有了显著提高，在各个领域得到普遍应用。

比如说，实体识别在旅游行业有着很大的发挥空间。实体识别在旅游行业的主要应用是对旅游景点的识别。例如，游客可以通过在旅游网站上检索来了解景点的信息，这个过程就涉及了对旅游景点的实体识别。目前对于旅游景点识别的困难主要有：旅游景点数量众多，建立旅游词库工作量大。据不完全统计，仅北京这一个城市的旅游景点就有 6 000 多个，幅员辽阔、地大物博的整个中国大地，旅游景点更是不计其数。旅游景点名称没有固定的格式和规律可循，给实体识别造成了困难，比如北京地安门大街附近的一个湖叫作「后海」，天坛公园里有一个小的景点叫作「皇穹宇」；景点名称经常采用缩写形式，也提高了实体识别的难度，比如「八达岭长城」「居庸关长城」都简称为「长城」。但现在的实体识别模型已经可以很好地解决这些难题了。还是以北京为例，我们首先需要构建一个北京旅游景点词库，词库建立之后经过一系列去重操作和人工审核，得到有效的景点词表。在此之后，通过该词表对旅游文本进行标注，并构建模型训练集和预测集。通过对训练集的反复学习训练，最终，对旅游景点实体识别的预测准确率可以做到很高，有人通过上述方法得出了 95% 的准确率。这样，游客想要了解各种旅游景点时就不会出现检索不到的情况了。

除此之外，实体识别在新闻传播领域的应用也很有前景。由于新闻报道会涉及体育、财经、科技、时尚、教育等社会生活的各个领域，所以对涉及领域的常见专有名词都需要有一定的识别精度。此外，新闻报道都具有时效性，所以也需要关注与热门资讯和突发事件相关联的实体，例如 2019 年 5 月，华为注册了「华为鸿蒙」商标，以及之前大火的电视剧《陈情令》等，这些新产生的、具有很大热度的实体都是需要识别的。现在的实体识别能够自动识别那些未被收录到词表中的词，并且从中发现新词，不仅满足了新闻传播领域精准识别实体的需要，还能帮助媒体行业进行更精准的内容挖掘、对内容进行更有效的监督管理，推动了新闻朝智能化方向发展。

我们已经讲过了人工智能对于图片、视频和文本这些非结构化数据的识别。在人工智能出现之前，这些数据就似汪洋大海一般一直存在，但人工智能的出现，才真正赋予了它们价值。AI 思维就是这样，在已有数据基础上挖掘其未竟的巨大的隐含价值，给予我们做出更好判断和决策的能力。人工智能对于非结构化数据的理解是 AI 思维的一个重要组成部分，我们只有充分理解了这些数据处理的方法论，才能更好地利用人工智能支持决策。

### 4.4 AI 理解商业数据

前面我们讲到了人工智能可以理解图像，具有人脸检测、商品检测等功能；可以理解视频，具有动作识别、社交分析等功能；可以理解语音和自然语言，具有语音识别、文本分类、实体识别等功能。这些都是从具体的某一类型的数据出发的，人工智能对这些非结构化数据的理解是为了更好地为我们的实践提供帮助。下面我们要具体讲解人工智能是如何通过其理解和学习能力直接为商业实践赋能的。

商业实践的基础是商业数据。所谓商业数据，是指在一个行业内各个环节中涉及数据的集合，包括企业内部数据、分销渠道数据、消费市场数据等。通过人工智能对商业数据的理解和分析，我们就可以知道在这个行业中，用户的需求是什么，哪些需求已经被满足了，哪些需求还没有被满足，整个行业还有什么「处女地」没有被挖掘，行业的创新点和决胜点在哪里，以及自己公司的优势、劣势和独特点在哪里。只有这样，我们才能准确地进行市场细分，确定目标用户，制定更加个性化、精准化的营销方案；也只有这样，我们才能准确地进行产品定位，提供让客户更加满意的服务。

人工智能可以通过 SIFT 特征等对图像进行理解，可以通过光流模型等对视频进行理解，可以通过 N-gram 模型等对语言进行理解，那么人工智能是如何理解复杂的商业数据的呢？

#### 4.4.1 你被标签化了吗？

我们在注册一个新软件时，软件会让我们勾选一些标签来更好地了解我们的喜好。在许多平台发文章前，平台也会让我们为文章勾选标签，因为这样可以帮助我们更好地定位内容与读者。我们每天都在被贴标签，我们也在给别人贴标签。当我们谈论起 80 后时，我们总认为他们是「叛逆」的一代；当我们谈论起 90 后时，我们觉得他们是「问题青年」；当我们谈论起 00 后时，我们说他们是「新新人类」。不管是哪个年代的人，我们都对他们有相对应的标签，甚至连「80 后」「90 后」都成了一种标签。这些标签都是人们根据他们的性格特点、人生经历做出的总结概括，在一定程度上反映了真实现象。有了这些标签，我们就可以更好地理解对方，甚至被贴上相同标签的人还能迅速拉近距离。现在很多人也通过贴标签的方式被大众熟知，比如网络上有的人因为和一些知名演员长得相像，就给自己贴上诸如「小林志玲」「小吴彦祖」等标签，迅速提升了知名度；还曾有某歌手凭借「四千年一遇的美女」的标签登上热搜成为「国民女神」。人工智能也是通过为数据打标签的方式来理解商业数据的。

标签是一种用来描述业务实体特征的数据形式。比如在描述一个用户时，我们会使用性别、年龄、地区、兴趣爱好、产品偏好等标签来反映该用户的特征。商业数据标签的生成过程以发生在时间和空间中的行为为媒介，将物的特征传递到人，人的特征传递到物。在这个过程中，物包括实物、商品、文本、图像等。标签传递过程形成了一个由人和物为节点，以行为为边的关系网。如图 4-20 所示，在很多实际商业场景下，需要把人打上标签，比如他经常和一些讲诚信的人合作，这样的社交行为多了，他也会被打上「诚信」标签；若他和一些经常毁约的人交往比较多，那和他合作就有一定的风险；他平时购买的运动用品比较多，他就会被贴上「运动」的标签；又或者是他平常喜欢收听科技类音频课的话，他就会被打上「科技」的标签。除了对人打标签外，对物也需要打标签。物包括自然语言的语料，比如说一部英文原声大片，其台词转录成文本以后，也可以打上标签。

图 4-20 商业数据标签生成过程示意图

#### 4.4.2 数据标签化

为数量巨大的商业数据打标签是一个庞大的工程。它有什么必要性呢？首先，根据上述分析我们知道，数据标签化，无论在理解人的属性行为还是理解对应内容的属性上，都是一套非常重要的方法论。其次，在信息爆炸的现在，商业数据的体量也是巨大的，而且它们结构十分复杂，分布在许多不同的数据库中。在实际场景中要应用这些数据时，我们发现很难把这些规模巨大结构复杂的数据直接分析或者用于模型训练。但是在实践中发现，通过分类，可以将杂乱无章的数据条理化，将数据标签化其实就是一种分类。标签化的数据结构更加简单，而且非常容易管理。标签本身就是在分析系统中产生的，所以具有较好的兼容性，可以直接作为训练数据导入训练模型，减少建模的数据准备时间，也极大地提高了数据使用效率。所以，不管是从人工智能理解数据的难易程度还是数据使用效率角度，数据标签化都有优越性。

标签通常是人为规定的高度精练的特征标识，如年龄、性别、兴趣爱好、产品偏好等，将这些标签集合在一起就能抽象出一个用户的信息全貌。首先我们要知道，标签体系基本上都是层级化的。每个标签体系都分为几大类，每个大类底下分别有几个小类，逐层分布。在标签体系中，最高层级的那一级标签称为「一级标签」，以此类推，底下是「二级标签」「三级标签」等。底层标签一般能捕捉到更细的特点信息，所以经常用于精准的广告投放和营销活动，比如说某个用户被贴了一个三级标签「购物方式 — 支付方式 — 信用卡」，在这个标签中，信用卡是底层标签，说明了该用户在购物时经常使用信用卡支付，信用卡对他的生活很重要，所以此时信用卡相关的广告投放对他就很有吸引力，又或者我们可以对应他信用卡支付的习惯向他推荐各种分期付款或者刷信用卡支付的商品。这样就满足了该用户的需要，还迎合了他的习惯，对提高广告投放效果和营销的精准度有很大作用。一般而言，每一个标签都只有一个含义，这样就不会发生同一层级的标签重复或者冲突的现象了。另外，信息粒度是指信息的相对大小和粗糙程度，不同层级的标签对应的信息粒度不同。一般来说，底层标签的信息粒度会相对更细。下面是一些常见类别下的细粒度标签。

表 4-3 常见类别下的细粒度标签

| 类别 | 标签 |
| --- | --- |
| 人口特征 | 年龄、出生日期、性别、地域、教育程度、职业、星座等 |
| 兴趣特征 | 兴趣爱好、浏览内容、收藏内容、使用App类型、使用网站类型、品牌偏好、产品偏好、阅读偏好、互动内容等 |
| 社会特征 | 婚姻状况、家庭情况、子女状况、社交渠道偏好等 |
| 消费特征 | 商品品类、价格区间、收入状况、已购商品、购买渠道偏好、最后购买时间、购买频次等 |

下面我们再来介绍一下各类标签构建的先后顺序。这个先后顺序需要综合考虑实际的业务需求以及构建的难易程度等。因为业务需求各有不同，所以我们这里就只介绍按照构建难易程度来排列的先后顺序，如图 4-21 所示，这是一个化妆品商家的用户标签体系。

图 4-21 化妆品商家用户标签体系示意图

原始数据就是我们数据库中这些数量庞大、结构混乱的数据。在原始数据基础上构建的是事实标签。事实标签的构建可以是对原始数据的直接提取，也可以是对原始数据的简单统计。事实标签的构建过程并不复杂，但是在这个过程中，我们加深了对数据的理解，也清楚了数据的分布情况，为后续标签打下了基础。在事实标签基础上分析加工形成的是分析标签，分析标签的建立是整个数据标签化过程中的重要步骤。最后构建的是预测标签。预测标签需要通过机器学习预测模型来建立，例如我们可以根据某用户对产品试用的评价来预测他是否购买该产品。需要注意的是，原始数据可能并不完善，比如数据中并没有该用户的年龄信息，这时就需要建立一个模型来预测。模型的预测也是建立在这个用户的其他行为特征上的，例如我们可以根据该用户是否上学、工作岗位等信息来预测他所在的年龄段。在商业场景中，为用户打标签的过程又称为「用户画像」。

#### 4.4.3 数据标签化助力企业营销

下面我们来看一下数据标签化是怎样在营销领域应用的。图 4-22 是一个银行的营销类标签体系。对于银行来说，数据标签化的第一步是统一用户标识，统一用户标识之后，用户的信息都汇聚在了一起，这样就可以对用户一贯的行为轨迹形成认知。这时再看银行数据，就能够有效地利用用户的属性数据、行为数据以及 CRM 数据等。属性数据包括年龄、性别、住址、习惯等；行为数据包括用户在银行存钱或者借款等一系列行为形成的数据；CRM 数据就是银行与用户进行过一些营销层面的沟通而形成的数据，比如银行向该用户推送过什么类型的广告和权益等都会有相关记录。对这些数据进行分析总结就可以形成基础行为标签和基础用户画像。当银行知道了用户的基础行为都对应了哪些标签，也就对用户画像有了一定的了解。通过基础行为标签和基础用户画像就可以形成场景化标签。比如要做营销，会有一套标签；要控制风险，会有一套标签；要推广一套产品，会有类似的其他标签；等等。场景化画像形成之后就可以对用户进行全方位的画像，比如用户是谁，哪些用户是有价值的，用户需要什么，对哪些商品感兴趣，如何选择沟通的媒介等。总体来看，数据标签化的应用过程就是用户数据在统一用户标识的基础上逐渐往上发展，最终形成对用户场景化认知的过程。

图 4-22 营销类标签体系示意图

图 4-23 是一套具体画像的例子。比如这套画像是针对金融机构用户的。例如，用户对旅游、电商以及交通出行等感兴趣，这些是一级行为标签；再往下细分，金融里面有理财、保险、贷款等二级标签；金融保险里又可以产生车险、少儿险这样的三级标签。这些标签都是根据用户的行为产生的，比如某个用户经常浏览金融理财页面，就会有金融理财标签。根据这些用户行为标签，还可以推出用户人口属性标签。但人口属性标签不全是事实性数据，比如我们不知道一个用户是否有车，但我们可以通过机器学习模型预测出来。假如这个用户平时上下班和周末都是乘坐地铁公交，或者是打车，那他有车的可能性就很低。通过行为数据还可以大致预测出用户所处人生阶段，是已婚还是未婚、中年还是老年等。

图 4-23 场景化用户画像示意图

通过上述数据能产生多个级别的标签维度，这些标签维度是对用户行为的刻画，通过机器学习方法从用户的行为又可以推知用户的其他属性。这些行为和属性维度可以用于场景化画像，预测客户在具体场景下会有什么需求。根据图 4-23 中金融机构的标签体系，可以预测出跟信用卡、保险、消费金融相关的用户画像，从而为实际的营销和运营活动服务和赋能。比如要做信用卡项目，可以将信用卡的广告信息推送给有境外游购物需求的客户；要做保险项目，可以将保险项目信息推送给有车险需求的客户；要做消费金融项目，可以将相关项目信息推送给有消费分期需求的客户；等等。因为客户的需求和推广的项目都是相对应的，广告营销的针对性和准确性就相对较高，客户购买产品和服务的可能性也就相对会有所提升。

#### 4.4.4 深度学习数据标签化

前面具体讲了数据标签化的过程，接下来我们看一下深度学习是怎样进行数据标签化的。深度学习是对人类神经网络的模拟，它对原始数据一层一层地进行分析和学习，最终会给一个句子、文本或者用户打上相应的标签。当我们拥有一套完整的标签体系之后，就可以将这套标签体系下的训练数据用来训练深度学习模型，深度学习模型就会知道哪些数据应该打上哪一类标签。经过深度学习后，人工智能就可以自动打标签了。如图 4-24，这是一个深度学习标签化的一个具体示例。

我们现在有一个句子「曹雪芹是中国古代四大名著之一《红楼梦》的作者」。如果要把这个句子打上标签，可能会有「名人」「图书」这样的相关标签。那么，这套标签是如何通过深度学习加工出来的呢？

图 4-24 深度学习标签化示意图

首先，深度学习并不知道「曹雪芹」是一个词，它也有可能将「芹」和「是」放在一起看作一个词。但通过大量语料训练以后，深度学习会发现「曹雪芹」更可能是一个词，因为很多语料中「曹」「雪」「芹」这三个字都是组合在一起的，所以「曹雪芹」三个字会聚集成关键词。通过大量文本训练后，深度学习能学出「名人」标签对应的关键词。因为名人经常与「曹雪芹」有关，所以「曹雪芹」就关联上了「名人」标签。同样，深度学习可以自动将「曹雪芹是中国古代四大名著之一《红楼梦》的作者」这句话中的「名著」「红楼梦」「作者」这些关键词贴上图书标签。深度学习通过大量的语料学习，知道哪几个字该放在一起形成词，哪几个字不是放在一起的，并且把关键词和相应的标签关联起来，这就是深度学习理解句子的方法论。

数据是 AI 思维的基础。大数据时代，只有理解了数据，人工智能才能产生决策、创造价值。数据标签化是人工智能处理数据和理解数据的重要工具，更是人工智能理解商业数据的关键环节。数据标签化使我们知道了用户最想要的是什么，在营销过程中，我们就可以精准地投放用户最想要看到的广告；在研发过程中，我们也可以设计出真正符合用户需求的产品。基于数据标签化的商业实践真正做到了有的放矢，它也为未来商业开辟了新的发展空间。

### 4.5 AI 理解消费数据

数据按结构类型分为结构化数据和非结构化数据，我们在前面几节讲过了图像、视频、音频、自然语言这些具有代表性的非结构化数据的处理。下面我们就来了解一下人工智能是如何处理结构化数据的。我们只挑选其中有代表意义的两种数据展开讨论：消费数据和社交数据。很多机构都有这样的数据，那么，通过人工智能来解读它们，可以得到什么价值呢？首先我们来看一下消费数据。

#### 4.5.1 消费数据

根据国家统计局数据，2019 年全年国内生产总值为 990 865 亿元，而全年社会消费品零售总额就高达 411 649 亿元。一个人的消费就是另外一个人的收入，用户的消费就是商家的利润来源，消费是拉动整个商业社会向前发展的驱动力，消费数据不仅能反映出社会整体的经济水平，而且对一个企业的经营和发展具有非常重要的指导意义。接下来我们就来了解一下什么是消费数据。

一般来说，消费数据指的是个人在商家购买商品而形成的数据，这个范围非常广泛，与我们的日常生活息息相关。大到买房的消费，小到买一块口香糖的消费，都是消费数据的一部分。

消费数据主要分为以下两类：第一类是购买不同类型的商品而形成的数据，例如在电商网站上的商品分为饰品、鞋靴、数码、电器等一系列类型，在日常生活中，用户也会根据自己需求的不同来购买形形色色的商品；第二类是在不同的时间购买商品而形成的数据，比如用户在春天购买的商品与秋天购买的商品的金额就会有所不同，又或者一个用户在「6·18」年中大促购买了一件商品觉得质量很好，然后在「双 12」时又购买了一件同样的商品。不论哪一类消费数据，都能够有效地反映出消费者的需求与特点。

消费数据的种类不同，用到的处理方法自然也就不同。下面介绍两种数据处理的模型框架：一个是推荐模型，一个是 CLV，也就是客户生命周期价值模型。它们分别对应上面说的两类消费数据相关的场景。

#### 4.5.2 推荐模型理解消费数据

听同事说最近新上了一部电影特别好看，我们周末路过电影院不自觉地就去看了这部电影；看到网上很多人说一家店的食物做得特别好吃，我们就会去品尝一下到底有多么美味。我们日常生活中的很多购物消费行为都会受到各种推荐的影响。这些推荐可能来自身边人的分享，也可能是商家的广告营销，它已经成为影响消费的一个重要因素。在人工智能领域中，推荐模型指的是通过分析数据，预测用户需求，再将满足需求的商品和服务推荐给用户的人工智能模型。随着商品种类和数量的快速增长，用户需要花费大量时间和精力才能挑选出最适合自己的商品，所以能够根据用户需求向用户推荐商品的推荐模型，有很大的应用价值。

提到推荐模型，就不得不提及世界上最大的在线影片租赁服务商奈飞。奈飞的电影租赁业务之所以能做到世界头部位置，很大一部分原因是奈飞网站搭建了电影推荐模型。当一个用户在奈飞网站看完一部电影后，他会根据自己的喜好为这部影片打分。这些分数就成为推荐模型的参考数据，推荐模型可以根据这些数据来判断用户是否喜欢某部电影，可能喜欢哪种类型的电影，然后根据预测出的用户需求来向用户推荐电影。为了让自己的推荐模型更加个性化、更加准确，奈飞在 2006 年还组织了一个推荐模型大奖赛。在这个比赛中，只要参赛者能够最大地优化奈飞的推荐模型，就能够赢得 100 万美元。在高额奖金的吸引下，很多做机器学习的高手都来参赛。最终比赛的结果是，奈飞的推荐模型的推荐准确率提升了 10%，整个产业界推荐模型的发展也因此向前迈进了一大步。权威杂志 IEEE Computer 为此曾专门介绍相关推荐模型。下面我们就来了解一下奈飞的推荐模型具体是怎么工作的。

首先我们要知道，奈飞的数据很多，构成了一个庞大的矩阵。在这个矩阵中，每一行对应的是一个用户对不同电影的评分数据，每一列对应的是一部电影收到的不同用户评分数据。比如说一个用户评价过 5 部电影，在奈飞的数据矩阵中，属于这个用户的那一行里就有 5 个非零的数字，来表示这个用户对电影的评分。奈飞大奖赛里获胜的团队用的就是矩阵分解的模型。矩阵分解就是把庞大的矩阵分解成两个小的矩阵，它采用的是降低维度的思路，是把原始矩阵用低维的子空间去拟合。原始矩阵在很高维的空间中，拟合就是用相对低维的空间，比如说用几百个或者几千个维度，来拟合原来更高维的数据。

降维更重要的目的是提取用户本质的行为。通过降维，我们可以在原来庞大的矩阵中提取本质信息，来代表用户兴趣中最稳定的部分，它也是模型可以用来预测行为的部分。比如说在矩阵中，有一个用户看过很多动作电影，而且对动作电影的评分都非常高，那么说明这个用户对动作电影感兴趣，如果向他推荐此类电影，他观看的可能性就会比较高。

通过这样的方法，我们可以预测新的信息。具体是如何实现的呢？当我们用矩阵分解模型把大矩阵分解成两个小的矩阵之后，需要再把这两个小矩阵乘起来，重构一个新矩阵。在这个重构的新矩阵里会有更多非零的数值，而且排列要比原来的矩阵稠密一些。这时就可以根据新矩阵中的非零数值进行推荐，比如选取用户所在的那一行预测出来最大的非零数值，再把这个非零数值所对应的商品或服务推荐给用户。

这就是矩阵分解「协同过滤」思想核心的地方。通过矩阵分解还原用户和商品数据中的规律，再通过重构矩阵来预测出原始矩阵中不存在，但是用户可能感兴趣的商品，然后把它们推荐给用户。为什么这个模型能够准确地预测用户的兴趣呢？比如说，如果许多人都看过某几部电影，那推荐模型就会认为这些人的需求相似，就可以把这个群体中一些人看过的其他电影，推荐给这个群体中没有看过这部电影的人。同样，推荐模型也会认为这些电影之间存在相似性，当这个群体之外的人看过这些电影中的一部，就可以向他推荐这些电影中的其他几部。

为了更好地理解推荐模型，我们下面来举一个实例。如图 4-25，最左边表示的是在某视频网站上观看电影的名称和相应的评分。行对应的是不同的用户，列对应的是不同的影片，分别是《红海行动》《我不是药神》《无名之辈》《芳华》《寻梦环游记》《神秘巨星》，数字表示的是不同用户对这些电影的评分。这个表格其实也可以类比实体商品的消费数据，只需要把影片换成实体商品就可以了。如果想知道一个用户是否对其他商品或商家感兴趣，感兴趣的程度有多高，都可以通过这类推荐模型实现。

从图 4-25 可以得知，第四个用户对《红海行动》的评分为 5 分，对《我不是药神》的评分为 8 分。如果想要继续知道这个用户对《无名之辈》的感兴趣程度，就需要把整个矩阵通过模型做分解。图 4-25 右边的部分展示了分解的过程。首先，我们将大的矩阵分解成两个小的矩阵，左边的矩阵每一行对应一个用户，右边的矩阵每一列对应一部电影。两个小矩阵的每一行或者每一列对应本质的兴趣特征，就像 DNA 存储了生物的基因信息一样。在左边的矩阵里，每一行是关于用户兴趣的 DNA；在右边的矩阵里，每一列是关于影片的 DNA。如果我想知道第四个用户对《无名之辈》的兴趣度有多大，我就可以把第四个用户的 DNA 和《无名之辈》影片的 DNA 乘在一起，算出一个数字，这个数字对应在这个未知的空格中。这个数可能是 8 也可能是 1，如果是 8，代表用户对这部影片的兴趣度很大，如果是 1，代表用户对这部影片的兴趣度很小。

图 4-25 某视频网站推荐模型示意图

现在我们来总结一下矩阵分解模型推荐功能的原理：首先，模型会把消费数据分解为两个小的矩阵，再把矩阵的行和列进行组合，相当于数学上的向量乘法，组合之后就可以预测出行和列交叉处的兴趣值。计算出来的兴趣值大小不一，但是数值越大代表用户的兴趣度越高，所以我们就可以根据兴趣值，向用户推荐他们没有购买消费过，但兴趣值很高的商品或服务。这是各大电商网站推荐商品的原理，也是视频网站上影片推荐的方式。只要用户购买过商品，结合用户的购买行为以及其他具有类似需求的用户的购买行为，矩阵分解模型就可以把最恰当的商品或服务推荐给用户，这是基于消费数据的推荐模型中最基本的框架。

奈飞的电影推荐，电商网站的商品推荐，金融机构向客户交叉销售产品，都可以应用这一类推荐模型。这种推荐模型能够从消费数据中提取规律，为用户推荐最适合他的选择，这是推荐模型最核心的思想。例如，随着生活节奏的加快，人们的闲暇时间越来越少，所以很多人会选择在网络上购物，而各大电商平台也都设置了购买之后的用户评价功能。如果一个用户在购买商品之后给了五星好评，那就说明他对这个商品很满意，这个商品符合他的需求，之后推荐模型就会向与这个用户具有相似喜好的其他用户推荐该商品。

#### 4.5.3 客户生命周期价值理解消费数据

接下来我们就来讲一下第二类消费数据处理的模型框架：客户生命周期价值。客户生命周期指的是从一个客户想了解企业的一些信息或者企业想开发这个客户开始，一直到客户和企业的业务关系以及业务的相关事宜完全结束并且处理完毕的这段时间。对于一个企业来讲，客户生命周期描述的是客户与企业的关系从开始到发展再到结束这样不同状态间运动的总体特征，形象地讲，客户与企业的关系经历了诞生、成长、成熟、衰老和死亡这样一个完整的生命过程。

客户生命周期分析经常应用于零售行业，指的是客户从成为某商家的客户并产生业务消费开始，经过消费成长、消费稳定和消费下降阶段，最后完全不在这个商家购物的过程。客户生命周期价值指的是一个公司或者产品预计在与客户的整个业务关系中获得的利润。让客户在生命周期中产生商业价值才是一个企业运营的最终使命，所以企业在运营过程中要尽一切可能来延长客户的生命周期，在客户的生命周期中要尽一切可能来产生商业价值。那么我们如何预测客户生命周期的价值呢？

一般来说，我们都是从已有的消费数据来推算客户生命周期的价值的。从一个简单的角度来讲，对一个客户的消费数据而言，最重要的三个变量分别是：最近一次消费时间（recency）、消费频率（frequency）、消费金额（monetary）。

图 4-26 表示的是一个客户在 6 个月内的购物行为。一个购物车代表一个月，这个客户在第 2 个月和第 5 个月购买了商品，其他时间没有买。在这里，总观察时长是 6，最近一次消费时间是 5，消费频率是 2。掌握了这些变量后，我们就可以计算很多跟消费者有关的指标，比如：客户在将来某个时间点活跃的概率、客户在将来某个时间段购买次数的预期值、客户剩余交易次数的预期值和客户剩余生命周期价值等。其中，客户剩余生命周期价值是 CLV 的一种体现。

图 4-26 客户生命周期的变量示意图

那么我们是如何计算这些指标的呢？最重要的是，我们需要一个描述客户消费行为的模型，下面来讲一个最基本的 CLV 模型：在每个时间点，客户都有一定的概率购买，也有一定的概率失活，也就是再也不买了。购买概率和失活概率本身也是变量，因此我们要用模型去表示它们。国际著名的《营销科学》（ Marketing Science ）杂志曾有一篇文章讲了这个模型，并得出了一个有趣的结论：上面说的客户在将来某个时间点活跃的概率、客户在将来某个时间段购买次数的预期值、客户剩余交易次数的预期值和客户剩余生命周期价值这 4 个与客户相关的指标，都是可以用最近一次消费时间和消费频率，以及购买概率、失活概率相关的模型参数来表示的。而这些模型参数通过人工智能计算可以求得最优解，将其代入前面提到的 4 个指标后，用最近一次消费时间和消费频率，就可以算出跟客户将来消费相关的一系列指标。这个结论表明了最近一次消费时间和消费频率在消费数据分析领域的重要性。同样，消费金额也有类似的作用。

在与客户相关的 4 个指标中，商家最关心的指标是客户剩余生命周期价值。顾名思义，客户剩余生命周期价值就是客户在之后的生命周期中还有多少价值。比如一个客户在一个行业中的生命周期已经到了消费稳定阶段，那这个客户的剩余生命周期价值就是在消费稳定以及消费下降这两个阶段所带来的价值。剩余生命周期价值的数值可以通过求利润率、每笔交易收入以及客户剩余交易次数的预期值的乘积来得出。商家可以根据客户的剩余生命周期价值来判断，这个客户还会不会购买自己的商品，能够购买多少商品，商家还要不要对这个客户进行营销，如何营销等。这样，商家既能有效地控制成本，又能有针对性地进行推广，广告营销的投资回报率也会相应提高。

除了上述模型外，在客户关系管理中的 RFM 模型也可以用来理解消费数据。RFM 模型能够有效地衡量客户的价值以及客户创造利润的能力，其中的 R、F、M 就是上面提到的理解消费数据最重要的三个变量：最近一次消费时间、消费频率、消费金额。这三个数据是零售商最感兴趣的三个数字，也是在预测客户价值过程中必不可少的三个指标。最近一次消费时间是指客户上一次的消费时间，最近一次消费时间越近的客户，对商品和服务印象越深，再次购买商品和服务的可能性就越大。消费频率是指客户在一定时间段内所购买商品或服务的次数，这个次数越多，那说明该客户对商品或服务的满意度越高，对于企业品牌有一定忠诚度，这样的客户重复购买商品或服务的可能性也就越大，而且还是持续性的。消费金额同理，金额越高说明客户对商品或服务越满意，再次购买的可能性就越大。

通过 RFM 模型可以把客户群区分开，得到不同价值的客户群，比如分成重要维持客户、重要发展客户、重要挽留客户、一般重要客户、一般客户和无价值客户等几个类别。这在智能化客户管理中很重要，很多企业已经对此进行了实践。比如说在没有对客户进行区分的情况下，某家商店可能会付出一定的成本给 1 000 位无差别的用户发短信，通知他们商店有新品上架，欢迎购买，但是可能只会收到 100 个订单。但是有了 RFM 模型之后，商家就可以只将这 1 000 条短信发给重要维持客户、重要发展客户这些购买概率大的客户，付出同样的成本，收到的订单数却会大大上升。而且当 RFM 模型对用户进行区分之后，发的短信就可以不用千篇一律了，比如给那些以前经常购买但有一段时间没有购买过商品的客户，短信的开头可以是「好久不见」，而对于那些在短时间内购买很多商品的客户，短信的开头可以是「恭喜您成为本店的 VIP 客户」。对于客户来说，这样千人千面的短信比那些同质化的短信更具有吸引力，他们再次光顾的可能性也就更大。

人工智能通过客户生命周期的价值来理解消费数据的重要意义是：根据客户有限的消费数据，可以推测他将来是否会继续消费，消费金额是多少。对于那些消费可能性高的客户，商家可以采取相应措施来刺激他们的消费；对于那些消费可能性低的客户，商家也可以采取一些措施来防止客户流失。

消费数据仿佛一棵茂盛的大树，盘根错节，枝蔓横生。但是 AI 思维就是要在这些看似复杂的数据中抓取其最本质的特征，为决策提供依据。理解消费数据的过程就是通过人工智能对这些数据抽丝剥茧、披沙拣金的过程。人工智能理解了消费数据，就可以通过推荐模型提高客户的购买率，可以通过客户的生命周期价值模型提高客户的价值。总之，人工智能理解了消费数据后，就可以从各个角度刺激客户的购买欲，增加企业商品和服务的销量，提高企业的商业价值，对企业的发展起到强有力的推动作用。理解了消费数据，我们就扣住了 AI 思维中的重要一环，在决策中真正做到对症下药，一针见血。

### 4.6 AI 理解社交数据

人类天生就是一种群居动物，社交是我们与生俱来的属性。几百年前人们鸿雁传书交流信息，几十年前人们通过电话与千里之外的人沟通感情。现在，我们动动手指就能在手机上和牵挂的人联系。日常生活中，我们也无时无刻不在与其他人沟通交流。我们离不开社交，每时每刻都在产生着社交数据。这些数据里包含着我们的喜怒哀乐，反映出我们与这个社会的联系，所以我们这一节就来介绍一下人工智能是如何理解社交数据的。

我们在进行社交的过程中，会和许多人建立关系，这样就形成了社交网络。由于我们的时间和精力是有限的，不可能和许多人保持紧密的社交关系。一般来说，一个人的社交网络最大只能包含 150 个人，平均在 124 个人左右。也许你听说过，不论你想要认识谁，只要通过 6 个人就可以认识他，这就是被美国社会学家斯坦利·米尔格拉姆（Stanley Milgram）证明的六度人脉理论。在社交网络中，每个人都是一个节点，人与人之间的关系就是社交网络的边。在现实生活中，以社交网络的节点为基础，通过社交网络的边，就可以拓展我们的人脉。

在社交网络中有一个重要的概念叫作中心性，它表示的是一个节点在网络中处于核心地位的程度。中心性有三种计算方法，分别是度中心性（degree centrality）、中介中心性（betweenness centrality）和紧密中心性（closeness centrality）。

第一种，度中心性。在社交网络中，度中心性表现的是一个节点与很多其他节点发生的直接联系。一个节点连接的节点越多，这个节点也就越重要，处于中心地位，这个是比较直观的指标。比如说，用户甲的社交账号上有 50 个好友，这就意味着有 50 个节点与甲相连接；如果在同样的社交平台上，用户乙的社交账号有 100 个好友，那么乙的度中心性就比甲要高，社交圈子比甲要广。

第二种，中介中心性。中介中心性是指一个节点出现在其他节点之间的最短路径的次数。如果这个节点的中介中心性高，它对整个网络信息的传播就会有很大的影响。例如我们的不少朋友可能都是通过某位社交达人才认识的，那么很显然，这位社交达人起到了中介作用。换句话说，就是这个节点相当于一个关口，和它相连的节点想要到其他节点都得经过它。

第三种，紧密中心性。紧密中心性指的是一个节点到其他所有节点最短路径的和，主要用来考察一个节点在传播信息时对其他节点的依靠程度。如果一个节点离其他节点越近，那么这个节点就不会受制于其他节点，它传播信息的时候也就越不需要依赖其他人。比如，在一个公司里，甲只认识本部门的几个人，而乙除此之外还认识部门外的其他人，那么乙的紧密中心性就要比甲高，因为甲要与非本部门的人进行信息传递可能需要经过乙的搭线，而乙因为本身就认识其他部门的人，则不存在这些限制。

通过这三种方式，我们就可以找到中心性高的节点，也就是找到在社交过程中会产生很大影响的人或者组织。中心的节点有着不可取代的重要性，它们更具有权威性、中枢性、核心性等特征，同时也具有更多的经验和影响力，对社区的形成起着决定性的作用。比如说在营销领域，我们需要挖掘意见领袖，这时候意见领袖就是中心节点，因为，通过意见领袖展开的营销才能更具影响力，更容易被大众相信，营销效果也才会提升。

#### 4.6.1 社区发现模型

下面谈谈什么是社区发现。社区是由一组连接紧密的节点组成的，并且这些节点与社区外部的节点连接稀疏。如图 4-27 所示，可以看出图上有三个社区，符合这样的规律。不同社区之间的关系稀疏，因而社区更多地反映的是网络中的个体之间的关联关系。研究网络中的社区对理解整个网络的结构和功能起到至关重要的作用，还可以帮助我们分析、预测整个网络各节点间的关系。社区发现就是在复杂网络中发现这些连接紧密的社区结构，或者说，社区发现就是网络中节点的聚类。举个例子，在一个班级里面，有的学生喜欢文学，有的学生喜欢绘画，有的学生喜欢数学，有相同喜好的学生之间联系会更为紧密和频繁，根据这些交流的频繁程度，可以把整个班级里的同学分成几个不同的社区。又或者说，在公司里，总会有几个联系较为紧密的小群体，例如，一部分人经常聚在一起讨论工作学习，还有一些人经常聚在一起讨论时尚潮流，根据他们之间联系的紧密程度和频率高低，很自然地能够发现这两个人群已经各自形成一个社区。

图 4-27 社区示意图

那么，我们怎么去进行社区发现呢？有很多常用的社区发现模型，例如吉尔韦安（Girvan）、纽曼（Newman）等人在 2003 年提出的 GN 社区发现方法（以两位开创者的名字首字母命名），就是社区发现中的一个比较经典的模型。GN 社区发现方法提出了边介数的概念。那么，什么是边介数呢？由网络中社区的定义可知，所谓社区就是指其内部节点的连接稠密，而与其他社区内的节点连接稀疏。这就意味着社区与社区之间联系的通道比较少，一个社区到另一个社区至少要通过这些通道中的一条。如果能找到这些重要的通道，并将它们移除，那么网络就自然而然地分出了社区。在实践中，我们用边的边介数来作为这个分割的依据。

边的边介数定义为网络中所有最短路径中经过该边的数目。跟节点的中介中心性类似，它反映了相应的边在整个网络中的作用和影响力。联系定义可知，通过社区内部的边的最短路径的数目相对较少（边介数较少），而通过社区之间的边的最短路径的数目则相对较多（边介数较多）。因为社区之间的联系只能经由这几条路径发生，就像车流要从停车场开到外面一样，如果停车场只有一条通道，那么车流都得从这条通道走，这条通道的使用率就是 100%，非常高；如果这个停车场内部有 5 条通道，车流可以分散开来，可能每个通道的平均使用率只有 20%，远没有前者高。

2『网络中的边介数，做一张术语卡片。（2021-02-28）』——已完成

GN 社区发现方法是基于删除边的方法，其本质是基于聚类中的分裂思想，原理上使用了边介数作为相似度的度量方法。GN 社区发现通过移除网络中的边来将整个网络划分成为合适的社区结构，将原来的网络分割成为任意数目的社区。在 GN 社区发现方法中，每次都会选择边介数高的边删除。GN 社区发现方法的步骤如下：

第一步，计算每一条边的边介数。

第二步，删除边介数最大的边。

第三步，重新计算网络中剩下的边的边介数。

重复上述步骤，直到产生合适的社区结构。

通过上述步骤，GN 社区发现方法可以较好地发现网络中存在的社区结构。在网络理论中，复杂网络是由数量繁多的节点和节点之间错综复杂的关系构成的。GN 社区发现方法用边介数的概念来探测边的位置，从而在盘根错节的复杂网络中划分出社区。

人与人之间的互动使其相互间产生了或紧密、或松散的联系，也在不经意间建立起了一张张连通社会各个层面的网络：通信往来、商业交易和互联网社交等。社区发现模型可以用来找出网络中的社区聚类，让人们洞察这些网络中隐藏的形形色色的「社区」，也可用于评估一个网络结构中个体组合或分裂的程度，并从中获取很多新发现。

#### 4.6.2 社区发现的应用一：推断用户未知信息

我们本来就生活在一张庞大的社交网络之中，随着社交媒体的兴起，社交网络变得越来越复杂。社区发现在近些年的不断发展，使得人们可以对复杂的社交网络进行分析，对于现实生活具有重要的应用意义。我们接下来就从推断用户未知信息、提高营销效果以及发现不良行为团伙三个方面剖析社区发现的应用价值。

社区发现的应用价值之一是推断用户未知信息。随着社交媒体的高度发展，其用户规模也在快速增长，有效的社区发现模型的应用也越来越重要。例如购物网站上的社交购物、商品推荐；微信、微博等媒体的社会舆情分析、意见领袖挖掘等，无论你想要找什么样的信息，都能通过社区发现模型获得启发。如果一个用户的某些信息尚且未知，但是我们知道他所在社区中很多人的信息，那么，我们就可以用这个社区中其他人的信息来推断该用户未知的信息。比如，你发现在朋友圈里有两群人，一群人花钱都大手大脚，入不敷出，另一群人生活稳定有规律，喜欢运动健身、阅读学习。这两群人都要向银行借钱，生活稳定有规律的这群人里，借钱的都还了，那么这群人里还没有借钱的人，一旦借钱，还钱的概率就大；花钱大手大脚的这群人里，不少人借的钱没还，那么，还没有借钱的人，一旦借钱，不还钱的概率就很大。

社区发现之所以可以用来推断用户未知信息，是因为我们可以在真实的社交网络中，通过社区发现找到节点之间的相互关系，利用已知节点的属性信息，推断出未知属性节点的缺失信息。基于社区发现的未知信息推断方法是基于两个假设：属性信息相似的人容易成为朋友，属性信息相似的人群倾向于聚集为社区。如果要推断一个社区中某些用户的未知信息，就要先从社交网络中选取若干个中心性较高的种子节点，通过这些节点的好友数据，运用社区发现模型进行划分，再根据节点的属性信息推断同社区好友的未知信息。实验结果表明，即使当给定的信息只占用户信息的 20% 时，某些用户属性也能够精确推断出来。所以，我们可以通过社区发现模型划分社区，根据未知属性节点所在社区的属性信息推断该节点的未知信息。

社交网络既是交流平台，也是市场营销等活动的平台。在人际关系网中，可以挖掘出具有不同兴趣、背景的社会团体，方便采用不同的宣传策略；在交易网站中，不同的社区代表不同购买力的客户群体，方便运营人员为他们推荐合适的商品。针对社交网络用户社区发现的方法，一般来说，可以基于用户发布的内容提取基于兴趣的用户社区；或者基于用户联系提取基于联系的用户社区；还可以将两个社区进行融合，形成兴趣和联系双重的用户社区。当社区形成之后，这个社区的成员可能就会拥有共同的兴趣或者联系，假如这个社区的大部分成员都喜欢网球，那我们就可以推断，社区的其他成员也很有可能喜欢网球。

社区发现的应用二：提高营销效果

社区发现模型在商业领域有着非常广泛的应用。在电子商务领域中，通过对消费者购买记录进行社区发现，能够挖掘出具有相同或相似购买兴趣的消费群体，发现相关商品的潜在客户并为其进行引导和推荐，进而提升商业价值。例如，通过社区发现模型，我们看到甲、乙两个消费者同属某一个社区，都买了简约风格的装饰画。这时，如果我们还想进一步为乙推荐商品，但苦于目前并没有太多数据，无法得知他的潜在需求是什么的话，可以通过甲的购物记录，发现甲还喜欢买智能手表，推知乙的潜在需求很可能是智能手表，进而做出相应推荐。

通过社区发现模型除了能找到消费者的未知潜在喜好，从而形成针对性的推荐外，还能找到社区中中心性高的核心人群，更好地实现营销触达。例如，我们要对一个社区的人群做营销投放，如果甲的中心性比乙高的话，那么通过甲来做社区整体营销触达的效果会比通过乙要好。因为，甲与社区里的其他人关系更为密切，也更有权威性，经由他来做营销传播，社区人群会更容易接受并采纳，甚至能够转化成实际行动。

在商业领域，有很多通过社区发现优化营销传播，从而实现商业价值的例子。例如阿里的年货节就利用了人工智能社区发现的手段，对品牌的目标人群进行分析，挖掘品牌目标潜在客户，使得其投放效果提升了将近 50%。他们的做法是从品牌方的受众人群出发，进行针对性的营销投放。年货节开发新用户时，先要通过机器学习模型对阿里年货节的受众人群进行预测。预测可以根据往年年货购买人群的数据进行，并以各大节假日促销时的购买数据作为参考，然后以同属一个社区的人有相似的兴趣爱好、价值观为前提假设，再通过社区发现模型找到与潜在消费者有相同或相似喜好的社区人群，利用社交网络关系进行广告投放。比如，一个社区内的大部分用户喜欢运动，经常购买运动类商品，那么我们就可以假设这个社区内的潜在用户也有相同的爱好，也有运动类商品的需求，在进行广告投放时，就可以向其推荐运动类商品。这样，推荐的商品会有很大的可能被购买，这个用户就从潜在用户转化为实际用户。新用户数量逐渐增加，品牌效益也会逐渐提升。

#### 4.6.3 社区发现的应用三：发现不良行为团伙

社交媒体的发展让我们能够更加方便快捷地获取资讯，但是人与人之间的交流由于缺乏面对面的真实接触，而变得虚拟了。我们不知道与我们在手机软件上相谈甚欢的那个人的真实面貌，更加不能分辨其中的真伪。在社交网络的保护下，各种诈骗犯罪行为变得越发猖獗。但是，社交网络可以是不良团伙牟利的蜜糖，也可以是将他们困于囹圄的砒霜。基于社交网络的社区发现，一个重要应用就是发现不良行为团伙，打击违法犯罪行为。

例如，每天在家和公司之间穿梭，很多人会选择驾车出行，但是经常穿梭于车水马龙之间，难免会出现一些磕磕碰碰的现象。因此，有车一族都会为汽车买保险。但是保险公司发现，近几年竟然出现了许多车险理赔的欺诈现象。例如，在交通事故理赔中有这样一条规定：如果发生的交通事故不涉及人员伤亡，仅造成轻微财产损失，不需要联系保险公司，可以自行拍摄车辆损失代替现场勘查。有不少人就利用这个漏洞骗取保险金。保险公司曾发现，有部分司机在短期内多次制造轻微相撞事故，骗取保险金。但是事情并没有到此为止，社区发现模型的及时介入使得警方发现了这些司机同在一家公司，于是警方经过调查发现，在这家公司工作的另外几名司机也有同样的诈骗行为，便一举拿下了这个诈骗团伙。

随着经济的高速发展，银行客户群体的规模不断扩大，但与此同时，银行客户群体中也混入了一些不法分子，他们伪装成正常客户群体到银行办理业务，意图牟取非法收入，并且这种违法行为现在越来越团伙化。「近朱者赤，近墨者黑。」团伙化犯罪的成员彼此之间都有联系，所以社区发现模型对于发现不良团伙、维护银行利益具有很大帮助。例如，现在有很多人在大量透支信用卡之后，潜逃或者隐瞒身份来逃避还款，极大地损害了银行的利益。为了防止这一现象的发生，一些大型银行借助社区发现模型建立了银行反欺诈系统。如果一个用户想要申请信用卡，银行会以这个申请用户为节点，调取这个用户所在社区里人员的相关信息，如果这个用户处于不良团伙之内，也就是说，在与他联系比较密切的人里面，大范围出现过类似于恶意透支信用卡这样的金融诈骗行为，那么这个用户就会被划入高欺诈风险的用户范围，他的信用卡申请很有可能会被驳回。此外，利用社区发现模型进行深入调查后，这些隐藏起来的不良团伙也更容易被追根究底，一网打尽。基于社区发现模型的银行反欺诈系统大大提高了银行防范风险的能力，在银行反欺诈领域有很大的应用价值。

人类社会离不开社交，要想让人工智能更好地服务于人类，离不开人工智能对社交数据的理解。正所谓打蛇打七寸，想理解数量庞大的社交数据，理清复杂的社交网络，重点在于理解中心性高的节点；在此之后，我们再结合社区发现模型，找到相应的社区群体，一方面可以精准营销，提升品牌效益，另一方面可以发现不良行为团伙，提升社会效益。不管从哪个角度出发，AI 思维都能够把握住事物的发展方向，帮我们做出正确的决策。