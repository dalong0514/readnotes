## 记忆时间

2020-01-22；2020-04-14；2021-08-20

## 目录

0101计算机里有什么

计算机里有什么可以从 2 个维度来看。功能组成上看，计算机里有什么组件、是做什么的、它们是如何连接起来的；从物理结构上看，这些东西是啥样子的，它们是如何被制作出来的。前一个维度的功能组成一直没变过，而后一个维度的物理结构一直在更新，典型如摩尔定律。

0102比特、字节与信息表示

有关计算机表示信息的三个核心思想：计算机处理的信息是数字信息而非模拟信息；计算机处理的信息是二进制编码的；较大的信息用比特组来表示。

0103深入了解 CPU

CPU 是一个小型指令系统，关键的一点是它能根据它所处理的数据决定下一步做什么。

## 0100. 硬件

然而，有些历史趋势却值得我们关注，尤其是这一点：以一定的成本，在给定大小的空间内能装进电路和设备的数量，随着时间而呈指数式增长。随着数字设备越来越强大和廉价，林林总总的机械系统已经被更为统一的电子系统所代替。

计算设备的历史悠久，不过早期的计算设备大多数是专用的，通常用于预测天文事件及其发生方位。例如，关于巨石阵，一个尚未证实的推测就认为它是一座天文观测站。公元前 100 年制造的安提基瑟拉机器就是一台天文计算机，其机械结构之精妙令人叹为观止。算盘之类的演算工具也已经使用了近千年时间，在亚洲尤为流行。计算尺发明于 17 世纪早期，也就是约翰·纳皮尔提出对数概念没多久之后。

虽然尚存在争议，但一般认为，当今意义上的计算机始于 19 世纪的英国，由查尔斯·巴贝奇提出。巴贝奇是一位科学家，对航海和天文学感兴趣，而这两项事业都需要通过写满了数值的表格来计算方位。巴贝奇花费了毕生精力来制造用于计算的设备，试图把冗长乏味、易出错的手工算术运算机械化。但由于各种原因，包括与资助人之间的关系疏离，他的雄心壮志始终没有得偿所愿。不过，他的设计是正确的，现代人利用他那个时代的工具和材料按其设计可以制造出他的机器。如今，在伦敦的科学博物馆、加州山景城的计算机历史博物馆等地，都能看到这样的机器。

一位年轻的女士受巴贝奇鼓舞而对数学和他那个计算设备产生了兴趣。这位女士就是诗人乔治·拜伦的女儿，奥古斯塔·爱达·拜伦，也就是后来的勒芙蕾丝伯爵夫人。她写过一份详细说明，讲述如何用分析引擎（巴贝奇所计划制造机器里最高级的一个）进行科学计算，并推测这种机器也可用于非数值计算，比如作曲。爱达·勒芙蕾丝通常被认为是世界上第一位程序员，编程语言 Ada 也是以她的名字而命名的。

在 19 世纪后期，赫尔曼·何乐礼为美国人口统计局设计并制造了制表机，用它制作人口统计数据表格要比手工快得多。何乐礼借用了雅卡尔织布机的思路，用卡片纸上的孔洞把人口统计数据编码成他的机器能处理的格式。令何乐礼名声大噪的是，1880 年的人口数据花了六年才制成表，而用了他的穿孔卡片和制表机之后，1890 年的数据仅一年就完工。他创立了一家公司，经过多次并购之后成为国际商业机器公司，也就是我们现在熟知的 IBM。

巴贝奇的机器是由齿轮、转轮、杠杆、拉杆组合起来的复杂机械，而 20 世纪电子学的发展使得人们有条件去设想没有运动部件的计算机会是什么样子。到了 20 世纪 40 年代，在费城的宾夕法尼亚大学，由布莱斯波·埃克特和约翰·莫奇利设计制造的 ENIAC（电子数值积分计算机的英文首字母缩写）横空出世，成为全电子计算机的最重要标志。ENIAC 占满了一间大屋，需要消耗很多电力，每秒钟能做大约五千次加法。它本来是为了计算弹道等军事用途而制造的，但直到 1946 年「二战」已结束多时它才完工。ENIAC 的一些部件，现存放在宾大的摩尔工程学院作展览。

巴贝奇清楚地意识到，计算设备可以把操作指令和数据保存为同样的形式，但 ENIAC 并没有把指令像数据那样保存在存储器中，而是通过扳动开关和重新连线来实现编程。第一台真正实现了存储程序的计算机于 1949 年在英国面世，称为 EDSAC（延迟存储电子自动计算机的英文首字母缩写）。

早期的电子计算机使用电子管作为基本计算元件。电子管是一种大小和形状类似于柱形电灯泡的电子设备，缺点是昂贵、易碎、笨重、费电。而随着 1947 年晶体管和 1958 年集成电路的相继发明，计算机的新时代才真正开始。用此技术制造的设备是当今的电子系统越来越小、越来越迅速以及越来越便宜的原因所在。

3『

[电子管_百度百科](https://baike.baidu.com/item/%E7%94%B5%E5%AD%90%E7%AE%A1)

电子管，是一种最早期的电信号放大器件。 被封闭在玻璃容器（一般为玻璃管）中的阴极电子发射部分、控制栅极、加速栅极、阳极（屏极）引线被焊在管基上。利用电场对真空中的控制栅极注入电子调制信号，并在阳极获得对信号放大或反馈振荡后的不同参数信号数据。

』

1、数字计算机包含处理器和存储器。处理器执行简单的指令，速度非常快。它可以根据早先计算的结果以及外界的输入，决定接下来做什么。存储器包含数据和处理数据的指令。

2、计算机是一种通用的机器。它从存储器中读取指令，而人把不同的指令放到存储器中，可以改变它要执行的计算。指令和数据要通过使用场景区分，一个人的指令可以是另一个人的数据。

3、图灵的结论：从能够执行完全相同的计算的意义上说，这种结构的所有计算机（包括你以后可能会看到的任何计算机）具有完全相同的计算能力。当然，它们的性能可能千差万别，但在不考虑速度和存储器容量的前提下，它们的能力则是等价的。最小最简单的计算机也能够完成大计算机所能完成的计算。的确，可以通过编程让任何计算机模拟其他计算机，而图灵正是这样证明了他的结论。

4、计算机的逻辑结构自冯·诺依曼之后并没有太大改变，但物理结构已经发生了巨大变化。摩尔定律已经应验了大约 50 年，成为迄今为止几乎完全兑现的预言。摩尔定律预言了在既定的空间和成本之下，个别器件的大小和价格会呈指数级下降，而它们的计算能力呈指数级增长。它们都是数字计算机：所有一切最终都要化简为比特，单独或成组地以数字形式表示信息，所谓信息可能是指令也可能是数据。这些比特的含义取决于它们的上下文。可以化简为比特的任何事物，都可以通过数字计算机来重现和处理。

从某种角度讲，互译结果相当不错，甚至还考虑到了一些语法问题。然而，翻译过程忽略了一个谈论计算机时至关重要的事实，即「比特」（bit）并不等同于「碎块」（piece），而「化简为比特」（reduced to bits）与「切分成碎块」（cut in pieces）的意思也完全不同。类似的结论同样适用于今天的语音识别、面部识别及其他图像处理：计算机可以做得很出色，但没人会说它们的能力可以与人类比肩。顺便说一下，如果你再试一次，翻译的结果又会不一样，其算法和底层的数据好像变化很频繁。

计算机可以下出特级大师水平的国际象棋，但在人脸识别上却不及幼儿。早些时候，计算机下国际象棋的水平其实很烂，但随着机器的速度不断加快，它下棋的水平明显提高，但这一切几乎完全是因为它比人类对手能够多看几步。类似地，语言翻译、语音识别等领域最大的进步，主要还体现在庞大的数据量（比如不同语言的平行文本数量），而有了这些数据计算机才能接近人的表现。

图灵测试想做的，就是在隐身、忽略响应时间等因素的前提下，通过对人和计算机提问，看看能否区分哪个是人，哪个是计算机。

尽管如此，我们还是有太多太多的事物不知道怎么用比特来表示，更不必说怎么用计算机来处理了。比如，日常生活中最重要的一些事物：艺术、创意力、真理、美、爱、荣誉和价值。我想在一定的时期内，这些事物仍将超出计算机的能力之外。

1『现实世界里知道如何用比特表示的事物只是一部分，而在这部分里能够用计算机处理的又只占一小部分。』

## 0101. 计算机里有什么

先大略看看计算机里面都有些什么东西。这个问题可以从两方面来看：逻辑上或者说功能上的组成，即每一部分是什么、做什么、怎样做、之间如何连接；以及物理上的结构，即每一部分长什么样子、如何建造起来的。

这些计算机看起来很不一样，用起来也感觉不一样，但这仅仅是表象，其实根本没区别。为什么这么说呢？可以拿汽车来打个大致的比方。在功能构成上，这一百多年来的汽车都是一样的。每辆汽车都有个发动机，通过燃烧某种燃料来驱动发动机运转，这样车就能开了；都有个方向盘，这样司机（也许可以比作软件）就能操纵车的方向；还有储存燃料的地方，以及留给乘客和行李的位置。但是这一个多世纪以来汽车在物理构成上却变化巨大：造车的材料日新月异，行驶越来越快，越来越安全，可靠性和舒适性与过去不可同日而语。

计算机也是一样的道理。当今的计算机在逻辑结构上和 20 世纪 50 年代的非常相似，但是物理指标的进步却远甚于汽车。当今的计算机和 50 年前的比起来，体积更小，价格更廉，运行更快，也更可靠，有些字面上的指标甚至提高了百万倍。计算机如此普及，其根本原因就在于此。

一件东西的功能表现与物理特性之间的区别，也就是说它能做什么与它是怎样建造起来的（或者说内部的工作方式）之间的区分，是很重要的。就计算机而言，「它是如何建造出来的」这个问题的答案在以惊人的速度变化着，「它运行起来有多快」也是如此，但是「它能做什么」的答案却没什么变化。后面将会反复提到这两方面的区别。

如果我们画一张抽象图展示计算机内部有什么，也就是它逻辑上或者功能上的体系结构，那么 Mac 和 PC 的结构都是如下图所示：一个处理器（CPU）、一些主存储器（内存）、一些大容量存储器（磁盘）和各种各样的其他部件，一组叫做总线的线缆把所有这些连接起来，在各部件之间传输信息。

计算机的基本组成，包括处理器、存放指令和数据的存储器以及输入输出设备，在 60 多年前就已经是标准了。这种体系结构通常称为冯·诺依曼体系结构，以约翰·冯·诺依曼的名字命名。他在 1946 年与阿瑟·勃克斯、赫尔曼·戈德斯坦共同撰写的论文《电子计算仪器逻辑设计的初步讨论》中描述了这种体系结构。尽管目前人们对于以冯·诺依曼来命名这种体系结构是否掩盖了其他人的贡献尚有争议，但这篇论文条理清晰，见解深刻，即便在今天也值得一读。例如，论文的第一句就指出：「为了让这台完整的设备成为通用的计算机器，它必须包含某些主要元件用于运算、存储数据、控制以及连接操作人员。」翻译成现在的术语就是，CPU 提供运算和控制功能，内存和磁盘用于存储数据，键盘、鼠标和显示器用于连接操作人员。

2『已下载论文「2020001Logical_Design_of_an_Electronic_Computing_Instrument」；已下载书籍「2020005计算机与人脑」；已下载原文书籍「2020005The_Computer_and_the Brain」、「2020006John_von_Neumann_Selected_Letters」。』

如果我们说计算机有大脑的话，处理器，或者叫中央处理单元（缩写为 CPU）就是计算机的大脑。处理器进行运算，来回搬运数据，并控制着一切别的操作。CPU 有一张指令表，它可以执行的操作是有限的，但执行起来速度异常之快，高达每秒钟几十亿次。它可以根据先前的计算结果决定接下来执行什么指令，所以在很大程度上，它可以主宰自己的命运。

比如你可能看到对 CPU 的描述是「英特尔双核酷睿 2.1 GHz」。这是什么意思呢？这款 CPU 是英特尔制造的，一片封装的内部实际上有两个 CPU。在这句话里，「核」的意思就是处理器。2.1 GHz 看起来更有趣。CPU 的速度大体上是以每秒钟执行的操作数量、指令数量或更小的动作数量来度量的。CPU 使用一个跟心跳或者钟表嘀嗒类似的内部时钟来控制基本操作的节拍，度量 CPU 速度的指标之一就是看这个内部时钟每秒振动多少次。每秒钟心跳一次或者嘀嗒一次就是 1 赫兹，记为 1 Hz。这个单位名称是为了纪念德国工程师海因里希·赫兹，他在 1888 年发现了产生电磁辐射的方法，由此直接导致无线电广播和其他无线系统的诞生。广播电台发射的广播信号频率为兆赫（百万赫兹），比如 102.3 MHz。现在的计算机通常运行在十亿赫兹的数量级上，也就是吉赫，记为 GHz。

主存储器，也就是随机访问存储器（缩写为 RAM，即内存 ），里面保存了处理器和计算机的其他部件正在活跃使用的信息；CPU 可以改变内存里的内容。内存里不仅保存了 CPU 正在处理的数据，还保存了让 CPU 如何处理数据所需运行的指令。这一点至关重要：通过把不同的指令加载进内存，就可以让计算机做不同的计算。这样，存储程序型计算机就成为通用的设备：同一台计算机，只要在内存里放上适当的指令，就可以运行文字处理程序、制作数据表格、上网浏览、收发电子邮件、计算纳税款，还可以播放电影。存储程序这个理念的重要性怎么强调都不为过。

内存是计算机运行的时候存储信息的地方。运行中的程序，比如 Word、iTunes 或浏览器，它们的指令就放在内存里；这些程序操作的数据，比如屏幕上显示的照片、正在编辑的文档、正在播放的音乐等，也是放在内存里的；而 Windows、Mac OS X 或其他操作系统，也就是能让你在同一时间运行多个应用程序的幕后功臣，它们运行时的指令还是放在内存里。

内存之所以被称为「随机访问」，是因为 CPU 能以同样的速度快速访问其中任何地方的信息。以任何顺序随机访问不同位置时，速度不会受到任何影响。与之相比，老式录像带的访问方式则称为「顺序访问」，比如想观看电影尾声的时候只能从头开始慢悠悠地「快进」，跳过前面的内容。

内存是易失性的，也就是掉电之后里面的内容会消失，当前活跃的信息就都丢掉了。所以要养成小心谨慎的好习惯，经常保存正在做的工作，尤其是在台式机上，不小心踢掉电源线可不是闹着玩的。

你计算机上的内存大小是有限的。表示容量的单位是字节。一字节大小的内存，可以放入单个字符（比如 W 或者 @），可以放入一个整数比如 42，还可以放入大数值的一部分。第 2 章会展示内存或者计算机其他部件中的信息是如何表示的，因为这是计算的一个基础问题，但在此之前，你可以把内存想象成一大堆完全一样的小盒子，上面从 1 开始编号到一二十亿，每个盒子里可以放进一小片信息。

内存的容量有多大？我正在用的这台笔记本电脑，内存有 20 亿个字节，或者说 2 吉字节，也就是 2 GB。有很多人还认为这么些内存太小了点。原因是，虽然所有程序同时开起来的时候，内存再多也不够用，但是内存越大，可供发挥的空间就越大，而这往往也可以说成是计算得越快。如果你想要计算机运行更快的话，多买内存看起来是最佳策略。

内存的容量很大，但还是有限的，并且掉电之后内容会消失。大容量存储器则能在掉电后仍保存着里面的信息。最常见的大容量级存储是磁盘，有时也称为硬盘或硬驱。磁盘能保存的信息比内存大得多，并且是非易失性的，也就是说，磁盘上的信息不论通电还是断电都一直在那里。于是数据、指令和其他信息都长期保存在磁盘上，仅在需要时临时读入内存。磁盘空间比内存便宜 100 倍，只是访问起来要慢得多。磁盘保存信息的方法是对旋转的金属盘片表面的磁性材料上的微小区间进行不同方向的磁化。计算机工作时的嗡嗡声和咔嗒声就是磁盘把磁头移向盘片表面正确位置时发出来的。

硬盘是用来展示逻辑结构和物理实现之区别的好例子。在 Windows 下运行资源管理器或者在 Mac OS X 下运行 Finder，可以看到硬盘里的内容组织为层次分明的文件夹和文件，但真正的数据是完全存放在旋转的机械装置、没有活动部件的集成电路或者其他存储设备里的。计算机里装的究竟是哪种「磁盘」其实无关紧要，事实上是硬盘里的硬件电路和操作系统里称为文件系统的重要软件一起创建了这种有组织的结构。

这种逻辑结构跟人的思维相当匹配，或者更合适的说法是，到如今我们已经完全习惯了这种组织方式，所以别的存储设备也提供了同样的组织方式，哪怕是它们使用了完全不同的物理方法来实现存储。比如说，CD-ROM 或者 DVD 使用了看起来跟硬盘上的文件系统一样的方式来储存信息；USB 设备、数码相机和可插存储卡等其他小玩意也都这样；就连现在已经完全淘汰的老古董软盘，在逻辑层次上看起来也完全一样。

在体系结构示意图里，这些设备看上去是通过一组线缆连接在一起的。借用电气工程的术语，这组线缆称为总线。实际上，计算机内部有好几组总线，每组总线都具有适合其功能的特性。比如 CPU 和内存之间的总线，线路短，传输快，但是价格贵；而连接到耳机插孔的总线，线路长，传输慢，但是价格便宜。有些总线在机箱外也露出了一部分，比如无所不在的通用串行总线，也就是把外设插入计算机所用的 USB 总线。

手机没有硬盘，但是有非易失性的闪存，这样它就能够在关机时仍保存电话本、应用程序和其他信息。手机能连接的外部设备没那么多，但还是可能有蓝牙、耳机和外部话筒插孔和 USB 接口等。

由于计算机里面的很多东西都抽象成了逻辑结构，所以能实际观察和触摸硬盘、集成电路芯片、制造芯片所用的晶圆等东西对于学习计算机是很有用的，而观察一些设备的进化史也很有趣。比如现在的笔记本电脑硬盘跟 10 年前的没什么区别，只是容量增大了 10 倍或者 100 倍，但从外表根本看不出来。另一方面，承载计算机部件的电路板却能看出明显的发展。部件的数量在减少，因为更多的电路被做到了内部，布线更加精细，电路的引脚比起 20 年前多了很多，也密集了很多。下图展示了一块 20 世纪 90 年代后期的台式机电路板，CPU、内存等部件安装或者插入到电路板上，通过反面的印刷线路连接起来。

计算机里的电子线路是由大量基本元件搭建起来的，但基本元件的类型却只有很少几种。其中最重要的一种是逻辑门电路，用来根据一个或两个输入值计算一个输出值，也就是用输入的电压或电流信号来控制输出的电压或电流信号。只要把足够多的门电路用正确的方式连接起来，就能执行任何计算。查尔斯·佩措尔德的《编码》是介绍这方面知识的好书，还有一些网站用图形动画的方式演示逻辑电路如何进行数学运算和其他计算。

2『已下载书籍「2020004编码」和原书「2020004Code」。』

当今最重要的电路元件是晶体管，是 1947 年由约翰·巴丁、瓦尔特·布拉顿和威廉·肖克利在贝尔实验室发明的，他们因此获得了 1956 年的诺贝尔物理学奖。在计算机里面，晶体管基本上就是个开关，也就是用电压控制电流通断的设备。任何复杂系统都可以构建在这么简单的基础之上。

门电路过去是用分立元件搭建的。制造 ENIAC 的时候，用的是跟灯泡差不多大小的电子管，而 20 世纪 60 年代的计算机则用的是铅笔上橡皮头那么大的单独的晶体管。下图展示了第一颗晶体管的仿制品（左边）、电子管和封装起来的 CPU 芯片。电子管大约 10 厘米长，CPU 实际的电路部分是在中间，大约 1 平方厘米。图上这么大的现代 CPU 芯片里则可以集成上亿颗晶体管。

如今的逻辑门电路是创建在集成电路上的。集成电路（integrated circuits）缩写为 IC，通常也称为芯片或微芯片。集成电路在一个平面里（很薄的硅片）包含了电路板的所有部件和布线，通过一系列复杂的光学和化学流程制造出来，这样就得到了没有分立元件也没有传统布线的电路。因此，集成电路比分立部件的电路小得多也可靠得多。芯片是在直径 12 英寸（30 厘米）的晶圆上批量制造的，然后把晶圆切割成独立的芯片，单独包装。芯片通常安装在比它大很多的封装壳上，用几十到几百条引脚连接到系统的其他部分。下图展示了一片封装起来的集成电路。实际的电路部分在中心，大约 1 平方厘米。

集成电路是 1958 年由罗伯特·诺伊斯和杰克·基尔比各自独立发明的。诺伊斯在 1990 年逝世，基尔比则在 2000 年因此获得了诺贝尔物理学奖。集成电路是数字电子设备的主角，但其他技术也发挥了作用，包括硬盘中的磁存储、CD 和 DVD 中的激光、光纤网络中的激光等。过去 50 年里，所有这一切都在尺寸、容量和价格方面发生了惊人的改进。

1965 年的时候，戈登·摩尔，也就是后来的 Intel 公司联合创始人及长期 CEO，发表了一篇文章《在集成电路里填入更多部件》。他注意到，随着技术的发展，在给定大小的集成电路内部可以制造的设备（主要是晶体管）数量大约每年翻一番。后来他又把这个速率修正为两年，另外有些人则认为是 18 个月。由于计算能力大体上可以用晶体管数量来代表，这就意味着计算能力只要两年或更短时间就能翻倍，也就是说，20 年下来可以翻十番，集成度提高 2 的 10 次方也就是大约 1000 倍，经过 40 年则可以提高 100 万倍或更多。

这种指数式增长，也就是通常说的摩尔定律，已经持续了 50 年，于是当今集成电路里的晶体管数量早已超过了 1965 年那时候的 100 万倍。巨大的量变引发了质变。摩尔定律的实际图表，尤其是处理器芯片的数据，显示出晶体管数量从 20 世纪 70 年代早期 Intel 8008 CPU 的几千个晶体管，已经发展到现在廉价家用笔记本电脑的处理器的上亿个。

用来描述电路规模的基本数值是集成电路里的特征尺寸（即其中的最小尺寸），比如导线的宽度。在过去的很多年里，这个数值在稳步缩减。我在 1980 年设计的第一片集成电路（也是我设计的唯一一片）用的是 3.5 微米的特征尺寸；如今的很多集成电路，特征尺寸是 32 纳米，也就是 32 米的 10 亿分之一，下一步将会是 22 纳米。对比一下，一张纸的厚度或者一根头发的粗细是 100 微米，即十分之一毫米。

1『之前在电脑报里看到的，CPU多少多少 nm 工艺，知道就是特征尺寸，即导线的宽度。』

集成电路的设计和制造是相当复杂的业务，竞争异常激烈。而且制造运行（生产线）也很昂贵，新建的工厂可以轻而易举花掉几十亿美元。如果一个公司的技术和资金跟不上，就会在竞争中严重处于劣势。如果一个国家没有这样的资源，就要为了这些技术依赖外国，存在严重的战略问题。

到某个阶段，摩尔定律会失效。以前曾多次有人断定摩尔定律的极限已到来，但后来又发现了突破极限的方法。然而现在，我们已经到了这样的阶段：有的电路里仅包含极少数原子，这么小的结构已经很难控制。CPU 速度已经不再每两年翻一番（部分原因是芯片越快散热越多），但是内存容量仍然在按这个规律翻倍。与此同时，现在的处理器在一片芯片里可以有多颗 CPU。

比较一下今天的个人电脑和 1981 年最初的 IBM PC，对比是惊人的：第一代 PC 的处理器主频是 4.77 MHz，现在一片 2.3 GHz CPU 的时钟频率快了大约 500 倍；第一代 PC 有 64 千字节内存（「千」缩写为 K），现在一台 4 GB 内存的计算机大约是它的 6 万倍；第一代 PC 至多有 750 KB 软盘，没有硬盘，现在机器的磁盘空间则增加了 100 万倍；第一代 PC 的显示器是 11 英寸的，只能在黑色背景上显示 24 行 80 列的绿色字符，而我写这本书的时候所用的 24 英寸显示器能显示 1600 万颜色；在 1981 年，买一台配备了 64 KB 内存、160 KB 单软驱的 PC 要花 3000 美元，相当于 30 年后的 5000-10000 美元，而现在买一台 2 GHz 处理器、4 GB 内存、400 GB 硬盘的笔记本电脑只要花几百美元。

1『老的显示器显示的是 80 列的绿色字符。原来这就是代码习惯设置 80 个字符一行的来源，至少算一个来源，还有个原因是方便多屏显示，细长的比较合适。』

20 世纪计算机科学的伟大发现之一是，现在的数字计算机、最初的 PC 以及再往前体积更大、计算能力更弱的老式计算机器，它们在逻辑或者功能上的特性是完全一样的。如果我们不考虑速度、存储容量这些因素，这些计算机可以做完全一样的计算。

## 0102. 比特、字节与信息表示

计算机表示信息的三个基本思想。首先，计算机是数字处理器。它们存储和处理离散的信息，这些信息表现为不连续的块，具有不连续的值，基本上就是一个个数值。而与之相对的模拟信息，则是平滑变化的值。其次，计算机用比特表示信息。比特就是二进制数字，即一个非 0 即 1 的值。计算机中的一切都用比特来表示。计算机内部使用二进制，而不是人们所熟悉的十进制。再次，较大的信息以比特组表示。数值、字母、单词、姓名、声音、照片、电影，以及处理这些信息的程序所包含的指令，都是用比特组来表示的。

为什么用二进制而不用十进制？因为制造只有两种状态（如开和关）的物理设备，比制造有十种状态的设备更容易。这种简单的性质在数不清的技术中都得到了利用，比如：电流（流动或不流动）、电压（高或低）、电荷（存在或不存在）、磁性（南或北）、光（亮或暗）、反射率（反光或不反光）。约翰·冯·诺依曼很早就清楚地认识到了这一点，他在 1946 年说过：「我们储存器中最基本的单位自然是采用二进制系统，因为我们不打算度量电荷的不同级别。」

为什么我们要知道或者要关心二进制数呢？这个问题问得好。至少在我的课上，理解另一种不熟悉的数制，相当于做了一次量化推理的练习，而有了这个训练之后，对我们习以为常的十进制的理解也将更深一层。除此之外，另一个意义在于，比特的数量在一定程度上揭示了涉及的空间、时间或者复杂性。再从根本上说，计算机值得我们花时间去理解，而二进制正是其运作的核心所在。

现实生活中也能找到一些与计算机无关的应用二进制的场景，或许是因为人们都认为大小、长短的加倍、减半是一种自然而然的运算。比如，高德纳在《计算机程序设计艺术》中描述了 14 世纪英国的酒器单位，分为 13 个二进制量级：2 吉耳是 1 超品（chopin），2 超品是 1 品脱，2 品脱是 1 夸脱，依此类推，直到 2 百瑞尔（barrel）是 1 豪格海（hogshead），2 豪格海是 1 派普（pipe），2 派普是 1 坦恩（tun）。这些单位中差不多还有一半仍然在英制液体度量体系中使用。当然，其中一些很令人陶醉的词，比如费尔金（firkin）和基尔德坎（kilderki）（2 费尔金是 1 百瑞尔），今天已经很难得见了。

2『已下载书籍「2020007计算机程序设计艺术」和原文书籍「2020007The_Art_of_Computer_Programming」。』

首先，我们谈一谈模拟与数字的区别。「模拟」（analog）与「类似的」（analogous）词根相同，表达的意思是：值随着其他因素变化而平滑变化。现实生活中的很多事物都具有模拟性质，比如水龙头或汽车方向盘。如果你想让车转个小弯，轻轻打一打方向盘即可，打多打少由你自己来定。拿它跟转向灯作个比较，后者要么开要么关，没有中间状态。在模拟装置中，某些事物（汽车转弯幅度）会随另一些事物（方向盘转动幅度）的变化平滑而连续地变化。变化过程没有间断，一个事物的微小变化就意味着另一个事物的微小变化。

数字系统处理的是离散值：可能的取值是有限的（转向灯只可能是关闭的或在左右方向打开）。某个事物小小的变化，要么不引发其他事物变化，要么就引发其他事物的突变，使其从一个离散的值跳到另一个离散的值。

比如手表。「模拟」手表有时针、分针和秒针，秒针每分钟转一圈。虽然现代的手表都由内部的数字电路控制，但时针和分针仍然随着时间流逝而平滑移动，而且三根表针都能走遍所有可能的位置。数字手表或手机时钟显示的时间只有数值。显示屏每秒变化一次，每分钟更新一次分钟的值，但不会显示分钟的小数位。

有人要问，为什么用数字而不用模拟呢？我们这个世界可是模拟的呀，而且手表、速度表等等模拟设备也更容易让人一目了然。但不管怎样，很多现代的技术都是数字的，而且我们这本书也是在讲述数字的故事。外部世界的数据 —— 声音、图片、运动、温度，等等一切，在输入端都会尽可能早地转换为数字形式，而在输出端则会尽可能晚地转换回模拟形式。原因就在于数字化的数据容易处理，无论最初来源是什么，数字化数据都可以用多种方式来存储、传输和处理，但模拟信息则不行。第 9 章将会介绍，通过删除冗余和不重要的信息，还可以压缩数字化信息。为了安全和隐私可以对它进行加密，可以将它与其他数据合并，可以复制它而不出错，可以通过互联网把它发送到任何地方，可以将它保存到几乎无限种设备中。而对于模拟信息，上述很多做法是根本行不通的。

与模拟系统相比，数字系统还有另一个优势，就是它更容易扩展。比如说，给模拟天文馆增加一颗新发现的星星，专业人员必须辛苦地做出光照效果来；而在数字天文馆，只要在数据文件里添加一行信息即可。我的数字手表可以连续不断地以百分之一秒显示时间流逝，而要让模拟手表做到这一点可就太难了。不过，模拟系统有时候也有它的优势，像泥版、石雕、羊皮纸、图书和照片等古老的媒体，都经历了数字格式未曾经历过的时间考验。

把照片转换为数字形式，应该是最容易想象的了。假设我们给自家的小猫拍张照片。胶卷相机的成像，是通过把胶片感光区曝露给被拍物体反射的光线实现的，胶片上不同区域接收到的不同颜色的光量不同，从而影响胶片上的染料。在胶片显影、印相时，彩色染料数量决定了显示出来的颜色变化。

对数码相机来说，镜头把影像聚焦到一块位于红、绿、蓝滤镜后面的矩形感光器阵列上，感光器由微小的光敏探测器组成。每个探测器存储一定数量的电荷，与落在它上面的光量成正比。这些电荷被转换为数字值，照片的数字表示就是这些表现光强度的数值序列。探测器越小，数量越多，电荷测量的结果就越精细，数字化图像就能越精确地反映原始的影像。

传感器阵列的每个单元都由一组能够捕获红、绿、蓝光的探测器构成，每个单元对应一个像素，即像元。3000×2000 像素的图像，包含 600 万个像元，或 600 万像素，对今天的数码相机而言并不算大。像素的颜色通常由三个值表示，分别代表红、绿、蓝光的强度，因此 600 万像素的图像总共要存储 1800 万个颜色值。屏幕在显示图像时，使用的是红、绿、蓝光三元组的阵列，其亮度与像素亮度一致。如果你用放大镜仔细观察手机或电脑屏幕，很容易看到每个独立的彩色块。

第二个模数转换的例子是声音，尤其是音乐。之所以说音乐是个不错的例子，原因在于以它为代表的数字信息的所有权，第一次引起了社会、经济和法律上的广泛关注。数字音乐与唱片或磁带不同，你可以在自己家的计算机里无限次地复制它，完全免费，而且还可以通过互联网把它复制发送到世界的任何角落，不会有任何音质损失，同样完全免费。唱片业把这当成了严重的威胁，试图通过法律或政治手段阻止数字音乐的拷贝。

什么是声音？音源通过振动或快速运动引起空气压力的波动，人的耳朵把这种压力变化转换为神经活动，经大脑解释之后就形成了「声音」。1870 年代，托马斯·爱迪生制造了一个叫做「留声机」的机器，这台机器能把声波转换为蜡筒上类似的螺旋沟槽，而通过这些沟槽又能再次创造出同样的气压波动来。把声音转换为沟槽就是「录音」，而从沟槽换回到气压波动就是「回放」。爱迪生的发明迅速地得到改进，1940 年代就出现了密纹唱片（long-playing record）或简称 LP，而且至今还在使用（尽管数量已经不多了）。麦克风随着时间推移把变化的声压转换为变化的值并记录下来，然后根据这些值在乙烯基的盘片上压制出与声压一致的螺旋沟槽。播放 LP 时，唱针随着沟槽起伏，其运动轨迹被转换为波动的电流，电流经过放大后驱动扬声器或耳机，通过它们的振动薄膜产生声音。

把空气压力随时间的变化形象地绘制出来并不难。其中压力可以用任何物理方法来表示，在此我们假设用电路中的电压。当然，电流、光的亮度，以及爱迪生发明的留声机中的纯机制装置都没有问题。

图中声波的高度表示声音强度或大小，水平方向的坐标轴表示时间：每秒钟声波的数量就是声调或频率。假设我们以固定时间间隔连续测量这条曲线的高度（在这里就是电压值），就会得到下图所示的这些垂直线条。

测量得到的数值连接起来与曲线近似，测量越频繁，越准确，结果也就越吻合。测量得到的数值序列是波形的数字化表示，可以存储、复制、操作它们，也可以把它们发送到任何地方。如果有设备把这些数值转换成对应的电压或电流，然后再通过电压或电流驱动音箱或耳机，就能够实现回放。从声波到数值是模数转换，相应的设备叫 A/D 转换器；反过来当然是数模转换，或者叫 D/A。转换过程并不是完美无缺的，两个方向的转换都会损失一点信息。但大多数情况下，这种损失是人所觉察不到的。

与 LP 唱片上的模拟沟槽不同，CD 用长长的螺旋状轨道在盘面的一侧记录数值。轨道上任意一个区块的表面要么平滑，要么是一个微小的凹坑。这些下凹或平滑的区块就是用来编码声波的数字值的，每个区块是一位，连续的多位表示二进制编码中的一个数值。光盘旋转时，一束激光照射到轨道上，而光电传感器则检测每个区块上反射回来的光量多少。如果光量不多，说明是凹坑；如果反射光很强，说明不是凹坑。标准 CD 编码采样率为每秒 44 100 次，而每次采样获得两个振幅值（立体声的左、右声道），精确度为 65 536（即 216，这并非巧合）分之一。轨道上的每个区块非常非常小，小到只有用显微镜才能看见，一张 CD 的表面上有 60 亿个小区块。（DVD 中的区块更小，由于区块更小，激光束频率更高，DVD 的存储容量近 5GB，而 CD 大约为 700MB。）

音频 CD 的出现几乎让 LP 没有了立足之地，相比之下，CD 的优点实在太多了：落上点灰尘也不用太担心了，更没有磨损一说，而且绝对小巧。但到了我写这本书的时候，LP 开始在某种程度上复苏，流行音乐 CD 的人气则日渐衰退。有朝一日，CD 很可能也会像 LP 一样变成古董，这倒让我很高兴，因为我收藏的音乐全部都是 CD 格式的。我现在完全拥有它们，而它们的存在将比我的生命更久远。CD 还有第二个用途，那就是作为存储、分发软件及数据的介质，不过这个功能已经被 DVD 取代，而 DVD 很可能又会被下载所取代。

声音和图片经常会被压缩，因为这两种媒体包含很多人类根本感知不到的细节。对于音乐，典型的压缩技术是 MP3，大约能把音频文件的体积压缩到原来的十分之一，同时几乎让人感觉不到音质下降。对于图片，最常用的压缩技术是 JPEG（是制定该标准的联合图像专家组  —— Joint Photographic Experts Group 的英文字头），它的压缩率也能达到 10 倍甚至更高。上文提到很多处理对数字信息能做，但对模拟信息却很难（或不可能），压缩就是一个例子。

1870 年代，摄影师埃德沃德·迈布里奇向世人证明，快速连续地显示一系列静态图片能够创造出运动的错觉。今天，电影显示影像的速度是每秒 24 帧，而电视大约是 25 到 30 帧，这个速度足以让人的眼睛把顺序播放的影像感知为动画。而通过组合（并同步）声音和影像，就可以创造出数字电影。而利用压缩技术减少空间占用，则催生了包括 MPEG（代表 Moving Picture Experts Group）在内的标准电影格式。实际上，视频的表示要比单纯的音频表示更复杂，一方面是它本身就复杂，另一方面很大程度上还因为它受到了电视的拖累，而电视在其存在的大部分时间内都是模拟的。模拟电视在世界范围内正逐渐被淘汰，而美国 2009 年已经将广播电视切换成了数字信号。

还有一些信息很方便以数字形式来表示，因为除了想好如何表示它之外，根本不需要做什么转换。比如这本书中的文字、字母、数字和标点符号，我们称为其普通文本。可以为其中每个字母指定一个唯一的数值，如 A 是 1，B 是 2 等等，这不就是一种数字化表示方法嘛。而事实也正是如此，只不过在表示标准中，A 到 Z 用的是 65 到 90，a 到 z 用的是 97 到 122，数字 0 到 9 用的是 48 到 57，而标点符号等其他字符用的是其他数值。这个表示标准叫做 ASCII，即 American Standard Code for Information Interchange（美国信息交换标准代码）。

不同地区有不同的字符集标准，但也有一个世界通用的标准叫 Unicode，它为所有语言的所有字符都规定了一个唯一的数值。这是一个非常庞大的字符集，人类的创造力是无穷无尽的，但在建立自身书写系统方面却很少有规则。目前，Unicode 涵盖的字符远远超过 100 000 个，而且这个数字还在稳步增长。可想而知，Unicode 中的大部分都是包括中文在内的亚洲字符集，但决不限于此。要了解 Unicode 都包含哪些字符集，可以访问 unicode.org，这个站点内容丰富。

3『[Unicode – The World Standard for Text and Emoji](https://home.unicode.org/) 和 [Overview – Unicode](https://home.unicode.org/basic-info/overview/)』

 表示数字信息的最基本单位是比特（bit）。英文 bit 是合并 binary digit（二进制数字）之后造出来的，造这个词的人是统计学家约翰·图基，时间是 1940 年代中期。（图基还在 1958 年发明了单词 software ——  软件。）
 
 一个比特表示开 / 关、真 / 假之类的二选一的情形没有问题，但我们经常还要面对更多选项，表示更复杂的事物。为此，可以使用一组比特，然后为不同的 0 和 1 的组合赋予不同的含义。比如，可以用两个比特来表示大学四年：新生（00）、大二（01）、大三（10）和毕业班（11）。如果再多考虑一种情况，比如研究生，那两个比特就不够用了，因为两个比特只有 4 种组合，没有第五种可能。但是三个比特没问题，实际上三个比特能表示 8 种不同的情况，这样我们就可以把教师、教工和博士后都包含进来。三个比特的全部组合为：000、001、010、011、100、101、110 和 111。比特数与它们所能表示的情况数之间有一个关系，很简单：N 个比特能表示 2^N 种组合，即 2×2×2…×2（乘 N 次）。
 
 由于计算机中的一切都是以二进制形式来处理，因此像大小、容量等概念一般都是用 2 的几次幂来表达的。如果有 N 比特，那么就有 2^N 种可能的值，所以知道 2 的幂是多少（比如到 2^10）是很有用的。但随着数值越来越大，完全记住它们也没有什么必要。好在有一种简便的方法，可以得到它们的近似值：2 的某次幂与 10 的某次幂接近，它们的对应关系严格有序，容易记忆.
 
 1『2 的 10 次方、20 次方、30 次方、40 次方、50 次方（每个幕隔 10 次方），分别对应于 10 的 3 次方、6 次方、9 次方、12 次方、15 次方（每个幕隔 3 次方）。这个真是相当的有趣啊，哈哈。』
 
 （这个对照表最后包含的表示大小的单位叫「拍」或 10^15，其英文单词发音不是「皮」而是「拍」。另外，书后附有一个更全的词汇表，列出了更多单位。）随着数值增长，这个近似值的误差也会增大，不过到了 10^15 这么大的时候误差也就 12.6%，所以还是可以在很大范围内使用的。经常会有人混淆上述 2 的幂与 10 的幂之间的关系（有时候是想用来支持他们的观点），于是 kilo 或 1K 可能是指 1000，但也可能指 2^10 即 1024。一般来说，这种混淆导致的误差并不大，因此在涉及很大的比特数时，用 2 和 10 的幂来做心算没什么问题。
 
 在所有现代计算机中，数据处理及内存组织的基本单位都是 8 个比特。8 比特被称为 1 字节，而字节（byte）这个词是由 IBM 的计算机设计师维尔纳·巴克霍尔兹（Werner Buchholz）在 1956 年发明的。一个字节可以编码 256 个不同的值（28，即 8 个 0 和 1 的所有不同组合），这个值可以是一个 0 到 255 间的整数，也可以是 ASCII 字符集中的一个字符，或者其他什么。通常，为了表示更大或更复杂的数据，需要用到多个字节的字节组。两个字节有 16 比特，也就是 16 位，可以表示 0 到 2^16-1（65 535）之间的数值。两个字节也可以表示 Unicode 字符集中的一个字符。
 
 1『Unicode 字符是 16 位编码的。』
 
 这是两个字符，即「东京」，每个字符占两个字节。四个字节是 32 位，既可以表示「东京」，也可以表示最大直至 2^32-1 的值，这个最大值大约是 43 亿。用一组字节表示什么都可以，但 CPU 自己特别定义了一些适中的字节组（比如表示不同大小的整数），以及处理这些字节组的指令。

二进制写起来太长了，比十进制格式长三倍还多，因此我们常用另一种替代数制，即十六进制。十六进制的基数是 16，因此也就有 16 个数字（就像十进制有 10 个数字，二进制有 2 个数字一样），分别是 0、1、…、9、A、B、C、D、E、F。每个十六进制数字表示 4 个比特，对于一般的数值，十六进制 0 相当于二进制 0000，依此类推，十六进制 9 相当于二进制 1001。接下去，十六进制 A 相当于二进制 1010（十进制 10），十六进制 B 相当于二进制 1011（十进制 11），依此类推，十六进制 F 相当于二进制 1111（十进制 15）。

除非你是程序员，否则能看到十六进制数的机会并不多。一个例子就是网页中的颜色值。前面说过，计算机中一个像素的颜色值大都使用三个字节来表示，一个表示红色分量，一个表示绿色分量，最后一个表示蓝色分量，这就是所谓的 RGB 编码。红绿蓝三个组分分别用一个字节表示，因此红色分量就有 256 种可能的值，三个组分中的绿色分量也有 256 种可能的值，同样，三个组分中的蓝色分量也有 256 种可能的值。于是一个像素可能的颜色值就是 256×256×256 种，听起来好多啊。我们可以用 2 和 10 的幂来简单估计一下这个数有多大。这个数是 2^8×2^8×2^8，即 2^24 或 2^4×2^20，大约是 16×10^6，即 1600 万。在描述计算机显示器的情况下，你可能听说过这个数（超过 1600 万种颜色！）。

一个深红色的像素可以表示为 FF0000，换句话说，就是红色分量最多，没有绿色和蓝色；而一个鲜蓝色（并非深蓝色），即类似很多网页中链接的颜色，可以表示为 0000CC。黄色是红加绿，因此 FFFF00 就是最深的黄色。阴影的灰色具有等量的红、绿、蓝组分，因此一个中等灰度的像素应该是 808080，也就是红、绿、蓝组分的数量都相等。黑色和白色分别是 000000 和 FFFFFF。

有时候，在某计算机的广告中，我们会看到「64 位」这个说法（Windows 7 家庭高级版 64 位）。什么意思呢？计算机在内部操作数据时，是以不同大小的块为单位的，这些块包含数值（32 位和 64 位表示数值比较方便）和地址，而地址也就是信息在 RAM 中的位置。前面所说的 64 位，指就是地址。大约 25 年前，16 位地址升级到了 32 位地址（足够访问 4GB 的 RAM），而现在 32 位又升级到 64 位。我不想预测什么时候会从 64 变成 128，总得过上好一阵子吧，先不必想那么多。

1『解答了很早之前的一个疑问。电脑多少位指的是地址的编码位数。』

关于比特和字节，我们讨论到现在最重要的是必须知道，一组比特的含义取决于它们的上下文，光看这些比特看不出来。一个字节可以只用 1 个比特来表示男或女，另外 7 个空闲不用，也可以用来保存一个不大的整数，或者一个 # 之类的 ASCII 字符，它还可能是另一种书写系统中一个字符的一部分，或者用 2、4 或 8 个字节表示的一个大数的一部分，一张照片或一段音乐的一部分，甚至是供 CPU 执行的一条指令的一部分。

事实上，一个程序的指令就是另一个程序的数据。从网上下载一个新程序，或者从 CD-ROM 或 DVD 中安装该程序时，它就是数据，所有比特将无一例外地被复制一遍。但在运行这个程序时，它的比特会被当成指令，CPU 在处理这些比特时，又会把它们当成数据。

## 0103. 深入了解 CPU

中央处理器如何工作？ 它处理什么，怎么处理？直观来讲，CPU 有一个小型指令系统，包含着它能够执行的基本操作。它可以做算术题，加、减、乘、除，跟计算器一样。它可以从 RAM 中取得要操作的数据，然后再把结果保存到 RAM，与很多计算器中的存储操作一样。CPU 还要控制计算机的其他组件，确保鼠标、键盘等外围设备输入的数据得到响应，让信息在屏幕上得以显示，同时还要控制和协调连接到计算机的其他所有器件。

最重要的是，它可以作出决定 —— 尽管是简单的决定：它可以比较数值（这个数比那个数大吗？） 或者比较其他数据（这段信息与那段信息一样吗？），还能根据结果决定接下来做什么。这一条最重要，因为这意味着 CPU 能做的虽然比计算器多不了多少，但它可以在无人看管的情况下完成自己的工作。正如冯·诺依曼所说的：「要让这种机器完全自动化，即让它在计算开始后不再依赖人工操作。」

由于 CPU 能根据它所处理的数据决定下一步做什么，因此它就能自己运行整个系统。虽然其指令系统并不大，或者说并不复杂，但 CPU 每秒可以执行数十亿次运算，所以它能完成极为复杂的处理。

我想了一下，就管这个编造的机器叫「玩具」计算机吧，因为它不是真的，但又具有真正计算机的很多特性。实际上，它跟 1960 年代末的小型机差不多是一个水平，某种程度上与冯·诺依曼论文中的例子相近。这个玩具有用来存储指令和数据的 RAM，还有一块额外的存储区叫累加器，其容量足以存储一个数值。累加器类似于计算器的显示屏，保存用户最近输入的数值，或者最近计算的结果。玩具还有一个指令表，只包含 10 个指令，都是前面提到过的基本操作。

CPU 反复执行简单的循环：从存储器中取得下一条指令，该指令正常情况下保存在存储器的下一个位置，但也可以是使用 GOTO 或 IFZERO 指定的位置；对指令进行译码，也就是搞清楚这条指令要干什么，然后为执行该指令做好准备；执行指令，从存储器中取得信息，完成算术或逻辑运算，保存结果，总之是执行与指令匹配的组合操作；然后再从头取得指令，开始下一次循环。真正的处理器也执行同样的「取指令－译码－执行」循环，只不过为了加快处理速度，还会配备精心设计的各种机制。但核心只有循环，与前面重复把数值加起来的例子一样。

真正计算机的指令比我们玩具计算机的多，但性质相同。比如，有更多移动数据的指令，更多完成算术运算及操作不同大小和类型数值的指令，更多比较和分支指令，以及控制计算机其他组件的指令。典型的 CPU 有几十到数百个不同的指令；指令和数据通常要占用多个内存位置，通常为 2 至 8 个字节。真正的处理器有多个累加器，通常是 16 或 32 个，所以可以保存多个中间结果，而且都是速度极快的存储器。真正的程序与我们的玩具示例相比可谓庞大，有的甚至多达数百万条指令。

计算机体系结构是研究 CPU 与其他计算机组件连接的一门学科。在大学里，它通常是计算机科学和电子工程的交叉领域。

计算机体系结构研究的一个问题是指令集，也就是处理器配备的指令表。是设计较多的指令去处理各式各样的计算，还是设计较少的指令以简化制造并提升速度？体系结构涉及复杂的权衡，要综合考虑功能、速度、复杂性、可编程能力（如果太复杂，程序员将无法利用其功能）、电源消耗及其他问题。用冯·诺依曼的话说：「一般来讲，运算器内在的经济性取决于期望的机器运行速度…… 与期望的简易性或低价位之间的折中。」

CPU 与 RAM 和计算机的其他组件是如何连接的？处理器非常快，通常执行一条指令只需要零点几纳秒。相对而言，RAM 则慢得让人难以忍受 —— 从存储器中取得数据或指令大概要花 25 到 50 纳秒。当然，这里的快指的是绝对速度，而慢则是相对于 CPU 而言。假如 CPU 不必等待数据，那它可能早就执行完上百条指令了。现代计算机会在 CPU 和 RAM 之间使用少量的高速存储器来保存最近使用过的指令和数据，这种高速存储器叫作缓存。如果可以从缓存中找到信息，那么就会比等待 RAM 返回数据快得多。

设计师在设计体系结构的时候也有一套方法，能够让处理器跑得更快。比如，可以把 CPU 设计为交替地取得和执行指令，而同一时刻会有几个指令处于执行过程的不同阶段，这种设计叫做流水线。（与汽车装配线很相似。）结果呢，虽然某个特定的指令仍旧要花同样的时间完成，但其他指令都有机会得到处理，从整体上看完成这些指令则会快很多。另一种方法是并行执行多条互不干扰、互不依赖的指令，就相当于多条平行的汽车装配线。有时候，只要指令的操作不会相互影响，甚至可以不按顺序执行。

另外一种可能是同时运行多个 CPU。今天的笔记本电脑，甚至连手机都已经有多个 CPU 了。英特尔酷睿双核处理器在一块集成电路芯片上集成了两个 CPU（核心）。在一块芯片上集成越来越多的处理器已经成为明显的趋势。由于集成电路特征尺寸越来越小，因而可以集成在一块芯片上的晶体管数量必将越来越多，这些晶体管可以构成更多 CPU，也可以构成更多缓存。

处理器应用的领域决定了设计者要权衡哪些要素。很长时间以来，处理器主要的应用领域是桌面计算机，而桌面环境下的电源和物理空间都比较充足。因此，设计者只要专注于让处理器尽可能地快就好了，电源是用之不竭的，而散热只要多加风扇就行。笔记本电脑要求的权衡要素有了明显不同，一方面空间有限，另一方面在不插电的情况下，笔记本要靠沉重又昂贵的电池供电。其他方面条件不变，笔记本处理器必然要相对慢一些，耗电少一些。

手机和其他超轻便设备进一步提高了设计要求，因为尺寸、重量和电源各方面都有了更多限制。此时，单靠小范围调整设计是行不通的。虽然英特尔是台式机和笔记本处理器的主要供应商，但几乎所有的手机都使用「ARM」处理器，因为它耗电更少。ARM 处理器是指获得英国 ARM Holdings 公司许可制造的处理器。

比较不同 CPU 的速度并不是特别有意义。即便是最基本的算术运算，其处理方式也可以完全不同，很难直接比较。比如，同样是计算两个数的和并保存结果，有的处理器需要用三个指令（比如我们的玩具计算机），有的则需要两个，而有的可能只需要一个。有的 CPU 具有并行处理能力，或者说能够同时执行多条指令，从而让这些指令在不同阶段上执行。为了降低处理器的耗电量，牺牲执行速度，甚至根据是不是电池供电动态调整速度都是很常见的。对于某个处理器比另一个处理器「更快」的说法，不必太当真，因为很多情况下都要具体问题具体分析。

说到这里，有必要花点时间简单介绍一下缓存，这是一个在计算领域中广泛适用的思想。在 CPU 中，缓存是容量小但速度快的存储器，用于存储最近使用的信息，以避免访问 RAM。通常，CPU 会在短时间内连续多次访问某些数据和指令。例如，加法计算程序循环体中的多条指令，对每个输入值都要执行一遍。如果这些指令存储在缓存中，就不用在每次循环时都从 RAM 读取它们，这会让程序的速度快 50 倍。类似地，把 Sum 存储在数据缓存中也能提高访问速度。

典型的 CPU 有两到三个缓存，容量依次增大，但速度递减，一般称为一级缓存、二级缓存和三级缓存。最大的缓存能存储以兆字节计的数据（我的 Macbook 有 3MB 的二级缓存），大多数 CPU 的指令和数据缓存都是独立的。缓存之所以有用，关键在于最近用过的信息很可能再次被用到，而把它们存储在缓存里就意味着减少对 RAM 的等待。缓存通常会一次性加载一组信息块，比如只请求一个字节，但会加载 RAM 中一段连续的地址。因为相邻的信息也可能被用到，要用的时候它们同样已经在缓存里了，换句话说，对邻近信息的引用也不需要等待。

除了发现性能提升之外，用户是感受不到这种缓存的。但缓存的思想却无处不在，只要你现在用到的东西不久还会用到，或者可能会用到与之邻近的东西，那运用缓存思维就没错。CPU 中的多个累加器本质上也是一种缓存，只不过是高速缓存而已。RAM 也可以作为磁盘的缓存，而 RAM 和磁盘又都可以作为网络数据的缓存。计算机网络经常会利用缓存加速访问远程服务器，而服务器本身也有缓存。

在使用浏览器上网的时候，你可能见过「清空缓存」的字眼。对网页中的图片和其他体积较大的资源，浏览器会在本地保存一份副本，因为再次访问同一网页时，使用本地副本比重新下载速度快。缓存不能无限地增长，因此浏览器会悄悄地删除旧项目，以腾出空间给新的，它还给你提供了删除所有缓存内容的命令。

你自己随时可以检验缓存的效果。比如可以做下面两个实验，一是打开 Word 或 Firefox 等大程序，看看从启动到加载完成并可以使用要花多长时间。然后退出程序，立即重新启动它。正常情况下，第二次启动的速度会明显加快，因为程序的指令还在 RAM 里，而 RAM 正在充当磁盘的缓存。使用其他程序一段时间后，RAM 里会填满该程序的指令和数据，原先的程序就会从缓存中被删除。

二是在谷歌里搜索几个不太常见的单词或短语，注意谷歌查询结果要花多长时间。接着再搜索同样的关键词。返回搜索结果的时间会明显缩短，因为谷歌已经在其服务器上缓存了搜索结果。这个缓存对其他搜索相同关键词的人也有好处，因为缓存在谷歌的服务器上，不在你的机器里。要验证缓存在谷歌服务器上，可以在你搜索完之后，让别人在他们自己的计算机上搜索同样的关键词。虽然不能完全保证，但一般来说第二次搜索速度会快很多。

人们很容易认为计算机不是 PC 就是 Mac，因为那是我们最常见到的。实际上，还有很多其他类型的计算机。这些计算机无论大小，都具有相同的核心特性，即都能完成逻辑运算，并且都具有类似的体系结构，只不过在设计的时候会不同程度地考虑成本、供电、大小、速度等因素。手机和平板电脑也是计算机，它们运行操作系统并支持更加丰富多样的运算环境。比这还小的系统是嵌入式系统，日常生活里能见到的几乎所有数字设备里都有嵌入式系统，比如数码相机、摄像机、GPS 导航系统、家电、游戏机，等等 。

更大的计算机在很多年前就已经实现多个 CPU 共享内存了。如果能把大任务分解成小任务，而分解后的小任务又可以通过不同 CPU 协作完成，CPU 相互之间不会出现太长的等待，也不会有太多的相互干扰，那么就能以这种方式加快完成大任务。除了在大型系统中广泛应用，这种集成多个处理器的多核芯片在个人计算机中也已经司空见惯，而且未来很可能会普及。

超级计算机往往有大量的处理器和大量的内存，这些处理器本身可能带有一些特殊指令，在处理某种数据时，它们比通用的处理器速度更快。今天的超级计算机通常是高速计算机集群，CPU 仍然是普通的 CPU，并没有什么特殊的硬件。网站 [Home | TOP500 Supercomputer Sites](https://www.top500.org/) 每六个月就重新公布一次全世界最快的 500 台计算机。最快速度的纪录不断被打破，几年前还能跻身排行榜前几名的计算机，今天可能已经在榜单上找不到了。2011 年 6 月最快的计算机有 50 多万个 CPU，每秒可以执行 8×10^15 次数学运算。

分布式计算指的是很多更加独立的计算机（比如不共享内存），而且地理上更加分散，甚至位于世界的不同地方。这样一来，通信更加成为瓶颈，但却能够实现计算机之间的远距离协作。大规模的 Web 服务，比如搜索引擎、在线商店和社交网络，都是分布式计算系统。在这种系统中，数以千计的计算机协作，可以为海量用户迅速地提供结果。

所有这些计算系统都有相同的基本原理。它们都使用通用处理器，可以通过编程完成无穷无尽种任务。每个处理器都有一个有限的简单指令表，能够完成算术运算、比较数据、基于前置计算结果选择下一条指令。不管物理结构的变化让人多么眼花缭乱，它们的一般体系结构从 1940 年代至今并没有太大的变化。

或许很难想象，这些计算机都具有相同的逻辑功能，可以完成一模一样的计算（暂且不论对速度和内存的要求）。1930 年代，这个结果就已经被几个人分别独立地证明过，其中包括英国数学家艾伦·图灵。对于非专业人员，图灵的手段最容易理解。他描述了一个非常简单的计算机（比我们的玩具计算机还简单），展示了它能够计算任何可以计算的任务。他描述的这种计算机，我们今天叫做图灵机。然后，他展示了如何创建一种图灵机，模拟其他图灵机，这种图灵机现在被称为通用图灵机。写一个模拟通用图灵机的程序很容易，而写一个程序让通用图灵机模拟真实的计算机也是可能的（尽管不容易）。实际上，从能够计算什么的角度讲，所有计算机都是等价的，尽管运行速度明显不可能等价。

第二次世界大战期间，图灵从理论转到实践：他领导开发了用于破译德军情报的计算机。1950 年，他发表了一篇名为「计算机器与智能」（Computing machinery and intelligence）的论文，其中提出一个测试（即今天所谓的图灵测试），人们可以通过该测试来评估计算机是否能表现出人类的智能。想象一下，一台计算机和一个人，通过键盘和显示器与另一个提问者交流。通过问答，提问者能确定哪个是人，哪个是计算机吗？图灵的想法是，如果不能明显地将二者区分开，那么计算机就表现出了智能的行为。

2『已下载原论文「2020002Computing-machinery-and-intelligence」。』