## 记忆时间

## 卡片

### 0101. 反常识卡 ——

这本书的主题核心，就是最大的反常识卡，并且注意时间脉络。

### 0201. 术语卡 —— JSON 和 JSON Line 格式的区别

CSV 和 XML 文件很流行，因为可以被 Excel 直接打开。JSON 文件很流行是因为它的开放性和与 JavaScript 的密切关系。JSON 和 JSON Line 格式的区别是 .json 文件是在一个大数组中存储 JSON 对象。这意味着如果你有一个 1GB 的文件，你可能必须现在内存中存储，然后才能传给解析器。相对的，.jl 文件每行都有一个 JSON 对象，所以读取效率更高。

### 0301. 人名卡 —— Dimitrios Kouzis-Loukas

Dimitrios Kouzis-Loukas，本书的作者。

Dimitrios Kouzis-Loukas has over fifteen years experience as a topnotch software developer. He uses his acquired knowledge and expertise to teach a wide range of audiences how to write great software, as well. He studied and mastered several disciplines, including mathematics, physics, and microelectronics. His thorough understanding of these subjects helped him raise his standards beyond the scope of "pragmatic solutions." He knows that true solutions should be as certain as the laws of physics, as robust as ECC memories, and as universal as mathematics.

Dimitrios now develops distributed, low-latency, highly-availability systems using the latest datacenter technologies. He is language agnostic, yet has a slight preference for Python, C++, and Java. A firm believer in open source software and hardware, he hopes that his contributions will benefit individual communities as well as all of humanity.

### 0401. 金句卡——

最后根据他写的非常震撼的话语——产生一张金句卡。

### 0501. 任意卡——

行动卡是能够指导自己的行动的卡。

## 目录

0101 Scrapy 介绍

Scrapy 是一个健壮的抓取网络资源的框架，它基于事件的架构，可以让我们进行串联操作，清洗、形成丰富数据，或存入数据库等等，同时不会有太大的性能损耗。

0201 理解 HTML 和 XPath

网页的 HTML、HTML 的树结构和用来筛选网页信息的 XPath。创建可靠的 XPath 表达式从 HTML 抓取信息。

0301 爬虫基础

0501 快速构建爬虫

快速构建爬虫，介绍更多关于爬虫的特点，模拟登陆、更快抓取、使用 APIs、爬 URL 的方法。

## 前言

Scrapy 是全世界网络抓取专家的秘密武器。在专家手中，Scrapy 节省了大量时间，表现出众，花费最少。

Dimitris Kouzis – Loukas 有超过 15 年的软件开发经历。同时他也参与到教学活动中，受众广泛。他精通数个领域，包括数学、物理和微电子。对这些学科的理解帮助使他得到了提高，超越了软件的「实用方案」。他认为，好的解决方案应该像物理学一样确定，像纠错内存一样拥有健壮性，并且像数学原理一样具有通用性。Dimitris 现在正在使用最新的数据中心技术，着手开发分布式、低延迟、高可用性的系统。他运用多个编程语言，但更偏爱 Python、C++ 和 Java。作为开源软硬件的坚定支持者，他希望对独立开发群体和整个人类做出贡献。

Lazar Telebak 是一名网络开发自由从业者，专精于网络抓取和利用 Python 库和框架进行网页索引。他的主要工作涉及自动化、网络抓取和数据导出，导出为 CSV、JSON、XML 和 TXT 等多种格式，或是导出到 MongoDB、SQLAlchemy 和 Postgres 等数据库。他还会使用网络前端技术：HTML、CSS、JS 和 Jquery。

第 1 章，Scrapy 介绍，向你介绍这本书和 Scrapy，使你对 Scrapy 框架和后面章节有清醒的认识。

第 2 章，理解 HTML 和 XPath，让爬虫初学者掌握基础的网页相关技术，以及后面会使用到的技术。

第 3 章，爬虫基础，我们会学习如何安装 Scrapy 和抓取网站。通过一步步搭建实例，让读者理解方法和背后的逻辑。学过这一章，你就可以抓取大部分简单站点了。

第 4 章，从 Scrapy 到移动应用，我们如何使用爬虫生成数据库和向移动应用提供数据支持。通过这一章，你会明白如何用网络抓取获益。

第 5 章，快速构建爬虫，介绍更多关于爬虫的特点，模拟登陆、更快抓取、使用 APIs、爬 URL 的方法。

第 6 章，Scrapinghub 部署，如何将爬虫部署到 Scrapinghub 云服务器，以尝试更快的可用性、简易部署和操作。

第 7 章，配置和管理，详细介绍利用 Scrapy 的配置文件对爬虫进行改进。

第 8 章，Scrapy 编程，使用底层 Twisted 引擎和 Scrapy 架构扩展爬虫功能。

第 9 章，如何使用 Pipelines，在不明显降低性能的条件下，举例实现 Scrapy 连接 MySQL、Elasticsearch、Redis、APIs 和应用。

第 10 章，理解 Scrapy 的性能，Scrapy 的工作机制，如何提高 Scrapy 的性能。

第 11 章，Scrapyd 分布式抓取和实时分析，最后一章介绍如何在多台服务器中使用 Scrapyd 以实现水平伸缩性，并将数据传送到 Apache Spark 进行实时分析。

## 0101. Scrapy 介绍

在本章中，我们向你介绍了 Scrapy 以及它的作用，还有使用这本书的最优方法。通过开发与市场完美结合的高质量应用，我们还介绍了几种自动抓取数据能使你获益的方法。下一章会介绍两个极为重要的网络语言，HTML 和 XPath，我们在每个 Scrapy 项目中都会用到。

Scrapy 是一个健壮的抓取网络资源的框架。作为互联网使用者，你可能经常希望可以将网上的资源保存到 Excel 中（见第 3 章），以便离线时使用或进行计算。作为开发者，你可能经常希望将不同网站的资源整合起来，但你清楚这么做的复杂性。Scrapy 可以帮助你完成简单和复杂的数据提取。使用 Scrapy，你只需进行一项设置，就可以抵过其它框架使用多个类、插件和配置。

从开发者的角度，你会喜欢 Scrapy 的基于事件的架构（见第 8 章和第 9 章）。它可以让我们进行串联操作，清洗、形成、丰富数据，或存入数据库等等，同时不会有太大的性能损耗。从技术上说，基于事件的机制，Scrapy 可以让吞吐量摆脱延迟，同时开放数千个连接。举一个极端的例子，假设你要从一个网站提取列表，每页有 100 个列表项。Scrapy 可以轻松的同时处理 16 个请求，假设每个请求在一秒内完成，每秒就可以抓取 16 个页面。乘以每页的列表数，每秒就可以抓取 1600 个列表项。然后，你想将每个列表项写入一个高并发的云存储，每个要花 3 秒。为了支持每秒 16 个请求，必须要并行进行 4800 个写入请求（第 9 章你会看到更多类似的计算）。对于传统的多线程应用，这需要 4800 个线程，对你和操作系统都是个挑战。在 Scrapy 中，4800 个并发请求很平常，只要操作系统支持就行。更进一步，Scrapy 的内存要求和你要抓取的列表项的数据量相关，而对于多线程应用，每个线程的大小都和一个列表的大小相当。

简而言之，速度慢或不可预测的网站、数据库或远程 API 不会对 Scrapy 的性能造成影响，因为你可以进行并发请求，用单线程管理。相比于多线程应用，使用更简单的代码反而可以同时运行几个抓取器和其它应用，这样就可以降低费用。

除了前面提到的性能的优点，以下是 Scrapy 其它让人喜爱的理由：1） Scrapy 可以读懂破损的 HTML。你可以在 Scrapy 上直接使用 BeautifulSoup 或 lxml，但 Scrapy 提供 Selector，一个相比 lxml 更高级的 XPath 解析器。它可以有效的处理破损的 HTML 代码和费解的编码。2）社区。Scrapy 有一个活跃的社区。可以查看 Scrapy 的邮件列表 [scrapy-users - Google 网上论坛](https://groups.google.com/forum/#!forum/scrapy-users) 和 Stack Overflow 上的数千个问题 [Highest Voted 'scrapy' Questions - Stack Overflow](https://stackoverflow.com/questions/tagged/scrapy)。多数问题在数分钟之内就会得到解答。[Scrapy | Community](https://scrapy.org/community/) 有更多的社区资源。3）由社区维护的组织清晰的代码。Scrapy 需要用标准的方式组织代码。你用 Python 来写爬虫和 pipelines，就可以自动使引擎的效率提高。如果你在网上搜索，你会发现许多人有使用 Scrapy 的经验。这意味着，可以方便地找人帮你维护或扩展代码。无论是谁加入你的团队，都不必经过学习曲线理解你特别的爬虫。4）注重质量的更新。如果查看版本记录（[Release notes — Scrapy 1.8.0 documentation](http://doc.scrapy.org/en/latest/news.html)），你会看到有不断的更新和稳定性 / 错误修正。

关于此书：目标和用法。对于此书，我们会用例子和真实的数据教你使用 Scrapy。大多数章节，要抓取的都是一个房屋租赁网站。我们选择它的原因是，它很有代表性，并可以进行一定的变化，同时也很简单。使用这个例子，可以让我们专注于 Scrapy。我们会从抓取几百页开始，然后扩展到抓取 50000 页。在这个过程中，我们会教你如何用 Scrapy 连接 MySQL、Redis 和 Elasticsearch，使用 Google geocoding API 找到给定地点的坐标，向 Apach Spark 传入数据，预测影响价格的关键词。

你可能需要多读几遍本书。你可以粗略地浏览一遍，了解一下结构，然后仔细读一两章、进行学习和试验，然后再继续读。如果你对哪章熟悉的话，可以跳过。如果你熟悉 HTML 和 XPath 的话，就没必要在第 2 章浪费太多时间。某些章如第 8 章，既是示例也是参考，具有一定深度。它就需要你多读几遍，每章之间进行数周的练习。如果没有完全搞懂第 8 章的话，也可以读第 9 章的具体应用。后者可以帮你进一步理解概念。我们已经尝试调整本书的结构，以让其既有趣也容易上手。但我们做不到用这本书教给你如何使用 Python。Python 的书有很多，但我建议你在学习的过程中尽量保持放松。Python 流行的原因之一是，它很简洁，可以像读英语一样读代码。对于 Python 初学者和专家，Scrapy 都是一个高级框架。你可以称它为「Scrapy 语言」。

开发高可靠高质量的应用，提供真实的开发进度表。为了开发新颖高质量的应用，我们需要真实和大量的数据，如果可能的话，最好在写代码之前就有数据。现在的软件开发都要实时处理海量的瑕疵数据，以获取知识和洞察力。当软件应用到海量数据时，错误和疏忽很难检测出来，就会造成后果严重的决策。例如，在进行人口统计时，很容易忽略一整个州，仅仅是因为这个州的名字太长，它的数据被丢弃了。通过细心的抓取，有高质量的、海量的真实数据，在开发和设计的过程中，就可以找到并修复 bug，然后才能做出正确的决策。

另一个例子，假设你想设计一个类似亚马逊的「如果你喜欢这个，你可能也喜欢那个」的推荐系统。如果在开始之前，你就能抓取手机真实的数据，你就可以快速知道一些问题，比如无效记录、打折商品、重复、无效字符、因为分布导致的性能问题。数据会强制你设计健壮的算法以处理被数千人抢购或无人问津的商品。相比较于数周开发之后却碰到现实问题，这两种方法可能最终会一致，但是在一开始就能对整个进程有所掌握，意义肯定是不同的。从数据开始，可以让软件的开发过程更为愉悦和有预测性。

快速开发最小化可行产品。海量真实数据对初创企业更为重要。你可能听说过「精益初创企业」，这是 Eric Ries 发明的词，用来描述高度不确定的企业发展阶段，尤其是技术初创企业。它的核心概念之一就是最小化可行产品（MVP），一个只包含有限功能的产品，快速开发并投放，以检测市场反应、验证商业假设。根据市场反应，初创企业可以选择追加投资，或选择其他更有希望的项目。

很容易忽略这个过程中的某些方面，这些方面和数据问题密切相关，用 Scrapy 可以解决数据问题。当我们让潜在用户尝试移动 App 时，例如，作为开发者或企业家，我们让用户来判断完成的 App 功能如何。这可能对非专家的用户有点困难。一个应用只展示「产品 1」、「产品 2」、「用户 433」，和另一个应用展示「Samsung UN55J6200 55-Inch TV」，用户「Richard S.」给它打了五星评价，并且有链接可以直接打开商品主页，这两个应用的差距是非常大的。很难让人们对 MVP 进行客观的评价，除非它使用的数据是真实可信的。一些初创企业事后才想到数据，是因为考虑到采集数据很贵。事实上，我们通常都是打开表格、屏幕、手动输入数据，或者我们可以用 Scrapy 抓取几个网站，然后再开始写代码。第 4 章中，你可以看到如何快速创建一个移动 App 以使用数据。

网络抓取让你的应用快速成长 —— Google 不能使用表格。让我们来看看表格是如何影响一个产品的。假如谷歌的创始人创建了搜索引擎的第一个版本，但要求每个网站站长填入信息，并复制粘贴他们的每个网页的链接。他们然后接受谷歌的协议，让谷歌处理、存储、呈现内容，并进行收费。可以想象整个过程工作量巨大。即使市场有搜索引擎的需求，这个引擎也成为不了谷歌，因为它的成长太慢了。即使是最复杂的算法也不能抵消缺失数据。谷歌使用网络爬虫逐页抓取，填充数据库。站长完全不必做任何事。实际上，想屏蔽谷歌，还需要做一番努力。

让谷歌使用表格的主意有点搞笑，但是一个普通网站要用户填多少表呢？登录表单、列表表单、勾选表单等等。这些表单会如何遏制应用的市场扩张？如果你足够了解用户，你会知道他们还会使用其它什么网站，或许已经有了账户。例如，开发者可能有 Stack Overflow 和 GitHub 账户。经过用户同意，你能不能直接用这些账户就自动填入照片、介绍和最近的帖子呢？你能否对这些帖子做文本分析，根据结果设置网站的导航结构、推荐商品或服务呢？我希望你能看到将表格换为自动数据抓取可以更好的为用户服务，使网站快速成长。

发现并实践。抓取数据自然而然会让你发现和思考你和被抓取目标的关系。当你抓取一个数据源时，自然会有一些问题：我相信他们的数据吗？我相信提供数据的公司吗？我应该和它们正式商谈合作吗？我和他们有竞争吗？从其他渠道获得数据花费是多少？这些商业风险是必然存在的，但是抓取数据可以让我们更早的知道，进行应对。

你还想知道如何反馈给这些网站或社区？给他们免费流量，他们肯定很高兴。另一方面，如果你的应用不能提供价值，继续合作的可能就会变小，除非找到另外合作的方式。通过从各种渠道获得数据，你可以开发对现有生态更友好的产品，甚至打败旧产品。或者，老产品能帮助你扩张，例如，你的应用数据来自两个或三个不同的生态圈，每个生态圈都有十万名用户，结合起来，你的应用或许就能惠及三十万人。假如你的初创企业结合了摇滚乐和 T 恤印刷行业，就将两个生态圈结合了起来，你和这两个社区都可以得到扩张。

在充满爬虫的网络世界做守法公民。开发爬虫还有一些注意事项。不负责任的网络抓取让人不悦，有时甚至是犯罪。两个最重要的要避免的就是拒绝访问攻击（DoS）和侵犯著作权。对于第一个，普通访问者每隔几秒才访问一个新页面。爬虫的话，每秒可能下载几十个页面。流量超过普通用户的十倍。这会让网站的拥有者不安。使用阻塞器降低流量，模仿普通用户。检测响应时间，如果看到响应时间增加，则降低抓取的强度。好消息是 Scrapy 提供了两个现成的方法（见第 7 章）。

对于著作权，可以查看网站的著作权信息，以确认什么可以抓取什么不能抓取。大多数站点允许你处理网站的信息，只要不复制并宣称是你的。一个好的方法是在你请求中使用一个 User-Agent 字段，告诉网站你是谁，你想用他们的数据做什么。Scrapy 请求默认使用你的 BOT_NAME 作为 User-Agent。如果这是一个 URL 或名字，可以直接指向你的应用，那么源网站的站长就可以访问你的站点，并知道你用他的数据做什么。另一个重要的地方，允许站长可以禁止爬虫访问网站的某个区域。Scrapy 提供了功能（RobotsTxtMiddleware），以尊重源网站列在 robots.txt 文件的意见（在 [https://www.google.com/robots.txt](https://www.google.com/robots.txt) 可以看到一个例子）。最后，最好提供可以让站长提出拒绝抓取的方法。至少，可以让他们很容易地找到你，并提出交涉。每个国家的法律不同，我无意给出法律上的建议。如果你觉得需要的话，请寻求专业的法律建议。

Scrapy 不是什么。最后，因为数据抓取和相关的名词定义很模糊，或相互使用，很容易误解 Scrapy。我这里解释一下，避免发生误解。1）Scrapy 不是 Apache Nutch，即它不是一个原生的网络爬虫。如果 Scrapy 访问一个网站，它对网站一无所知，就不能抓取任何东西。Scrapy 是用来抓取结构化的信息，并需要手动设置 XPath 和 CSS 表达式。Apache Nutch 会取得一个原生网页并提取信息，例如关键词。它更适合某些应用，而不适合其它应用。2）Scrapy 不是 Apache Solr、Elasticsearch 或 Lucene；换句话说，它和搜索引擎无关。Scrapy 不是用来给包含「爱因斯坦」的文档寻找参考。你可以使用 Scrapy 抓取的数据，并将它们插入到 Solr 或 Elasticsearch，如第 9 章所示，但这只是使用 Scrapy 的一种途径，而不是嵌入 Scrapy 的功能。3）最后，Scrapy 不是类似 MySQL、MongoDB、Redis 的数据库。它不存储和索引数据。它只是提取数据。也就是说，你需要将 Scrapy 提取的数据插入到数据库中，可行的数据库有多种。虽然 Scrapy 不是数据库，它的结果可以方便地输出为文件，或不进行输出。

## 0201. 理解 HTML 和 XPath

编程语言的不断进化，使得创建可靠的 XPath 表达式从 HTML 抓取信息变得越来越容易。在本章中，你学到了 HTML 和 XPath 的基本知识、如何利用 Chrome 自动获取 XPath 表达式。你还学会了如何手工写 XPath 表达式，并区分可靠和不够可靠的 XPath 表达式。

为了从网页提取信息，了解网页的结构是非常必要的。我们会快速学习 HTML、HTML 的树结构和用来筛选网页信息的 XPath。HTML、DOM 树结构和 XPath，从这本书的角度，键入网址到看见网页的整个过程可以分成四步：1）在浏览器中输入网址 URL。URL 的第一部分，也即域名（例如 gumtree.com），用来搜寻网络上的服务器。URL 和其他像 cookies 等数据形成了一个发送到服务器的请求 request。2）服务器向浏览器发送 HTML。服务器也可能发送 XML 或 JSON 等其他格式，目前我们只关注 HTML。3）HTML 在浏览器内部转化成树结构：文档对象模型（DOM）。4）根据布局规范，树结构转化成屏幕上的真实页面。

1、URL 包括两部分：第一部分通过 DNS 定位服务器，例如当你在浏览器输入 https://mail.google.com/mail/u/0/#inbox 这个地址时，产生了一个 mail.google.com 的 DNS 请求，后者为你解析了一台服务器的 IP 地址，例如 173.194.71.83。也就是说，https://mail.google.com/mail/u/0/#inbox 转换成了 https://173.194.71.83/mail/u/0/#inbox。URL 其余的部分告诉服务器这个请求具体是关于什么的，可能是一张图片、一份文档或是触发一个动作，例如在服务器上发送一封邮件。

HTML 文档。服务器读取 URL，了解用户请求，然后回复一个 HTML 文档。HTML 本质是一个文本文件，可以用 TextMate、Notepad、vi 或 Emacs 等软件打开。与大多数文本文件不同，HTML 严格遵循万维网联盟（World Wide Web Consortium）的规定格式。这个格式超出了本书的范畴，这里只看一个简单的 HTML 页面。如果你打开 http://example.com，点击查看源代码，就可以看到 HTML 代码，如下所示。

为了便于阅读，我美化了这个 HTML 文档。你也可以把整篇文档放在一行里。对于 HTML，大多数情况下，空格和换行符不会造成什么影响。尖括号里的字符称作标签，例如 \<html> 或 \<head>。\<html> 是起始标签，\</html> 是结束标签。标签总是成对出现。某些网页没有结束标签，例如只用 \<p> 标签分隔段落，浏览器对这种行为是容许的，会智能判断哪里该有结束标签 \</p>。\<p> 与 \</p> 之间的内容称作 HTML 的元素。元素之间可以嵌套元素，比如例子中的 <\div> 标签，和第二个 \<p> 标签，后者包含了一个 \<a> 标签。

有些标签稍显复杂，例如 \<a href="http://www.iana.org/domains/example">，带有 URL 的 href 部分称作属性。最后，许多标签元素包含有文本，例如 \<h1> 标签中的 Example Domain。对我们而言，\<body> 标签之间的可见内容更为重要。头部标签 \<head> 中指明了编码字符，由 Scrapy 对其处理，就不用我们浪费精力了。

树结构。不同的浏览器有不同的借以呈现网页的内部数据结构。但 DOM 树是跨平台且不依赖语言的，可以被几乎所有浏览器支持。只需右键点击，选择查看元素，就可以在浏览器中查看网页的树结构。你看到的树结构和 HTML 很像，但不完全相同。无论原始 HTML 文件使用了多少空格和换行符，树结构看起来都会是一样的。你可以点击任意元素，或是改变属性，这样可以实时看到对 HTML 网页产生了什么变化。例如，如果你双击了一段文字，并修改了它，然后点击回车，屏幕上这段文字就会根据新的设置发生改变。在右边的方框中，在属性标签下面，你可以看到这个树结构的属性列表。在页面底部，你可以看到一个面包屑路径，指示着选中元素的所在位置。重要的是记住，HTML 是文本，而树结构是浏览器内存中的一个对象，你可以通过程序查看、操作这个对象。在 Chrome 浏览器中，就是通过开发者工具查看。

1『HTML 是文本，而树结构是浏览器内存中的一个对象；Chrome 里快捷键 option+command+I 进入。』

浏览器中的页面。HTML 文本和树结构和我们平时在浏览器中看到的页面截然不同。这恰恰是 HTML 的成功之处。HTML 文件就是要具有可读性，可以区分网页的内容，但不是按照呈现在屏幕上的方式。这意味着，呈现 HTML 文档、进行美化都是浏览器的职责，无论是对于功能齐备的 Chrome、移动端浏览器、还是 Lynx 这样的文本浏览器。也就是说，网页的发展对网页开发者和用户都提出了极大的开发网页方面的需求。CSS 就是这样被发明出来，用以服务 HTML 元素。对于 Scrapy，我们不涉及 CSS。

既然如此，树结构对呈现出来的网页有什么作用呢？答案就是盒模型。正如 DOM 树可以包含其它元素或是文字，同样的，盒模型里面也可以内嵌其它内容。所以，我们在屏幕上看到的网页是原始 HTML 的二维呈现。树结构是其中的一维，但它是隐藏的。例如，在下图中，我们看到三个 DOM 元素，一个 \<div> 和两个内嵌的 \<h1> 和 \<p>，出现在浏览器和 DOM 中。

用 XPath 选择 HTML 元素。如果你以前接触过传统的软件工程，并不知道 XPath，你可能会担心，在 HTML 文档中查询某个信息，要进行复杂的字符串匹配、搜索标签、处理特殊字符、解析整个树结构等繁琐工作。对于 XPath，所有的这些都不是问题，你可以轻松提取元素、属性或是文字。在 Chrome 中使用 XPath，在开发者工具中点击控制台标签，使用 \$x 功能。例如，在网页 http://example.com/ 的控制台，输入 \$x ('//h1')，就可以移动到 \<h1> 元素，如截图所示。你在控制台中看到的是一个包含所选元素的 JavaScript 数组。如果你将光标移动到这个数组上，你可以看到被选择的元素被高亮显示。这个功能很有用。

XPath 表达式。HTML 文档的层级结构的最高级是 \<html> 标签，你可以使用元素名和斜杠线选择任意元素。例如，下面的表达式返回了 http://example.com/ 上对应的内容。注意，\<p> 标签在 \<div> 标签内有两个，所以会返回两个。你可以用 p[1] 和 p[2] 分别返回两个元素。从抓取的角度，文档的标题或许是唯一让人感兴趣的，它位于文档的头部，可以用下面的表达式找到：

```
$x('//html/head/title')
  [ <title>Example Domain</title> ]
```
对于大文档，你可能要写很长的 XPath 表达式，以获取所要的内容。为了避免这点，两个斜杠线 // 可以让你访问到所有的同名元素。例如，//p 可以选择所有的 p 元素，//a 可以选择所有的链接。//a 可以用在更多的地方。例如，如果要找到所有 \<div> 标签的链接，你可以使用 //div//a。如果 a 前面只有一个斜杠，//div/a 会返回空，因为在上面的例子中 \<div> 标签下面没有 \<a>。

```
$x('//p')
  [ <p>...</p>, <p>...</p> ]
$x('//a')
  [ <a href="http://www.iana.org/domains/example">More information...</a> ]

$x('//div//a')
  [ <a href="http://www.iana.org/domains/example">More information...</a> ]
$x('//div/a')
  [ ]
```

你也可以选择属性。http://example.com/ 上唯一的属性是链接 href，可以通过下面的方式找到；你也可以只通过 text( ) 函数选择文字；可以使用 * 标志选择某层下所有的元素，例如：

```
$x('//a/@href')
[href="http://www.iana.org/domains/example"]

$x('//a/text()')
["More information..."]

$x('//div/*')
[<h1>Example Domain</h1>, <p>...</p>, <p>...</p>]
```

寻找特定属性，例如 @class、或属性有特定值时，你会发现 XPath 非常好用。例如，//a[@href] 可以找到所有链接，//a[@href="http://www.iana.org/domains/example"] 则进行了指定的选择。当属性值中包含特定字符串时，XPath 会极为方便。例如：

```
$x('//a[@href]')
[<a href="http://www.iana.org/domains/example">More information...</a>]
$x('//a[@href="http://www.iana.org/domains/example"]')
[<a href="http://www.iana.org/domains/example">More information...</a>]
$x('//a[contains(@href, "iana")]')
[<a href="http://www.iana.org/domains/example">More information...</a>]
$x('//a[starts-with(@href, "http://www.")]')
[<a href="http://www.iana.org/domains/example">More information...</a>]
$x('//a[not(contains(@href, "abc"))]')
[ <a href="http://www.iana.org/domains/example">More information...</a>]
```

在 http://www.w3schools.com/xsl/xsl_functions.asp 在线文档中你可以找到更多类似的函数，但并非都常用。在 Scrapy 终端中可以使用同样的命令，在命令行中输入：

    scrapy shell "http://example.com"

终端会向你展示许多写爬虫时碰到的变量。其中最重要的是响应，在 HTML 中是 HtmlResponse，这个类可以让你在 Chrome 使用 xpath() 方法 \$x。下面是一些例子：

```
response.xpath('/html').extract()
  [u'<html><head><title>...</body></html>']
response.xpath('/html/body/div/h1').extract()
  [u'<h1>Example Domain</h1>']
response.xpath('/html/body/div/p').extract()
  [u'<p>This domain ... permission.</p>', u'<p><a href="http://www.iana.org/domains/example">More information...</a></p>']
response.xpath('//html/head/title').extract()
  [u'<title>Example Domain</title>']
response.xpath('//a').extract()
  [u'<a href="http://www.iana.org/domains/example">More information...</a>']
response.xpath('//a/@href').extract()
  [u'http://www.iana.org/domains/example']
response.xpath('//a/text()').extract()
  [u'More information...']
response.xpath('//a[starts-with(@href, "http://www.")]').extract()
  [u'<a href="http://www.iana.org/domains/example">More information...</a>']
```

这意味着，你可用 Chrome 浏览器生成 XPath 表达式，以便在 Scrapy 爬虫中使用。

1『scrapy shell "http://example.com"，会显示出爬的结果，而且自动进入 ipython 交互界面。可以在 ipython 输入一些列命令，比如 response.xpath('/html').extract()。』

使用 Chrome 浏览器获得 XPath 表达式。Chrome 浏览器可以帮助我们获取 XPath 表达式这点确实对开发者非常友好。像之前演示的那样检查一个元素：右键选择一个元素，选择检查元素。开发者工具被打开，该元素在 HTML 的树结构中被高亮显示，可以在右键打开的菜单中选择 Copy XPath，表达式就复制到粘贴板中了。

下面展示一些 XPath 表达式的常见使用。先来看看在维基百科上是怎么使用的。维基百科的页面非常稳定，不会在短时间内改变排版。

1）取得 id 为 firstHeading 的 div 下的 span 的 text；2）取得 id 为 toc 的 div 下的 ul 内的 URL；3）在任意 class 包含 ltr 和 class 包含 skin-vector 的元素之内，取得 h1 的 text，这两个字符串可能在同一 class 内，或不在。实际应用中，你会在 XPath 中频繁地使用 class。在这几个例子中，你需要记住，因为 CSS 的板式原因，你会看到 HTML 的元素总会包含许多特定的 class 属性。这意味着，有的 \<div> 的 class 是 link，其他导航栏的 \<div> 的 class 就是 link active。后者是当前生效的链接，因此是可见或是用 CSS 特殊色高亮显示的。当抓取的时候，你通常是对含有某个属性的元素感兴趣的，就像之前的 link 和 link active。XPath 的 contains() 函数就可以帮你选择包含某一 class 的所有元素。

```
//h1[@id="firstHeading"]/span/text()
//div[@id="toc"]/ul//a/@href
//*[contains(@class,"ltr") and contains(@class,"skin-vector")]//h1//text()
```

4）选择 class 属性是 infobox 的 table 的第一张图片的 URL；5）选择 class 属性是 reflist 开头的 div 下面的所有 URL 链接；6）选择 div 下面的所有 URL 链接，并且这个 div 的下一个相邻元素的子元素包含文字 References；7）取得所有图片的 URL。

```
//table[@class="infobox"]//img[1]/@src
//div[starts-with(@class,"reflist")]//a/@href
//*[text()="References"]/../following-sibling::div//a
//img/@src
```

1『属性 @href 和 @src 都表示网页链接；[] 中括号里的表达式感觉就是「过滤器」。』

提前应对网页发生改变。爬取的目标常常位于远程服务器。这意味着，如果它的 HTML 发生了改变，XPath 表达式就无效了，我们就不得不回过头修改爬虫的程序。因为网页的改变一般就很少，爬虫的改动往往不会很大。然而，我们还是宁肯不要回头修改。一些基本原则可以帮助我们降低表达式失效的概率：

1、避免使用数组序号。Chrome 常常会在表达式中加入许多常数。如果 HTML 上有一个广告窗的话，就会改变文档的结构，这个表达式就会失效。解决的方法是，尽量找到离 img 标签近的元素，根据该元素的 id 或 class 属性，进行抓取，例如：

```
//*[@id="myid"]/div/div/div[1]/div[2]/div/div[1]/div[1]/a/img
//div[@class="thumbnail"]/a/img
```

2、用 class 抓取效果不一定好。使用 class 属性可以方便的定位要抓取的元素，但是因为 CSS 也要通过 class 修改页面的外观，所以 class 属性可能会发生改变，例如下面用到的 class；过一段时间之后，可能会变成：

```
//div[@class="thumbnail"]/a/img
//div[@class="preview green"]/a/img
```

3、数据指向的 class 优于排版指向的 class。在上一个例子中，使用 thumbnail 和 green 两个 class 都不好。thumbnail 比 green 好，但这两个都不如 departure-time。前面两个是用来排版的，departure-time 是有语义的，和 div 中的内容有关。所以，在排版发生改变的情况下，departure-time 发生改变的可能性会比较小。应该说，网站作者在开发中十分清楚，为内容设置有意义的、一致的标记，可以让开发过程收益。

4、id 通常是最可靠的。只要 id 具有语义并且数据相关，id 通常是抓取时最好的选择。部分原因是，JavaScript 和外链锚点总是使用 id 获取文档中特定的部分。例如，下面的 XPath 非常可靠：

```
//*[@id="more_info"]//text( )
```

相反的例子是，指向唯一参考的 id，对抓取没什么帮助，因为抓取总是希望能够获取具有某个特点的所有信息。例如：

```
//[@id="order-F4982322"]
```

这是一个非常差的 XPath 表达式。还要记住，尽管 id 最好要有某种特点，但在许多 HTML 文档中，id 都很杂乱无章。

1『 id 属性优先考虑，有意义的 id 更佳，有意义的 class 属性可优先考虑。』

## 0301. 爬虫基础

对所有学习 Scrapy 的人，本章也许是最重要的。你学习了爬虫的基本流程 UR2IM、如何自定义 Items、使用 ItemLoaders，XPath 表达式、利用处理函数加载 Items、如何 yield 请求。我们使用 Requests 水平抓取多个索引页、垂直抓取列表页。最后，我们学习了如何使用 CrawlSpider 和 Rules 简化代码。我们刚刚从一个网站提取了信息。它的重要性在哪呢？答案在下一章，我们只用几页就能制作一个移动 app，并用 Scrapy 填充数据。

Vagrant：本书案例的运行方法。本书有的例子比较复杂，有的例子使用了许多东西。无论你是什么水平，都可以尝试运行所有例子。只需一句命令，就可以用 Vagrant 搭建操作环境。在 Vagrant 中，你的电脑被称作「主机」。Vagrant 在主机中创建一个虚拟机。这样就可以让我们忽略主机的软硬件，来运行案例了。本书大多数章节使用了两个服务 —— 开发机和网络机。我们在开发机中登录运行 Scrapy，在网络机中进行抓取。后面的章节会使用更多的服务，包括数据库和大数据处理引擎。

根据附录 A 安装必备，安装 Vagrant，直到安装好 git 和 Vagrant。可以打开几个窗口输入 vagrant ssh，这样就可以打开几个终端。输入 vagrant halt 可以关闭系统，vagrantstatus 可以检查状态。vagrant halt 不能关闭虚拟机。如果在 VirtualBox 中碰到问题，可以手动关闭，或是使用 vagrant global-status 查找 id，用 vagrant halt <ID> 暂停。大多数例子可以离线运行，这是 Vagrant 的一大优点。

```
vagrant up --no-parallel
```

3『[Vagrant by HashiCorp](https://www.vagrantup.com/) | [Documentation - Vagrant by HashiCorp](https://www.vagrantup.com/docs/index.html) | [docker/kitematic: Visual Docker Container Management on Mac & Windows](https://github.com/docker/kitematic) | [Releases · docker/kitematic](https://github.com/docker/kitematic/releases)，官网下载安装 Vagrant 以及虚拟机 docker。docker 也可以做 Vagrant 的 Provider，但目前没有配置成功，暂时还是用 VirtualBox 做的 Provider。』

R2IM—— 基础抓取过程。每个网站都是不同的，对每个网站进行额外的研究不可避免，碰到特别生僻的问题，也许还要用 Scrapy 的邮件列表咨询。寻求解答，去哪里找、怎么找，前提是要熟悉整个过程和相关术语。Scrapy 的基本过程，可以写成字母缩略语 UR2IM，见下图：URL、Request、Response、Items 和 More URLs。

一切都从 URL 开始。你需要目标网站的 URL。我的例子是 https://www.gumtree.com/，Gumtree 分类网站。例如，访问伦敦房地产首页 http://www.gumtree.com/flats-houses/london，你就可以找到许多房子的 URL。右键复制链接地址，就可以复制 URL。其中一个 URL 可能是这样的：https://www.gumtree.com/p/studios-bedsits-rent/split-level。但是，Gumtree 的网站变动之后，URL 的 XPath 表达式会失效。不添加用户头的话，Gumtree 也不会响应。这个留给以后再说，现在如果你想加载一个网页，你可以使用 Scrapy 终端，如下所示：

```
scrapy shell -s USER_AGENT="Mozilla/5.0" <your url here  e.g. http://www.gumtree.com/p/studios-bedsits-rent/...>
```

3『用户头 header 可以通过在浏览器里输入「http://httpbin.org/get?a=123&b=456」获得。』

要进行调试，可以在 Scrapy 语句后面添加 –pdb，例如：

```
scrapy shell --pdb https://gumtree.com
```

我们不想让大家如此频繁的点击 Gumtree 网站，并且 Gumtree 网站上 URL 失效很快，不适合做例子。我们还希望大家能在离线的情况下，多多练习书中的例子。这就是为什么 Vagrant 开发环境内嵌了一个网络服务器，可以生成和 Gumtree 类似的网页。这些网页可能并不好看，但是从爬虫开发者的角度，是完全合格的。如果想在 Vagrant 上访问 Gumtree，可以在 Vagrant 开发机上访问 http://web:9312/，或是在浏览器中访问 http://localhost:9312/。

让我们在这个网页上尝试一下 Scrapy，在 Vagrant 开发机上输入：

```
$ scrapy shell http://web:9312/properties/property_000000.html
```

得到一些输出，加载页面之后，就进入了 Python（可以使用 Ctrl+D 退出）。

请求和响应。在前面的输出日志中，Scrapy 自动为我们做了一些工作。我们输入了一条地址，Scrapy 做了一个 GET 请求，并得到一个成功响应值 200。这说明网页信息已经成功加载，并可以使用了。如果要打印 reponse.body 的前 50 个字母，我们可以得到：

```
>>> response.body[:50]
'<!DOCTYPE html>\n<html>\n<head>\n<meta charset="UTF-8"'
```

这就是这个 Gumtree 网页的 HTML 文档。有时请求和响应会很复杂，第 5 章会对其进行讲解，现在只讲最简单的情况。

1『估计指的是有关「用户登录」的信息。』

抓取对象。下一步是从响应文件中提取信息，输入到 Item。因为这是个 HTML 文档，我们用 XPath 来做。首先来看一下这个网页；页面上的信息很多，但大多是关于版面的：logo、搜索框、按钮等等。从抓取的角度，它们不重要。我们关注的是，例如，列表的标题、地址、电话。它们都对应着 HTML 里的元素，我们要在 HTML 中定位，用上一章所学的提取出来。先从标题开始。

在标题上右键点击，选择检查元素。在自动定位的 HTML 上再次右键点击，选择复制 XPath。Chrome 给的 XPath 总是很复杂，并且容易失效。我们要对其进行简化。我们只取最后面的 h1。这是因为从 SEO 的角度，每页 HTML 只有一个 h1 最好，事实上大多是网页只有一个 h1，所以不用担心重复。

2『注意，要在开发者工具里先点左上角的「选取」按钮，然后再去页面里拾取元素。或者直接用快捷键「command+shift+C」。』

提示：SEO 是搜索引擎优化的意思：通过对网页代码、内容、链接的优化，提升对搜索引擎的支持。让我们看看 h1 标签行不行：

```
>>> response.xpath('//h1/text()').extract()
[u'set unique family well']
```

很好，完全行得通。我在 h1 后面加上了 text ()，表示只提取 h1 标签里的文字。没有添加 text () 的话，就会这样：

```
>>> response.xpath('//h1').extract()
[u'<h1 itemprop="name" class="space-mbs">set unique family well</h1>']
```

我们已经成功得到了 title，但是再仔细看看，还能发现更简便的方法。Gumtree 为标签添加了属性，就是 itemprop=name。所以 XPath 可以简化为 //*[@itemprop="name"][1]/text ()。在 XPath 中，切记数组是从 1 开始的，所以这里 [] 里面是 1。选择 itemprop="name" 这个属性，是因为 Gumtree 用这个属性命名了许多其他的内容，比如「You may also like」，用数组序号提取会很方便。接下来看价格。价格在 HTML 中的位置如下：

```
<strong class="ad-price txt-xlarge txt-emphasis" itemprop="price">£334.39pw</strong>
```

1『注意，里面的 name 或者 price 必须还是双引号，之前试过单引号不行。』

我们又看到了 itemprop="price" 这个属性，XPath 表达式为 //*[@itemprop="price"][1]/text ()。验证一下：

```
>>> response.xpath('//*[@itemprop="price"][1]/text()').extract()
[u'\xa3334.39pw']
```

注意 Unicode 字符（£ 符号）和价格 350.00pw。这说明要对数据进行清理。在这个例子中，我们用正则表达式提取数字和小数点。使用正则方法如下：

```
>>> response.xpath('//*[@itemprop="price"][1]/text()').re('[.0-9]+')
[u'334.39']
```

提取房屋描述的文字、房屋的地址也很类似，如下：

```
//*[@itemprop="description"][1]/text()
//*[@itemtype="http://schema.org/Place"][1]/text()
```

相似的，抓取图片可以用 //img [@itemprop="image"][1]/@src。注意这里没使用 text ()，因为我们只想要图片的 URL。假如这就是我们要提取的所有信息，整理如下；这张表很重要，因为也许只要稍加改变表达式，就可以抓取其他页面。另外，如果要爬取数十个网站时，使用这样的表可以进行区分。目前为止，使用的还只是 HTML 和 XPath，接下来用 Python 来做一个项目。

一个 Scrapy 项目。目前为止，我们只是在 Scrapy shell 中进行操作。学过前面的知识，现在开始一个 Scrapy 项目，Ctrl+D 退出 Scrapy shell。Scrapy shell 只是操作网页、XPath 表达式和 Scrapy 对象的工具，不要在上面浪费太多，因为只要一退出，写过的代码就会消失。我们创建一个名字是 properties 的项目；先看看这个 Scrapy 项目的文件目录。文件夹内包含一个同名的文件夹，里面有三个文件 items.py, pipelines.py 和 settings.py。还有一个子文件夹 spiders，里面现在是空的。后面的章节会详谈 settings、pipelines 和 scrapy.cfg 文件。

```
$ scrapy startproject properties
$ cd properties
```

1、定义 items。用编辑器打开 items.py。里面已经有代码，我们要对其修改下。用之前的表里的内容重新定义 class PropertiesItem。还要添加些后面会用到的内容。后面会深入讲解。这里要注意的是，声明一个字段，并不要求一定要填充。所以放心添加你认为需要的字段，后面还可以修改。

我们还会加入一些杂务字段，也许和现在的项目关系不大，但是我个人很感兴趣，以后或许能用到。你可以选择添加或不添加。观察一下这些项目，你就会明白，这些项目是怎么帮助我找到何地（server，url），何时（date），还有（爬虫）如何进行抓取的。它们可以帮助我取消项目，制定新的重复抓取，或忽略爬虫的错误。这里看不明白不要紧，后面会细讲。利用这个表修改 PropertiesItem 这个类。修改文件 properties/items.py 如下：

2、编写爬虫。已经完成了一半。现在来写爬虫。一般的，每个网站，或一个大网站的一部分，只有一个爬虫。爬虫代码来成 UR2IM 流程。当然，你可以用文本编辑器一句一句写爬虫，但更便捷的方法是用 scrapy genspider 命令，如下所示：

```
$ scrapy genspider basic web
```

使用模块中的模板「basic」创建了一个爬虫「basic」：properties.spiders.basic。一个爬虫文件 basic.py 就出现在目录 properties/spiders 中。刚才的命令是，生成一个名字是 basic 的默认文件，它的限制是在 web 上爬取 URL。我们可以取消这个限制。这个爬虫使用的是 basic 这个模板。你可以用 scrapy genspider –l 查看所有的模板，然后用参数 –t 利用模板生成想要的爬虫，后面会介绍一个例子。查看 properties/spiders/basic.py file 文件，它的代码如下：

```
import scrapy
class BasicSpider(scrapy.Spider):
    name = "basic"
    allowed_domains = ["web"]
start_URL = (
        'http://www.web/',
    )
    def parse(self, response):
        pass
```

import 命令可以让我们使用 Scrapy 框架。然后定义了一个类 BasicSpider，继承自 scrapy.Spider。继承的意思是，虽然我们没写任何代码，这个类已经继承了 Scrapy 框架中的类 Spider 的许多特性。这允许我们只需写几行代码，就可以有一个功能完整的爬虫。然后我们看到了一些爬虫的参数，比如名字和抓取域字段名。最后，我们定义了一个空函数 parse ()，它有两个参数 self 和 response。通过 self，可以使用爬虫一些有趣的功能。response 看起来很熟悉，它就是我们在 Scrapy shell 中见到的响应。下面来开始编辑这个爬虫。start_URL 更改为在 Scrapy 命令行中使用过的 URL。然后用爬虫事先准备的 log () 方法输出内容。修改后的 properties/spiders/basic.py 文件为：

```
import scrapy
class BasicSpider(scrapy.Spider):
    name = "basic"
    allowed_domains = ["web"]
    start_URL = (
        'http://web:9312/properties/property_000000.html',
    )
    def parse(self, response):
        self.log("title: %s" % response.xpath(
            '//*[@itemprop="name"][1]/text()').extract())
        self.log("price: %s" % response.xpath(
            '//*[@itemprop="price"][1]/text()').re('[.0-9]+'))
        self.log("description: %s" % response.xpath(
        '//*[@itemprop="description"][1]/text()').extract())
        self.log("address: %s" % response.xpath(
            '//*[@itemtype="http://schema.org/'
            'Place"][1]/text()').extract())
        self.log("image_URL: %s" % response.xpath(
            '//*[@itemprop="image"][1]/@src').extract())
```

总算到了运行爬虫的时间！让爬虫运行的命令是 scrapy crawl 接上爬虫的名字：

```
$ scrapy crawl basic
```

成功了！不要被这么多行的命令吓到，后面我们再仔细说明。现在，我们可以看到使用这个简单的爬虫，所有的数据都用 XPath 得到了。来看另一个命令，scrapy parse。它可以让我们选择最合适的爬虫来解析 URL。用 --spider 命令可以设定爬虫：

```
$ scrapy parse --spider=basic http://web:9312/properties/property_000001.html
```

你可以看到输出的结果和前面的很像，但却是关于另一个房产的。

3、填充一个项目。接下来稍稍修改一下前面的代码。你会看到，尽管改动很小，却可以解锁许多新的功能。首先，引入类 PropertiesItem。它位于 properties 目录中的 item.py 文件，因此在模块 properties.items 中。它的导入命令是；然后我们要实例化，并进行返回。这很简单。在 parse () 方法中，我们加入声明 item = PropertiesItem ()，它产生了一个新项目，然后为它分配表达式；


```py
from properties.items import PropertiesItem

item['title'] = response.xpath('//*[@itemprop="name"][1]/text()').extract()
```

最后，我们用 return item 返回项目。更新后的 properties/spiders/basic.py 文件如下：

```
import scrapy
from properties.items import PropertiesItem
from scrapy.loader import ItemLoader
class BasicSpider(scrapy.Spider):
    name = "basic"
    allowed_domains = ["web"]
    start_URL = (
        'http://web:9312/properties/property_000000.html',
    )
    def parse(self, response):
        item = PropertiesItem()
        item['title'] = response.xpath(
            '//*[@itemprop="name"][1]/text()').extract()
        item['price'] = response.xpath(
            '//*[@itemprop="price"][1]/text()').re('[.0-9]+')
        item['description'] = response.xpath(
            '//*[@itemprop="description"][1]/text()').extract()
        item['address'] = response.xpath(
            '//*[@itemtype="http://schema.org/'
            'Place"][1]/text()').extract()
        item['image_URL'] = response.xpath(
            '//*[@itemprop="image"][1]/@src').extract()
        return item
```

现在如果再次运行爬虫，你会注意到一个不大但很重要的改动。被抓取的值不再打印出来，没有「DEBUG：被抓取的值」了。你会看到：

```
DEBUG: Scraped from <200  
http://...000.html>
  {'address': [u'Angel, London'],
   'description': [u'website ... offered'],
   'image_URL': [u'../images/i01.jpg'],
   'price': [u'334.39'],
   'title': [u'set unique family well']}
```

这是从这个页面抓取的 PropertiesItem。这很好，因为 Scrapy 就是围绕 Items 的概念构建的，这意味着我们可以用 pipelines 填充丰富项目，或是用「Feed export」导出保存到不同的格式和位置。

1『这么说，Scrapy 里 items 是个核心概念。』

4、保存到文件。试运行下面：

```
$ scrapy crawl basic -o items.json
$ cat items.json
```

不用我们写任何代码，我们就可以用这些格式进行存储。Scrapy 可以自动识别输出文件的后缀名，并进行输出。这段代码中涵盖了一些常用的格式。CSV 和 XML 文件很流行，因为可以被 Excel 直接打开。JSON 文件很流行是因为它的开放性和与 JavaScript 的密切关系。JSON 和 JSON Line 格式的区别是 .json 文件是在一个大数组中存储 JSON 对象。这意味着如果你有一个 1GB 的文件，你可能必须现在内存中存储，然后才能传给解析器。相对的，.jl 文件每行都有一个 JSON 对象，所以读取效率更高。不在文件系统中存储生成的文件也很麻烦。利用下面例子的代码，你可以让 Scrapy 自动上传文件到 FTP 或亚马逊的 S3 bucket。

2『JSON 和 JSON Line 格式的区别，做一张术语卡片。』

```
$ scrapy crawl basic -o "ftp://user:pass@ftp.scrapybook.com/items.json "
$ scrapy crawl basic -o "s3://aws_key:aws_secret@scrapybook/items.json"
```

注意，证书和 URL 必须按照主机和 S3 更新，才能顺利运行。另一个要注意的是，如果你现在使用 scrapy parse，它会向你显示被抓取的项目和抓取中新的请求；当出现意外结果时，scrapy parse 可以帮你进行 debug，你会更感叹它的强大。

5、清洗 —— 项目加载器和杂务字段。让我们让它看起来更专业些。我们使用一个功能类，ItemLoader，以取代看起来杂乱的 extract () 和 xpath ()。我们的 parse () 进行如下变化：

```py
def parse(self, response):
    l = ItemLoader(item=PropertiesItem(), response=response)
    l.add_xpath('title', '//*[@itemprop="name"][1]/text()')
    l.add_xpath('price', './/*[@itemprop="price"]'
           '[1]/text()', re='[,.0-9]+')
    l.add_xpath('description', '//*[@itemprop="description"]'
           '[1]/text()')
    l.add_xpath('address', '//*[@itemtype='
           '"http://schema.org/Place"][1]/text()')
    l.add_xpath('image_URL', '//*[@itemprop="image"][1]/@src')
    return l.load_item()
```

是不是看起来好多了？事实上，它可不是看起来漂亮那么简单。它指出了我们现在要干什么，并且后面的加载项很清晰。这提高了代码的可维护性和自文档化。（自文档化，self-documenting，是说代码的可读性高，可以像文档文件一样阅读）

1『注意，作者代码里漏掉了一个包的导入，from scrapy.loader import ItemLoader。[Item Loaders — Scrapy 1.8.0 documentation](https://docs.scrapy.org/en/latest/topics/loaders.html)』

ItemLoaders 提供了许多有趣的方式整合数据、格式化数据、清理数据。它的更新很快，查阅文档可以更好的使用它，[Maze Found | Read the Docs](http://doc.scrapy.org/en/latest/topics/loaders)。通过不同的类处理器，ItemLoaders 从 XPath/CSS 表达式传参。处理器函数快速小巧。举一个 Join () 的例子。//p 表达式会选取所有段落，这个处理函数可以在一个入口中将所有内容整合起来。另一个函数 MapCompose ()，可以与 Python 函数或 Python 函数链结合，实现复杂的功能。例如，MapCompose (float) 可以将字符串转化为数字，MapCompose (unicode.strip, unicode.title) 可以去除多余的空格，并将单词首字母大写。让我们看几个处理函数的例子。

你可以使用 Python 编写处理函数，或是将它们串联起来。unicode.strip () 和 unicode.title () 分别用单一参数实现了单一功能。其它函数，如 replace () 和 urljoin () 需要多个参数，我们可以使用 Lambda 函数。这是一个匿名函数，可以不声明函数就调用参数：

```py
myFunction = lambda i: i.replace(',', '')
```

可以取代下面的函数：

```py
def myFunction(i):
    return i.replace(',', '')
```

使用 Lambda 函数，打包 replace () 和 urljoin ()，生成一个结果，只需一个参数即可。为了更清楚前面的表，来看几个实例。在 scrapy 命令行打开任何 URL，并尝试；要记住，处理函数是对 XPath/CSS 结果进行后处理的的小巧函数。让我们来看几个我们爬虫中的处理函数是如何清洗结果的；完整的列表在本章后面给出。如果你用 scrapy crawl basic 再运行的话，你可以得到干净的结果如下；最后，我们可以用 add_value () 方法添加用 Python（不用 XPath/CSS 表达式）计算得到的值。我们用它设置我们的「杂务字段」，例如 URL、爬虫名、时间戳等等。我们直接使用前面杂务字段表里总结的表达式，如下：

```py
l.add_value('url', response.url)
l.add_value('project', self.settings.get('BOT_NAME'))
l.add_value('spider', self.name)
l.add_value('server', socket.gethostname())
l.add_value('date', datetime.datetime.now())
```

记得 import datetime 和 socket，以使用这些功能。现在，我们的 Items 看起来就完美了。我知道你的第一感觉是，这可能太复杂了，值得吗？回答是肯定的，这是因为或多或少，想抓取网页信息并存到 items 里，这就是你要知道的全部。这段代码如果用其他语言来写，会非常难看，很快就不能维护了。用 Scrapy，只要 25 行简洁的代码，它明确指明了意图，你可以看清每行的意义，可以清晰的进行修改、再利用和维护。

你的另一个感觉可能是处理函数和 ItemLoaders 太花费精力。如果你是一名经验丰富的 Python 开发者，你已经会使用字符串操作、lambda 表达构造列表，再学习新的知识会觉得不舒服。然而，这只是对 ItemLoader 和其功能的简单介绍，如果你再深入学习一点，你就不会这么想了。ItemLoaders 和处理函数是专为有抓取需求的爬虫编写者、维护者开发的工具集。如果你想深入学习爬虫的话，它们是绝对值得学习的。

6、创建协议。协议有点像爬虫的单元测试。它们能让你快速知道错误。例如，假设你几周以前写了一个抓取器，它包含几个爬虫。你想快速检测今天是否还是正确的。协议位于评论中，就在函数名后面，协议的开头是 @。看下面这个协议：

```py
def parse(self, response):
    """ This function parses a property page.
    @url http://web:9312/properties/property_000000.html
    @returns items 1
    @scrapes title price description address image_URL
    @scrapes url project spider server date
    """
```

这段代码是说，检查这个 URL，你可以在找到一个项目，它在那些字段有值。现在如果你运行 scrapy check，它会检查协议是否被满足：

```
$ scrapy check basic
----------------------------------------------------------------
Ran 3 contracts in 1.640s
OK
如果url的字段是空的（被注释掉），你会得到一个描述性错误：
FAIL: [basic] parse (@scrapes post-hook)
------------------------------------------------------------------
ContractFail: 'url' field is missing
```

当爬虫代码有错，或是 XPath 表达式过期，协议就可能失效。当然，协议不会特别详细，但是可以清楚的指出代码的错误所在。综上所述，我们的第一个爬虫如下所示。

7、提取更多的 URL。到目前为止，在爬虫的 start_URL 中我们还是只加入了一条 URL。因为这是一个元组，我们可以向里面加入多个 URL，例如；不够好。我们可以用一个文件当做 URL 源文件；还是不够好，但行得通。更常见的，网站可能既有索引页也有列表页。例如，Gumtree 有索引页：http://www.gumtree.com/flats-houses/london：

```py
start_URL = (
    'http://web:9312/properties/property_000000.html',
    'http://web:9312/properties/property_000001.html',
    'http://web:9312/properties/property_000002.html',
)

start_URL = [i.strip() for i in  
open('todo.URL.txt').readlines()]
```

一个典型的索引页包含许多列表页、一个分页系统，让你可以跳转到其它页面。因此，一个典型的爬虫在两个方向移动：1）水平 —— 从索引页到另一个索引页。2）垂直 —— 从索引页面到列表页面提取项目。

在本书中，我们称前者为水平抓取，因为它在同一层次（例如索引）上抓取页面；后者为垂直抓取，因为它从更高层次（例如索引）移动到一个较低的层次（例如列表）。做起来要容易许多。我们只需要两个 XPath 表达式。第一个，我们右键点击 Next page 按钮，URL 位于 li 中，li 的类名含有 next。因此 XPath 表达式为 //*[contains (@class,"next")]//@href。

对于第二个表达式，我们在列表的标题上右键点击，选择检查元素；这个 URL 有一个属性是 itemprop="url"。因此，表达式确定为 //*[@itemprop="url"]/@href。打开 scrapy 命令行进行确认：

```
$ scrapy shell http://web:9312/properties/index_00000.html
>>> URL = response.xpath('//*[contains(@class,"next")]//@href').extract()
>>> URL
[u'index_00001.html']
>>> import urlparse
>>> [urlparse.urljoin(response.url, i) for i in URL]
[u'http://web:9312/scrapybook/properties/index_00001.html']
>>> URL = response.xpath('//*[@itemprop="url"]/@href').extract()
>>> URL
[u'property_000000.html', ... u'property_000029.html']
>>> len(URL)
30
>>> [urlparse.urljoin(response.url, i) for i in URL]
[u'http://..._000000.html', ... /property_000029.html']
```

很好，我们看到有了这两个表达式，就可以进行水平和垂直抓取 URL 了。

3『

下面代码是「2020061Website_Scraping_with_Python」中通过 BS4 实现的翻页功能。

```
next_page = soup.find('li', class_='next')
if next_page:
    a = next_page.find('a', href=True)
    if a:
        qm = a['href'].find('?')
        if '#' in product_page:
            product_page = product_page[:product_page.find('#')]
        queue.append(product_page + '#' + a['href'][qm + 1:]
```

』

8、使用爬虫进行二维抓取。将前一个爬虫代码复制到新的爬虫 manual.py 中（cp basic.py manual.py）；在 properties/spiders/manual.py 中，我们通过添加 from scrapy.http import Request 引入 Request，将爬虫的名字改为 manual，将 start_URL 改为索引首页，将 parse () 重命名为 parse_item ()。接下来写新的 parse () 方法进行水平和垂直的抓取：

```py
def parse(self, response):
    # Get the next index URL and yield Requests
    next_selector = response.xpath('//*[contains(@class,"next")]//@href')
    for url in next_selector.extract():
        yield Request(urlparse.urljoin(response.url, url))

    # Get item URL and yield Requests
    item_selector = response.xpath('//*[@itemprop="url"]/@href')
    for url in item_selector.extract():
        yield Request(urlparse.urljoin(response.url, url), 
        callback=self.parse_item
```

提示：你可能注意到了 yield 声明。它和 return 很像，不同之处是 return 会退出循环，而 yield 不会。从功能上讲，前面的例子与下面很像。

```py
next_requests = []
for url in...
   next_requests.append(Request(...))
for url in...
   next_requests.append(Request(...))
return next_requests
```

yield 可以大大提高 Python 编程的效率。做好爬虫了。但如果让它运行起来的话，它将抓取 5 万张页面。为了避免时间太长，我们可以通过命令 - s CLOSESPIDER_ITEMCOUNT=90（更多的设定见第 7 章），设定爬虫在一定数量（例如，90）之后停止运行。开始运行：

1『 mark 下，之前代码卡在主网页的地址弄错，应该：http://web:9312/properties/index_00000.html。一直以为是重命名函数弄错。』

查看输出，你可以看到我们得到了水平和垂直两个方向的结果。首先读取了 index_00000.html, 然后产生了许多请求。执行请求的过程中，debug 信息指明了谁用 URL 发起了请求。例如，我们看到，property_000029.html, property_000028.html ... 和 index_00001.html 都有相同的 referer (即 index_00000.html)。然后，property_000059.html 和其它网页的 referer 是 index_00001，过程以此类推。

这个例子中，Scrapy 处理请求的机制是后进先出（LIFO），深度优先抓取。最后提交的请求先被执行。这个机制适用于大多数情况。例如，我们想先抓取完列表页再取下一个索引页。不然的话，我们必须消耗内存存储列表页的 URL。另外，许多时候你想用一个辅助的 Requests 执行一个请求，下一章有例子。你需要 Requests 越早完成越好，以便爬虫继续下面的工作。

我们可以通过设定 Request () 参数修改默认的顺序，大于 0 时是高于默认的优先级，小于 0 时是低于默认的优先级。通常，Scrapy 会先执行高优先级的请求，但不会花费太多时间思考到底先执行哪一个具体的请求。在你的大多数爬虫中，你不会有超过一个或两个的请求等级。因为 URL 会被多重过滤，如果我们想向一个 URL 多次请求，我们可以设定参数 dont_filter Request () 为 True。

9、用 CrawlSpider 二维抓取。如果你觉得这个二维抓取单调的话，说明你入门了。Scrapy 试图简化这些琐事，让编程更容易。完成之前结果的更好方法是使用 CrawlSpider，一个简化抓取的类。我们用 genspider 命令，设定一个 -t 参数，用爬虫模板创建一个爬虫：

```
$ scrapy genspider -t crawl easy web
Created spider 'crawl' using template 'crawl' in module:
  properties.spiders.easy
```

现在 properties/spiders/easy.py 文件包含如下所示：

```py
...
class EasySpider(CrawlSpider):
    name = 'easy'
    allowed_domains = ['web']
    start_URL = ['http://www.web/']
    rules = (
        Rule(LinkExtractor(allow=r'Items/'),  
callback='parse_item', follow=True),
    )
    def parse_item(self, response):
        ...
```

这段自动生成的代码和之前的很像，但是在类的定义中，这个爬虫从 CrawlSpider 定义的，而不是 Spider。CrawlSpider 提供了一个包含变量 rules 的 parse () 方法，以完成之前我们手写的内容。现在将 start_URL 设定为索引首页，并将 parse_item () 方法替换。这次不再使用 parse () 方法，而是将 rules 变成两个 rules，一个负责水平抓取，一个负责垂直抓取：

```py
rules = (
Rule(LinkExtractor(restrict_xpaths='//*[contains(@class,"next")]')),
Rule(LinkExtractor(restrict_xpaths='//*[@itemprop="url"]'),
         callback='parse_item')
)
```

两个 XPath 表达式与之前相同，但没有了 a 与 href 的限制。正如它们的名字，LinkExtractor 专门抽取链接，默认就是寻找 a、href 属性。你可以设定 tags 和 attrs 自定义 LinkExtractor ()。对比前面的请求方法 Requests (self.parse_item)，回调的字符串中含有回调方法的名字（例如，parse_item）。最后，除非设定 callback，一个 Rule 就会沿着抽取的 URL 扫描外链。设定 callback 之后，Rule 才能返回。如果你想让 Rule 跟随外链，你应该从 callback 方法 return/yield，或设定 Rule () 的 follow 参数为 True。当你的列表页既有 Items 又有其它有用的导航链接时非常有用。你现在可以运行这个爬虫，它的结果与之前相同，但简洁多了：

```
$ scrapy crawl easy -s CLOSESPIDER_ITEMCOUNT=90
```

1『注意，开始自己码的一直调试不好，后来看源码后发现，少载入了 CrawlSpider 库，开头应该加上：from scrapy.spiders import Rule, CrawlSpider。』

## 0501. 快速构建爬虫

使用 FormRequest 进行登录，用请求 / 响应中的 meta 传递变量，使用了相关的 XPath 表达式和 Selectors，使用 .csv 文件作为数据源等等。

第 3 章中，我们学习了如何从网页提取信息并存储到 Items 中。大多数情况都可以用这一章的知识处理。本章，我们要进一步学习抓取流程 UR2IM 中两个 R，Request 和 Response。

一个具有登录功能的爬虫。你常常需要从具有登录机制的网站抓取数据。多数时候，网站要你提供用户名和密码才能登录。我们的例子，你可以在 http://web:9312/dynamic 或 http://localhost:9312/dynamic 找到。用用户名「user」、密码「pass」登录之后，你会进入一个有三条房产链接的网页。现在的问题是，如何用 Scrapy 登录？

让我们使用谷歌 Chrome 浏览器的开发者工具搞清楚登录的机制。首先，选择 Network 标签（1）。然后，填入用户名和密码，点击 Login（2）。如果用户名和密码是正确的，你会进入下一页。如果是错误的，会看到一个错误页。一旦你点击了 Login，在开发者工具的 Network 标签栏中，你就会看到一个发往 http://localhost:9312/dynamic/login 的请求 Request Method: POST。

提示：上一章的 GET 请求，通常用来获取静止数据，例如简单的网页和图片。POST 请求通常用来获取的数据，取决于我们发给服务器的数据，例如这个例子中的用户名和密码。

点击这个 POST 请求，你就可以看到发给服务器的数据，其中包括表单信息，表单信息中有你刚才输入的用户名和密码。所有数据都以文本的形式发给服务器。Chrome 开发者工具将它们整理好并展示出来。服务器的响应是 302 FOUND（5），然后将我们重定向到新页面：/dynamic/gated。只有登录成功时才会出现此页面。如果没有正确输入用户名和密码就前往 http://localhost:9312/dynamic/gated，服务器会发现你作弊，并将你重定向到错误页面：http://localhost:9312/dynamic/error。服务器怎么知道你和密码呢？如果你点击左侧的 gated（6），你会发现在 RequestHeaders（7）下有一个 Cookie（8）。

提示：HTTP cookie 是通常是一些服务器发送到浏览器的短文本或数字片段。反过来，在每一个后续请求中，浏览器把它发送回服务器，以确定你、用户和期限。这让你可以执行复杂的需要服务器端状态信息的操作，如你购物车中的商品或你的用户名和密码。

总结一下，单单一个操作，如登录，可能涉及多个服务器往返操作，包括 POST 请求和 HTTP 重定向。Scrapy 处理大多数这些操作是自动的，我们需要编写的代码很简单。

我们将第 3 章名为 easy 的爬虫重命名为 login，并修改里面名字的属性，如下：

```py
class LoginSpider(CrawlSpider):
    name = 'login'
```

提示：本章的代码 github 的 ch05 目录中。这个例子位于 ch05/properties。