## 记忆时间

## 目录

0401 How Universal are Turing Machines?

0501 Algorithms and Heuristics

0601 Memory: Information and Section Codes

## 0401. How Universal are Turing Machines?

W hat are the limits to what a computer can do? Must all computers be composed of Boolean logic and registers, or might there be other kinds, even more powerful? These questions take us to the most philosophically interesting topics in this book: Turing machines, computability, chaotic systems, Goedel's incompleteness theorem, and quantum computing — topics at the center of most discussions about what computers can and cannot do.

Because computers can do some things that seem very much like human thinking, people often worry that they threaten our unique position as rational beings, and there are some who seek reassurance in mathematical proofs of the limits of computers. There have been analogous controversies in human history. It was once considered important that the Earth be at the center of the universe, and our imagined position at the center was emblematic of our worth. The discovery that we occupied no central position — that our planet was just one of a number of planets in orbit around the Sun — was deeply disturbing to many people at the time, and the philosophical implications of astronomy became a topic of heated debate. A similar controversy arose over evolutionary theory, which also appeared as a threat to humankind's uniqueness. At the root of these earlier philosophical crises was a misplaced judgment of the source of human worth. I am convinced that most of the current philosophical discussions about the limits of computers are based on a similar misjudgment.

计算机的能力极限是什么？所有计算机必须由布尔逻辑和寄存器构成吗？或者，是否存在其他类型且功能更强大的计算机？这些问题将我们引入本书最具哲学意味的主题：图灵机、可计算性、混沌系统、哥德尔的不完备定理以及量子计算机。这些主题涉及一个热点话题，即计算机能做什么以及不能做什么。

因为计算机的一些行为方式与人类的思维过程十分相似，因此有些人担忧计算机会威胁到人类独为万物之灵的地位。还有些人试图用数字来证明计算机能力的局限性，以寻求慰藉。人类历史上曾发生过类似的争论。比如，曾经有一段时间，人类坚信地球是宇宙的中心。实际上，我们想象出的中心位置是人类价值的象征。当发现我们所在的星球并非处于宇宙中心，而只是围绕太阳旋转的众多行星之一时，许多人感到苦恼不安。随后，天文学的哲学意义变成争论的焦点。另一个类似的争论是关于进化论的。进化论也被视为对人类独特性的威胁。早年的这些哲学危机源自对人类自身价值的错误认知。我坚信，目前关于计算机能力极限的大部分争论同样源于类似的错误认知。

### 4.1 Truing Machines

The central idea in the theory of computation is that of a universal computer  — that is, a computer powerful enough to simulate any other computing device. The general-purpose computer described in the preceding chapters is an example of a universal computer; in fact most computers we encounter in everyday life are universal computers. With the right software and enough time and memory, any universal computer can simulate any other type of computer, or (as far as we know) any other device at all that processes information.

One consequence of this principle of universality is that the only important difference in power between two computers is their speed and the size of their memory. Computers may differ in the kinds of input and output devices connected to them, but these so-called peripherals are not essential characteristics of a computer, any more than its size or its cost or the color of its case. In terms of what they are able to do, all computers (and all other types of universal computing devices) are fundamentally identical.

The idea of a universal computer was recognized and described in 1937 by the British mathematician Alan Turing. Turing, like so many other computing pioneers, was interested in the problem of making a machine that could think, and he invented a scheme for a general-purpose computing machine. Turing referred to his imaginary construct as a「universal machine,」since at that time the word「computer」still meant「a person who performs computations.」

To picture a Turing machine, imagine a mathematician performing calculations on a scroll of paper. Imagine further that the scroll is infinitely long, so that we don't need to worry about running out of places to write things down. The mathematician will be able to solve any solvable computational problem no matter how many operations are involved, although it may take him an inordinate amount of time. Turing showed that any calculation that can be performed by a smart mathematician can also be performed by a stupid but meticulous clerk who follows a simple set of rules for reading and writing the information on the scroll. In fact, he showed that the human clerk can be replaced by a finite-state machine. The finite-state machine looks at only one symbol on the scroll at a time, so the scroll is best thought of as a narrow paper tape, with a single symbol on each line.

Today, we call the combination of a finite-state machine with an infinitely long tape a Turing machine. The tape of a Turing machine is analogous to, and serves much the same function as, the memory of a modern computer. All that the finite-state machine does is read or write a symbol on the tape and move back and forth according to a fixed and simple set of rules. Turing showed that any computable problem could be solved by writing symbols on the tape of a Turing machine — symbols that would specify not just the problem but also the method of solving it. The Turing machine computes the answer by moving back and forth across the tape, reading and writing symbols, until the solution is written on the tape.

I find Turing's particular construction difficult to think about. To me, the conventional computer, which has a memory instead of a tape, is a more comprehensible example of a universal machine. For instance, it is easier for me to see how a conventional computer can be programmed to simulate a Turing machine than vice versa. What is amazing to me is not so much Turing's imaginary construct but his hypothesis that there is only one type of universal computing machine. As far as we know, no device built in the physical universe can have any more computational power than a Turing machine. To put it more precisely, any computation that can be performed by any physical computing device can be performed by any universal computer, as long as the latter has sufficient time and memory. This is a remarkable statement, suggesting as it does that a universal computer with the proper programming should be able to simulate the function of a human brain.

图灵机

计算理论的核心思想是通用计算机，这是一类足以模拟任何类型的计算装置的计算机。我们在前几章讨论过的一般用途的计算机就属于通用计算机。实际上，我们在日常生活中遇到的大多数计算机都是通用计算机。只要安装了合适的软件，拥有足够多的时间和存储，任何通用计算机都可以模拟其他类型的计算机，或者我们所知的任何信息处理设备。

这种通用性原理产生的一个结果是，两台计算机在能力方面唯一重要的区别在于它们的运算速度和内存大小。虽然各种计算机所连接的输入和输出设备各有不同，但这些外部设备并不是计算机的关键特征，甚至没有计算机的大小、价格和外部颜色等特征重要。从本质上来说，所有类型的计算机和通用计算设备在能做哪些事上是基本相同的。

1937 年，英国数学家艾伦·图灵（Alan Turing）提出了通用计算机的概念。像许多其他计算机先驱一样，图灵对制造一台会思考的机器很感兴趣，并且提出了一种设计通用计算机的方案。图灵将设想的装置称为「通用机」，因为当时「计算机」（computer）一词特指那些「执行计算任务的计算员」。

为了更具象地描绘出计算机的计算原理，我们设想这样一个场景，有一位数学家正在纸质卷轴上进行数学运算。假设这条卷轴的长度是无限的，所以不必担心因缺纸而无法记录运算数据的情况。只要计算问题可解，那么无论它涉及多少步运算，数学家都能将其解答出来，尽管这么做会花费大量时间。图灵证明，只要按照一套在纸上读写信息的简单规则，一个头脑愚笨但细致的职员也可以完成聪明的数学家所做的任何计算。实际上，他证明在计算这件事情上，有限状态机可以代替人类。有限状态机每次只查看卷轴上的一个字符，因此我们最好将纸质卷轴想象为一条细长的纸带，其中每行只有一个字符。

如今，我们将有限状态机和无限长的纸带的结合体称为图灵机。图灵机中的纸带类似于现代计算机的内存，两者的功能大致相同。有限状态机所做的事情只有两件：在纸上读取或写入字符，以及根据简单的固定规则来回移动。图灵还证明，任何可计算的问题都能通过在图灵机的纸带上读写字符解决，这些字符不仅可以描述问题本身，还可以指明问题的解决方法。图灵机的求解方式是，不断地在纸带上前后移动、读写字符，并计算答案，直到答案出现在纸带上。

我觉得图灵构想的模型难以理解。对我来说，带有内存而非纸带的传统计算机更能轻易地解释清楚什么是通用计算机。例如，我更容易理解如何通过传统的计算机编程来模拟图灵机，反之则不然。令我感到惊叹的不是图灵构想出来的模型，而是他提出的假设，即只存在一种类型的通用计算机。据我们所知，物理世界中的任何设备都不会比图灵机拥有更强大的计算能力。更准确地来说，只要具备足够多的时间和存储空间，任何一种通用计算机都能完成所有物理计算装置所能完成的计算任务。这是一个了不起的结论，它暗示了只要我们在通用计算机上进行合理的编程，就有可能模拟出人类大脑的功能。

### 4.2 Levels of Power

How can Turing's hypothesis be true? Surely some other kind of computer could be more powerful than the ones we have described. For one thing, the computers we have discussed so far have been binary, that is, they represent everything in terms of 1 and 0. Wouldn't a computer be more powerful if it could represent things in terms of a three-state logic, like Yes, No , and Maybe? No, it would not. We know that a three-state computer would be able to do no more than a two-state computer, because you can simulate the one using the other. With a two-state computer, you can duplicate any operation that can be performed on a three-state computer, by encoding each of the three states as a pair of bits — 00 for Yes , say, and 11 for No , and 01 for Maybe. For every possible function in three-state logic, there is a corresponding function in two-state logic which operates on this representation. This is not to say that three-state computers might not have some practical advantage over two-state computers: for instance, they might use fewer wires and therefore might be smaller, or cheaper to produce. But we can say for certain that they would not be able to do anything new. They would just be one more version of a universal machine.

A similar argument holds for four-state computers, or five-state computers, or computers with any finite number of states. But what about computers that compute with analog signals — that is, signals with an infinite number of possible values? For example, imagine a computer that uses a continuous range of voltages to indicate numbers. Instead of just two or three or five possible messages, each signal could carry an infinite number of possible messages, corresponding to the continuous range of voltages. For instance, an analog computer might represent a number between 0 and 1 by a voltage between zero and one volt. The fraction could be represented to any level of precision, no matter the number of decimal places, by using the exact corresponding voltage.

Computers that represent quantities by such analog signals do exist, and in fact the earliest computers worked this way. They are called analog computers , to distinguish them from the digital computers we have been discussing, which have a discrete number of possible messages in each signal. One might suppose that analog computers would be more powerful, since they can represent a continuum of values, whereas digital computers can represent data only as discrete numbers. However, this apparent advantage disappears if we take a closer look. A true continuum is unrealizable in the physical world.

The problem with analog computers is that their signals can achieve only a limited degree of accuracy. Any type of analog signal — electrical, mechanical, chemical — will contain a certain amount of noise; that is, at a certain level of resolution, the signal will be essentially random. Any analog signal is bound to be affected by numerous irrelevant and unknown sources of noise: for example, an electrical signal can be disturbed by the random motion of molecules inside a wire, or by the magnetic field created when a light is turned on in the next room. In a very good electrical circuit, this noise can be made very small — say, a millionth the size of the signal itself — but it will always exist. While there are an infinite number of possible signal levels, only a finite number of levels represent meaningful distinctions — that is, represent information. If one part in a million in a signal is noise, then there are only about a million meaningful distinctions in the signal; therefore, information in the signal can be represented by a digital signal that uses twenty bits (2 20 = 1,048,578). Doubling the number of meaningful distinctions in an analog computer would require making everything twice as accurate, whereas in a digital computer you could double the number of meaningful distinctions by adding a single bit. The very best analog computers have fewer than thirty bits of accuracy. Since digital computers often represent numbers using thirty-two or sixty-four bits, they can in practice generate a much larger number of meaningful distinctions than analog computers can.

Some people might argue that while the noise of an analog computer may not be meaningful, it is not necessarily useless. One can certainly imagine computations that are helped by the presence of noise. Later, for example, we will describe computations requiring random numbers. But a digital computer, too, can generate random noise if randomness is called for in a computation.

计算能力等级

图灵的假设为何得以成立？有些类型的计算机的功能确实比我们所提到的计算机更为强大。目前为止，我们所讨论的都是二进制的计算机，也就是说，它们用 1 和 0 表示一切。如果计算机采用包含三种状态的逻辑来表示一切，比如「是」「否」「可能」等，那么它的计算能力是否会更强大呢？答案是否定的。我们已经知道，三态逻辑计算机的功能不会比双态逻辑计算机更强大，因为我们可以使用后者模拟出前者。双态逻辑计算机可以模拟出三态逻辑计算机所能执行的所有运算，方法就是用一组二进制位对三种状态分别编码，比如用 00 表示「是」，用 11 表示「否」，用 01 表示「可能」。三态逻辑计算机中的每种可能的功能都可以在双态逻辑计算机中找到对应的功能。不过，这并不意味着三态逻辑计算机不具备任何实际应用层面的优势。实际上，这类计算机所需的电线更少，因此体积更小，造价更低廉。不过，我们可以肯定的是，三态逻辑计算机并不是一种创新，而是另一种版本的通用计算机。

同理，四态逻辑计算机、五态逻辑计算机以及任何有限态逻辑计算机也是如此。那么，如果计算机采用模拟信号来进行计算，情况又如何呢？换言之，这种信号具有无数多种可能的数值。例如，假设一台计算机用一段连续的电压值范围来表示数。对应于连续分布的电压值，每个信号都能携带无数种信息，而不仅仅是两种、三种或五种。例如模拟计算机可通过 0～1 伏特之间的电压表示 0～1 之间的数。只要电压值足够精确，无论这个分数有多少位小数，分数的表示都可达到任意精度。

不过，采用模拟信号来表示定量数值的计算机确实存在。事实上，最早的计算机就是以这种方式工作的。为了区别于前面所说的数字计算机，它们被称为模拟计算机。数字计算机中的信号只包含几种离散的信息。有些人可能会认为，模拟计算机的功能更强大，因为它们用连续的值来表示数据，而数字计算机只能用离散的值来表示数据。实际上，模拟计算机并无特别优势，因为真正的连续流在物理世界中是无法实现的。

模拟计算机的缺点是，它们的信号精度是有限的。所有类型的模拟信号都会包含一定量的噪声，比如电子信号、机械信号和化学信号等。也就是说，当达到一定精度后，信号基本上是随机的。所有的模拟信号都会受到许多不相关的和未知噪声源的影响。例如，电子信号会受到电线中分子的随机运动的干扰，或者受到邻居房间中灯亮起时产生的磁场的干扰。虽然在良好的电路中，这种噪声可降至很低的水平，比如信号本身的百万分之一，但它依然存在。因此，虽然信号拥有无数种强度水平，但真正起到有意义的区分作用（即表示信息）的信号数目却是有限的。如果信号中有百万分之一的噪声，那么对于信号来说，只有约一百万种有意义的差异。因此，用一个 20 个二进制位的数字信号可以表示出信号中的全部信息（2^20=1 048 578）。若想让模拟计算机中的有意义的差异翻倍，就必须将所有东西的精确度都提高一倍。然而，对于数字计算机来说，只需增加一个二进制位即可。目前性能最好的模拟计算机的精确度不超过 30 个二进制位。由于数字计算机通常会用 32 位或者 64 位来表示数，因此数字计算机产生的有意义的差异比模拟计算机更高。

有些人可能会反驳，虽然模拟计算机中的噪声看起来没有意义，但也并非毫无用处。我们当然可以设想噪声有助于计算，例如，我们接下来要讲的随机数。不过，如果计算中涉及随机性，数字计算机也能生成随机噪声。

### 4.3 Random Numbers

How can a digital computer generate randomness? Can a deterministic system like a computer produce a truly random sequence of numbers? In a formal sense, the answer is No, since everything a digital computer does is determined by its design and its inputs. But the same could be said of a roulette wheel — after all, the ball's final landing place is determined by the physics of the ball (its mass, its velocity) and the spinning wheel. If we knew the exact design of the apparatus and the exact「inputs」governing the spin of the wheel and the throw of the ball, we could predict the number on which the ball would land. The outcome appears random because it exhibits no obvious pattern and is difficult, in practice, to predict.

Like the roulette wheel, a computer can produce a sequence of numbers that is random in the same sense. In fact, using a mathematical model, the computer could simulate the physics of the roulette wheel and throw a simulated ball at a slightly different angle each time in order to produce each number in the sequence. Even if the angles at which the computer throws the simulated ball follow a consistent pattern, the simulated dynamics of the wheel would transform these tiny differences into what amounts to an unpredictable sequence of numbers. Such a sequence of numbers is called a pseudorandom sequence, because it only appears random to an observer who does not know how it was computed. The sequence produced by a pseudorandom number generator can pass all normal statistical tests of randomness.

A roulette wheel is an example of what physicists call a chaotic system  — a system in which a small change in the initial conditions (the throw, the mass of the ball, the diameter of the wheel, and so forth) can produce a large change in the state to which the system evolves (the resulting number). This notion of a chaotic system helps explain how a deterministic set of interactions can produce unpredictable results. In a computer, there are simpler ways to produce a pseudorandom sequence than simulating a roulette wheel, but they are all conceptually similar to this model.

Digital computers are predictable and unpredictable in exactly the same senses as the rest of the physical world. They follow deterministic laws, but these laws have complicated consequences that are extremely difficult to predict. It is often impractical to guess what computers are going to do before they do it. As is true of physical systems, it does not take much to make a computation complex. In computers, chaotic systems — systems whose outcomes depend sensitively on the initial conditions — are the norm.

随机数

数字计算机如何才能产生随机性呢？像计算机这样的确定性系统能否产生真正意义上的随机数序列？严格来讲，答案是否定的。因为计算机中的一切都取决于其设计和输入。对于轮盘机来说也是如此，毕竟，球最终停下来的位置取决于球的物理特性（质量、速度）以及轮盘机的物理因素。如果我们能掌握轮盘机的具体设计信息、控制轮盘转动的「输入」和球的投掷，就能预测出球会落在哪一个数字上。由于球的运动并无明显的模式可循，所以结果是随机的，在实践中难以预测。

和轮盘机一样，计算机可以产生相同意义上的随机数序列。实际上，计算机可以基于数学模型模拟出轮盘机的物理结构，并每次以略有不同的角度投掷一个模拟球，从而产生随机数序列中的某个数。即使计算机投掷模拟球的角度遵循相同的模式，但动态模拟轮盘机的过程会将这些微小的角度差异转换为无法预测的序列。这种数的序列称为伪随机数序列，因为其随机性只有对计算过程一无所知的观察者来说，才是随机的。由伪随机数发生器产生的序列，可以通过所有标准的随机性统计测试。

轮盘机是物理学家称之为混沌系统的一个例子。在这个系统中，初始条件（比如投掷、球的质量、轮盘的直径等）的微小扰动会对系统的最终状态带来巨大影响。混沌系统的概念解释了为何一个确定性的交互系统中会出现不可预测的结果。在计算机中，有比模拟轮盘机更简单地生成伪随机数序列的方法。不过，在概念上，这些方法与轮盘机模型是相似的。

1-3『混沌系统，第一次知道这个概念是在书籍「复杂」里，原来伪计算机里随机数的实现原理也是靠它。（2021-10-10）』

如同物理世界中的万物，计算机既是可预测的，也是不可预测的。虽然它们都遵循确定性原则，但这些原则导致的复杂结果很难预测。在计算机完成某件事之前，我们难以预测它将会做什么。与物理系统一样，计算很容易变复杂。在计算机中，混沌系统随处可见，其结果敏感地取决于初始条件。

### 4.4 Computability

While a universal computer can compute anything that can be computed by any other computing device, there are some things that are just impossible to compute. Of course, it is not possible to compute answers to vaguely defined questions, like「What is the meaning of life?」or questions for which we lack data, like「What is the winning number in tomorrow's lottery?」But there are also flawlessly defined computational problems that are impossible to solve. Such problems are called noncomputable.

I should warn you that noncomputable problems hardly ever come up in practice. In fact, it is difficult to find examples of a well-defined noncomputable problem that anybody wants to compute. A rare example of a well-defined, useful, but noncomputable problem is the halting problem. Imagine that I want to write a computer program that will examine another computer program and determine whether or not that program will eventually stop. If the program being examined has no loops or recursive subroutine calls, it is bound to finish eventually, but if it does have such constructs the program may well go on forever. It turns out that there is no algorithm for examining a program and determining whether or not it is fatally infected with an endless loop. Moreover, it's not that no one has yet discovered such an algorithm; rather, no such algorithm is possible. The halting problem is noncomputable.

To understand why, imagine for a moment that I do have such a program, called Test-for-Halt, and that it takes the program to be tested as an input. (Treating a program as data may seem strange, but it's perfectly possible, because a program, just like anything else, can be represented as bits.) I could insert the Test-for-Halt program as a subroutine in another program, called Paradox, which will perform Test-for-Halt on Paradox itself. Imagine that I have written the Paradox program so that whatever Test-for-Halt determines, Paradox will do the opposite. If Test-for-Halt determines that Paradox is eventually going to halt, then Paradox is programmed to go into an infinite loop. If Test-for-Halt determines that Paradox is going to go on forever, then Paradox is programmed to halt. Since Paradox contradicts Test-for-Halt, Test-for-Halt doesn't work on Paradox; therefore, it doesn't work on all programs. And therefore a program that computes the halting function cannot exist.

The halting problem, which was dreamed up by Alan Turing, is chiefly important as an example of a noncomputable problem, and most noncomputable problems that do come up in practice are similar to or equivalent to it. But a computer's inability to solve the halting problem is not a weakness of the computer, because the halting problem is inherently unsolvable. There is no machine that can be constructed that can solve the halting problem. And as far as we know, there is nothing that can perform any other computation that cannot be performed by a universal machine. The class of problems that are computable by a digital computer apparently includes every problem that is computable by any kind of device. (This last statement is sometimes called the Church thesis, after one of Turing's contemporaries, Alonzo Church. Mathematicians had been thinking about computation and logic for centuries but — in one of the more dazzling examples of synchrony in science — Turing, Church, and another British mathematician named Emil Post all independently invented the idea of universal computation at roughly the same time. They had very different ways of describing it, but they all published their results in 1937, setting the stage for the computer revolution soon to follow.)

Another noncomputable function, closely related to the halting problem, is the problem of deciding whether any given mathematical statement is true or false. There is no algorithm that can solve this problem, either — a conclusion of Goedel's incompleteness theorem, which was proved by Kurt Goedel in 1931, just before Turing described the halting problem. Goedel's theorem came as a shock to many mathematicians, who until then had generally assumed that any mathematical statement could be proved true or false. Goedel's theorem states that within any self-consistent mathematical system powerful enough to express arithmetic, there exist statements that can neither be proved true nor false. Mathematicians saw their job as proving or disproving statements, and Goedel's theorem proved that their「job」was in certain instances impossible.

Some mathematicians and philosophers have ascribed almost mystical properties to Goedel's incompleteness theorem. A few believe that the theorem proves that human intuition somehow surpasses the power of a computer — that human beings may be able to「intuit」truths that are impossible for machines to prove or disprove. This is an emotionally appealing argument, and it is sometimes seized upon by philosophers who don't like being compared to computers. But the argument is fallacious. Whether or not people can successfully make intuitive leaps that cannot be made by computers, Goedel's incompleteness theorem provides no reason to believe that there are mathematical statements that can be proved by a mathematician but can't be proved by a computer. As far as we know, any theorem that can be proved by a human being can also be proved by a computer. Humans cannot compute noncomputable problems any more than computers can.

Although one is hard pressed to come up with specific examples of noncomputable problems, one can easily prove that most of the possible mathematical functions are noncomputable. This is because any program can be specified in a finite number of bits, whereas specifying a function usually requires an infinite number of bits, so there are a lot more functions than programs. Consider the kind of mathematical function that converts one number into another — the cosine, say, or the logarithm. Mathematicians can define all kinds of bizarre functions of this type: for example the function that converts every decimal number into the sum of its digits. As far as I know, this function is a useless one, but a mathematician would regard it as a legitimate function simply because it converts every number into exactly one other number. It can be proved mathematically that there are infinitely more functions than programs. Therefore, for most functions there is no corresponding program that can compute them. The actual counting involves all kinds of difficulties (including counting infinite things and distinguishing between various degrees of infinity!), but the conclusion is correct: statistically speaking, most mathematical functions are noncomputable. Fortunately, almost all these noncomputable functions are useless, and virtually all the functions we might want to compute are computable.

可计算性

尽管通用计算机可以计算其他所有类型的计算装置能做的计算，但有些问题本身就是不可解的。当然，有些定义模糊的问题，比如「生命的意义是什么」，或者缺乏数据支持的问题，比如「明天彩票的中奖号码是什么」，其答案无法通过计算得出。不过，有些定义明确的计算问题也无法通过计算解决，这类问题被称为不可计算的问题。

我需要提醒大家的是，实践中几乎不会出现不可计算的问题。而且，我们很难找到一个定义明确且大家都愿意去研究的不可计算的问题。一个罕见的例子是停机问题，虽然这个问题定义明确，且有实际用处，却是不可计算的。假设我们现在要编写一个判断其他程序是否会在有限时间内停止工作的计算机程序。如果被检查的程序中没有循环和子程序递归调用，那么它必然会停止工作。然而，如果这个程序中满足上述条件，那么它可能会永远地运行下去。可以证明的是，并不存在一种算法可以检查并判定某个程序是否会陷入无限循环状态。此外，并不是没有人去寻找这一算法，而是这种算法根本不存在。停机问题是不可计算的。

若想了解这背后的原因，我们先假设找到了解决停机问题的程序，其名为「停机检测」程序。这一程序将被检测的程序作为输入。虽然将程序作为输入可能看起来很诡异，但这是完全有可能的，因为程序和其他事物一样，可以用二进制位来表示。将「停机检测」这个程序作为一个子程序嵌入另一个名为「悖论」的程序中，「悖论」程序将对自己进行停机检测。假设「悖论」程序所要做的事情就是做出与「停机检测」程序输出的决定相反的动作。如果「停机检测」程序判定「悖论」程序最终会停机，那么便会通过编程让「悖论」进入无限循环状态；如果「停机检测」程序判定「悖论」程序会进入无限循环状态，就会通过编程让「悖论」程序最终停机。由于「停机检测」程序和「悖论」程序两者存在矛盾，「停机检测」程序无法判定「悖论」程序是否会停机。因此，「停机检测」程序并不是对所有程序都有效，所以不存在一个可以解决停机问题的程序。

图灵设想的停机问题是一个非常重要的不可计算问题，实践中遇到的大多数不可计算问题都与它类似或者相同。虽然计算机无法解决停机问题，但这并非计算机的弱点，因为停机问题本质上是不可解的，因此也无法建造出能解决停机问题的机器。据我们所知，不存在其他装置能完成而通用计算机完成不了的计算。显然，数字计算机可以计算所有其他类型的装置能计算的问题。这个结论有时也被称为「丘奇命题」，以纪念图灵的同时代数学家阿朗佐·丘奇（Alonzo Church）。在他的研究基础上，数学家花费了大量时间来研究计算和逻辑问题。图灵、丘奇和另一位名为埃米尔·波斯特（Emil Post）的英国数学家几乎同时提出了通用计算机这一概念，这是一个体现科学同步性的典型案例。虽然他们采用的描述方法各不相同，但都于 1937 年发表了各自的成果，这为即将到来的计算机革命奠定了基础。

还有一个与停机问题密不可分的不可计算问题 —— 判定任何给定数学命题的真假。没有算法能够解决这个问题，这是库尔特·哥德尔（Kurt Goedel）于 1931 年证明的哥德尔不完备性定理中的一个结论，该结论的提出时间正好在图灵提出停机问题之前。哥德尔的定理使许多数学家大为震惊，因为在这之前，他们认为任何数学命题要么为真，要么为假。哥德尔的定理表明，在任何一个足以描述算术运算的逻辑自洽的数学体系中，一定存在着既不能被证实也不能被证伪的命题。数学家向来视证明命题的真假为天职，而哥德尔的定理却证明，他们的工作在某些情况下是不可能完成的。

一些数学家和哲学家将所有的未解之谜都归咎于哥德尔不完备性定理。还有少数人认为，这条定理证明了在某种程度上，人类的直觉超越了计算机的能力，因为人类能够通过直觉推导出机器无法判定真假的事实。从感情色彩上来说，这个论调很吸引人，那些不喜欢将计算机和人类相提并论的哲学家有时会抓住这个论点不放。然而，这个论点是靠不住的，无论人类是否能完成计算机所不能完成的直觉上的跳跃，哥德尔不完备性定理都没有说明，存在数学家能证明但计算机无法证明的数学命题。据我们所知，凡是能由人类证明的定理，计算机也能证明。计算机无法解决的不可计算问题，人类也无法解决。

尽管我们很难找到不可计算问题的具体例子，但可以轻易地证明大多数数学函数都是不可计算的。这是因为，所有程序都可以通过有限位的二进制位来表示。不过表示一个函数需要无数位的二进制位，因此函数要远多于程序。我们可以考虑一下那些将一个数字转换成另一个数字的数学函数，比如余弦函数或者对数函数。数学家总能定义出各种奇怪的函数。比如，将十进制数转换成其位数之和的函数。在我看来，这个函数毫无用处，但数学家认为这是一个正规的函数，因为它能将一个数字转换成另一个数字。应用数学可以证明：函数是无限的，远远多于程序。因此对于大多数函数来说，没有对应的程序可以计算它们。如果对函数和程序进行实际计数，会遇到各种困难，比如计算无限的事物，区分不同程度的无限性等。不过，这个结论是正确的，因为统计表明：大多数数学函数是不可计算的。幸运的是，几乎所有这些不可计算的函数都没有用处。实际上，我们想计算的函数都是可计算的。

2『函数是不可计算问题的典型案例，做一张信息数据卡片。（2021-10-10）』—— 已完成

### 4.5 Quantum Computing

As noted earlier, the pseudorandom number sequences produced by computers look random, but there is an underlying algorithm that generates them. If you know how a sequence is generated, it is necessarily predictable and not random. If ever we needed an inherently unpredictable random-number sequence, we would have to augment our universal machine with a nondeterministic device for generating randomness.

One might imagine such a randomness-generating device as being a kind of electronic roulette wheel, but, as we have seen, such a device is not truly random because of the laws of physics. The only way we know how to achieve genuinely unpredictable effects is to rely on quantum mechanics. Unlike the classical physics of the roulette wheel, in which effects are determined by causes, quantum mechanics produces effects that are purely probabilistic. There is no way of predicting, for example, when a given uranium atom will decay into lead. Therefore one could use a Geiger counter to generate truly random data sequences — something impossible in principle for a universal computer to do.

The laws of quantum mechanics raise a number of questions about universal computers that no one has yet answered. At first glance, it would seem that quantum mechanics fits nicely with digital computers, since the word「quantum」conveys essentially the same notion as the word「digital.」Like digital phenomena, quantum phenomena exist only in discrete states. From the quantum point of view, the (apparently) continuous, analog nature of the physical world — the flow of electricity, for example — is an illusion caused by our seeing things on a large scale rather than an atomic scale. The good news of quantum mechanics is that at the atomic scale everything is discrete, everything is digital. An electric charge contains a certain number of electrons, and there is no such thing as half an electron. The bad news is that the rules governing how objects interact at this scale are counterintuitive.

For instance, our commonsense notions tell us that one thing cannot be in two places at the same time. In the quantum mechanical world this is not exactly true, because in quantum mechanics nothing can be exactly in any place at all. A single subatomic particle exists everywhere at once, and we are just more likely to observe such a particle at one place than at another. For most purposes, we can think of a particle as being where we observe it to be, but to explain all observed effects we have to acknowledge that the particle is in more than one place. Almost everyone, including many physicists, find this concept difficult to comprehend.

Might we take advantage of quantum effects to build a more powerful type of computer? As of now, this question remains unanswered, but there are suggestions that such a thing is possible. Atoms seem able to compute certain problems easily, such as how they stick together — problems that are very difficult to compute on a conventional computer. For instance, when two hydrogen atoms bind to an oxygen atom to form a water molecule, these atoms somehow「compute」that the angle between the two bonds should be 107 degrees. It is possible to approximately calculate this angle from quantum mechanical principles using a digital computer, but it takes a long time, and the more accurate the calculation the longer it takes. Yet every molecule in a glass of water is able to perform this calculation almost instantly. How can a single molecule be so much faster than a digital computer?

The reason it takes the computer so long to calculate this quantum mechanical problem is that the computer would have to take into account an infinite number of possible configurations of the water molecule to produce an exact answer. The calculation must allow for the fact that the atoms comprising the molecule can be in all configurations at once. This is why the computer can only approximate the answer in a finite amount of time. One way of explaining how the water molecule can make the same calculation is to imagine it trying out every possible configuration simultaneously — in other words, using parallel processing. Could we harness this simultaneous computing capability of quantum mechanical objects to produce a more powerful computer? Nobody knows for sure.

Recently there have been some intriguing hints that we may be able to build a quantum computer that takes advantage of a phenomenon known as entanglement. In a quantum mechanical system, when two particles interact, their fates can become linked in a way utterly unlike anything we see in the classical physical world: when we measure some characteristic of one of them, it affects what we measure in the other, even if the particles are physically separated. Einstein called this effect, which involves no time delay,「spooky action at a distance,」and he was famously unhappy with the notion that the world could work that way.

A quantum computer would take advantage of entanglement: a one-bit quantum mechanical memory register would store not just a 1 or a 0; it would store a superposition of many 1's and many 0's. This is analagous to an atom being in many places at once: a bit that it is in many states (1 or 0) at once. This is different from being in an intermediate state between a 1 and a 0, because each of the superposed 1's and 0's can be entangled with other bits within the quantum computer. When two such quantum bits are combined in a quantum logic block, each of their superposed states can interact in different ways, producing an even richer set of entanglements. The amount of computation that can be accomplished by a single quantum logic block is very large, perhaps even infinite.

The theory behind quantum computing is well established, but there are still problems in putting it to use. For one thing, how can we use all this computation to compute anything useful? The physicist Peter Shor recently discovered a way to use these quantum effects — at least, in principle — to do certain important and difficult calculations like factoring large numbers, and his work has renewed interest in quantum computers. But many difficulties are still there. One problem is that the bits in a quantum computer must remain entangled in order for the computation to work, but the smallest of disturbances — a passing cosmic ray, say, or possibly even the inherent noisiness of the vacuum itself — can destroy the entanglement. (Yes, in quantum mechanics even a vacuum does strange things.) This loss of entanglement, called decoherence , could turn out to be the Achilles heel of quantum mechanical computers. Moreover, Shor's methods seem to work only on a specific class of computations which can take advantage of a fast operation called a generalized Fourier transform. The problems that fit into this category may well turn out to be easy to compute on a classical Turing machine; if so, Shor's quantum ideas would be equivalent to some program on a conventional computer.

If it does become possible for quantum computers to search an infinite number of possibilities at once, then they would be qualitatively, fundamentally more powerful than conventional computing machines. Most scientists would be surprised if quantum mechanics succeeds in providing a kind of computer more powerful than a Turing machine, but science makes progress through a series of surprises. If you're hoping to be surprised by a new sort of computer, quantum mechanics is a good area to keep an eye on.

This leads us back to the philosophical issues touched on at the beginning of the chapter — that is, the relationship between the computer and the human brain. It is certainly conceivable, as at least one well-known physicist has speculated (to hoots from most of his colleagues), that the human brain takes advantage of quantum mechanical effects. Yet there is no evidence whatsoever that this is the case. Certainly, the physics of a neuron depends on quantum mechanics, just as the physics of a transistor does, but there is no evidence that neural processing takes place at the quantum mechanical level as opposed to the classical level; that is, there is no evidence that quantum mechanics is necessary to explain human thought. As far as we know, all the relevant computational properties of a neuron can be simulated on a conventional computer. If this is indeed the case, then it is also possible to simulate a network of tens of billions of such neurons, which means, in turn, that the brain can be simulated on a universal machine. Even if it turns out that the brain takes advantage of quantum computation, we will probably learn how to build devices that take advantage of the same effects — in which case it will still be possible to simulate the human brain with a machine.

The theoretical limitations of computers provide no useful dividing line between human beings and machines. As far as we know, the brain is a kind of computer, and thought is just a complex computation. Perhaps this conclusion sounds harsh to you, but in my view it takes nothing away from the wonder or value of human thought. The statement that thought is a complex computation is like the statement sometimes made by biologists that life is a complex chemical reaction: both statements are true, and yet they still may be seen as incomplete. They identify the correct components, but they ignore the mystery. To me, life and thought are both made all the more wonderful by the realization that they emerge from simple, understandable parts. I do not feel diminished by my kinship to Turing's machine.

量子计算

如前所述，计算机产生的伪随机数序列看起来是随机的，但实际上是由一个潜在的算法产生的。如果你知道序列的生成过程，那么这个序列必定是可预测的，而不是随机的。如果我们需要一个无法预测的随机数序列，就必须在通用计算机中添加一个能产生随机性的非确定性装置。

有些人认为，这种随机性生成装置是一种电子式的轮盘机。然而，正如我们所看到的，由于物理规律的限制，这种装置并不是真正随机的。唯一能真正产生不可预测的结果的方法是依靠量子力学。在关于轮盘机的经典物理学模型中，原因决定了结果。然而，量子力学与之不同，它产生的结果是完全随机的。例如，一个给定的铀原子何时会衰变为铅原子，这是不可预测的。因此，我们可以用一个盖革计数器来生成真正的随机数序列。从原理上来说，这是通用计算机永远无法做到的。

量子力学引出了一系列关于通用计算机的问题，目前无人能给出答案。初看起来，量子力学与数字计算机非常拟合，因为「量子」与「数字」两词传达了相同的理念。量子现象与数字现象一样，只存在于离散状态之中。从量子的角度来看，物理世界呈现出来的连续和模拟的特性，比如电流的流动，只是我们在比原子尺度更宏观的尺度上的所见所闻导致的错觉。好消息是，在量子力学的世界中，原子尺度上的一切都是离散的，一切都是数字化的。电荷由一堆电子组成，而电子不能再被分割。坏消息则是，在微观尺度下，物体之间相互作用的物理规律是违反常识的。

举例来说，常识告诉我们，同一个物体不可能同时出现在两个地方。然而，在量子力学的世界中，这并不完全正确，因为没有任何物体所处的位置是完全精确的。一个亚原子粒子可以同时存在于所有的空间，只是我们在某一地点观测到它们的概率高于其他地点而已。在大多数情况下，我们可以认为粒子就位于我们观测到它们的那个地方，但为了解释我们观察到的所有现象，我们必须承认粒子的位置不止一个。几乎所有人都难以理解这个概念，包括许多物理学家。

我们可以利用量子效应制造出更强大的计算机吗？目前为止，这个问题依然没有答案，但有迹象表明，这是有可能的。原子似乎更擅长于解答某些问题，比如原子之间是如何相互作用的，而这些问题恰恰是传统计算机难以解答的。例如，当两个氢原子和一个氧原子结合形成一个水分子时，这些原子就以某种方式计算出了两键之间的角度应该为 107 度。根据量子力学原理，数字计算机也可以大致计算出这一角度，但需要耗费很长的时间。而且，若想计算结果越准确，所用的时间就越长。然而，一杯水中的每个分子几乎可以瞬间完成此运算。为何单个分子的运算速度比数字计算机快得多呢？

计算机计算这个量子力学问题之所以需要很长时间，原因在于，它必须考虑该水分子的无数多种可能的原子组态，才能得出精确的答案。同时，计算过程中还要考虑这些因素：构成水分子的原子可以同时处于所有的组态。这是计算机在有限时间内只能得到近似答案的原因。为了解释水分子为何能完成同样的计算，我们可以假设水分子能同时得出所有的原子组态，换句话说，它采用了并行计算的方式。我们能否利用量子力学对象的这种并行计算能力制造出更强大的计算机呢？无人知晓确切的答案。

最近出现了一些引人关注的迹象，我们似乎可以利用一种被称为量子纠缠的现象制造出量子计算机。在量子力学系统中，当两个粒子互相作用时，它们的命运就会连接在一起，这种连接方式超出了我们在经典物理世界中的认知。当我们测量其中一个粒子的某些属性时，会干扰到另一个粒子的测量值，即使这两个粒子在物理空间上相隔甚远。爱因斯坦称这种没有时延的物理效应为「远距离作用的幽灵」，他对世界竟然以这种方式运行而感到不快。

量子计算机可以利用量子纠缠效应来建造，这样一来，一个二进制位的量子寄存器存储的不再是一个 1 或者一个 0，而是许多个 1 和 0 的叠加态。这种情况类似于一个原子同时存在于多个地方，一个二进制位也可以同时处于多种状态（1 或 0）。不过，这种叠加态区别于 1 和 0 之间的中间态，因为 1 和 0 叠加之后还能与量子计算机中的其他二进制位产生纠缠效应。当两个这样的量子二进制位在量子逻辑块中组合时，它们产生的叠加态会以不同的方式相互作用，进而产生更为丰富的纠缠行为。因此，单个量子逻辑块可以完成的计算量非常大，甚至可能是无限的。

虽然量子计算背后的理论已经相当成熟，但在使用它的过程中仍然存在一些问题。比如，我们如何利用量子计算来实现有价值的计算？物理学家彼得·肖尔（Peter Shor）提出了一种方法，它可以利用量子效应来完成某些意义重大且难度很高的计算，比如大数因式分解的计算。这一工作重新点燃了人们对量子计算机的兴趣。不过，前进的道路上依然存在许多困难，其中一个便是，量子计算机中的二进制位必须始终处于纠缠状态才能使计算生效，一旦出现极小的扰动，比如由宇宙射线或者真空本身的噪声引起的扰动，就会破坏纠缠效应。是的，在量子力学中，即使真空的特性也很怪异。量子纠缠效应的丧失现象被称为脱散，它可能会成为量子计算机的致命弱点。此外，肖尔的方法似乎只适用于特殊类型的计算问题，这类计算需要用到被称为广义傅立叶变换的快速运算。经典的图灵机也能轻易地解决这一类问题；如果真是如此，肖尔的量子算法将无异于传统计算机中的某些程序。

如果量子计算机确实能同时搜索无限多种可能性，那么其计算能力将从本质上超过传统计算机。如果真的能够利用量子力学制造出比图灵机更强大的计算机，大部分科学家都会惊叹不已。然而，科学正是在一系列出人意料之中取得进步的。如果你希望出现一种令人惊叹的新型计算机，那么量子力学是一个值得关注的领域。

这将我们又带回到了本章开头所讨论的哲学话题：计算机和人类大脑之间的关系。这当然是可以想象的，因为至少有一位著名物理学家推测，人类大脑利用了量子力学效应。然而，没有任何证据证明事实就是如此。当然，神经元的物理特性取决于量子力学，就如同晶体管的物理特性取决于量子力学。不过，没有证据表明神经元的信息处理发生在量子力学这一级，而非经典物理学这一级。也就是说，没有证据表明必须用量子力学来解释人类的思维。实际上，我们可以在传统计算机上模拟出神经元中所有相关的计算属性。如果事实真是如此，那么我们也可以模拟出由数百亿神经元组成的神经网络。这也就意味着，我们能通过一台通用计算机模拟出大脑。即使事实证明，大脑得益于量子计算的优势，我们也有可能利用量子效应制造出对应的装置。在这种情况下，用计算机来模拟大脑仍是可能的。

计算机在理论上的局限性并不意味着人类和计算机之间存在一条有实际意义的分界线。实际上，大脑相当于一台计算机，而思维只不过是一种复杂的计算。虽然这个结论听起来可能很残酷，但在我看来，这丝毫不妨碍人类思维的非凡价值。「思维是复杂的计算」这一说法和生物学家所说的「生命是一种复杂的化学反应」一样，两者都是正确的，但并不完整。它们都说出了正确的那一部分，但忽视了其中隐藏的谜团。我认为，生命和思维都是从简单、易懂的事物中萌生而来的，这使它们变得更为奇妙。我不会因为自己与图灵机的亲密关系而感到人类是多么卑微。

## 0501. Algorithms and Heuristics

W hen I was an undergraduate at MIT, one of my roommates had several dozen pairs of socks, each pair with a slightly different color or design. He frequently postponed doing his laundry until he was completely out of clean socks, so whenever he washed them he had the not inconsiderable task of matching them all up again in pairs. Here is the way he would do it: First, he would pull a random sock out of the pile of clean laundry, then he would extract another sock at random and compare it to the first to see if it matched. If it didn't, he would throw the second sock back and pull out another one. He would keep doing this until he found a match, and then he would go through the same sequence all over again with a new sock. Since he had to look though a lot of laundry, the process went very slowly — especially at the beginning, because there were a lot more socks to be examined before a match turned up.

He was studying for a degree in mathematics, and was apparently taking some kind of course in computers. One day when he had hauled his laundry basket back to our rooms, he announced,「I have decided to use a better algorithm for matching my socks.」What he meant was that he was now going to use a procedure of a fundamentally different nature. He pulled out the first sock and set it on the table, then he pulled out the next sock and compared it with the first sock; since it didn't match, he set it next to the first. Now each time he pulled out a sock he would compare it to the growing row of socks on the table. When he found a match, he would bundle that pair together and throw them in his sock drawer. When he didn't, he would add the unmatched sock to the row. Using this method, he was able to pair up his socks in a small fraction of the time it had previously required. His parents, who had paid a great deal of money for his college education, would have been proud to know that he was putting his newfound learning to such practical use.

在麻省理工学院读本科时，我有这样一位室友，他备有几十双袜子，每双袜子的颜色或样式都略有不同，在将所有干净的袜子穿完之前，他一般都不会洗袜子。因此，每次洗完袜子后，他都要完成一项艰巨的任务 —— 给袜子配对。他是这么做这项工作的：先从一堆干净的袜子中随机抽出一只，然后再随机抽取另一只与第一只袜子进行比较，看是否匹配。如果配对失败，他会将第二只袜子扔回去，然后抽出另一只袜子。他会不断地重复这个过程直至找到一双匹配的袜子。然后，他会再拿出一只新袜子并重复这个过程。这种配对方式进度十分缓慢，因为需要翻看许多袜子，尤其是在开始阶段，需要翻看更多袜子才能配对成功。

室友当时正在攻读数学学位，他还选修了一门计算机类的课程。有一天，他把洗衣篮带进宿舍并宣布道：「我决定利用更好的‘算法'来配对袜子。」他的意思是采用一种完全不同的方法。他取出第一只袜子放到桌子上，然后取出另一只和第一只进行比较；如果这两只不是同一双袜子，他就把第二只袜子放到第一只旁边。现在他每次取出一只袜子，都会和桌上越来越多的袜子进行比较。一旦配对成功，就将它们绑在一起并丢进装袜子的抽屉里；当两者不匹配时，他就把这只袜子放到桌上的袜子中。自从他采用这一方法后，只需很短的时间就能将所有袜子分好类。他父母为他的大学教育花费了一大笔钱，如果知道他在实践中运用所学的新知识，一定会备感骄傲。

### 5.1 The Algorithmic Guarantee

With or without socks, an algorithm is a fail-safe procedure, guaranteed to achieve a specific goal. The word「algorithm」comes from the name of the Arabian mathematician al-Khwarizmi, who wrote down an extensive collection of algorithms in the ninth century. The word「algebra,」in fact, comes from al jabr (「the transposition」), a term in the title of one of his books. Many of al-Khwarizmi's algorithms are still used today. He described them, of course, in Arabic, which may be why this language gained a reputation as the language of magic spells. (It has even been suggested that the incantation「abracadabra」is a corruption of al-Khwarizmi's full name, Abu Abdullah abu Jafar Muhammad ibn Musa al-Khwarizmi.)

Computer algorithms are usually expressed as programs. Since the term refers to the sequence of operations rather than the particular way they are described, it is possible to express the same algorithm in many different computer languages, or even to build it into hardware by connecting the appropriate registers and logic gates.

Usually, many algorithms can compute the same result. As in the sock example, different algorithms require different amounts of time to complete a given task. Certain algorithms may also offer other kinds of advantages: they may use only a small amount of a computer's memory, or they may require a particularly simple pattern of communication that is easy to wire into its hardware. The difference in speed and memory requirements between a good algorithm and a bad one is often a factor of thousands or even millions. Sometimes the discovery of a new algorithm allows you to solve previously intractable problems.

Because an algorithm can be implemented in many different ways and can be applied to problems of varying size, you can't judge how fast an algorithm is by measuring the time that elapses before a solution to your particular problem is reached. The time will vary with the method of implementation and the size of the problem. Instead, we usually describe the speed of an algorithm by how much the time required to complete the task grows along with the size of the problem. In the sock-pairing example, most of the time is spent in pulling the socks out of the basket, so we can compare the two sock algorithms by asking how the number of socks pulled out in each one compares to the total number of socks. Assume that there are n socks in the laundry basket. In the first algorithm, finding two that match requires pulling out and putting back an average of half the remaining socks, so the number of sock removals is proportional to the square of the number of socks. In analyzing algorithms, we usually don't bother to calculate the exact constant of proportionality; instead, we simply say that the algorithm is order n 2 , meaning that for large problems the time required for completion grows as the square of the problem size. This means that if there are ten times as many socks, the first algorithm will take a hundred times as long, so it is not a very good algorithm to use for pairing large numbers of socks. In the second algorithm, however, each of the n socks is pulled out only once, so the algorithm is order n. When you use the second algorithm to sort ten times as many socks, the task will take only ten times as long.

One of the greatest joys in computer programming is discovering a new, faster, more efficient algorithm for doing something — particularly if a lot of well-respected people have come up with worse solutions. Computer scientists can gain fame and admiration — at least among other computer scientists — by discovering a faster algorithm for a common problem. Since a bad algorithm can take weeks to solve a problem that a good algorithm can solve in minutes, the classical form of algorithmic one-upmanship is to write a new program and compute the right answer while your colleague's inferior program is still running.

Often the best algorithm is not obvious. Consider the problem of sorting a deck of sequentially numbered cards into ascending order. One way to do this is to start by looking through the entire deck for the lowest-numbered card. This card is removed and it becomes the first card of the sorted output pile. Next we look for the lowest card among those that remain. This in turn is removed and placed on top of the first card. The process is repeated until the unsorted cards are exhausted and the deck is arranged in ascending order. This procedure requires looking through the entire deck each time a card is extracted. Since there are n cards, each of which requires n comparisons, the run time of the algorithm is order n 2.

If we know that the cards are sequentially numbered from 1 to n, then we can sort them by a different method — one that uses a recursive definition, like the Logo procedure for drawing a tree described in chapter 3. To sort a deck of cards recursively, go through the deck once, moving cards with a value lower than the mean to the bottom half of the deck and leaving the cards with higher-than-average value in the top half. Then sort the two halves of the deck using the same algorithm. Applying the same algorithm recursively to each half of the deck will involve applying it recursively to each half of the half-decks, and so on. Each step of the recursion halves the number of cards to be sorted; the recursion ends when there is only one card — in which case, it is already sorted. Because this algorithm involves repeatedly dividing the cards until you are examining only one, it will require a time proportional to the number of times n cards can be divided — or the logarithm to the base 2 of the number of cards. So the order of this algorithm is n log n. (If you don't remember what logarithms are, never mind. They are all small numbers, so they can be safely ignored.)

There is an even more elegant recursive algorithm, which doesn't require the cards to be sequentially numbered; it would be useful for putting a large number of business cards into alphabetical order, for example. This algorithm, called merge sort , is harder to understand, but it's so beautiful that I cannot resist describing it. The merge-sort algorithm depends on the fact that it's easy to merge two already sorted stacks into a single sorted stack by successively pulling top-ranked cards off the top of one or the other of the stacks; this merge procedure is a subroutine of the algorithm, and the algorithm works like this: If your stack consists of only one card, then that card is already sorted. Otherwise divide the stack in half, and recursively use the merge-sort algorithm by sorting each half and then combining them using the merge procedure described above. That's all there is to it. (If this sounds too simple to work, you might want to try it with a few cards. Start with eight.) The merge-sort algorithm is a good example of the almost mysterious power and elegance of recursion.

A sorting algorithm that requires just n log n steps, like merge sort, is pretty fast. In fact, it is about the fastest algorithm possible. Proving that particular statement is beyond the scope of this book, but the reasoning that underlies the proof is interesting. It can be shown by counting the number of possible orderings of n cards. From this count, it is possible to compute that n log n bits of information must be known in order to put the cards in the correct order. Since each comparison of two cards produces only 1 bit of information (either the first card is greater than the second, or it is not), then to sort n numbers requires at least n log n comparisons, so in this case the merge-sort algorithm is about as good as any other. Books have been written on the topic of choosing the proper sorting algorithm. In many cases, where certain constraints are put on the sorting, or particular knowledge is available about the objects being sorted, the fastest possible sorting algorithm remains unknown. Still, on the scale of tasks for which we would like to design algorithms, sorting is considered relatively easy.

An example of a difficult task is called the traveling salesman problem. Imagine that a traveling salesman has to visit n cities. Given the traveling distance between each of the cities, in what order should the salesman visit the cities to minimize total distance traveled? No one knows an algorithm that is order n 2 , or order n 3 , or n to any power, which will accomplish this. The best algorithm known is order 2 n , meaning that the time required grows exponentially with the size of the problem. If we add ten more cities to the salesman's itinerary, the problem gets a thousand times harder (2 10 = 1,024). If we add thirty more cities, it gets about a billion times harder (2 30 is about 10 9 ). Exponential algorithms are not much use when problems get large, but for the traveling salesman problem they are the best algorithms we know. The fastest computer in the world, working for billions of years, would not have enough time to find the best route for just a few thousand cities.

The traveling salesman problem may seem unimportant, but it turns out to be equivalent to a lot of other problems — the so-called N-P complete problems (N-P stands for「nondeterministic polynomial」) — that it would be very useful to solve. A fast solution to the traveling salesman problem would lead immediately to a solution of these additional problems: for example, certain codes used for protecting secret information would become easy to break. Anyone who uses these codes is betting that no fast algorithm for solving the traveling salesman problem will ever be found. It's probably a safe bet.

No predictable technical breakthroughs in computers will help solve the traveling salesman problem, since even a computer a billion times faster will still be stumped by the addition of a few more cities. Exponential algorithms are just too slow to use for large problems. What could make a difference is the invention of a new algorithm: no one has ever proved that a fast algorithm for the traveling salesmen problem cannot exist. The study of algorithms has made significant progress in just the last few decades, and finding a fast one for the traveling salesman problem — or else proving that a fast one does not exist — remains one of computing's holy grails.

算法的保证性

算法是一种失效安全（fail-safe）机制，能确保达成既定目标，上述配对袜子的情况就是一种示例。「算法」一词来源于阿拉伯数学家阿尔 - 赫瓦里兹米（al Khwarizmi）的名字，他在 9 世纪写下了大量关于算法的著作。实际上，「algebra」（代数）一词来源于他一本书的书名 aljabr（移项）一词。阿尔 - 赫瓦里兹米提出的许多算法直到今天仍在被使用。当然，他写下这些算法时用的是阿拉伯文，这可能就是阿拉伯语被认为是具有神奇魔力的语言的原因。甚至有人认为「abracadabra」（咒语）一词是阿尔 - 赫瓦里兹米的全名（Abu Abdullah abu Jafar Muhammad inb Musa al-Khwarizmi）的误写。

计算机算法通常以程序的形式呈现。算法指的是一系列运算步骤，而不是这些运算步骤的方式。因此，同一种算法可以用不同的计算机语言来描述，甚至可以通过连接合适的寄存器和逻辑门，将算法直接嵌入到硬件中。

在通常情况下，许多不同的算法可以计算出相同的结果。正如配对袜子的例子，不同的算法完成同一任务所需的时间是不同的，但结果是一样的。某些算法还具有其他方面的优势，比如占用的计算机内存很小，或者所需的通信模式很简单，可轻易通过硬件来实现。在时间和内存需求方面，好的算法和差的算法的差别通常可以达到数千甚至上百万倍。有时，一个新算法能帮助你解决以前非常棘手的问题。

因为算法可以通过多种不同的方式来实现，并且可以应用于不同规模的问题，因此在得到具体问题的解决方案之前，我们无法通过测量算法的运行时间来判断算法的效率。算法的运行时间会随着实现方式和问题的规模的变化而变化。因此，我们通常根据完成任务所需的时间和问题的规模来评估算法的速度。在袜子配对的例子中，从洗衣篮中取出袜子的过程耗费了大部分时间，因此，根据每种算法取出袜子的次数和袜子总数的关系，我们就可以比较两种算法的速度。假设洗衣篮中袜子的总数为 n，在第一种算法中，找到一双匹配的袜子所需的取出和放回的平均次数为剩余袜子数目的一半，因此取出袜子的次数与袜子数目的平方成正比。在分析算法时，我们通常不用计算出准确的比例，只需知道算法的阶次为 n^2，这意味着对于输入规模很大的问题，计算所需的时间与输入问题的规模的平方成正比。如果袜子的数目增加 10 倍，那么第一种算法所需的时间将增加 100 倍。所以当袜子数目很大时，这不是一个好的算法。在第二种算法中，n 只袜子中的每只都只取出一次，因此算法的阶次为 n。如果你用第二种算法来配对 10 倍于原数目的袜子，完成任务所需的时间只是原来的 10 倍。

计算机编程最大的乐趣之一就是找到一种更快、更高效的新算法，尤其当许多资深专家都无法提出更优算法时。如果计算机科学家能为某种常见的问题找到更快的算法，那么他们会收获很多赞赏和荣誉，至少在计算机科学家的圈子里是这样。如果用差的算法来解决某个问题，可能需要数周，而好的算法可能仅需几分钟。因此，对于算法而言，胜人一筹的方式就是编写出一个新程序，计算出正确的解，而此时你的同事还在运用差劲的程序做计算。

一般而言，最好的算法并不是显而易见的。例如，如何将一副打乱的纸牌（顺序编号）按照升序的方式进行排序？一种方案是，先浏览全副纸牌并找到编号最小的那张，将其作为排序输出的第一张牌。接下来再在剩余纸牌中寻找最小编号的纸牌，然后将其放到第一张牌的上面。依次重复该过程，直到消除所有未排序的纸牌，将所有纸牌都按升序排列。在这个过程中，每找出一张纸牌都需要遍历整副纸牌。由于共有 n 张纸牌，而且每张牌都需要进行 n 次比较，因此算法的运行时间的阶次为 n^2。

如果我们知道纸牌是按从 1 到 n 进行编号的，那么就可以通过不同的方法对它们进行排序，比如采用递归定义的方法，这与第 3 章的 Logo 画树程序类似。以递归方式排序纸牌的方法如下：首先从头到尾浏览一遍纸牌，将比纸牌编号平均值小的纸牌移到下半区，比纸牌编号平均值大的纸牌移到上半区。然后使用同样的算法分别对两个半区进行排序，而且对这副牌的每一半递归地应用这一算法，即对半副牌的一半递归地应用这一算法，以此类推。每一轮迭代后，待排序的纸牌数目都会减半；当只剩最后一张牌时，递归过程结束，排序也结束。因为这个算法不断将纸牌分为两半，直到确定只有一张纸牌时为止，所以它所需的时间与 n 张纸牌被对半分开的次数成正比，即次数为「以 2 为底数的纸牌的对数」。所以这个算法的阶次为 nlogn。如果你不清楚对数的定义，请不用在意。它们的数值很小，可以忽略不计。

1-2-3『真是意外收获，竟然在这里吃透了快速排序法，之前一直有点不明白的地方原来关键在于：开始的时候就知道最小数和最大数，那么对半的平均数是可以算出来的，按「半数」来切割。之前「卡在」的地方是第一次切割的时候我怎么知道「半数」在哪里，原来前提条件是你开始就知道，这一点吴军在谷歌方法论将快速排序时也忘记给读者点出来了。快速排序法的解释做一张任意卡片。（2021-10-10）』—— 已完成

还有一种更巧妙的递归算法，它不需要对纸牌进行连续的按序编号。这种算法可以有效地用于处理按字母顺序排序的大量名片等类似问题。这种算法被称为归并排序，它虽然看起来难以理解，但十分美妙，我非常喜欢这种算法。归并排序基于这一事实：将两个已经排好序的序列合并为一个有序的序列十分容易，只需依次从其中一个序列的顶部取出排在首位的卡片。这个合并过程只是归并排序算法中的一个子程序，整个算法的工作流程是这样的：如果序列只剩一张卡片，那么这张卡片就已经排好序；否则，就将序列分为两半，在每一半上迭代使用归并算法，即对每一半进行排序，然后使用上述合并过程将两者组合。这就是归并排序算法的全部过程。如果这个过程听起来太过简单，你可以先在少数几张卡片上试试这个算法，比如从 8 张卡片开始。归并排序算法是展示递归方法的神秘力量和精妙之处的一个很好的例子。

如果一个排序算法（如归并排序算法）只需 nlogn 步，那是非常高效的。事实上，这是最快的算法。不过，这个结论的证明不在本书的范围之内，实际上，其证明的推理十分有趣。我们可以通过统计 n 张卡片的排列数来说明这一点。根据这个数目，我们可以计算出为了将纸牌排好序，必须得到 nlogn 个二进制位的信息。由于每次比较两张卡片只能生成 1 个二进制位的信息，比如第一张牌的编号大于第二张牌的编号，或者相反。因此，对 n 个数进行排序，至少需要进行 nlogn 次比较。在这种情况下，归并排序算法不逊于任何算法。至于如何选择合适的排序算法，市面上有很多相关的书籍。在多数情况下，我们需要在排序中增加一些限制条件，或者已知排序对象的具体情况。最快的算法仍不得知。尽管如此，对于现有待解决的问题的规模来说，排序问题还是相对容易解决的。

旅行推销员问题是一个典型的难以求解的案例。假设一位旅行推销员需要访问 n 座城市，且给定每两座城市之间的距离，旅行推销员应该以何种顺序访问这些城市才能最小化旅行距离呢？没有人能找到阶次为 n^2、n^3 或者 n 的任意次幂的算法来解决这个问题。目前已知的最佳算法的阶次为 2^n，这意味着求解时间会随着问题规模的增加呈指数级增长。如果我们在旅行推销员的行程中增加 10 座城市，那么这个问题的难度会增加 1 000 倍（2^10 = 1 024）。如果我们再增加 30 座城市，问题的难度会增加 10 亿倍（2^30 约等于 10^9）。当问题规模变得更大时，这种指数算法的效率会变得很低。然而，对于旅行推销员问题来说，这已经是我们所知道的最快的算法了。即使目前世界上运行速度最快的计算机连续工作数十亿年，也无法及时找到几千座城市之间的最佳访问路径。

虽然旅行推销员问题看起来无关紧要，但事实证明，它与许多其他问题很类似，即所谓的 NP 完全问题（NP complete problem），其中 NP 代表「非确定性多项式」。如果这类问题能得到解决，将会大有益处。如果能找到旅行推销员问题的快速解法，便能得到这些问题的解法。例如，某些用于保护秘密信息的代码能被快速破译。使用这些密码的人一定希望永远不要找到旅行推销员问题的快速算法，而这很可能会成真。

在计算机领域，没有哪一种可预期的技术突破将有助于解决旅行推销员问题。因为即便计算机的运算速度提高 10 亿倍，只要增加几座城市就会被难住。指数算法的效率太低，无法用于解决大规模的问题。也许，只有发明一种新算法才可能有所作为。好消息是，至今仍无证据显示，并不存在解决旅行推销员问题的快速算法。在过去几十年内，这类算法的研究取得了重大进展，找到该问题的快速算法或者证明并不存在这种快速算法，仍然是计算科学界的圣杯。

### 5.2 Settling for Almost Always

As hard as the traveling salesman problem is, it is not one of the most difficult problems to solve on a computer. Some problems are known to require even more than exponential time to solve. As discussed in the previous chapter, there are noncomputable problems that we know no algorithm can solve. Even when algorithms exist for certain problems, they are not necessarily the best approach. An algorithm, by definition, is guaranteed to get the job accomplished, but this guarantee of success often comes at too high a price. In many cases, it is more practical to use a procedure that only almost always gets the right answer. Often,「almost always」is good enough. A rule that tends to give the right answer, but is not guaranteed to, is called a heuristic. It is often more practical to use a heuristic than an algorithm: for instance, there are many effective heuristics for the traveling salesman problem — procedures that will provide an almost optimal route very quickly. In fact these heuristics usually do find the best route, although they are not absolutely guaranteed to do so. A real-life traveling salesman would presumably be happier with a good, fast heuristic than with a slow algorithm.

A simple example of the use of heuristics is the game of chess. A talented programmer who is only an average chess player can write a chess-playing program that will consistently beat the programmer. Such a program is not an algorithm, because it is not guaranteed to win. Heuristics make educated guesses; good heuristics usually make the right guess. Some of the most impressive behaviors of computers are the result of heuristics rather than of algorithms. (Philosophers have written a great deal of nonsense about「the limitations of computers」when what they are really talking about are the limitations of algorithms.)

A good chess-playing program can be written based on the following heuristics:

1 Estimate the relative strength of each player's position by counting the number of pieces of each type remaining on the board.

2 Move so as to put yourself in the strongest possible position a few moves in the future.

3 Expect your opponent to adopt a strategy similar to your own.

Each of these rules is only an approximation of the ideal strategy, and it is possible to imagine situations in which each is actually wrong. The relative strength of a player's position, for example, depends not just on the number of pieces but also on their position. A good position can often be more advantageous than an extra piece. Regardless, the first heuristic is generally correct; in most cases, having more pieces is better. Even before computers, chess players developed a simple method of numerically scoring the relative strengths of two players' positions by assigning one point for a pawn, three for a bishop, five for a rook, and so on, and using the total score of each player's remaining pieces as a measure of strength.

Based on these heuristics, you can write a chess-playing program that will trace out all plausible lines of play for the next few moves. Of course, it would be preferable if the program considered all lines of play, plausible or not, all the way to the end of the game. This was easy in the game of tic-tac-toe, but where chess is concerned it is impractical even for the fastest computers. In a typical midgame chess position, a player has about thirty-six potential legal moves, each of which leads to thirty-six possible responses by the opponent. Since the average chess game lasts for more than eighty moves, the computer would have to search something on the order of 36 80 possibilities, or about 10 124 possibilities. Such a search could not be accomplished by the fastest modern computers in hundreds of years. The problem is that the possible lines of play grow exponentially with the number of moves; it is thus impractical to look more than about five to ten moves ahead — which is why computers use the heuristics I have just listed for evaluating their moves.

Let us for the moment accept the second heuristic as correct and agree that the best line of play is the one that optimizes a player's position a few moves into the future. Let us further specify that the chess-playing program will look six moves ahead. According to the first heuristic, the program will evaluate the strength of both players after the sixth move by counting the number of pieces on each side remaining on the board and scoring them using the point system I have described. The relative strengths of the two players in any position considered will be judged by the difference between those scores.

Given these assumptions, what is the best way for the program to choose its next move? It's not enough for the computer to choose a move leading to the most favorable sequence of six future moves, because every alternate move in that sequence will be determined by the opponent. Instead, we must assume that the opponent will always try to choose a line of play that will favor the opponent's relative position; this is the assumption embodied in the third heuristic. To predict the opponent's line of play, the computer must place itself in the position of the opponent. The computer chooses a move by evaluating each legal move that can be made by its own side — white, say. The procedure for evaluating a possible move to be made by white depends on invoking a procedure for evaluating a possible move to be made by black, and vice versa. In effect, the computer follows every possible line of play for six moves, alternately putting itself in the position of black and white. The program tries out the moves on an imaginary board inside the memory of the computer, in much the same way that a chess master imagines lines of play「inside his head.」The programs evaluating white's and black's positions call each other as subroutines, recursively. The recursion terminates after six levels, when the computer evaluates the score by counting the pieces.

Most chess-playing programs incorporate additional heuristics to abort searches of implausible lines of play and to search deeper in branches that involve the exchange of pieces. There are also more elaborate systems for evaluating positions without searching — for example, systems that award points for keeping control of the center or for protecting the king. Each of these heuristics is just an additional guess, and each one can improve the search in some situations at the cost of potentially making mistakes in others. With various refinements, this basic search procedure is at the heart of nearly every chess-playing program. It is effective because it takes advantage of the speed of the computer to consider many millions of alternate lines of play. Among these many millions of alternatives, there is often a variation that will surprise the programmer, or even an experienced human chess player. This ability to surprise is what allows the machine to play a better game of chess than the programmer.

Chess-playing machines have a long and sometimes dishonorable role in the history of computing. The eighteenth-century Hungarian inventor Wolfgang von Kempelen captured the world's imagination with a chess-playing automaton in the form of a mechanical turbaned Turk. As it turned out, the machine worked only because a chess-playing midget was hidden inside it. In 1914, the Spanish engineer Luis Torres y Quevedo built a mechanical device that played a simplified game of chess without the help of a concealed human being, and in the late 1940s Claude Shannon described how a computer could be programmed with a set of chess-playing heuristics similar to those listed here. Still, it was many years before computers were fast enough to play a decent game of chess, which comforted not a few philosophers who argued that chess playing was an example of the unique powers of the human mind. Modern computers, using the same heuristics, can now beat the best human chess players in the world (witness the victory in 1997 of IBM's Deep Blue over Garry Kasparov), so the philosophers have shifted the argument to other domains.

The simple search heuristics work because there are a relatively small number of responses to consider for each move. In checkers, where there are even fewer possible responses to each move, machines based on heuristics began beating the best human players in the 1960s. In the Chinese/Japanese game of Go, on the other hand, humans still reign, because the larger board (19 × 19) affords far more possible moves. (I prefer playing Go to chess, precisely because a search is less useful; it makes my impatience less of a disadvantage.)

解决问题的万能方法

尽管旅行推销员问题难以通过计算机求解，但它还不是最难求解的问题。有些问题的求解耗时远远超出指数级别。正如前一章所讨论的，算法无法解决不可计算的问题，而且，即便找到了某些问题的算法，它们也不一定是最优的方法。根据定义，算法必须确保完成任务，但确保成功的承诺通常需要付出很大的代价。在许多情况下，使用一种几乎总能得到正确答案的方法更为实际。通常来说，「几乎总能」已经足够好了。这种能够尽力给出正确答案，但并不确保给出的答案一定正确的规则被称为「启发式方法」。一般而言，更为实际的做法是使用启发式方法而非算法。例如，存在很多能有效解决旅行推销员问题的启发式方法，它们能快速地提供近似的最优路径。事实上，这些启发式方法得出的路径通常是最优的，尽管它们并不能完全保证做到这一点。在现实生活中，旅行推销员可能更希望拥有一个有效、快速的启发式方法，而非一个缓慢的算法。

国际象棋游戏是应用启发式方法的一个简单范例。一位具有编程天赋的程序员可能棋艺一般，但可以写出一个能够击败自己的国际象棋程序。这样的程序并不是一种算法，因为它并不能保证每局都赢。启发式方法会做出有根据的猜测，而好的启发式方法做出的猜测几乎都是正确的。最令人印象深刻的一些计算机行为往往来自启发式方法而非算法。哲学家虽然写下了很多关于「计算机局限性」的无稽之谈，但谈论的实际上只是算法的局限性。

若想编写出一款优秀的国际象棋程序，可以遵循如下启发式方法：

1、通过统计棋盘上每种棋子的数目来评估双方的相对实力。

2、走出会使自己在之后占据最优地位的那步棋。

3、设想对方也会采取相似的策略。

上面的每条规则只是接近于理想的策略，并且在某些情况下，每条规则都可能是错误的。比如，下棋双方的强弱关系可能不仅取决于棋子的数目，还取决于棋子的位置。一颗处于好位置的棋子的作用通常比多一颗棋子的作用还要大。无论如何，第一条启发式方法通常是正确的，即在大多数情况下，拥有更多棋子总是更有利。早在计算机出现前，国际象棋选手就已经发明了一种简单的方法来为下棋双方的实力定量计分：兵为 1 分，象为 3 分，车为 5 分等，并用选手剩余棋子的总分值作为该场棋局实力的衡量标准。

基于这些启发式方法，你可以编写出一款国际象棋程序，该程序能计算出接下来几步合理可行的走法。当然，程序最好能够计算出从游戏开始到结束之间所有的走法。在井字游戏中，很容易实现这一点，而对于国际象棋来说，即使速度最快的计算机也难以实现这一点。在典型的国际象棋中局阶段，可供下棋双方的每种走法都能使对方想出 36 种可能的应对之法。由于国际象棋平均每局超过 80 步，因此计算机需要搜索的可能性数目约为 3680，也就是 10124 种可能性。即便运算速度最快的现代计算机耗时几百年，也无法完成这种规模的搜索过程。这个问题的关键点在于，可能的走法会随着下棋步数的增加呈指数级增长。因此，一般至多考虑前 5～10 步的走法。这就是为什么计算机需要使用上面列出的启发式方法来评估走法的原因。

我们暂且认为第二条启发式方法是正确的，也就是同意能使自己在几步之后处于最有利地位的走法是最佳的。假设国际象棋程序只考虑 6 步棋前的情况。根据第一条启发式方法，这个程序将统计 6 步之后双方棋盘上剩余的棋子数量，并根据上述计分系统打分，由此来评估双方的实力。无论双方处于何种态势，双方的相对实力按得分的高低来判定。

基于上述假设条件，程序应该如何走出最好的一步棋呢？计算机只根据对自己最有利的未来 6 步棋来选择第一步棋的走法是不够的，因为在这 6 步中，每隔一步棋的走法还决定于对方的走法。因此，我们必须假设对方始终会采取对他自己最有利的走法，这就是第三条启发式方法包含的假设。为了预测对手的走法，计算机必须将自己置于对手的位置。计算机在选择下一步棋时会评估自己所能采取的所有走法，同时还要评估对方可能会采取的应对之法，反之亦然。事实上，计算机会分别站在双方的位置，全面考虑双方在未来 6 步棋中所有可能的走法。程序会在计算机内存中的虚拟棋盘上尝试不同的走法，这就如同国际象棋大师在头脑中想象各种可能的走法。评估双方棋局形势的程序互为子程序递归调用。这个递归过程迭代 6 次后结束，此时计算机就会通过统计所剩棋子的数目来评估双方的分数。

大多数国际象棋程序还会采用其他的启发式方法，其目的是放弃一些不合理的搜索，或者在涉及棋子互换的分支中进行更深入的搜索。还有很多精细的系统不借助搜索便可以完成棋局评估。例如，有些系统会为能够控制中心或者保卫「国王」的棋子加分。每种启发式方法都只是一种猜测，在某些情况下，它们都能改善搜索效率，但代价是在其他情况下可能会犯错。经过各种改进，这个基本的搜索过程几乎是所有国际象棋程序的核心。国际象棋程序能利用计算机的运算速度优势来考虑上百万种走法，因而效率极高。在这上百万种走法中，总有一种会让程序员，甚至有经验的人类棋手感到意外。这种制造意外的能力能使计算机比程序员走出更好的棋。

在计算科学史上，国际象棋游戏机有着漫长且偶尔口碑不佳的历程。18 世纪，匈牙利发明家沃尔夫冈·冯·肯佩伦（Wolfgang Von Kempelen）通过一个下国际象棋的自动装置激发了全世界的想象力，这个装置很像一个机器人。后来的事实证明，这个机器之所以能运转，是因为其内部藏有一个会下棋的侏儒。1914 年，西班牙工程师路易斯·托雷斯·克韦多（Luis Torresy Quevedo）制造了一台机械装置，该装置可以在没有人类帮助的情况下，下一种简化版的国际象棋。20 世纪 40 年代末，克劳德·香农描述了如何用计算机编程实现国际象棋启发式走法的思路，其中用到的方法与上文列出的三条规则类似。尽管如此，直到很多年以后，计算机的运算速度才快到足以玩国际象棋这样复杂的游戏。这对不少哲学家来说是一个不小的打击，因为他们认为下国际象棋是人类心智所具有的独特能力。现代计算机利用同样的启发式方法击败了世界上最优秀的国际象棋棋手 —— 1997 年，深蓝计算机击败了加里·卡斯帕罗夫（Garry Kasparov）。此后，哲学家又将争论转移到了其他领域。

简单的启发式方法之所以有效，是因为每步棋需要考虑的可能性相对较少。在跳棋中，每步需要考虑的可能性则更少。早在 20 世纪 60 年代，基于启发式方法的机器就开始击败人类冠军。不过，在中国和日本的围棋游戏中，人类选手仍然处于统治地位，因为 19×19 的棋盘更大，可以提供更多种可能的走法。相比于国际象棋，我更喜欢玩围棋，因为启发式方法在后者中所起的作用更小，我不至于因急躁而处于下风。

1『深度学习打破了作者的这个观点，2016 年是 AI 元年，哈哈。（2021-10-10）』

### 5.3 Fitness Landscapes

The use of heuristics to search through a set of possibilities is ubiquitous in computer programming and has applications far more important than game playing. This is often the way computers find「creative」solutions to problems — usually problems whose solutions are known to be among a large but finite set of possibilities called a search space. The search space in chess is the set of all possible lines of play; the search space in the traveling salesman problem consists of all possible routes linking the cities on the salesman's list. Since these spaces are too large to search exhaustively, heuristics are used to reduce the area to be searched. In the case of small search spaces, such as in the game of tic-tac-toe, the exhaustive search is preferable, because it is guaranteed to find the right answer.

Generally, the reason that a search space is large is because the possibilities are produced by forming combinations of simpler elements — the individual moves in chess, the city-to-city hops in the traveling salesman problem. This combining of elements leads to a combinatorial explosion of possibilities — an explosion that grows exponentially with the number of elements being combined. Since the possibilities are built from combinations of elements, there is a sense of distance in the space; combinations that share common elements are「closer」than the combinations that do not. This is why it is called a「space」and not just a set of possibilities. To extend the analogy, we can imagine the possibilities as lying in a two-dimensional landscape sometimes known as a fitness landscape. The desirability, or score, of each possible solution is represented by the altitude of a point in the landscape. If similar possibilities have similar scores, then nearby points will have similar altitudes, so the landscape will have well-defined hills and valleys. In this analogy, finding the best solution is like finding the top of the highest hill. Taking the traveling salesman problem as an example, we can imagine each point in the landscape as representing a particular travel itinerary for the salesman. The height of each point represents the distance the salesman must travel, with points representing efficient travel itineraries at higher altitudes. The best itinerary will be at the top of the highest hill.

One of the simplest ways of searching such a space is to compare points at random and remember the best one found. The number of points that can be searched this way is generally limited only by the amount of time available, and the procedure can be applied to any type of space. It is the equivalent of parachuting scouts into various locations in the landscape and asking them to report back their altitude. It is not a very efficient way to find the top of a hill. If the space is large, then in any practical amount of time only a tiny portion of the possibilities will be investigated, and therefore the best point found is unlikely to be one of the highest.

In a search space like that of the traveling salesman problem, where nearby points are likely to have similar scores, it is usually better to use a procedure that searches a path through the space by traveling from point to nearby point. Just as the best method for finding a peak in a hilly landscape is to walk uphill, the equivalent heuristic is to choose the best of nearby solutions found in the search space. In the traveling salesman problem, for example, the computer might vary the best-known solution by exchanging the order of two of the cities in the itinerary. If this variation leads to a more efficient tour, then it is accepted as a superior solution (a step uphill); otherwise, it is rejected and a new variation is tried. This method of search will wander through the space, always traveling in an uphill direction, until it reaches the top of a hill. At this point, the solution cannot be improved by exchanging any pair of cities.

The weakness of this method, which is called hill climbing , is that although you thereby reach the top of a hill, it is not necessarily the highest hill in the landscape. Hill climbing is a heuristic, not an algorithm. There are other heuristics similar to hill climbing which are less likely to get you stuck on top of one of the foothills. For instance, you could repeat the hill-climbing process many times, starting from different random locations (that is, you might ask the parachutists to climb uphill). Or you could occasionally take a step downward to avoid getting stuck. There are many such variations, each with its own advantages and disadvantages.

Heuristics like hill climbing work well on the traveling salesman problem and produce good answers in a short amount of time. Even when thousands of cities are involved, it is usually possible to find at a good solution to the problem by starting out with a reasonable guess and improving it by hill climbing. So why is the traveling salesman problem considered so difficult? Using heuristics, we can almost always get almost the best itinerary. But a method that almost always works is not an algorithm. Periodically, a great deal of fuss is made by someone who has「solved」the traveling salesman problem; so far, all anyone has actually done is come up with a new heuristic. Fast heuristic solutions to the traveling salesman problem are not difficult to dream up; it is finding a fast algorithm that is the difficulty.

There are many problems for which we do not need exactly the right answer every time — problems for which we can accept a less-than-perfect solution. Even when we want a perfect answer, we may not be able to afford it. For such problems, computers can produce an educated and well-considered guess. Because the computer is able to consider an enormous number of combinations and possibilities, such a guess will often surprise the programmer. When a computer uses heuristics, it is capable both of surprises and mistakes — which makes it a little more like a person and a little less like a machine.

适应度地形

这种利用启发式方法搜索一系列可能性的做法在计算机程序设计中随处可见，这种方法还能应用于比游戏更重要的领域。启发式方法是计算机解决某些问题时发现「创造性」的解的常用方式，这类问题的解存在于数量巨大但有限的可能性集合之中，被称为搜索空间。国际象棋的搜索空间是所有可能走法的集合；旅行推销员问题的搜索空间是旅行推销员历程表中各个城市之间所有可能的路线。因为这些空间很大，难以穷举，因此可以使用启发式方法缩小搜索的范围。如果搜索空间本身就比较小，例如井字游戏，那就应该优先选择穷举式的搜索方法，因为这样能保证找到正确的答案。

通常来说，搜索空间之所以很大，是因为简单元素的组合产生了各种可能性，比如国际象棋中的每种走法、旅行推销员问题中各个城市之间的每条路径。元素的组合会致使可能性的「组合爆炸」，这里的「爆炸」是指可能性数目随着组合元素数目的增加而呈指数级增长。由于元素的组合形成了各种可能性，因此搜索空间内有一种距离感，有些组合之间享有相同元素，它们之间的关系比其他没有共享元素的组合更为「紧密」。这是搜索空间又被称为「空间」而不仅是「可能性集合」的原因。为了进一步说明这种类比关系，我们可以设想可能性集合位于一个二维平面上，这个平面有时被称为「适应度地形」（fitness landscape）。每种解的可取性或者得分由地形上的某个高度来表示。如果相似的可能性具有相近的分数，那么相近的点也具有相似的高度，因此这个地形中会有界限分明的丘陵和山谷。在这个类比中，找到最优解的方法就相当于找到最高的山顶。以旅行推销员问题为例，我们可以将地形上的每个点想象为旅行推销员制定的一个行程路线，每个点的高度代表旅行推销员的行程距离，其中更高效的行程路线对应的是最高的山顶。所以，代表行程最短的点位于最高的山顶上。

最简单的空间搜索方法是，比较随机选择的两个点，并记录找到的最佳点。这种方式的搜索范围虽然会受到时间的限制，但能应用于所有类型的空间。这相当于将侦察兵空降至不同的地点，并要求他们报告自己所在位置的海拔高度。显然，这种寻找山顶的方法的效率并不高。如果搜索空间非常大，那么在一段时间内只能搜索到小部分的可能性，而且找到的最佳点也可能不是全局最优的。

在类似于旅行推销员问题的搜索空间里，相近的点可能具有相似的分数，在这样的搜索空间中，更好的一种路线搜索方式是从某点出发搜索其临近点。正如在丘陵地形中，走上坡路才是最好的登顶方法，对应的启发式方法就是在搜索空间中选择局部范围内的最优解。例如，在旅行推销员问题中，计算机通过对调行程中某两个城市的次序，便可以变换最优解。如果改变城市的次序会缩短行程距离，则将它当作更好的解（向上坡迈了一步）；否则就放弃，尝试新方案。这种搜索方法就像在搜索空间中漫游，始终朝上坡方向前进，直至到达山顶。到达山顶以后，再交换任何两个城市的次序都无法得到更好的解。

这种方法被称为爬山法，其缺点在于，你到达的山顶不一定是地形上的最高点。爬山法是一种启发式方法而非算法。还有一些与爬山法相似的启发式方法，它们能降低你困在某个山麓的概率。例如，你可以从不同的随机地点多次重复爬山过程，也就是说，你可以命令那些空降兵登山；或者，为了避免被困住，你可以后退一步。还存在很多种不同的方法，每种都有其自身的优点和缺点。

1-3『随机爬山法，第一次听到是万维钢精英日课里研读书籍「混乱」时。（2021-10-10）』

像爬山法这样的启发式方法能够很好地解决旅行推销员问题，它能在很短的时间内得到令人满意的答案。即便涉及的城市有几千座，通常也可以从一个合理的猜测出发，利用爬山法不断改进，从而找到更好的解。那么，为什么旅行推销员问题如此难以解决呢？虽然我们几乎总能用启发式方法得到几乎最优的行程路径，但「几乎总能有效的方法」并不是一种算法。每隔一段时间，就会有人宣称「解决」了旅行推销员问题，并带来一阵喧嚣。实际上，只不过是提出了一种新的启发式方法。对于旅行推销员问题来说，其困难不在于找到高效的启发式方法，而是找到高效的算法。

对于很多问题来说，我们并不会每次都得出完全正确的答案，而是会接受一个不太完美的答案。即使我们想得到一个完美的答案，也可能无法承担时间成本。对于这些问题，计算机可以给出一个有理有据且考虑周全的解。因为计算机能够考虑数量庞大的组合和可能性，其得出的推测往往会为程序员带来惊喜。当计算机使用启发式方法时，它既能制造惊喜也能犯错，这使它更像人类，而非机器。

## 0601. Memory: Information and Section Codes

So far, we have mostly ignored the limitations imposed on a computer by the size of its memory. An idealized universal computer has an infinite memory, but a real computer's memory is limited, usually by expense. As long as the size of the memory is adequate for the task at hand, we are free to ignore the limitation, but some memory-intensive algorithms and applications store such large amounts of data that the amount of memory available becomes an important consideration. Applications that manipulate representations of the physical world — such as images, sounds, or three-dimensional models — are often memory-intensive. Knowing how much memory is required for a given application is important not only in judgments about whether or not the computer is big enough to handle it but also for estimating the time required to process the information.

The bit — the unit of measure for information — is appropriate both for the communication of information and for its storage. In a sense, communication and storage are just two aspects of the same thing: communication sends a message from one place to another; storage「sends」a message from one time to another. Unless you are accustomed to thinking in four-dimensional spacetime terms, this equivalence between moving and storing may seem strange, but think of mailing a letter as a means of communication which has aspects of both. Mailing a letter to someone else is a way to move information in space; mailing a letter to yourself is a way to store information in time. When examined closely, any form of communication is seen to have both a spatial and a temporal aspect. One way that electronic computers store information is to constantly recirculate it, in the electronic equivalent of a self-addressed letter.

We know that computers with n bits of memory can hold up to n bits of information, but how do we determine how many bits are required to represent a given piece of information? For instance, how many bits are in the words of this book? Calculating the answer turns out to be not particularly easy; in fact, there are several different correct answers. Thinking about this question leads us to ideas about compression, error detection and correction, random numbers, and secret codes.

The number of bits required to send or store a given piece of data will depend on how the data is encoded. One way to represent a complex message, like the text of a book, is to represent it as a sequence of simpler parts: for example, the characters that spell out the text. In this common representation, the number of bits in the message is equal to the number of characters in the text times the number of bits per character. The text of this book contains about 250,000 characters, and my computer uses a code that requires 8 bits (a byte) to store a character, so the size of the file that the computer uses to store the text is about 2 million bits. You may be tempted to conclude that this book contains about 2 million bits of information, since that's how much memory the computer uses to store the text, but this is only one measure of the information — a measure that depends on the representation of the message. It's a useful measure, because it tells you not only how much memory the computer needs to store the information but also how much time is needed to process it. For example, if I know that my computer can write information on a disk at 20 million bits per second, and I know that it uses a 2-million-bit representation of this book, then I can calculate that it will take about 1/10 of a second to store the book in a file on disk.

The problem with using the number of characters times 8 as a measure of the number of bits in the text is that it depends on the representation scheme used by the computer. A different computer, or a different application program running on the same computer, might store the very same sequence of characters in a different number of bits. For instance, with 8 bits per character, it is possible to represent 256 different characters, but the text in this book uses fewer than 64 different characters — 26 letters, upper and lower case, numerals, and punctuation marks. Therefore, a more efficient code would represent each character by using 6 bits of information (2 6 = 64), and hence compress the representation of the text to only 1.5 million bits.

It would be nice to have a measure of information that didn't depend on the form of representation. A more fundamental measure of information would be the minimum number of bits required to represent the text. This is easy to define, but not necessarily easy to calculate.

0601 存储：信息和密码

到目前为止，我们基本上没有考虑过计算机因存储空间而受到的限制。理想的状态是，通用计算机具有无限的存储空间。然而，对于具体的计算机来说，其存储空间时常受限于成本，而且总是有限的。只要存储空间足以满足当前的任务需求，我们就可以不用考虑这一限制。不过，一些算法和应用程序具有大量数据需要被存储，所以存储空间便成为一个重要的考虑因素。用于表示物理世界的应用程序通常需要占用大量存储空间，比如图像、声音和三维模型等。知道应用程序需要多少存储空间非常重要，因为它不仅可以用于判断计算机是否拥有足够的空间运行该程序，还可以用于估算处理信息所需的时间。

作为测量信息的单位，二进制位适用于信息的传输和存储。从某种意义上来说，传输和存储是同一事物的两个不同方面：传输是将一条信息从一个地点发送到另一个地点，而存储是将一条信息从一个时刻「发送」到另一个时刻。除非你习惯于在四维时空思考问题，否则可能难以理解将传输和存储等同视之的想法。邮寄信件不仅是一种通信方式，还同时具备传输和存储两方面的特性。给他人寄信是在空间范畴内传输信息的一种方式，而给自己寄信是在时间范畴内存储信息的一种方式。实际上，所有形式的通信都具有空间和时间两方面的属性。电子计算机存储信息的一种方式就是，让信息不断循环流通，这就相当于在电子世界给自己寄信。

我们知道，存储容量为 n 个二进制位的计算机最多可以存储 n 位的信息。然而，对于给定的信息，我们如何确定需要多少位呢？若想计算出这个问题的答案，并不容易。实际上，存在若干不同的正确答案。如果进一步思考这个问题，我们会接触到压缩、查错和纠正、随机数、密码等概念。

数据的编码方式决定了传输和存储一段数据所需的位数。对于一些复杂的信息来说，比如一本书中的文字，可以将其表示为一串更简单的序列，例如，用组成文本的字符表示这本书。在这种常用的表示方法中，文本信息的位数等于文本中的字符数乘以每个字符所用的位数。本书约有 250 000 个字符，而我的笔记本电脑存储一个字符需要 8 位（1 个字节）。因此，用于存储本书的计算机文件约为 200 万个位。你可能会就此得出结论，这本书之所以包含 200 万个位的信息，是因为计算机用于存储本书的存储空间就这么大。不过，这只是信息的一种度量方式，这种度量方式取决于信息的表示形式。这是一种有效的衡量标准，因为它不仅告知了计算机需要多少存储空间来存储信息，还告知了处理这些信息所需的时间。例如，如果我知道自己的计算机以每秒 2000 万位的速度向磁盘中写入信息，并且知道需要用 200 万位来表示本书，那么我就可以计算出将这本书存入磁盘的时间 ——1/10 秒。

不过，用字符数乘以 8 得到的数字作为文本信息的度量值存在一定的问题：此时文本信息的位数取决于计算机使用的字符表示方法。不同的计算机或者在同一台计算机上运行的不同应用程序，存储完全相同的字符序列所需的位数可能不尽相同。例如，如果用 8 位来表示每个字符，则可表示 256 种不同的字符，但本书中的字符数少于 64 种 —— 大写和小写的字母共 26 个，再加上数字和标点符号。因此，更有效的编码方式是使用 6 位（26=64）表示一个字符，这样可将本书压缩至 150 万位左右。

如果存在一种不依赖信息表示形式的测量方式，那就最好不过了。一种更本质的测量信息的方式是表示文本最少需用多少个位。这种方式虽然很容易定义，但不容易计算。

### 6.1 Compression

How much can we compress a given text without losing information? Reducing the number of bits per character from 8 to 6 is a simple form of compression. Other compression methods are based on taking advantage of regularities in the text. For instance, the letters T and E occur far more often in most English-language texts than the letters Q and Z. A more efficient code would use a shorter sequence of 1's and 0's to represent the more common letters. Using a variable-length encoding of characters to achieve a more compact representation was a trick used by early telegraph operators and radio amateurs. In Morse code, the letter E is represented by a single dot and the letter T by a single dash. Less common letters, like Q and Z, are represented by sequences of up to four dots and dashes. Because a third type of signal — a space — is used to mark the ends of letters, the dots and dashes of Morse code don't exactly correspond to 1's and 0's, but the principle is similar.

To make a variable-length code using 1's and 0's, it is necessary to choose the code patterns carefully, so that a stream of bits can be broken unambiguously into characters. This is possible as long as no bit sequence used to represent a character begins with a subsequence of 1's and 0's used to represent another character. For example, all common characters could be represented by 4 bits, starting with a 1, while less common characters might have 7 bits and start with a 0. This would allow the stream of bits to be divided unambiguously into short and long characters. Choosing a variable-length code that best takes advantage of the relative frequency of the various letters will result in substantial compression of the text. In the case of the text of this book, it would reduce the number of bits from the original 2 million to about 1 million, a compression of 50 percent.

Any method of compressing takes advantage of regularities in the data. The code just described takes advantage of regularities in the rate of occurrence of single characters, but there are other regularities that can be exploited. For instance, not all pairs of adjacent letters occur with equal frequency in this book. The letter Q is almost always followed by the letter U, and the letter Z is never followed by the letter K. By developing a system of variable-length code for pairs of letters rather than for individual letters, we can take advantage of the fact that two-letter combinations do not occur with equal frequency. The code can use shorter sequences of bits for the more common pairs and long sequences for pairs that occur rarely. If we use this method, the number of bits required to store the text of this book could be reduced by a further 10 percent, to an average of about 3.5 bits per letter.

A still more efficient code would take advantage of regularities that occur in longer sequences of letters. For example, the word「the」occurs about 3,000 times in this text. It would be advantageous to use a code that uses a relatively short sequence of bits to represent this entire word. Similarly, there are many other words, such as「computer」and「bit,」that occur so often in this particular text that they, too, are worth encoding.

There are also regularities beyond these statistical regularities in the letter sequences. For instance, there are regularities in grammar, sentence structure, and punctuation that allow further compression of the text. But at some point we will begin to get diminishing returns. In the end, the compression that uses the best available statistical methods would probably reach an average representation size of fewer than 2 bits per character — about 25 percent of the standard 8-bit character representation.

Compression works fairly well on text, but it works even better on signals that are representations of the real world, like sounds and pictures. These signals are usually read into the computer by a process known as analog-to-digital conversion. Such inputs — the intensity level of a sound, say, or the brightness of a light — are usually continuously variable, analog signals. For example, a dot, or pixel , in a black-and-white photograph may be either white or black or any of the infinite shades of gray in between. Since the computer has no way of representing an infinite number of possibilities, it simplifies the signal by reducing each pixel to one of a finite set of levels of gray. Typically, the number of gradations is an exact power of 2, so that it will fit into a specific number of storage bits. For instance, the brightness of a dot in a black-and-white picture might be represented by 8 bits, so that 256 shades of gray could be represented. A higher-quality image would be represented with a 12-bit code, producing 4,096 shades of gray. A color image might use 24 bits per dot — 8 bits for the intensity of each of the three primary colors.

The other parameter determining the quality of a photographic image is its resolution  — that is, the number of pixels used to represent it. A high-resolution image produced by an array of 1,000 × 1,000 dots will be a more accurate representation than an image with a resolution of 100 × 100. However, since the representation of the high-resolution image uses 1,000,000 pixels instead of 10,000, your computer will need 100 times as much memory for storage, and processing the image will take 100 times as long. Quality costs.

Since high-resolution images can contain a large number of bits, it is often desirable to compress them, in order to reduce the cost of storage and transmission. This is especially true of moving images, which typically contain from 24 to 100 frames per second. Fortunately, images are relatively easy to compress, because there is a high degree of regularity in an image. In most pictures, the intensity and color of a particular pixel is often nearly identical to the intensity and color of neighboring pixels. Two pixels representing adjacent parts of the same cheek in the image of a face, say, would likely be very similar in brightness and color. Most image-compression algorithms take advantage of this similarity. An image-compression algorithm may represent areas of uniform brightness and color by having just a few numbers represent the color and size of the area. Other image-compression methods take advantage of more complex forms of regularity: for example, similar textures in different parts of the image. For moving pictures, such as television broadcasts, compression methods generally take advantage of the similarity of sequential frames. Using such techniques, one can often compress the representation of a photograph by a factor of 10 and of a moving image by a factor of 100. Similar compression methods can be applied to sounds.

These compression methods lead to a counterintuitive notion of the amount of information contained in a picture. If the minimum number of bits required to represent the image is taken as a measure of the amount of information in the image, then an image that is easy to compress will have less information. A picture of a face, for example, will have less information than a picture of a pile of pebbles on the beach, because the adjacent pixels in the facial image are more likely to be similar. The pebbles require more information to be communicated and stored, even though a human observer might find the picture of the face much more informative. By this measure, the picture containing the most information would be a picture of completely random pixels, like the static on a damaged television set. If the dots in the image have no correlation to their neighbors, there is no regularity to compress. Such pictures look completely meaningless to us — and may truly be meaningless — but they require the greatest amount of information in order to be represented by a computer.

The minimal representation measure of information does not correspond well to our intuition about information content because the computer is making no distinction between meaningful and meaningless information. The computer must represent the color of every pixel, or the position of every pebble on the beach, even though these details may not be important to the viewer. Deciding what information is and is not meaningful is a subtle art; it depends on how an image is being used and who is using it. The position of a tiny blemish on an X-ray image might be irrelevant to an untrained eye but very meaningful to a physician. A great artist like Picasso may be able to「compress」the image of a complex scene into a few simple lines, but in doing so he exercises a complex judgment in deciding which aspects of the image communicate the most meaning (see Figure 23 ).

If the computer were to compress an image by storing only the meaningful information, then the number of bits in the representation would correspond more closely to our commonsense notion about the information contained in the image. For example, the computer might represent the array of random pixels by indicating that this image has no regularity and no meaningful information. When asked to reconstruct this picture, it could simply generate another array of random pixels. The details — which pixel was which exact shade — would be different in the original and the reconstructed images, but these differences would be meaningless to the human eye.

FIGURE 23

Picasso sketch

Many image- and sound-compression algorithms discard certain meaningless information in order to reduce the size of the representation. These so-called lossy compression algorithms assume that a certain level of detail in the image or sound will be ignored by the eye or the ear. Lossy compression methods are generally used when it is known that the decompressed information will serve some particular purpose. For instance, if a particular detail appears only in a single frame of a movie, it may be safe to throw it away, since it will go unnoticed.

There is another important form of image representation, which is able to achieve an even greater degree of compression than the methods just described. If the process that generates the original image is known, then it may be more efficient to store a record of that process rather than a record of the image itself. For instance, if the image was a drawing, created by a person who drew a series of lines, then the drawing may be represented by storing a list of the lines — a representation scheme often used by computers for making simple line drawings.

The idea of representing something by storing the procedure or program that generated it is applicable to other types of data as well, such as sounds. Where sound is concerned, the technique may seem no more profound than the notion of recording a piece of music by writing down the score, but in a computer the score is able to represent every detail necessary to reproduce the original — the tuning of the instruments, the bowing of the violins, even the mood of the orchestra. If an object can be generated by a computer, then by definition there is a precise procedure for generating it, so a description of that procedure will serve as representation of the object.

This leads us to another measure of information: The amount of information in a pattern of bits is equal to the length of the smallest computer program capable of generating those bits. This definition of information holds whether the pattern of bits ultimately represents a picture, a sound, a text, a number, or anything else. The definition is interesting, because it allows for any type of regularity within the pattern. In particular, it subsumes all the methods of compression described above. (It might seem that such a definition depends on the details of the computer's machine language, but recall that any computer can simulate any other, so the measure from one computer to another will vary only by the small amount of code needed to perform the simulation.)

Once a string of information is compressed as much as possible, it will exhibit no regularity. This is true because any regularity would be an opportunity for further compression. The string of 1's and 0's representing optimally compressed text would look completely random, like the record of the flipping of a coin. In fact, many mathematicians use this property of incompressibility as a definition of randomness — a satisfyingly simple definition, but one that often is not very useful in practice, since it's very difficult to tell whether or not a given string of bits is random in this sense. It's easy to decide, when we recognize any regularity, that a string can be compressed — but we can't prove that the string cannot be compressed if we see no pattern. The pseudorandom number sequences described in chapter 4 are a good example of sequences that appear random but have an underlying pattern. By the above definition of randomness they are highly nonrandom, because a very long sequence can be briefly described, simply by describing the algorithm that produced it — in this case, the roulette-wheel simulation.

压缩

在不丢失信息的情况下，我们能将文本压缩多少呢？一种简单的压缩方式是，将每个字符的位数从 8 位减少至 6 位。有的压缩方式会利用文本的规律性。例如，在英文文本中，字母 T 和 E 出现的频率远高于字母 Q 和 Z。那么更高效的编码方法就是，用更短的 1、0 序列来表示出现频率更高的字母。早期电报员和无线电业爱好者使用的一个技巧是，利用长度可变的字符编码方式来实现更紧凑的表示形式。在莫尔斯码中，单个点代表字母 E，单条画线代表字母 T。其他不常见的字母，比如 Q 和 Z，则由一列长度最多为 4 的点和画线组成。不过，还有第三种信号 —— 停顿，用于表示字母的结束。因此，莫尔斯码的点和画线并不完全对应于二进制位的 1 和 0，但其基本原理是相似的。

如果使用 1、0 序列来表示一个长度可变的字符代码，必须仔细选择编码方式，以便将二进制位组成的信息流毫无歧义地分解成单个字符。只要某个字符的二进制位序列的开头部分与其他字符的子序列不同，那么这个分解就是有可能的。例如，用 4 位表示常见的字符且首位字符为 1；用 7 位表示不常见的字符且首位字符为 0。此时，二进制位信息流就能被准确地划分为短字符和长字符。选择一种充分利用不同字母相对频率的可变长度字符编码，可以有效地实现文本的压缩。以本书文本为例，这种方法能将原来的 200 万个位缩减至 100 万个位左右，压缩比例达 50%。

所有的压缩方法都利用了数据的规律性。上面介绍的编码方式利用了单个字符出现频率的规律性，还存在其他规律性可供利用。例如，在本书中，并非所有相邻的字母组都以同等频率出现，字母 Q 后面几乎总是跟着字母 U，但字母 Z 后面绝不会出现字母 K。我们可以为双字母组而非单个字母设计一个采用可变长度字符编码的系统，充分地利用双字母组合出现频率的非均等性。在编码中，可以使用较短的二进制位序列表示更常见的字母组，使用较长的二进制位序列来表示很少出现的字母组。这种方式可以使存储本书所需的位数再压缩 10%，达到平均每字符 3.5 位的压缩水平。

利用多字母组合的规律性进行编码，效率将会更高。例如，在本书中，「这」一字出现了约 3 000 次。如果使用一个较短的二进制位序列来表示这个字，将非常有效。同理，有许多字词也适合用这种方式来编码，比如「计算机」「二进制位」等这些经常在本书中出现的词。

除了字母组合的统计规律，还存在其他方面的规律性。例如，语法、句子结构和标点符号等也具有规律性，这些都可以用于进一步压缩文本。不过，在某些时刻，我们得到的边际收益会逐渐递减。如果使用目前最佳的统计方法，表示每个字符的位数最终可压缩至不到 2 位，这个压缩水平是 8 位标准字符代码的 25%。

压缩在文本中的应用效果相当不错，但它在表示现实世界的信号时的应用效果更好，比如声音和图像等。这些信号通过被称为模数转换（analog-to-digitalconversion）的过程输入计算机。这些输入通常是连续的模拟信号，比如声音的强度、光线的亮度等，在黑白照片中，点或像素可以为白色、黑色，或者介于两者之间的任意灰色。由于计算机无法表示无数多种可能性，因此它通过将每个像素转换成有限的灰度集合中的某个值来简化信号。通常来说，灰度的等级数目为 2 的整数次幂，这样便能与存储的位数相匹配。例如，可以用 8 位来表示黑白图片中的像素点亮度，因此共有 256 种灰度。也可以用 12 位来表示更高质量的图片，此时对应的灰度值有 4 096 种。计算机通常用 24 位来表示彩色图片中的像素点，其中每种三原色的亮度各由一个 8 位表示。

另一个决定图像质量的参数是分辨率，即照片中的像素数目。一张由 1 000×1 000 点阵组成的高分辨率图像比一张分辨率为 100×100 的图像更加清晰。不过，由于前者的像素点数为 1 000 000，而非 10 000，因此计算机需要的存储空间将增加 100 倍，需要的图像处理时长也会增加 100 倍。

由于高分辨率图像包含大量数据，因此我们通常希望将其压缩，以降低存储和传输成本。对于动态影像来说，尤其如此，其每秒包含的图像帧数为 24～100。幸运的是，图像容易压缩，因为它们具有高度的规律性。在大多数图像中，单个像素点的颜色和亮度通常与临近点的几乎相同。例如，在一张关于人脸的图像中，面颊中相邻部位的两个像素点具有非常相似的亮度和颜色。大多数图像的压缩算法都利用了这种相似性。对于亮度和颜色一致的区域，图像压缩算法只用几个数就可以表示其颜色和大小。其他图像压缩算法采用了更为复杂的规律形式，例如，图像中不同区域的纹理的相似性。对于诸如电视转播等动态影像来说，压缩算法通常利用前后相继的各帧图像之间的相似性，这种方法可以实现 10 倍的图像压缩比，以及 100 倍的视频压缩比。类似的压缩方法可以应用于声音信息的压缩。

对于图像所包含的信息量来说，这种压缩方法有些违反直觉。如果用表示图像所需的最少位数作为图像信息的度量，那么更易于压缩的图像包含的信息量更少。例如，一张面部图像的信息量比一堆沙滩鹅卵石图像的信息量更少，因为面部图像中相邻的像素点更加相似，而传输和存储一张鹅卵石图像则需要更多的信息量，即使人类观察者认为面部图像的信息更为丰富。根据这种度量方式，包含信息量最多的图像将是由随机像素点组成的图像，如同充满噪声和杂讯的电视机屏幕。如果图像中的像素点和相邻像素点之间没有任何相关性，那么压缩图像时就没有规律可循。虽然这样的图像对我们来说毫无意义，其计算机表示却具有最大的信息量。

信息度量的最小表示方法与我们对于信息内容的直观认识并不完全一致，因为计算机没有区分有意义和无意义的信息，它只需记录每个像素点的颜色，或者沙滩上每个鹅卵石的位置，即使这些细节对我们来说并不重要。判定哪些信息有意义、哪些信息没有意义是一门精妙的艺术，这取决于图像的使用方式和使用者。在外行看来，X 光片上的一点小瑕疵无关紧要，但对于医生而言，它却意义非凡。像毕加索这样伟大的艺术家能够将复杂的景象「压缩」成几条简单的线条，但为了实现这一点，他需要经过复杂的判断来决定用图像的哪些部分传达最重要的意义（见图 6-1）。

如果计算机通过只存储有意义的信息来压缩图像，那么表示图像所需的位数将更接近于我们对图像信息量的直观认知。例如，当表示由随机像素阵列组成的图像时，计算机可以指出该图像无规律性，其信息毫无意义。当要求计算机重构这张图片时，它可以简单地生成一张由随机像素阵列组成的新图像，而新图像和源图像在诸如像素灰度等细节方面存在差别，不过，这些差别在人眼中并无意义。

许多有关图像和声音的压缩算法会丢弃一些无意义的信息，以减少表示的信息量，这类算法被称为有损压缩。这类算法假设，人类的眼睛和耳朵会忽略图像和声音中的某些细节。有损压缩法一般适用于已知解压的信息用于何种目标的情况。例如，如果电影中的某个细节只出现在一帧图像中，那么将这帧图像丢弃也不失为一种安全的做法，因为没有人会注意到这个细节。

还有另外一种重要的图像压缩方法，它能实现比上述方法更高的压缩程度。如果已知原始图像的生成过程，那么与存储原始图像相比，存储这个生成的过程可能更加有效。例如，如果图像是由画家绘制的一系列线条组成的画作，那么可以通过存储线条列表来表示这幅画作，计算机经常使用这种方法制作简单的线条画。

通过存储事物的生成过程或程序来表示该事物的方法也适用于其他类型的数据，比如声音等。当涉及声音对象时，这种方法就类似于通过乐谱来记录音乐的方法。不过在计算机中，乐谱记录了生成原声音乐所需的全部细节，包括乐器的音调、小提琴的运弓方法，甚至乐队的演奏情绪。如果某个对象能由计算机生成，那么根据定义，计算机中一定有该对象的准确生成过程，而关于此过程的描述便可作为该对象的表示。

这样，我们又可以得出一个关于信息度量的结论：一个二进制位模式的信息量等同于能够生成这些二进制位的最短计算机程序的长度。无论这个二进制位模式最终表示的是图像、声音、文本、数字，还是其他事物，这种关于信息的定义都是成立的。这个定义相当有趣，因为它考虑到了模式中的各类规律，特别是包含了上述所有的压缩方法。这个定义似乎依赖于计算机的机器语言。不过，任何计算机都可以模拟其他的计算机，因此，信息度量之差仅是用于模拟所需的少许代码而已。

当信息被尽可能地压缩后，就不再具有规律性。这是因为任何规律性都是一次压缩信息的机会。文本经过最优压缩后，表示文本的 1 和 0 序列看起来完全是随机的，就像随机投掷硬币得到的记录一样。事实上，许多数学家将不可压缩性作为定义随机性的方式。虽然这个定义很简洁，但在实际应用中并没有多大用处，因为这种定义方式很难判断一串序列是不是随机的。当我们识别到字符串的规律性时，不难判断出该序列可以被压缩；但如果找不到字符串的规律性，并不能证明该序列不可被压缩。第 4 章描述的伪随机数就是这样的例子：它们虽然看起来是随机的，却具有一种潜在的规律模式。根据上面关于随机性的定义，这里的伪随机数具有高度的非随机性，因为通过伪随机数生成的算法能将一长串数字中的规律简明扼要地归纳出来，轮盘机模拟程序就属于这种情况。

### 6.2 Encryption

Those sequences that appear random but have a hidden underlying pattern can be used to create codes for encrypting data. Imagine, for example, that I wish to send a secret message to a friend of mine. If we both have a copy of the same random-number generator, so that we can generate the same sequence of pseudorandom numbers, we could use this sequence to hide the content of the message from anyone who might intercept it. Let's say that the message to be transmitted is a stream of bits representing characters, using the standard 8-bit-per-character representation. This standardized representation could presumably be interpreted and understood by any eavesdropper; it is what cryptographers call the plain text of the message. To encrypt the message, we pair each bit in the plain text to the corresponding bit in the pseudorandom bit stream. If the pseudorandom bit is 1, we invert the corresponding plain-text bit. If the pseudorandom bit is 0, we leave the corresponding plain-text bit alone. This will invert about half the bits in the plain text, but the eavesdropper won't know which half. Unless the eavesdropper knows the pseudorandom sequence, this string of 1's and 0's will be utterly meaningless. My friend the recipient, on the other hand, knows how to generate exactly the same random sequence, which can be used to reinvert the inverted bits, thereby restoring (decrypting) the message. This method, or something very similar to it, is at the heart of most encryption schemes.

Encrypting a message is analogous to sending it in a locked box that can be opened only with a special key. In the encryption method just described, the key is the random-number generator. Anyone who has the key is able to perform the conversion. In the example above, the same key is used for encryption and decryption, but it is also possible to construct codes that use different keys for encryption and decryption. In a public encryption scheme, the keys for encryption and decryption are different, and an eavesdropper who knows the encryption key will not thereby know the key needed to decrypt. This method of transmitting messages is extremely useful. For example, if I wish to receive an encrypted message, I can publicize the description of the key necessary to encode messages to me. Anyone will be able to send me a secret message, whether I know them or not. Since the public key tells the sender only how to encrypt the message, not how to decrypt it, others will not be able to unlock the encoded message. Only my private key, which I keep secret, allows an encrypted message to be converted back into plain text. This is called public key encryption. Public key encryption solves an important practical problem: for example, many businesses that accept credit card numbers over the Internet publish their own public key, so that customers can encrypt their credit card numbers without fear that they will be intercepted and read.

The public-key-encryption scheme is also useful in reverse, for authenticating messages. In this case, I publicize the key for decrypting a message but keep secret my key for encryption. Whenever I want to send out a message that I wish to「sign」as being verifiably from me, I encrypt the message with my private key. Any recipient of the message can use the public key for decrypting the message. They will know it was really from me because only someone who knows my private key could have encrypted such a message.

加密

有些序列虽然表面上看起来是随机的，却包含了隐藏的固有规律模式，它们可用于制作由数据加密的编码。例如，我想给朋友发送一条秘密信息。如果我们都有相同的伪随机数生成器，就能生成一串相同的伪随机数序列。然后，我们可以利用这个序列将信息内容隐藏起来，他人将无法窃取。假设要传输的信息是用字符表示的二进制位信息流，采用每字符 8 位的标准格式，任何窃听者都可以看懂这种标准化的表示形式，密码学家称之为明文（plain text）。为了加密信息，我们将明文中的位和伪随机数序列中的位一一配对。如果伪随机数序列中的位是 1，则置换对应的明文；如果伪随机数序列中的位是 0，则保持对应的明文不变。这样，明文中约有一半的位数被置换，但窃取者不会知道是哪一半，除非他们都知道伪随机数序列，否则这些由 1 和 0 组成的序列对他们来说毫无意义。在另一头，我的朋友知道如何生成完全相同的伪随机数序列，该序列能用于置换那些已经被置换的序列，从而重构（解密）出原始信息。这种方法或者其他类似的方法是大多数加密系统的核心。

对信息加密相当于将其放到只有使用特殊的密钥才能打开的带锁的箱子里。在刚才介绍的加密方法中，这把钥匙是伪随机数生成器，所有拥有这把钥匙的人都能执行置换操作。在上述例子中，加密和解密所使用的是同一把密钥，不过，也可以在加密和解密过程中使用不同的密钥。在公共加密体系中，加密和解密使用的密钥是不一样的，知道加密密钥的破译者不知道解密所需的密钥。这种传输信息的方式非常有用。例如，如果我想接收加密后的信息，就可以对外公布加密信息所需的密钥。这样，任何人都可以向我发送秘密信息，无论我是否认识他们。由于公开密钥只会告知发送者如何加密信息，而不会告知如何解密信息，所以其他人无法破译这些编码后的信息，只有我私下保存的密钥才能将密文转换成明文。这种方法被称为公共密钥加密法。公共密钥加密法解决了一类重要问题。比如，许多在互联网上接收信用卡账号支付的商家会发布他们的公钥，这样客户就能加密传输他们的信用卡账号，无须担忧账号被拦截和窃取。

此外，这种公共密钥加密法在信息认证方面也大有用处。在这种情况下，我会公开用于解密的公钥，而用于加密的密钥则不公开。当我想发送一条信息，并希望用其签名来证明该信息来自我时，我就用密钥对其加密。任何接收到这条信息的人都可以用公钥解密这条信息。他们也能确定这条信息来自我，因为只有知道我的密钥的人才能加密这条信息。

### 6.3 Error Detection

Encoding and decoding bits have many other applications besides compression and security. For instance, there are situations in which we may represent a message using more bits than necessary, in order to reduce the chance of error. Codes that use some form of redundancy for detecting transmission errors — for example, a 0 that was received as a 1 — are called error-detecting codes. Other codes, called error-correcting codes , contain enough redundant information to correct as well as detect such errors.

An obvious form of redundancy is to send a message more than once. Sending a message twice allows for error detection. If two copies of the same message are transmitted, but slightly different messages are received, then there must have been some error in transmission. A simple error-correcting code would be to repeat the message three times. Assuming that only one of the messages was corrupted, then the recipient would reconstruct the correct message by choosing the two copies that were the same.

Fortunately there are error-detecting and error-correcting codes that achieve results with far less redundancy. A commonly used scheme for detecting errors is a parity code. This scheme can detect a single-bit error in a message of any length by the addition of one redundant bit. As a specific example of a parity code, consider the 8-bit code often used for transmitting characters over noisy communications lines. The eighth bit, called the parity bit , is 1 if, and only if, the number of 1's in the seven other bits is even. This means that the number of 1's in the 8-bit sequence should always be odd. If noise in the transmission line causes a 1 to be received as a 0, or vice versa, then the 8-bit message that is received will have an even number of 1's. The recipient therefore will be able to detect that there has been an error. Similar parity schemes are used for detecting errors in the memory systems of computers. One bit of parity can be used to detect an error in a message of any number of bits. A limitation of this simple parity code is that it's good at detecting only a single error. A message that has two inverted bits will have the correct parity, even though the data is incorrect.

By using multiple parity bits, it is possible to detect multiple errors. It is also possible to give the recipient enough information not only to detect an error but to correct it; that is, the recipient is able to reconstruct the original message even though there has been an error. An example of such a code is the two-dimensional parity code illustrated in Figure 24.

This code contains 9 bits of message information and 6 bits of parity. The message bits are arranged in 3 rows of 3 bits each. There is 1 parity bit for every horizontal row and 1 for every vertical column. A single-bit error in a message will cause two parity failures to be detected, one in a row and one in a column. The recipient of the message will then know that the bit at the intersection of the failing row and the failing column is incorrect and should be inverted. If there is an error in transmission of one of the parity bits, on the other hand, then either a row or a column will show an incorrect parity but not both. The bits are drawn in a two-dimensional pattern to help you visualize the structure of the code, but they can be transmitted in any order. Such error-correcting codes are often used to protect each word in the memory of a large computer. Using similar techniques, many other codes can be constructed which will detect and/or correct various types and numbers of errors.

FIGURE 24

An error correction code with 9 data bits and 6 check bits

Error-correcting codes are able to deal with signaling errors that occur in transmission and storage of information, but what about errors in computation itself? It turns out that it is also possible to build logic blocks that produce correct answers even if some of the blocks from which they're constructed are operating incorrectly. Again, the basic tool is some form of redundancy. One way to build a fault-tolerant logic block is to copy each logic block three times. A majority-voting block, such as is shown in Figure 12 of chapter 2 , can be used to combine the answers from the three copies. If one of the copies makes an error, it will be outvoted by the others. This simple method protects against any single error (except within the majority-voting block itself).

Given a well-defined set of possible errors — such as wires breaking, switches getting stuck, and 0's switching to 1's — it is possible to construct an arbitrarily reliable computing device out of arbitrarily unreliable components. This task simply requires using enough redundant logic in a systematic form. For instance, if you could build a new type of switching device — say, a molecular switch — that was extremely fast, or extremely inexpensive, but failed 20 percent of the time, you could still use it to build a computer that produced answers with 99.99999-percent reliability, by building the proper redundancy into the circuitry.

Does this mean that you can construct an arbitrarily reliable computer? Not exactly. While a computer can be constructed so as to eliminate a particular type of error, errors of unanticipated types may occur which produce correlated errors in the redundant module. For instance, the burnout of one module may cause another module to overheat, or some kind of magnetic pulse may even cause all modules to err simultaneously. Engineers are capable of designing a logic block that can deal with any type of error they can imagine, but the history of technology shows that our imaginations are not always sufficient. The most dramatic failures are usually surprises.

A second reason not to expect a perfect computer is that most computer failures are not caused by incorrect operation of the logic. They stem from errors in design — usually in the design of the software. Programmed computers, including their software, are by far the most complex systems ever designed by human beings. The number of interacting components in a computer is orders of magnitude larger than the number of components in the most complex airplane. Modern engineering methods are not really up to designing objects of such complexity. A modern computer can have literally millions of logical operations going on simultaneously, and it is impossible to anticipate the consequences of every possible combination of events. The methods of functional abstraction described in the previous chapters help keep the interactions under control, but these abstractions depend on everything interacting as expected. When unanticipated interactions occur (and they do), the assumption on which the abstraction rests breaks down, and the consequences can be catastrophic. In a practical sense, the behavior of a large computer system, even if no failures occur, is sometimes unpredictable — and this is the overarching reason that it is impossible to design a perfectly reliable computer.

查错

除了压缩和加密之外，编码和解码还有许多其他用途。例如，在某些情况下，为了降低出错率，我们会在必要的位之外再附加几位。一种被称为查错码（error-detection code）的冗余码可用于检测传输过程中发生的错误，比如传输的是 0，但接收时却变成了 1。还有一类被称为差错校正码（error-correction code）的代码，它们是一种包含了足够冗余信息的代码，可用于检测和校正此类错误。

一种显而易见的冗余形式是多次发送信息。将信息发送两次就能起到检测差错的作用。如果一方发送了同一信息的两个副本，但对方接收到的两条信息却略有不同，这意味着在传输过程中一定出现了差错。一种简单的差错校正码是将同一信息重复发送三次。假设其中只有一条信息受到了破坏，那么接收者可以通过另外两份相同的副本来重构正确的信息。

幸运的是，有些查错码和差错校正码可以用更少的冗余信息达到同样的结果。一种常用的查错码是奇偶校验码（parity code）。这种方法可以通过增加一个冗余位来检测任意长度信息中出错的一个位。举一个奇偶校验码的具体例子，当在嘈杂的通信线路中传输字符时，我们经常采用 8 位码。编码中的第 8 位即被称为奇偶校验位，当且仅当前 7 位中 1 的数目为偶数时，则此位才为 1。这意味着 8 位中的 1 的数目始终为奇数。如果传输线路中的噪声导致 1 变成了 0，或者相反，那么接收到的 8 位中的 1 的数目就变成了偶数。据此，接收者可以检测到差错的存在。计算机存储系统也采用类似的奇偶校验码来检测错误。使用一个奇偶校验位，我们就能检测出任意长度信息中的错误。这种简单的奇偶校验码的局限性在于，一个奇偶校验码只能检测单个位的错误。如果一个信息中出现两个位同时被置换的数据错误，那么即便数据本身是不准确的，其奇偶性仍是正确的。

不过，使用多个奇偶校验位便可以检测出多个错误。此外，还可以给接收者提供足够的信息，使其不仅用于检测错误，还可以用于纠正错误。也就是说，即使信息存在差错，接收者也能够重构原始消息。图 6-2 所示的二维奇偶校验码就是一个具体的例子。

图 6-2 使用 9 个数据位和 6 个奇偶校验位的差错校正码

这种代码包括 9 个数据位和 6 个奇偶校验位。表示信息的 9 位排列成 3 行，每行 3 位。每行和每列各有一个奇偶校验位。当信息中的某一个数据位出现错误时，两个奇偶校验位会同时检测出异常，也就是能确定该异常位于某一行某一列。据此，信息接收者就会知道位于异常行和异常列交叉位置的那个数据位是不正确的，应该将其置换。另一方面，如果一个奇偶校验位在传输中出现错误，那么就只有某一行或某一列的奇偶校验结果是异常的，这样便不会出现某一行和某一列同时出现异常的情况。为了可视化编码结构，可以用二维模式来表示这些位，但代码可以按照任何顺序传输。这种差错校验码通常用于保护大型计算机存储空间中的字符。我们可以使用类似的技术设计出许多种其他代码，用于检测或者纠正不同类型和数目的错误。

差错校正码能够处理信息传输和存储过程中出现的错误。那么计算本身的错误又当如何呢？事实证明，即使组成逻辑块的某些子模块无法正常工作，也依然可以搭建出能产生正确答案的逻辑块。同样，基本的检测工具还是某种形式的冗余。构建容错逻辑块的一种方法是，将每个逻辑块复制三份。如第 2 章图 2-3 所示，可以将少数服从多数逻辑块用于统筹这三个复制逻辑块的输出。如果其中一个逻辑块出现了错误，就以少数为由将其排除在外。这种简单的方法能够防止单个模块出现错误（除非少数服从多数逻辑块本身会出错）。

如果给定一组可能发生的、定义明确的错误，比如断线、开关失灵以及 0 变成 1 等，那么无论元件多么不可靠，都可以用它们构造出具有任意可靠性的计算装置。此任务只需以系统的形式组装具有足够冗余度的逻辑元件便可完成。例如，如果你发明了一种新型开关，比如分子开关，它的速度非常快，或者价格非常便宜，但会在 20% 的时间内出错，通过在电路中搭建恰当的冗余结构，你仍然能够利用它制造出一台具有 99.99999% 可靠性的计算机。

这是否意味着你可以搭建一台具有任意可靠性的计算机呢？答案并非如此。尽管在搭建计算机时，某种特定类型的错误可以被消除，但还是有可能发生意料之外的错误，它们会在冗余逻辑块之间产生关联故障。举个例子，某个烧毁的逻辑块可能会导致另一个逻辑块的温度过高，或是某种形式的磁场脉冲会导致所有逻辑块同时出错。工程师设计的逻辑块只能处理他们预想范围内的错误。技术的发展史表明，人类的考虑并非始终万无一失，惨痛的失败通常发生在意料之外。

奢望建造一台完美的计算机是不现实的，因为大多数计算机故障并不是由错误的逻辑运算造成的，而是源于错误的设计，通常是错误的软件设计。计算机及其软件是迄今为止人类设计出的最为复杂的系统。计算机中交互的元件数目比最复杂的飞机的元件数目还要多出几个量级。现代工程技术并不足以支撑设计如此复杂的物体。一台现代计算机可同时运行的逻辑运算高达数百万条，因此我们无法预测出各个事件所有可能的组合的后果。虽然前几章介绍的功能抽象的方法有助于控制这些交互行为，但这些功能抽象基于一个前提，即一切交互作用都按照设想的那样进行。当出现预期之外的交互行为时（实际中的确会出现），这些功能抽象基于的假设便不再成立，其后果可能是灾难性的。实际上，即使没有出现故障，大型计算机的行为有时也是不可预测的。这是无法设计出一台绝对可靠的计算机的重要原因。