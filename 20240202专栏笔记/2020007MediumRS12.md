## 记忆时间

## 目录

20200304Top-3-Selenium-Functions-.md

20200304Web-scraping-in-5-minutes.md

20200318My-top-10-Python-packages-for-data-science.md

20200318Productivity-tips-for-Jupyter.md

20200319Build-an-API-with-Laravel.md

## 20200304Top-3-Selenium-Functions-.md

get(). The get command launches a new browser and opens the given URL in your Chrome Webdriver. It simply takes the string as your specified URL and opens it for testing purposes. If you are using Selenium IDE, it is similar to open command.

    driver.get("https://google.com");

The ‘driver’ is your Chrome Webdriver on which you are going to perform all actions and it looks like this after executing the command above:

find\_element(). This function is crucial when you want to access elements on the page. Let’s say we want to access the「Google search」button in order to perform a search. There are many ways to access elements, but my preferred one is to find XPath of the element. XPath is the element's ultimate location on the web page.

By clicking F12 you will inspect the page and get the background information about the page you are at. By clicking the selection tool, you will be able to select elements. After finding the button, click on the right at the blue marked section and Copy element’s「Full Xpath」.

    self.driver.find_element_by_xpath('/html/body/div/div[4]/form/div[2]/div[1]/div[3]/center/input[1]')

That is the full command in order to find the specific element. I prefer full XPath over the regular XPath because regular one can be changed if the element in a new session changes and then next time you perform your script it does not work.

send_keys() and click(). I added a bonus function for you so we can follow through with this example and make it fully work. Send_keys function is used for typing the text into a field to you selected by using the find\_element function. Let’s say we want to type「plate」into google and perform a search. We already have our「Google search」button, now we just need to type in the text and click the button with the help of click function.

```py
google_tray = self.driver.find_element_by_xpath('/html/body/div/div[4]/form/div[2]/div[1]/div[1]/div/div[2]/input')
google_tray.send_keys("plate")
google_search = self.driver.find_element_by_xpath('/html/body/div/div[4]/form/div[2]/div[1]/div[3]/center/input[1]')
google_search.click()
```

## 20200304Web-scraping-in-5-minutes.md

### 01. Introduction

we can face the web-scraping-challenge by using just a few Python public libraries:

1. ‘requests’ calling ‘import requests’

2. ‘BeautifulSoup’ calling ‘from bs4 import BeautifulSoup’

3. We may also want to use also ‘tqdm\_notebook’ tool calling ‘from tqdm import tqdm\_notebook’ in case we need to iterate through a site with several pages. For example, suppose we’re trying to extract some information from all the job posts in https://indeed.co.uk after searching ‘data scientist’. We’ll probably have several result pages. The ‘bs4’ library will allow us to go all over them in a very simple way. We could do this also using just a simple for loop, but the ‘tqdm_notebook’ tool provides visualization about the evolution of the process, what meshes quite nicely if we’re scraping hundreds or thousands of pages from a site in one run.

But anyway, let’s go step by step. For this article, we’ll be scraping Penguin’s list of the 100 must-read classic books ([100 Must-Read Classic Books, As Chosen By Our Readers | Fiction, Novels & More](https://www.penguin.co.uk/articles/2018/100-must-read-classic-books/)). And we’ll try to get only the title and the author for each one of them. This simple notebook and others are available in my [gonzaferreiro/Simple_web_scraper](https://github.com/gonzaferreiro/Simple_web_scraper) (including the entire project about scraping indeed and running a classification model).

2『已经 forked 作者的源码「2020010Simple_web_scraper」。』

1『

因为网页刷不出来，试着输出内容，放到自己的服务器里看。

```py
import requests 
from bs4 import BeautifulSoup
import re

url = 'https://www.penguin.co.uk/articles/2018/100-must-read-classic-books/'
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')

## 试着输出内容，放到自己的服务器里看
with open('result.txt', 'w') as f:
    f.write(r.text)
```

』

### 02. Hands-on application

In this case, the text seems to be contained inside ‘\<div class=”cmp-text text”>’. The \<div> tag is nothing more than a container unit that encapsulates other page elements and divides the HTML document into sections. We may find our data under this or another tag. For example, the \<li> tag is used to represent an item in a list, and the \<h2> tag represents a level 2 heading in an HTML document (HTML includes 6 levels of headings, which are ranked by importance).

In this case, we didn’t have any special parameter in our URL. But let’s go back to our previous example of scraping Indeed.co.uk. In that scenario, as we mentioned before, we’d have to use tqdm_notebook. This tool works as a for loop iterating through the different pages:

```py
for start in tqdm_notebook(range(0, 1000 10)):
    url = “https://www.indeed.co.uk/jobs?q=datascientist&start{}".format(start)
    r = requests.get(url)
    soup = BeautifulSoup(r.text,’html.parser’)
```

Pay attention to how we’re specifying to go from page 0 to 100, jumping from 10 to 10, and then inserting the ‘start’ parameter into the url by using start={}”.format(start). Amazing! We already have our HTML code in a nice format. Now let’s obtain our data! There are several ways of doing this, but for me, the neatest way is to write a simple function to be executed just after creating our BeautifulSoup object.

Previously we saw that our data (the title + author info) was in the following location: ‘\<div class=”cmp-text text”>’ . So what we’ll have to do is dismantle that code sequence, in order to find all the ‘cmp-text text’ elements in the code. For that, we’ll use our BeautifulSoup object:

    for book in soup.find_all('div', attrs={'class':'cmp-text text'})

In this way, our soup object will go through all the code, finding all the ‘cmp-text text’ elements and giving us the content of each one of them. A good idea to understand what we’re getting on each iteration is to print the text calling ‘book.text’. In this case, we’d see the following:

What we can see here, is that the text under our ‘cmp-text text’ element contains the title + author, but also the brief description of the book. We can solve this in several ways. For the sake of this example, I used regex calling ‘import re’. Regex is a vast topic by its own, but in this case, I used a very simple tool that always comes in handy and you can learn easily.

    pattern = re.compile(r'(?<=. ).+(?=\n)')

Using our ‘re’ object, we can create a pattern using the regex lookahead and lookbehind tools. Specifying (?<=behind_text) we can look for whatever follows our ‘match\_text’. And with (?=ahead_text) we are indicating for anything followed by ‘match_text’, ’. Once regex found our code, we can tell it to give us anything that’s a number, only letters, special characters, some specific words or letter, or even a combination of all this. In our example, the ‘.+’ expression in the middle basically means ‘bring me anything’. So we’ll find everything in between ‘. ‘ (the point and the space after the book’s position in the list) and a ‘\n’, that’s a jump to the next line. Now we only have to pass our ‘re’ object the pattern, and the text:

    appender = re.findall(pattern,text)

This will give us a list, for example, with the following content:

    [‘The Great Gatsby by F. Scott Fitzgerald’]

Easy peasy to solve calling the text in the list and splitting it at ‘ by ‘, to after store title and author in different lists. Let’s put everything together now in a function as we said before:

```py
def extract_books_from_result(soup):
    returner = {'books': [], 'authors': []}
    for book in soup.find_all('div', attrs={'class':'cmp-text text'}):
        try:
            text = book.text
            pattern = re.compile(r'(?<=. ).+(?=\n)')
            appender = re.findall(pattern,text)[0].split(' by')
            # Including a condition just to avoid anything that’s not a book Each book should consist in a list with two elements: 
            # [0] = title and [1] = author
            if len(appender) > 1:
                returner['books'].append(appender[0])
                returner['authors'].append(appender[1])
                
        except:
            None
            
    returner_df = pd.DataFrame(returner, columns=['books','authors']) 
    
    return returner_df
```
And finally, let’s run all together:

```py
url = 'https://www.penguin.co.uk/articles/2018/100-must-read-classic-books/'
r = requests.get(url)
soup = BeautifulSoup(r.text,'html.parser')
results = extract_books_from_result(soup)
```

Unfortunately, web scraping is not always that easy, since there are several web sites that don’t want us sneaking around. In some cases, they will have a very intricated HTML to avoid rookies, but in many, they will be blocking good people like us trying to obtain some decent data. There are several ways of not being detected, but sadly, this article has gone away from me and my promise of learning web scraping in five minutes. But don’t worry, I’ll be writing more about this in the near future.

## 20200318My-top-10-Python-packages-for-data-science.md

### 01. Data processing

pandas. Developed by Wes McKinney more than a decade ago, this package offers powerful data processing capabilities. For people with a SAS background, it is a bit like SAS data steps. You can do sorting, merging, filtering, etc. The key difference is in pandas, you call a function to perform these tasks. By the way, I was really amazed to know that Wes McKinney was able to develop pandas after only a few years of Python experience. Some people are just really gifted! His book Python for Data Analysis is highly recommended if you are just starting out your Python data science journey.

3『就是之前看过的书籍「利用 Python 进行数据分析」。』

numpy. Pandas builds on top of this package numpy. So when you will often rely on this package for basic data manipulations. For example, when you need to create a new column based on the age of the customer, you need to do something like:

    df['isRetired'] = np.where(df['age']>=65, 'yes', 'no')

qgrid. An amazing package that allows you to sort, filter, and edit DataFrames in Jupyter Notebooks.

### 02. Graphing

The next three packages are all to do with graphing — which is a key step in exploratory data analysis.

matplotlib. This package allows you to do all sorts of graphs. If you are using it in a Jupyter Notebook, remember to run this line of code to enable the display of the graphs:

    %matplotlib inline

seaborn. With the help of this package, you can make matplotlib graphs looking much more attractive.

plotly. Nowadays we come across interactive graphs everywhere. They really offer a much better user experience. When we hover the mouse over a line plot we expect some text to pop up. When we select a line, we expect it to stand out from the other lines. Sometimes we would like to zoom into parts of the graph. These are the areas where interactive graphs shine. plotly allows you to build interactive graphs easily with a Jupyter Notebook. Over time, I see a lot of potential in sending Jupyter Notebooks with beautiful plotly graphs to the stakeholders.

### 03. Modeling

statsmodels. This package allows you to build Generalized Linear Models (GLMs) which are still widely used by actuaries today. It also offers time series analysis and other statistical modelling capabilities.

scikit-learn. This is the main machine learning package allowing you to complete most machine learning tasks, including classification, regression, clustering, and dimensionality reduction. I also use model selection and pre-processing functions. From k-fold cross-validation to scaling data and encoding categorical features, it has so much to offer.

lightgbm. This is one of my favorite machine learning packages for Gradient Boost Machine (GBM). I gave a talk in the 2018 Data Analytics seminar about this package. For a fraction of the time and effort needed to build GLMs, you could run a GBM, look at the importance matrix to find out the most important features for your model and have a good initial understanding of the problem. This can be a stand-alone step, or a quick first step before building a full GLM that’s more readily accepted by the stakeholders.

lime. Model interpretation still remains a challenge for machine learning models like GBM. When stakeholders don’t understand a model, they can’t trust it, and as a result, there’s no adoption.

However, I feel model interpretation packages like lime is starting to change this. It allows you to examine each model prediction and work out what’s driving the prediction.

### 20200318Replacing-Excel-with-Python.md

GitHub repo link: [ank0409/Ditching-Excel-for-Python: Functionalities in Excel translated to Python](https://github.com/ank0409/Ditching-Excel-for-Python)

2『已经 forked 项目「2020011Ditching-Excel-for-Python」。』

Importing Excel Files into a Pandas DataFrame. Though read_excel method includes million arguments but I will make you familiarise with the most common ones that will come very handy in day to day operations. I’ll be using the Iris sample dataset which is freely available online for educational purpose. Please follow the below link to download the dataset and ensure to save it in the same folder where you are saving your python file.

1『感觉后面的内容太零碎，没看了。』

## 20200318Productivity-tips-for-Jupyter.md

I’ve been very busy working on my MRes project in recent weeks, having very little sleep. This made me seek ways to improve my workflow in the most important tool of my work: Jupyter Notebook/Jupyter Lab. I collected all the hacks & tips in this piece, hoping that other researchers may find those useful:

Note: To make it easy-to-use, I collected the snippets presented below into a Python3 package (jupyter-helpers). You can get it with:

    pip install jupyter_helpers

It will work out of the box, but if you wish the best experience, these dependencies are highly recommended:

```py
pip install ipywidgets
jupyter labextension install @jupyter-widgets/jupyterlab-manager
```

1『 jupyter labextension install @jupyter-widgets/jupyterlab-manager，提示没有该安装命令。』

### 06. Selectively import from other notebooks

For some time I was trying to follow data/methods/results separation, having three Jupyter notebooks for each larger analysis: data.ipynb, methods.ipynb and results.ipynb. To save time on useless re-computation of some stuff I wanted to have selective import from data and methods notebooks for use in the results notebook. This was described in this SO thread, and I still hope to see some suggestions.

[python - Selectively import from another Jupyter Notebook - Stack Overflow](https://stackoverflow.com/questions/54317381/selectively-import-from-another-jupyter-notebook)

### 07. Scroll to the recently executed cell

You may have noticed that previously shown Notifications class made the notebook scroll down on exception to the offending cell (Figure 1). This can be disabled by passing scroll\_to\_exceptions=False.

If you conversely, want more auto-scrolling, you could use the underlying helper function to mark the cell that you ended working with at night to quickly open your notebook back on it in the morning:

```py
from jupyter_helpers.utilities import scroll_to_current_cell
scroll_to_current_cell(preserve=True)
```

## 20200319Build-an-API-with-Laravel.md

3『

[Laracasts: Laravel 5.7 From Scratch](https://laracasts.com/series/laravel-from-scratch-2018)

[Laravel API Tutorial: How to Build and Test a RESTful API - mouseleo - 博客园](https://www.cnblogs.com/mouseleo/p/10777175.html)

[connor11528/december-2018-meetup: Build an API with Laravel 5.7](https://github.com/connor11528/december-2018-meetup)

[Build an API with Laravel 5.7 - Employbl - Medium](https://medium.com/employbl/build-an-api-with-laravel-5-7-b3aa16ca2e69)

[Build authentication into your Laravel API with JSON Web Tokens (JWT)](https://medium.com/employbl/build-authentication-into-your-laravel-api-with-json-web-tokens-jwt-cd223ace8d1a)

```py
$ git clone
$ cp .env.example .env
$ composer install

$ touch database/database.sqlite
$ php artisan migrate
$ php artisan db:seed
$ php artisan serve

$ ./vendor/bin/phpunit
```

』

### 01. Create a new Laravel app

    composer create-project laravel/laravel meetup --prefer-dist '5.7'

### 02. Hook up the database

For this application we’re going to use SQLite. All that’s required to run SQLite locally is a blank file sitting in the database folder.

    $ touch database/database.sqlite

Now that we’ve created the raw database file, edit the .env file use sqlite. Delete the other database connection info and replace it:

```
DB_CONNECTION=sqlite
DB_DATABASE=database/database.sqlite
```

The .env file is not checked into version control and holds database connection info and API keys.


### 03. Create a database model, table and controller

For the next step we’re going to create a Task model with a few options:

    $ php artisan make:model Task --migration --resource --controller

The above options can be abbreviated to -mcr for the same effect.

These options:

--migration: This creates a file in database/migrations for creating the tasks table. We use this file to articulate what columns exist on the Task table.

--resource --controller: This creates a controller within app/Http/Controllers. The controller class will be fully equipped with CRUD methods.

To see all the options for any artisan command, prefix “help” to the command, like so:

    $ php artisan help make:model

Update the create task table migration to include a title and description field:

1『去 database/migrations/2020_03_19_134153_create_tasks_table.php。』

```py
public function up()
{
    Schema::create('tasks', function (Blueprint $table) {
        $table->increments('id');
        $table->string('title');
        $table->text('description');
        $table->timestamps();
    });
}
```

The timestamps field, included by default, automatically add created\_at and updated\_at fields to the table in the database.

To actually run the migration and create our database tables use:

    $ php artisan migrate

### 04. Add routes

Now that we have a database, we need to be able to access our data and serve up JSON. These are defined in routes/api.php.

```py
Route::get('/tasks', 'TaskController@all')->name('tasks.all');
Route::post('/tasks', 'TaskController@store')->name('tasks.store');
Route::get('/tasks/{task}', 'TaskController@show')->name('tasks.show');
Route::put('/tasks/{task}', 'TaskController@update')->name('tasks.update');
Route::delete('/tasks/{task}', 'TaskController@destory')->name('tasks.destroy');
```

Here we’re taking advantage of a Laravel feature called Route Model Binding. By passing in the id for a task resource we can inject the task itself into the controller below. If you don’t want to match on the id field and instead match on something like username, you could define getRouteKeyName to be username. More info in the docs above. This gives functionality similar to LinkedIn where your personal URL is /in/yourusername instead of /in/2398094392432 which would be a user id.

3『[Routing - Laravel - The PHP Framework For Web Artisans](https://laravel.com/docs/5.7/routing#route-model-binding) | [Database Testing - Laravel - The PHP Framework For Web Artisans](https://laravel.com/docs/5.7/database-testing#writing-factories)』

### 05. Define the actions in the controller

Now that we have route endpoints set up we need to actually do things when users make API calls. These actions are defined in the TaskController referenced above:

```php
<?php

namespace App\Http\Controllers;

use App\Task;
use Illuminate\Http\Request;

class TaskController extends Controller
{
    public function index()
    {
        $tasks = Task::all();

        return response()->json($tasks);
    }

    public function store(Request $request)
    {
        $request->validate([
            'title' => 'required',
            'description' => 'required'
        ]);

        $task = Task::create($request->all());

        return response()->json([
            'message' => 'Great success! New task created',
            'task' => $task
        ]);
    }

    public function show(Task $task)
    {
        return $task;
    }

    public function update(Request $request, Task $task)
    {
        $request->validate([
           'title'       => 'nullable',
           'description' => 'nullable'
        ]);

        $task->update($request->all());

        return response()->json([
            'message' => 'Great success! Task updated',
            'task' => $task
        ]);
    }

    public function destroy(Task $task)
    {
        $task->delete();

        return response()->json([
            'message' => 'Successfully deleted task!'
        ]);
    }
}
```

In order for creating tasks to work, we need to specify on the Task model what fields we can write to:

```php
<?php

namespace App;

use Illuminate\Database\Eloquent\Model;

class Task extends Model
{
    protected $fillable = [
        'title',
        'description'
    ];
}
```

If you want to make all fields writable on a model you could instead do protected \$guarded = []. For our purposes, either one will work. The purpose of this extra check is to protect against Mass Assignment vulnerabilities where malicious users could write to fields that you don’t expect them to.

Woohoo! After all that it works! But don’t take my word for it. Let’s write some tests.

### Testing

### 1A. Create a test

Really tests should come first. First generate the test.

    $ php artisan make:test TaskTest

To run all of the tests use ./vendor/bin/phpunit from the command line. The default test will pass because it asserts true is true.

When writing tests it’s very helpful to rely on the die and dump command dd(). This will output what you specify and stop the program execution.

We’ll also want to specify more information about our testing database. In phpunit.xml add two lines toward the bottom of the file:

```php
    ...
    <php>
        <env name="APP_ENV" value="testing"/>
        <env name="BCRYPT_ROUNDS" value="4"/>
        <env name="CACHE_DRIVER" value="array"/>
        <env name="MAIL_DRIVER" value="array"/>
        <env name="QUEUE_CONNECTION" value="sync"/>
        <env name="SESSION_DRIVER" value="array"/>
        <env name="DB_CONNECTION" value="sqlite"/>
        <env name="DB_DATABASE" value=":memory:"/>
    </php>
</phpunit>
```

This specifies connecting to a SQLite database for testing that’s stored in memory.

1『单元测试的配置。』

### 1B. Create a factory

To simplify creating tasks in our tests we can use Model Factories.

    $ php artisan make:factory TaskFactory --model=Task

These factories can be used for seeding your development database and have automatic access to the faker php library. This is helpful for mocking out names, addresses, email addresses, text and more. The factories are located in the database/factories folder.

```php
<?php

use Faker\Generator as Faker;

$factory->define(App\Task::class, function (Faker $faker) {
    return [
        'title'       => $faker->sentence(),
        'description' => $faker->text()
    ];
});
```

### 1C. (optional) Create a seeder

Then create a seeder to check it works:

    $ php artisan make:seeder TaskSeeder

And the code for the seeder will invoke the factory to create ten unique tasks:

```php
<?php

use App\Task;
use Illuminate\Database\Seeder;

class TaskSeeder extends Seeder
{
    public function run()
    {
        factory(Task::class, 10)->create();
    }
}
```

We can use the php artisan db:seed command to invoke the run command in database/seeds/DatabaseSeeder.php:

```php
<?php

use Illuminate\Database\Seeder;

class DatabaseSeeder extends Seeder
{
    public function run()
    {
        $this->call(TaskSeeder::class);
    }
}
```

By running the seeder you now have ten tasks to play with in the database

```php
$ php artisan db:seed
Seeding: TaskSeeder
Database seeding completed successfully.
```

To verify it works, you can use the Artisan Tinker command. This allows you to directly view and manipulate your database.

```php
$ php artisan tinker
Psy Shell v0.9.9 (PHP 7.1.7 — cli) by Justin Hileman
>>> $tasks = \App\Task::all();
```

If all is well there will be ten records in the output with jibberish titles and descriptions. You could also view these records in a database viewing software such as Sequel Pro (for Mac).

### 1D. Write the test!

In the test for tasks we specify that we’d like to use DatabaseMigrations. This will set up our testing database from our migration files before the first test runs.

```php
<?php

namespace Tests\Feature;

use App\Task;
use Illuminate\Foundation\Testing\DatabaseMigrations;
use Tests\TestCase;

class TaskTest extends TestCase
{
    use DatabaseMigrations;

    /** @test */
    public function it_will_show_all_tasks()
    {
        $tasks = factory(Task::class, 10)->create();

        $response = $this->get(route('tasks.index'));

        $response->assertStatus(200);

        $response->assertJson($tasks->toArray());
    }

    /** @test */
    public function it_will_create_tasks()
    {
        $response = $this->post(route('tasks.store'), [
            'title'       => 'This is a title',
            'description' => 'This is a description'
        ]);

        $response->assertStatus(200);

        $this->assertDatabaseHas('tasks', [
            'title' => 'This is a title'
        ]);

        $response->assertJsonStructure([
            'message',
            'task' => [
                'title',
                'description',
                'updated_at',
                'created_at',
                'id'
            ]
        ]);
    }

    /** @test */
    public function it_will_show_a_task()
    {
        $this->post(route('tasks.store'), [
            'title'       => 'This is a title',
            'description' => 'This is a description'
        ]);

        $task = Task::all()->first();

        $response = $this->get(route('tasks.show', $task->id));

        $response->assertStatus(200);

        $response->assertJson($task->toArray());
    }

    /** @test */
    public function it_will_update_a_task()
    {
        $this->post(route('tasks.store'), [
            'title'       => 'This is a title',
            'description' => 'This is a description'
        ]);

        $task = Task::all()->first();

        $response = $this->put(route('tasks.update', $task->id), [
            'title' => 'This is the updated title'
        ]);

        $response->assertStatus(200);

        $task = $task->fresh();

        $this->assertEquals($task->title, 'This is the updated title');

        $response->assertJsonStructure([
           'message',
           'task' => [
               'title',
               'description',
               'updated_at',
               'created_at',
               'id'
           ]
       ]);
    }

    /** @test */
    public function it_will_delete_a_task()
    {
        $this->post(route('tasks.store'), [
            'title'       => 'This is a title',
            'description' => 'This is a description'
        ]);

        $task = Task::all()->first();

        $response = $this->delete(route('tasks.destroy', $task->id));

        $task = $task->fresh();

        $this->assertNull($task);

        $response->assertJsonStructure([
            'message'
        ]);
    }
}
```

1『文件位置在「/meetup/tests/Feature/TaskTest.php」，要体会到测试的重要。』

In the above test we make a few different types of assertions about the JSON structure and content. The ->fresh() command will populate the data by running a new query. This is especially helpful for comparing data after we run the update or delete operations.

You can use dd() combined with getContent() to inspect the response values when you are writing your tests: dd(\$response->getContent());

1『

目前在浏览器里还是获取不了资源：http://localhost:8000/api/tasks/all。报错：

```php
Illuminate\Database\QueryException
Database (database/database.sqlite) does not exist. (SQL: PRAGMA foreign_keys = ON;)
http://localhost:8000/api/tasks/all
```

确实在控制器里没有读取本地数据库的操作。

php artisan route:list

看了下 api 都已经建好了。

已解决，有 2 个原因导致的：

1、数据库放到本地的服务器上。

2、文章里路由器那边的代码有问题。Route::get('/tasks', 'TaskController@all')->name('tasks.all'); 应该改为 Route::get('/tasks', 'TaskController@index')->name('tasks.all');

因为控制器那边的函数是 index()。

』

1『

步骤总结：

1、新建项目：「composer create-project laravel/laravel meetup-api」。

2、配置里改数据库的信息。

3、建模型、资源和数据迁移，可以一个命令搞定「php artisan make:model Task -mrc」。

4、修改迁移设置文件。

5、迁移数据「php artisan migrate」。

6、增加路由器设置「routes/api.php」。

7、控制器设置。

8、模型文件设置。

9、单元测试文件的配置「phpunit」，发现 7.0 版本的框架已经自动生成了；创建测试「php artisan make:test TaskTest」。

10、创建 factory「php artisan make:factory TaskFactory --model=Task」，并修改 factory 文件。

11、创建 seeder「php artisan make:seeder TaskSeeder」，并修改 seeder 文件。还要修改数据库的 seeder 文件「DatabaseSeeder.php」。

12、跑模拟写入数据「php artisan db:seed」。

有几个验证 api 是否成功的办法。最直接的在浏览器里输入 endpoint，比如「http://localhost:8000/api/tasks/」。

比如查看做好的路由器列表「php artisan route:list」。

比如进 tinker 界面用命令行查看：

```php
$ php artisan tinker
Psy Shell v0.9.9 (PHP 7.1.7 — cli) by Justin Hileman
>>> $tasks = \App\Task::all();
```

』