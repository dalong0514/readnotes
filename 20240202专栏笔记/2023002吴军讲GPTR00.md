## 记忆时间

请总结以下文档的主要内容和核心观点，并列举文档中的例子说明，要求 1500 字以上。

doc_id: 9193c90f-067b-48d1-998f-3f95128d7402

GPT-4-AskYourPDF（2023-06-03）：

这份文档是吴军关于 ChatGPT 的讲解，主要内容和核心观点可以概括为以下几点：

ChatGPT 的能力和局限性：ChatGPT 是一个基于语言模型的人工智能，它能够从大量的信息中挑选出用户需要的部分。然而，它的能力是有限的，主要表现在信息形式的转换，例如语音识别和机器翻译。它不能创造新的知识，只能整理和复制现有的信息。因此，那些不创造新知识，只是整理现有信息的职业，如金融分析师、财经媒体记者、律师助理等，可能会被机器替代。

对职业的影响：文档列举了一些可能被 ChatGPT 替代的职业，包括金融分析师、大部分财经媒体记者、律师助理、普通文员、短视频制作人、大部分公众号写手等。这些职业的共同特点是本身不产生信息，只是整理和复制现有的信息。

对投资的影响：ChatGPT 的出现可能会对投资产生影响。例如，当一家企业拥有了一项别人都没有的技术，特别是这项技术被认为有可能改变世界的时候，它的估值就会暴涨。2016 年，当 AlphaGo 打败李世石的时候，Google 的股价就暴涨过一轮。2023 年当微软投资了 OpenAI 一百亿美元，占了它大约 1/3 的股份后，微软的股价在一个月内相比大盘大约上涨了 10%。

对技术趋势的看法：文档指出，任何能成为趋势的技术，都会有几十年的发展机会。例如，电子商务就是一个例子，从互联网兴起后，它就是趋势，但是各种机会持续了 20 多年。而如果一项技术，或者一个新领域，只存在几年的时间，错过前两年就没有机会了，那这样的所谓机会不要也罢。

对新技术的理性看待：文档强调，我们应该理性看待 ChatGPT 带来的变化，不要被炒作所迷惑。ChatGPT 不是第一个被热炒的话题，也不会是最后一个。我们应该从投资、从业者、消费者等多个角度去理解它。例如，从投资者的角度看，我们应该关注的是这个技术是否能带来实际的收益，而不是它是否被炒得火热。从从业者的角度看，我们应该关注的是这个技术是否能提高我们的工作效率，而不是它是否能让我们一夜之间成名。从消费者的角度看，我们应该关注的是这个技术是否能解决我们的实际问题，而不是它是否被媒体大肆宣传。

在文档中，吴军给出了一些具体的例子来说明他的观点：

AlphaGo 和 Google 的股价：2016 年，当 AlphaGo 打败李世石的时候，Google 的股价就暴涨过一轮。这是一个很好的例子，说明了新技术的出现和成功应用，可以对企业的股价产生重大影响。

微软投资 OpenAI：2023 年当微软投资了 OpenAI 一百亿美元，占了它大约 1/3 的股份后，微软的股价在一个月内相比大盘大约上涨了 10%。这个例子说明了投资者对新技术的期待，以及新技术可能带来的投资回报。

电子商务的发展：电子商务就是一个例子，从互联网兴起后，它就是趋势，但是各种机会持续了 20 多年。这个例子说明了任何能成为趋势的技术，都会有几十年的发展机会。

总的来说，这份文档主要讨论了 ChatGPT 的能力和局限性，它对职业、投资和技术趋势的影响，以及我们应该如何理性看待新技术。文档中的例子进一步说明了这些观点。

## 目录

0000 发刊词：ChatGPT 神话背后的原理是什么？

## 卡片

### 0101. 新知卡

新知卡，是有什么理论模型、推断证据、故事或行动，挑战了你的既有认知。新知卡的信息结构是：已知 + 新知 + 例子。「已知」建议挑选大众或自己以前的观念或掌握的知识；「新知」建议挑选自己更新之后的新知。「例子」建议使用自己日常生活中碰到过的事情，这样更容易记忆。新知卡举例如下：

“”“
已知：通过熟人找工作更容易。

新知：社会学家马克·格兰诺维特的弱联系理论告诉我们，在大多数人寻找工作的过程中，弱联系的那些人起到的作用最大。

例子：读本科时，我的第一份实习工作是一个统计论坛上素未谋面的朋友介绍的。
“”“

请通过文档提炼一张新知卡，新知的主题是：「认识到人类的认知可以是科学研究而不光是哲学思辩的对象还不到 150 年」。尽可能多的列举文档中的例子，并将例子具体展开。开始前请先重新阐述任务。

#### 1.1 新知卡 —— 

#### 1.2 新知卡 —— 

### 0201. 术语卡

术语卡记录的是阅读中出现过的陌生术语或概念，有时候是学术词汇，有时候是作者发明的黑话。如果不是作者首创的术语，建议找到最原始的论文或者资料，查询谁首先提出了这个术语，分辨哪些是作者真正意义上的原创贡献，哪些是作者改善前人研究，哪些是作者故意曲解他人术语。一旦养成这种习惯，你的独立思考能力就会增强。术语卡常见的信息结构是：「标题 + 定义 + 解释 + 例子」。试举如下：

“”“
标题：什么是执行意图？

定义：它是一种制订计划的方式。认知心理学家彼得·戈尔维策将「我要减肥十斤」这种制订计划的方式称为「目标意图」，将使用「如果...那么...」（if...then...）的句式来制订计划的方式称为「执行意图」。

解释：执行意图通过在大脑中提前规划执行计划的时间、地点，从而更易引发行动。

例子：你可以将「我要多运动」改写为「如果到了每天傍晚 5 点，那么我就去操场跑步」。前者是目标意图，后者是执行意图。
“”“

请通过文档提炼一张标题为「大语言模型」的术语卡，尽可能多的列举文档中的例子，并将例子具体展开。开始前请先重新阐述任务。

术语卡记录的是阅读中出现过的陌生术语或概念，有时候是学术词汇，有时候是作者发明的黑话。如果不是作者首创的术语，建议找到最原始的论文或者资料，查询谁首先提出了这个术语，分辨哪些是作者真正意义上的原创贡献，哪些是作者改善前人研究，哪些是作者故意曲解他人术语。一旦养成这种习惯，你的独立思考能力就会增强。术语卡常见的信息结构是：「标题 + 定义 + 解释 + 例子」。试举如下：

“”“
标题：什么是执行意图？

定义：它是一种制订计划的方式。认知心理学家彼得·戈尔维策将「我要减肥十斤」这种制订计划的方式称为「目标意图」，将使用「如果...那么...」（if...then...）的句式来制订计划的方式称为「执行意图」。

解释：执行意图通过在大脑中提前规划执行计划的时间、地点，从而更易引发行动。

例子：你可以将「我要多运动」改写为「如果到了每天傍晚 5 点，那么我就去操场跑步」。前者是目标意图，后者是执行意图。
“”“

请写一张标题为「格式塔心理学」的术语卡，尽可能多的列举文档中的例子，并将例子具体展开。开始前请先重新阐述任务。

#### 2.1 术语卡 —— 大语言模型

标题：大语言模型

定义：大语言模型是一种人工智能模型，它通过学习大量的文本数据，理解和生成人类语言。这种模型的训练通常需要大量的计算资源和数据。

解释：大语言模型通过学习大量的文本数据，理解语言的语法、语义和上下文关系，从而能够生成连贯、有意义的文本。它们可以用于各种应用，如机器翻译、问答系统、文本生成等。

例子：

GPT-3：这是由 OpenAI 开发的大语言模型，它有 1750 亿个参数，能够理解和生成人类语言。GPT-3 可以用于各种应用，如写作助手、聊天机器人、编程助手等。

BERT：这是由 Google 开发的大语言模型，它通过双向的训练，理解文本的上下文关系。BERT 在各种自然语言处理任务中表现出色，如情感分析、命名实体识别等。

T5：这是由 Google 开发的大语言模型，它将所有自然语言处理任务视为文本生成问题。T5 在各种自然语言处理任务中表现出色，如文本分类、文本生成等。

参考：专栏「2023002吴军讲GPT」

唯一编码：202306031710

## 0000. 发刊词：ChatGPT 神话背后的原理是什么？

关于 ChatGPT，你应该已经听到了很多讨论、看到了很多信息。可惜的是，绝大部分人在谈论 ChatGPT 时，甚至没有搞清楚最基本的概念。

ChatGPT 是什么？我先下个判断：它是一个被神化的语言模型的衍生品。

1972 年，当贾里尼克博士在 IBM 沃森实验室建立人类第一个语言模型，并且创造出这个专有名词时，一定不会想到这个概念今天会这么火。

二十年后，也就是 1991 年，我在清华大学开始从事语音识别工作，我和郭进博士最早构建了中文的语言模型，然后将这个名词翻译成了中文，「语言模型」这四个字便从此出现在中国的会议刊物上。同样，我们也不会想到，这个原本那么生僻的名词，今天成了中文媒体上的热门词。

1993 年，贾里尼克成为了约翰霍普金斯大学语言与语音处理中心（CLSP）的主任，而我在 1996 年成为了他的学生，并且在他和库旦普的指导下，利用（美国）自然科学基金会（NSF）构建第二代语言模型。第二代模型比第一代复杂很多，但是效果好不少。

2010 年，贾里尼克走完了他传奇的一生。这一年，Google 的杰夫·迪恩等人实现了第一个真正实用的深度学习系统 —— Google 大脑，然后用它构建出第三代语言模型。这个模型比第二代更上了一层楼。

两年后，也就是 2012 年，我回到 Google，启动了计算机问答的项目。到了 2014 年，我们已经能够利用语言模型回答一些很复杂的问题了，在后面的课程还会详细展开。

我梳理这段历史是想告诉你，今天，ChatGPT 使用的语言模型采用了深度学习算法进行训练，但是这几十年发展下来，语言模型还是语言模型，不是什么其他的东西。

这就如同今天特斯拉的电动汽车还是汽车，虽然和本茨早期的汽车、福特的内燃机车、丰田的混合动力汽车不同，但它不是飞机。

2020 年疫情期间，我在几次内部讲座中介绍未来科技时，特别强调要关注一个叫做 GPT-3 的语言模型。当时开发它的 OpenAI 还是一家小公司，因此听众并不对它感兴趣，所有人关心的都是大公司的动态，没有一个人事后向我提出任何有关语言模型的问题。

又过了两年，ChatGPT 一夜爆红，大家才都开始谈论语言模型、GPT-3，以及后来的 GPT-4。

媒体从来是扮演加速师的角色，在赞扬的同时贩卖恐惧。绝大部分赞誉是炒作，这并不可怕，因为 20 世纪技术的发展常常伴随着泡沫。但几乎所有的恐惧都是自己在吓唬自己，这才是可怕之处。我不知道如果贾里尼克能活过来，会如何评论这种现象。如果他想给大家降降温，大家是否会说他老糊涂了，跟不上发展了。

但是稍微了解一点宗教历史的人都会知道，只有神具有让人们既赞誉，又恐惧的特点。ChatGPT 已经成为了一尊新的神。

看到很多人今天对 ChatGPT 的反应，让我想起了当年哥伦布的一则故事。

1503 年，哥伦布的船队到达了牙买加，岛上的原住民对哥伦布他们表示欢迎，并为他们提供食品。但由于有水手盗窃了当地人的物品，几个月后，当地人不再为他们供应食品了。

哥伦布是一个对天文和地理颇为感兴趣的人，他航海时恰巧带了一本天文年鉴，里面记载了 1475 到 1506 年月食出现的时间。于是哥伦布就想到，可以借此扭转不利局势。

在一个将要发生月食的夜晚，哥伦布约见了当地的部落首领，告诉他，神对当地人不招待哥伦布和船员的事情愤怒了，神会使月亮「愤怒地发红」然后消失，以表达惩罚。

果然，当晚月亮变红了，然后逐渐消失。哥伦布的儿子斐迪南在航海日记这样写道：

（土著人）从四面八方赶到船上，悲伤地哀嚎著，带着食品和各类供给，恳求他们向神尽可能地说情，不要将怒火发泄在他们头上……

这时，哥伦布走进船舱去假装「祈祷」，其实是在用沙漏计时，当长达 48 分钟的月全食快结束时，他告诉惊魂未定的原住民，神即将原谅他们。不久后，月球开始走出地球阴影，露出了亮光。哥伦布对说，神已经宽恕了他们。此后原住民就把哥伦布当成了神的代理人，老老实实地给他的船队提供食物。

世界上各种自然神，都是人创造出来的，但是人从此匍匐于它们的脚下。而人这么做，是因为不知道各种自然现象背后的原因，简单地讲，就是不懂得科学原理。

今天，当 ChatGPT 这尊新的神被树立起来的时候，我不知道有多少人想成为哥伦布，又有多少人的行为和当年的土著人没有差别。如果想成为前者，就需要知道 ChatGPT 背后的科学原理，就如同需要掌握基本的天文地理知识一样。

很多人一听说了解科学原理就失去了耐心，希望我马上给出一个答案或者行动指南。但根据我对我这十几年来上百万读者的观察，凡是能够耐心听我讲讲道理的人都进步了，凡是懒得动脑筋，想很快得到一些结论或者行动指南的人，还在原地踏步。我相信得到的同学，一定都属于前者。

听完这门课，你会了解什么呢？

首先，这是一门让你成为哥伦布，而不是土著人的课程，它不长，也不难懂，里面没有任何公式，而是一个讲原理的课程。听完之后，你会知道 ChatGPT 以及类似的产品是怎么一回事，它们能做什么、不能做什么，这就如同哥伦布懂得月食的原理。听完这门课，你会像哥伦布一样，当看到别人在朝拜月亮时至少不会跟着起哄。

我在课程中所讲述的观点，不仅来自于我个人。我和 Google、苹果和微软等公司负责人工智能项目的负责人，和 MIT、斯坦福等大学这个领域的教授，以及年轻一代刚获得博士学位的机器学习从业者都进行了讨论。课程中给出的观点和结论包括他们的看法，只不过我以讲述人的方式给你讲出来。

其次，这不是一门讲商业机会的课程，指望着听完课就能从 ChatGPT 上挣一笔快钱的人，可能会失望了。但是，认真听完课的人，至少不会因为 ChatGPT 亏钱。人一辈子挣钱的机会很多，但关键是在机会到来时得有本钱，本钱输掉了，机会真的到来时就会倍感无力。

最后，我知道也会有人不同意我的看法。我的想法是，我们尽可以保留自己的观点，不必说服我，我也不会去说服谁，毕竟思想的多元化让世界变得美丽。

顺便说一句，既然是得到的课程，版权当然属于得到，因此不欢迎媒体断章取义地转载。

重要的事情交代完了，接下来，就让我们来了解 ChatGPT。

## 0101. 计算机回答问题、写诗难吗？

为什么 ChatGPT 今天会这么火？因为它可以模仿人类对话。而且跟以前那些对话系统不一样的是，它不再是尽讲一些不着边际的话了，ChatGPT 讲出来的话非常切题，而且看上去像是真人讲出来的。

面对 ChatGPT，你应该已经接受到了足够多的信息，有人赞扬它，有人不以为然，有人用它贩卖恐惧。不过，那些内容里九成以上的理解都带有想象色彩，甚至违反了基本的信息论和物理学原理。所以在讲具体内容之前，我们先要对 ChatGPT 有一个理性的认识。

今天 ChatGPT 让人感到神奇的地方主要有两个：

一个是它能回答复杂的问题。另一个是它能写作，你告诉它写什么，它就能给你写一篇能交差的公文。

那么让计算机完成这两项任务难吗？

确实，这两件事在上个世纪五六十年代都被认为是很难的。当时的科学家们认为，如果计算机能够很好地解决语音识别、语言翻译、回答复杂的问题、命题写作，或者对文章进行摘要，就表明它和人一样聪明。人们当时甚至觉得，这件事和治愈癌症、水变油一样难。

而到了 2000 年之后，虽然全面治愈癌症、水变油这两件事还是遥遥无期，但是上面这些和自然语言处理有关的工作，已经被确认比想象得要容易得多。

发刊词里提到，2010 年，Google 的杰夫·迪恩等人实现了第一个真正实用的深度学习系统 —— Google 大脑，用它构建出了第三代语言模型。

两年后，也就是 2012 年，我在 Google 启动了计算机问答的项目。经过两年的工作，到了 2014 年的时候，无论是利用语言模型回答问题，还是写作，都已经大部分得到了解决。我们来看看当时计算机可以做到什么样的水平。

### 1.1 计算机回答问题

先看计算机自动问答。这是我 2014 年离开谷歌前负责开发的自动问答系统，我来试着问它一个问题：

天为什么是蓝色的？

回答是：阳光在透过大气层到达地球时会发生折射，空气中的气体让不同颜色的光散射到各个方向。蓝光因为波长短，比其他颜色折射率高。

你看，这个答案既简明、又准确，远远比绝大部分物理老师能够写出的答案要好。

再问一个和生活相关的：

怎么清洗洗碗机？

回答是：将 1 杯白醋装入洗碗机安全碗中，并将其放在空洗碗机的底部。将洗碗机设置为在热水循环中运行。醋会分解任何残留的食物、油脂、肥皂渣、残留物和任何其他残留的污垢。

我们可以继续问，比如：如何烤蛋糕？从太空上看，天为什么是黑色的？

问题：从太空上看，天为什么是黑色的？

回答：在太空或月球上，因为没有大气可以散射光。太阳光沿直线传播，没有散射，所有颜色都聚集在一起。朝太阳看，我们因此看到明亮的白光；而向远处看，我们只会看到虚空的黑暗。

问题：如何烤蛋糕？

回答：第 1 步：准备烤盘。…

第 2 步：让配料达到室温。…

第 3 步：预热烤箱。…

第 4 步：将干成分搅拌在一起。…

第 5 步：混合黄油和糖。…

第 6 步：加入鸡蛋。…

第 7 步：交替添加干湿成分。…

第 8 步：将面糊倒入烤盘中烘烤。…

这几个问题的结果，都来自刚才提到的那个自动问答系统。当然，原版的问题和答案都是英文的，我用 Google 翻译给翻译成了中文，中间改动的字数不超过 10 个。因此，你可以基本上认为，上面所有的结果都是计算机产生的。

不知道你对这几个回答是否满意，我猜想，就算知识面比较广的大学毕业生，也未必能够比得过这个问答系统。事实上，这个系统虽然是我做的，但它对于很多问题的回答，显然比我个人给出的更好。

类似地，在 2021 年，Google 的工程师开发了一个解决奥数问题的人工智能系统，它的得分比那些有计算机博士学位的开发者的得分还要高不少。

到了今天，ChatGPT 产生的结果类似，质量也差不多，不过它能够回答的问题要多一些。而且，它能回答更多的「难的问题」。

什么叫难的问题？对计算机来说，所谓的容易问题，就是跟事实有关的问题，比如「是谁」「什么时候」「在哪里」「发生了什么事」等等。容易问题的回答，早在 2008 年前后就实现了。而所谓「难的问题「，是指」为什么」和「怎么做」这两大类问题。2014 年，机器能够完美回答 40% 的「难的」问题，而现在，ChatGPT 能回答的难的问题要多得多。

关于计算机是如何回答问题的，我们后面还会讲细节。通过这个例子，我想提醒你的是，让计算机回答问题并不是一件高不可攀的事情，在今天，让它做得比人好是完全能够办得到的。

### 1.2 计算机写作

接下来，我们来看看计算机写作。

2022 年年底，我参加了美国一个媒体从业者的活动，几家大报纸的记者编辑都在。我们谈到了计算机写作的能力，他们再次确认，今天相当多的纸媒文章，特别是财经类的文章，主要是计算机写的。当然，写什么是人定的，写完之后的编辑修改也是由人完成的。在这个过程中，人和机器要来回来去进行好几次的迭代，也就是修改稿件。

你看，今天计算机的写作水平不仅超出大多数人的想象，甚至比很多人的写作水平都要高。

但是计算机写作本身并不神秘。我在 Google 时做过一个「玩具」，编了个程序来写唐诗。我给你念念这个程序写的两首诗，你不妨看看计算机的水平怎么样。

第一首：

空愁走百川，微露贵乡还。

故园人不见，远望忆长安。

这首诗是怎么写出来的呢？非常简单，我只是用李白的诗做训练数据，训练了一个语言模型，然后花两天时间写了个程序。我给了它一个关键词「忆长安」，然后计算机就写出来了。

再看第二首：

东城淡日初破晓，薄云远帆送客棹。

临亭桥边渔家忙，镜山湖东春色早。

第二首诗稍微复杂一点，我用了更多的唐诗数据，训练了一个更大的模型，在写作程序中还加了一些限制和规则，然后让计算机产生了这首诗。

不过，这两首诗都有很多改进的空间。首先由于今天很多汉字的读音和古代是有所不同的，我没有使用平仄信息，这两首诗可能不符合平仄的要求。另外，当时我只是好奇，想看看计算机能写出什么样的诗，所以编这个程序花的时间不多，像词性等信息都没有加进去。如果真想让计算机写唐诗，稍微做得细致一点，写出来的会比这两首漂亮得多。

无独有偶，2019 年，我和我的师兄郭毅可教授进行交流时发现，他也写了一个写唐诗的程序，原理和我的程序大致相同，我在后面还会详细介绍。

### 1.3 ChatGPT 的能力

当然，ChatGPT 能做的事情，远比我和郭教授花几天时间写的程序能做的事情多得多。但是我想说明的是，它并不是像很多局外人想得那样，一夜之间实现了很多过去没有的功能。可以说，它就是科技发展顺理成章的结果。

为什么 2023 之前，我们没有看到 ChatGPT 这样的问答产品呢？

今年 3 月底，我约 Google 负责搜索和人工智能的高级副总裁谈了 ChatGPT 的事情。他告诉我 ——

ChatGPT 能做的事情，Google 的产品和服务其实三年前就能做了，只是没有包装成 ChatGPT 这样的产品而已。因为公司认为，有更重要的事需要人工智能来做。

此外，提供不受限制的问答服务，对于 Google 这样的公司来讲，还有很多法律问题。比如，Google 如果就法律问题给出建议，它会惹很多麻烦，搞不好会赔上百万美元。但是至少在今天，大家还不会为难 OpenAI，因为和它这样的初创公司打官司挣不到多少钱的。

那今天 ChatGPT 被炒得那么火，它的能力到底怎么样？实事求是说，有做得好的，也有不尽如人意的。

现在人们使用 ChatGPT 的目的大致可以归结为三个：

第一个是信息查询，过去 Google 是给出有答案的网页链接，但今天 ChatGPT 是直接给出答案。

第二个是让它帮助写作业，这是很多大中学生使用它的原因。

第三个则是写一些应付差事的汇报，这主要是职场上的人士使用。

这三件事的核心是两个，一个是理解自然语言，明白人的意图；另一个是产生自然语言的文本，满足人的要求。

在理解自然语言这方面，ChatGPT 做得比较好，但是在产生内容方面的表现就不稳定了。有时让人眼睛一亮，有时也让人大跌眼镜，特别是当你对它刨根问底时，得到的答案、内容经常让人哭笑不得。

至于为什么它的表现不稳定，这和它之前使用的训练数据有关，也和人对它提出的不同要求有关。

虽然 GPT 自称是可以自己产生训练数据，但实际上，它要依赖人为产生的初始数据，才能产生新的数据。没有好的初始数据，它产生的数据也是乱七八糟的，当然，训练出的模型质量也就无法保证了。

比如我们前面提到，计算机很早就能帮助人类写财经文章，这是因为这方面的数据多，而且这类文章格式固定。今天一些人觉得，用 ChatGPT 写每周汇报很方便，也是因为这种汇报的格式很固定。关于数据和格式的重要性，我们后面还会讲到。

另外，两位刚毕业的从事机器学习的博士和我讲了如何更好地利用 ChatGPT 写作，这个我们后面还会讲到。大家可能也注意到了，ChatGPT 写出来的文章好坏可以相差很远，这里面是有技巧的。

由于 ChatGPT 是一个新鲜玩意，人们常常会在它表现好的时候给它点赞，而那些不好的表现，就被不知不觉地给忽略了。

关于 ChatGPT 所产生的内容质量，微软、Google 等公司内部都做了详细的评测，当然它们都没有公布结果。不过，根据《大西洋月刊》的报道，ChatGPT 今天更像是一个玩具，而不是一个能够替代人的工具。

下节预告：由于 ChatGPT 是基于语言模型的自然语言处理系统，它的优势和局限性都来自语言模型本身。因此，我们要了解 ChatGPT，就需要多少知道一些语言模型的原理。

下一讲，我们就来说一说，语言模型这个出现了很多次的概念，到底是什么。

## 0201. ChatGPT 的本质是什么？

很多人喜欢说「某某技术横空出世」这样的话，渲染一种很神秘的氛围。但我要提醒一个事实：新技术往往不神秘。世界上很多爆红的产品和服务，其实使用的技术都是半个世纪之前的。

比如，如果你听过我讲 5G 的课，会知道 5G 通信中最关键的编码技术，低密度奇偶检查码（LDPC）是上个世纪 60 年代提出的，90 年代前甚至被认为是无用的，但是后来 5G 用上了。

ChatGPT 所依赖的语言模型技术也是如此，它到今天也有 50 多年的历史了。换句话说，ChatGPT 的底层技术是 50 年前的。

今天在媒体上，很多没有开发过相应技术的人，甚至很多学了一点技术的研究生都会讲，技术发展了这么多年，不该早就超越了当时的极限了吗？

其实，无论是在工业上，还是在信息产业上，技术确实是可以改变的，但是物理学和信息论的原理和极限是无法改变的。虽然今天人工智能使用的语言模型和 50 年前在技术上有很大的不同，但是它们的原理是一样的。这一讲，我们就来把 ChatGPT 的底层技术，也就是语言模型说明白。

### 2.1 什么是语言模型？

什么是语言模型？顾名思义，就是对人类的语言建立数学模型。这里面最重要的关键词是「数学」。语言模型不是逻辑框架，不是生物学的反馈系统，而是由数学公式构建的模型。

那么解决和语言相关的问题，为什么要用数学模型呢？

因为这是一条捷径。我们想让计算机理解自然语言，让计算机回答问题，或者进行几种语言之间的翻译，第一反应会是，应该让计算机理解人的语言，让它学会语法…… 但是人们经过很多次尝试，都失败了，或者说至少到今天还没有做到。而换个思路，如果想办法把这些问题变成数学问题，然后通过计算，就能间接解决这些自然语言处理的问题。于是，语言模型的想法就被提出来了。

最初提出语言模型概念的，是我们前面提到的贾里尼克博士。

1972 年，他正式离开康奈尔大学，在 IBM 负责和语言处理有关的各个项目。当时，IBM 垄断着全世界的计算机市场，在当时的 IT 领域可以讲是独孤求败。于是贾里尼克就要挑战一下自己，做一些传统计算机做不到的事情，特别是和人工智能有关的事情。于是贾里尼克为研究部门挑选了语音识别这个课题，也就是让计算机把人说的话识别出来。

为什么是这个课题呢？语音识别从 1946 年开始就有人研究，这个历史和电子计算机的历史几乎一样长，但研究了二十多年依然不得要领。当时的计算机，只能识别百十来个单词，而且错误率高达 30%，这样的技术当然是没有什么使用价值的。

当时就像前面说的，人们主要是从语音学和语言学入手研究这个问题。贾里尼克和之前那些研究人员所不同的是，他不是语音学家，也不是语言学家，而是一个数学基础非常好的通信专家，是当时美国大学信息论课程教科书的作者。因此，贾里尼克的想法不受之前的限制，他以一种独特的视角来看待自然语言处理问题，把它们都看成是一种通信问题。

自然语言和通信问题，这两件事又有什么关联呢？

根据香农确立的现代通信原理，所谓的通信，也被称为信道的编码和解码，无非是信息源先产生一个原始信息，然后在接收方还原一个和原始信息最接近的信息。

比如，你传输一句话，「中国是一个古老的国家」。在传输前要对它进行编码，比如编成 010101111000…... 但是，传输中一定会有噪音和信号损失，接收方接收到的编码可能是 1010111000…... 这样就翻译不回原来的句子了。

那怎么办呢？我们可以把和接收到的编码相似的句子都列举出来。比如：

国中是一个古老的国家

中国是一个古老的国家

国是一个古老的国家

中国一个古老的国

等等。

然后，通信系统会计算哪一种可能性的概率最大，然后把它选出来。只要传输中的噪音不是太大，而且传输的信息有冗余，我们就都能复原原来的信息。贾里尼克做研究的时候，已经有很多通信的算法来做这件事，我在《数学之美》这本书中已经有所介绍。

从这个角度来看待语音识别也是一样的。当人和人交谈的时候，我说「中国是一个古老的国家」这句话，在空气中或者电话线上传播的是声音的波形，而在接听者那里听到的，其实是带有噪音的声音，他需要接收声音的波形，来还原讲话人说的话。只要噪音不是太大，人是能够做到这件事的。

当然，要让计算机来做这件事，不是让计算机学着人的做法去理解语言，而是最好能够让计算机计算出来哪一种可能的语句概率最大。这种计算自然语言每个句子概率的数学模型，就是语言模型。

比如在刚才讲的例子中：

「国中是一个古老的国家」的概率是 0.05，「中国是一个古老的国家」的概率是 0.2，其它几个候选句子的概率都是 0.01，于是我们就认为，概率最高的「中国是一个古老的国家」，就是讲话人讲的句子。

### 2.2 如何消除不确定性？

当然，爱动脑筋的人会马上指出，如果我想讲的就是那些小概率的事情呢？你用最大的概率来预测不是就错了吗？

这个观察很敏锐，这确实是语言模型的问题。而解决这个问题的办法就是利用更多的上下文信息，消除所有的不确定性。

比如，第一代语言模型用的上下文信息就很少，但是到了 GPT-3，就用到了前后 2000 个词的信息，包括标点符号等，都算成是词。

由于自然语言中有信息冗余，在这么多上下文里，几乎就不存在不确定性了。这也是为什么今天 ChatGPT 产生的语句，已经很像人说的话了。但从本质上讲，它的原理依然是在很多种候选中，选择一个概率或者是可能性最大的句子。这一点是没有改变的。

### 2.3 模型的概率怎么计算？

接下来的问题是，这个概率该怎么计算，或者说好坏该如何评估呢？

早期的语言模型只是看上下文。还拿「中国是一个古老的国家」举例子，在这句话里，「中国」这个词就比「国中」放到开头要来的通顺。什么叫通顺？就是大家都这么说，用的时候多，并不一定要符合语法。事实上，在生活中很多常见的说法其实都是病句，但是语言模型不考虑这种情况，它只是认为人们说得多的就是好的句子。

那怎么准确计算这个概率呢？就要做一些统计了，统计一下在相同的上下文的条件下，每个词具体出现了多少次。因此，语言模型也被称为是统计语言模型，因为它模型的参数，也就是用来计算各种概率的参数，都是靠统计得出的。

讲到统计之前，我们先要做一个说明，就是今天对于语言模型参数的统计并不是简单的数数，而是要用很复杂的机器学习方法反复计算。我们后面会讲语言模型的三个发展阶段，就是根据如何得到模型参数来区分的。但是为了简单起见，我们可以先把它理解为数数。

为了统计出语言模型的参数，就需要事先准备好大量的文本供统计使用。

比如有两个句子，「天为什么是蓝色的」和「天为什么是绿色的」，哪一个概率更大？

我们很容易想到，「蓝色」的概率更大，因为我们经常会在文本中看到这句话，比如在 1 亿篇文章和书籍的章节中看到了 100 次，它的概率就是百万分之一。而后一句话没有看到，概率就认为是零。

但是，如果某句话，比如「天为什么是黄色的」在统计的文本中出现了一次，是否我们可以认为它比那些没有出现过的句子概率大呢？这就不好说了，因为这些小概率事件，出现不出现，都有很大的随意性。出现两次的随机事件，也未必能说明它比出现一次的随机事件发生的概率更大。

为了避免这种所谓小概率事件所带来的噪音，我们能做的就是增加数据量。

2000 年前后，我在训练语音识别所使用的语言模型时，只用到了几千万个英语的句子。但是到了 2012 年，我负责开发计算机问答时，训练的数据就扩大到当时互联网上能找到的全部、上百亿个句子，也就是说，十多年增加了上千倍。

今天，ChatGPT 的语言模型所用的训练数据量也是很大的，第一个版本使用的 GPT-3 用了大约 5000 亿个词，换算成句子大约是 500 亿个。GPT-4 因为模型规模增加了将近一个数量级，训练数据可能增幅更大了。

当然，提高语言模型的准确性，光增加数据量不够，还需要保证数据的质量，这一点我们后面会讲到。

### 2.4 如何利用语言模型写唐诗？

好，语言模型介绍得差不多了。现在，我们用上刚刚讲的知识，解释一下如何利用语言模型写唐诗。

唐诗大多属于韵律诗，它不仅最后一个字押韵，而且每一句诗都是按照一组两个字或者三个字的单元构成的。

比如王之涣的《登鹳雀楼》：「白日依山尽，黄河入海流，欲穷千里目，更上一层楼。」每一句都可以拆成「2-3」组合。杜甫的《登高》：「无边落木萧萧下，不尽长江滚滚来。」每一句都可以拆成「2-2-3」组合。

于是，我们就把所有的绝句和律诗拆成「2-3」或者「2-2-3」组合，然后以两个字或者三个字作为基本的统计单元进行统计，得到它们在上下文中的概率，然后就可以写出一句句概率比较大的诗句了。

当然，我们还要再根据每一个词的语义，把唐诗中出现的词归类，让一首诗中的每一句符合同一个主题。这里面还有一些具体的细节，我就省略了。总之，只要搞懂了语言模型的道理，有现成的诗供学习，让计算机写诗并不是一件很难的事情。

类似地，让计算机写每周汇报，方法也大致相似，因为这类文章几乎都遵循固定的模版。美国很多作家在测试了 ChatGPT 后，对它进行了逆向工程，认为它是按照五段论写作的，也就是包括开头和结论，以及中间的三个要点，当然这个要点可以增加或者减少，要点之间再做一些承接和转折。五段论是美国初中教学生们写作的基本方法，美国大部分篇幅不长的公文，比如电子邮件，都是这么写的。

下节预告：

你可能会好奇：一开始，语言模型只能完成语音识别、手写体输入的纠错、拼写纠错这些任务。而现在，ChatGPT 已经可以回答我们提出的问题，写出很像样的文章了。两相比较，真是强太多了。这当中发生了什么呢？

下一讲，我们就来讲讲语言模型的发展过程，以及它对人工智能带来的影响。

## 0301. 语言模型是如何进化的？

上一讲我们说了，ChatGPT 的本质是语言模型，就是对人类的语言建立数学模型，也解释了语言模型的工作原理。今天各种语言模型的底层技术，其实出现在 50 年以前。

不过，不用我说你也知道，像 GPT-3 和 GPT-4 这样的语言模型，已经比早期的语言模型强大太多了。如同汽车、移动电话和计算机在不断进步一样，语言模型诞生之后也在持续进步。

这一讲，我们就讲讲语言模型进化的历史，说说自然语言处理的要点。

我知道，大部分同学不会做这方面的工作，但是了解一下自然语言处理的发展趋势和目前方法的局限性，可以帮我们理性看待 ChatGPT，消除对于人工智能的迷信。

### 3.1 阶段一

语言模型发展的第一个阶段是上个世纪 90 年代之前，当时，贾里尼克等人用它解决了语音识别的问题。

在此之前，计算机只能识别百十来个英语单词，识别的错误率还非常高。在采用通信中的算法，并且引入语言模型之后，IBM 的语音识别系统可以识别 22000 个英文单词，而且错误率从过去的 30% 左右降低到了 10% 以内。

这意味着什么？当识别的错误率控制在 10% 以内，也就是每识别十个词出错不到一个词的时候，就有应用价值了。因为这时候，人可以根据自己的知识纠正那一个错字。

这件事，对当时的一个年轻人触动很大，他就是还在卡内基梅隆大学读博士的李开复。

在征得导师同意后，李开复决定抛开卡内基梅隆大学所走的传统的人工智能道路，利用 IBM 的方法实现一个实用的语音识别系统。后来他做到了，并且成为当时美国的科技新星，还到 CBS 电视台向观众展示了他打造的语音识别系统。随后，很多家公司高薪挖他，这让他彻底离开学术界，进入工业界。

但是公平地讲，当时的语音识别系统离实用还差得远，和今天的更是完全无法相比。不过，这件事起码证明了，语言模型确实有用。

上世纪 90 年代还发生了一件事，就是贾里尼克的一个下属 —— 彼得∙布朗 —— 利用语言模型实现了英语和法语之间的自动翻译。

相比语音识别，机器翻译的难点在于要准备两种语言的语言模型，而且最好利用两种语言一一对应的句子来做训练。如果你用狄更斯的英语小说和雨果的法语小说来训练，它们对应不上，效果自然就不会好。由于当时计算机上一一对应的多语种文本只有《圣经》和少量的联合国文件，因此布朗的翻译系统效果并不好。

不久之后，布朗干脆离开学术界，成为著名对冲基金文艺复兴技术公司的技术负责人，他建立数学模型解决问题的本事让基金和他本人都挣了很多钱。

在当时，布朗的成果并没有受到学术界的重视。直到十多年后，德国科学家弗朗茨∙奥科在 Google 用类似的方法比较好地解决了机器翻译问题，布朗最初的那篇论文才成为那个领域被引用数量最多的论文之一。当然，奥科也用了比当时其它研究机构多上万倍、比布朗多上百万倍的数据。不过，这都是后话了。

### 3.2 阶段二

说回语言模型。

进入到 90 年代后，人们发现，单纯增加训练数据的做法并不是十分有效。贾里尼克自己做了一个统计，发现不论数据如何增加，都会有大量的语言学现象覆盖不了。于是，人们开始考虑用语言中更深的知识和信息建立语言模型。语言模型的发展由此进入到了第二阶段。

我就是在那个背景下进行语言模型研究的。当时，我们用了语言中的语法和语义信息，可以在不增加数据量的情况下大大提高语言模型的能力。

比如，读音相似的词，在不同的主题中，出现的可能性相差巨大。举个例子，和「事实」（不含音调）同音的词就有四十多个，在过去，到底是哪一个就很不好确定。但是加入语法和语义信息后，我们就知道 ——「时事」「事实」「失实」这些词会在时政新闻中经常出现，而「失事」「史实」「诗史」等词会在历史题材的文章中经常出现。很多时候，确定了主题，就知道该使用什么词语了。

当语言模型加入了语法和语义信息后，语言模型的训练，以及使用它计算概率时，就不再是简单的统计了。

在此之前的语言模型，稍微有一些概率论和统计学背景，学过一门自然语言处理课程的人基本都能构建。但是此后的语言模型，公式非常复杂，绝大部分人是搞不懂的。

当然，把更多信息，比如语法信息、语义信息，加入语言模型后，人们面临的更大的挑战是，训练模型的计算量变得特别大。

关于训练模型的计算量，我们后面还会讲到，这里举一个小例子：

在我之前，IBM 有一位科学家，他是世界上第一个成功利用语义信息训练语言模型的人。他主要的工作，不是改进算法或者数学模型，而是写了一个程序，能够把 IBM 沃森实验室所有空闲的服务器都用上，然后计算了一年，完成了模型的训练。

要知道，沃森实验室是一个有上千名科学家的大型研究机构。这么大的研究机构，这么多的服务器，在空闲时间计算一年，你想想这是多大的计算量。

由于摩尔定律的作用，也就是计算机处理器的性能每 18 个月翻一番，我们有能力进行越来越复杂的计算，能处理越来越多的数据，也能利用和存储更多的信息。但是，这个计算量的增长是超过计算能力的提升的。

比如，2012 年，我们在 Google 做机器问答时，就需要对几百亿个句子做深入的语法分析，提取各种有用的信息，而不是简简单单地进行统计。这个计算量，即便是 Google 也感到很吃力。

今天，ChatGPT 的语言模型利用的语言信息更加丰富，计算量也更大，即使利用大量的计算能力更强的 GPU，计算的成本也很高。

除了挖掘语言中更多的信息，从上世纪 90 年代起，科学家还提出了自适应的语言模型。也就是说，一个人使用语言模型的过程中，他个人输入的信息和间接提供的反馈信息，能够被用来改进语言模型。

比如，我们用语言模型构建一个拼音输入法，使用者在不断输入中文时，他输入的中文文本就被用来做训练数据了。这样一来，这个输入法软件就会越用越聪明。

当然，这种做法不仅需要个人数据，还需要大量的存储空间。因为这时候，不同的人使用的语言模型就已经不同了。

今天的 ChatGPT 其实就有这个功能，它会根据用户的输入不断迭代优化。你如果用它写几篇文章，然后不断对那些文章进行修改，它就会学到你的行文习惯，然后根据你的行文习惯输出内容。但是，如果人们刻意误导它，输入垃圾信息，它输出的文本的质量也会跟着下降。

刚才说了，挖掘语言中更深层的信息，能有效提高语言模型的能力，但与此同时，也让计算量变得巨大。因此，为了让计算机更有效率地工作，在 2000 年之后，很多语言模型的研究工作就转到提高算法效率上了。

比如，我在博士工作的后期，花了一年多的时间改进训练方法，将计算量减少了两到三个数量级。也就是说，只需要之前千分之一到百分之一的计算资源，就能做同样的事情。

### 3.3 阶段三

到了 2010 年前后，Google 开发了深度学习的工具 Google 大脑：一方面，能够更有效地利用计算资源，这使得语言模型能够越做越大；另一方面，也让模型计算出的概率越来越准确。语言模型的发展由此进入第三阶段。

在随后的几年里，Google 做了一件事，就是把之前的语言模型重新训练一遍。在训练数据没有增加的情况下，模型的准确性提高了很多，相应产品的质量也提高了。

由于数据量的增加和语言模型准确率的提高，到了本世纪的第二个 10 年，计算机就逐渐做到了从写一句话到写一个完整的段落。

在此之前，无论是语音识别，还是机器翻译，你都需要给它一个完整的句子，让它生产一个具有同等信息量的句子。比如，你给它一句中文，它产生相应的英文；或者你给它一个句子的语音，它产生相应的文字。输入输出是 1:1。

而当语言模型足够强大之后，你给它一点输入，它就可以产生大量输出了。

比如，你问它一个问题，可能只有一句话，它生成的答案可能有一大段文字。你给它几个关键词，让它生成一份简历，它可能会给你产生一页纸的内容。

了解信息论原理的同学肯定会知道，输入的信息量少、输出的信息量多，这中间的差异必须有办法给补回来，否则输出的内容里一定会充满不确定性。

补充的信息从哪里来呢？实际上，都是存在于语言模型中的，需要先把信息输入语言模型。这时的语言模型就是所谓产生式的了。GPT 中的 G，就代表 Generative，产生。

比如，GPT-3 的训练数据就包括几十万本图书、几乎全部维基百科的内容。这么多的数据还只是它所用数据的五分之一不到。实际上，单是维基百科的内容，几乎就能回答今天绝大部分人提出的所有有意义的问题了。因此，ChatGPT 的表现比很多人好并不奇怪，因为人一辈子学习的内容可能连它的千分之一都没有。

当然，光有信息还不够，为了保证一段文字中各个句子之间的连贯性，还需要语言模型能够计算出几个句子出现在一起的概率。在段落这个层级进行计算，要比在句子这个层级计算，计算量大出成千上万倍，而不是几倍几十倍。

同时，为了让语言模型产生的段落不太发散，通常还要用一些模版对生成的文字进行限制。这也是为什么 ChatGPT 写汇报和简历能写得很好，写不受限制的文章就显得非常枯燥的原因。

类似的，今天的写作软件通常只能应付有套路的写作要求，比如写财经报道、写大学申请材料等。所谓的「套路」，其实就是计算机使用的模版。今天，美国的作家群体普遍认为，ChatGPT 只能按照一定的模版写作。

当然，要让 ChatGPT 能够回答好问题，或者和人进行对话，还需要让它理解人的问题。这件事相对写作本身要容易一些。

今天，计算机对自然语言做语法分析，可以做得比人还准确。通过句子的语法结构，可以理解常见的实词，也就是名词、动词和形容词的含义。通常来说，句子的含义是这些词的含义的叠加，于是，它就能理解这句话的含义。

当然，在丰富的人类语言中，也经常有例外，就是一句话中所有的词你都懂，但是这句话听不懂。对于这种情况，计算机也没有办法，这时候，它就会胡乱回答，甚至给出完全相反的答案。

下节预告：

语言模型的算法其实不仅能处理和自然语言相关的问题，从理论上讲，用其它数据建立类似的模型，就能处理大量的、原本需要利用人类智能才能解决的问题。这些内容，我们下一讲再讲。

## 0401. ChatGPT 的能力边界在哪

上一讲，我们详细说了语言模型的进化历史。

ChatGPT 在今天被热炒，主要的原因不是因为它能和人聊天，或者能帮助人做作业。其实做作业这件事它做得并不好，虽然有些中学和大学的问题它能够解决，但是对于绝大部分问题，它给出的答案都是车轱辘话。

那 ChatGPT 被热炒的原因是什么呢？其实，ChatGPT 真正可怕的地方在于，按照当前的速度发展下去，不断扩大应用领域，它可能可以解决很多原本需要人类才能解决的问题。

现在问题来了，都有哪些问题是 ChatGPT 能解决的？哪些是它不能解决的呢？

前面说了，ChatGPT 的基础是语言模型，因此，它的极限也被语言模型的极限所限制。这一讲，我们就看看语言模型都能做什么事情。理解了这个问题，你也就知道了 ChatGPT 的能力边界。

我把语言模型能做的事情分为三类：

### 4.1 第一类：信息形式转换

第一类是将信息从一种形式转换为另一种形式，无论是语音识别还是机器翻译，都属于这一类。

在语音识别中，输入的信息是语音声波，输出的信息是文字，它们是一一对应的，因此是信息在形式上的转化。机器翻译也是如此，是从一种语言的编码，转换成另一种语言的编码。

不过值得指出的是，任何形式的信息转换通常都会损失一些信息。比如，在机器翻译中，语言中所蕴含的文化常常就损失掉了。这倒不是机器的问题，在用人进行的翻译的时候，也经常会出现这种现象。比如，唐诗翻译成英语往往就显得乏味，英文的诗歌翻译成中文，也常常显得平淡无奇。有些贯通中西的翻译家，会试图把文化的元素加回去，但是计算机做不到这一点。

在这一类事情中，一个通常不被人们注意的应用是在医学领域，比如基因测序。

任何物种的 DNA 都是四种碱基 ATCG 的组合，当然，它们不是随意排列的，并非所有的组合都是合理的。比如，不同物种同一功能的碱基片段其实是差不多的，每一个基本的单元就有点像文本中的文字。因此，根据一段碱基，有时候就能识别下一段碱基。

当我们进行基因测序时，要把缠绕在一起的 DNA 序列剪开，一段段地识别。而剪开的时候，就有可能剪坏，因此，通常都是把 DNA 复制很多份，剪开以后做对比，以免每一份都没有剪好，识别错了。

当然，对于没有剪好，或者识别得不是很清楚的片段，就可以通过语言模型识别、纠正错误。只不过这项工作所使用的语言模型是基于碱基对的，不是基于文字的。

此外，还有一件事也属于这个范畴，就是让计算机写简单的程序。

2014 年，著名的机器翻译专家奥科在离开 Google 之前领导过这样一个项目，就是让人把要做的事情描述清楚，然后让计算机写 Python 程序。

奥科的想法很简单，既然能够让计算机将一种人类语言的文本翻译成另一种人类语言的文本，就应该能将自然语言描述的文本翻译成机器语言的脚本，也就是程序。

在 2014 年的时候，奥科的团队已经能把功能描述清楚的简单任务书变成 Python 程序。不过，当时的困难是，人其实也无法把自己的想法非常准确地用自然语言写清楚。

从信息论的角度看，如果有了完美的算法，这一大类问题都可以得到完美的解决。对于这些事情，最终人是做不过机器的。

### 4.2 第二类：根据要求产生文本

语言模型能做的第二类事情是根据要求产生文本。今天 ChatGPT 做的主要工作，像回答问题、回复邮件、书写简单的段落，都属于这一类。

这一类工作，输入的信息量明显少于输出的信息量。从信息论的角度看，这会产生很大的不确定性，需要额外补充信息。而补充的信息的来源，其实就是语言模型中所包含的信息。因此，如果语言模型中包含了某个话题的很多相关信息，它就可以产生高质量的文本；否则，它给出的答案或者所写的内容就不着边际。

这一类应用对于语言模型来讲是最难的。这倒不是因为语言模型做得不够好，而是因为站在信息论的角度看，不可能通过少量信息得出更多的信息。因此，这类工作其实或多或少都需要人工干预。

今天，除了 ChatGPT，还有很多类似的写作软件，它们写出来的内容看上去都不错。但是，在这些软件背后，其实有一个由人组成的编辑团队，他们会从几十篇候选文章中挑出一篇提供给用户。今天，在硅谷地区还有一些评估内容质量的外包公司，他们有专人评估计算机产生的文本质量，然后反馈给计算机继续学习、改进。

我就 ChatGPT 的写作水平，专门询问了两位 ChatGPT 的深度用户。他们本身就是研究机器学习的博士，出于工作的需要，天天都在分析 ChatGPT 的写作水平。

他们告诉我，一种最大化的发挥 ChatGPT 写作能力的做法，就是你和它反复迭代。他们是这样做的：

先给 ChatGPT 提要求，让它写一篇文章。绝大部分人到此为止了，但是他们会对机器写的文章提出新的修改要求，然后它就会重新给你写，然后你再提要求。这样一来二去，几次迭代下来，文章质量就大有提高了。

这两个人一个是美国人，一个是中国人，他们对 ChatGPT 最终写出来的文章评价差异还是很大的：美国人认为，质量一般，可以作为邮件发出，但不精彩，不能作为自己的写作，否则别人会觉得自己水平太低；而中国人因为母语不是英语，觉得它写得不错，省了自己很多时间，虽然同样水平的文章他也能写出，但是可能要花更多的时间选择用词和语法。

当然，有人可能会觉得，ChatGPT 对于一些专业问题给出的答案，甚至比专家还好。这种现象是存在的，正如我们前面所讲，它学过的知识可能是我们的成千上万倍。但那是因为其他专家已经就所提出的问题进行过了研究，有现成的知识可以提供给它。

比如，你如果问计算机「天为什么是蓝色的」，能得到完美的答案，那是因为之前有物理学家进行了研究，并且他们的解释得到了更多物理学家的认可。也就是说，还是有人工干预在先。甚至于很多问题，其实在互联网上就有比较好的问题答案配对。ChatGPT 这一类软件只是把它们整理出来。

相反，硅谷几家大公司的研究发现：ChatGPT 做小学算术应用题，甚至参加一些语文考试，比它参加高中的 AP 课考试，以及研究生入学考试，比如医学院的 MCAT 考试，成绩要差得多。原因就是，那些小学生的题它没见过，而 AP 课和 MCAT 考试都是标准化的，有很多过去的考试题可以找到。

不过，虽然 ChatGPT 不能自己创造答案，但它还是很有价值的，它可以减少人的工作量。这就如同你在参加物理考试时，计算器可以节省时间一样。但是如果你不懂物理学的内容，即便有了趁手的工具，也照样考不出来。

### 4.3 第三类：信息精简

语言模型能做的第三类事情是把更多的信息精简为较少的信息。

比如，为一篇长文撰写摘要，按照要求进行数据分析，分析上市公司的财报，都属于这方面的工作。

这一类工作，输入的信息多，输出的信息少，因此只要算法做得好，就不会出现信息不够用的问题。

将信息由多变少，就会面临一个选择，选择保留哪些信息，删除哪些信息。

比如，为一本书写摘要，不同的人会写出不同的摘要，他们对于书中哪些是重点内容、哪些是次要内容会有不同的看法。类似的，对于某个上市公司的季度财报，不同的分析师会有不同的看法，他们会按照自己的想法挑选数据作为证据。

同样的，把更多的信息进行精简，也会得出不同的结果，这就要看算法是如何设计的，它所依赖的语言模型之前都统计过什么样的信息等等。

对于这一类工作，最终计算机会做得比大部分人更好。这不仅是因为计算机阅读和处理数据快，语言模型强大，更是因为它在做摘要、做分析或者剪辑视频时，能够做到比人客观。

比如，今天很多人分析财报，会有先入为主的看法，然后根据自己的看法选择数据，有意无意忽略那些重要但不合自己想法的数据。还有很多人在做摘要时，喜欢断章取义。这些问题，计算机通常都能够避免。

但是，计算机的算法也有一个问题，就是缺乏个性化。

我们人类，通常不同的人对于同一本书会有不同的看法。同样是阅读《红楼梦》，有的人把它当作宝黛爱情故事来读，有的人把它当作官僚家庭的生活来读，也有人将它当作中国农耕社会的缩影来读。类似地，同样是将一部电影剪辑成短片，不同人挑选的片段也会不同。

但是，机器做这种事情，结果都是千篇一律的。

这就如同生产线出现之前，手工制作的产品，每一件都有自己的特点；而大机器生产之后，所有的产品都是标准化的。

但是总的来讲，在这方面，人是做不过机器的。这就如同绝大部分手工产品的质量都不如大机器生产的好那样。

### 4.4 前景展望

ChatGPT 和同类的产品，目前已经能完成上面说的大部分工作，虽然有时候做得还不是很好。不过，由于它进步的速度非常快，所以在未来，肯定可以完成更多的工作，对现有能做的工作也会做得更好。

2019 年的时候，著名的人工智能专家（也是我的师兄）郭毅可院士做过这样的估计：

2024 年，计算机能够对描述非常清晰的任务进行编程；

2026 年，完成中学生水平的作文；

2028 年，编辑视频；

2049 年，创作最畅销的小说。

郭毅可院士自己还做了一个项目，就是让计算机根据歌曲《东方之珠》生成了一部几分钟的电视片，并且在香港表演了。

这个电视片的内容和画面，实际上是对《东方之珠》歌词的理解和视频化翻译。很多视频画面超出了人的想象，但也有些地方理解得还很肤浅，停留在字面上。比如，它把东方之珠的「珠」字理解为大水滴，因此在唱到「东方之珠」时，画面是一个不断变化的水滴。

了解了 ChatGPT 能做什么事情，擅长做什么事情，我们就知道如何取长补短了。比如，今天有很多从别人文章中摘出几句话，做一个短视频到抖音上挣流量的人，今后这些内容就不用人来做了，机器可以做得更好。

下节预告

下一讲，我们就来具体看看，ChatGPT 和类似的产品是如何回答大家的问题的。

## 0501. ChatGPT 的回答从哪里来？

我们在前面讲到，ChatGPT 回答问题，通常答案都比问题来得长，需要补上额外的信息，这件事是怎么做到的呢？我们这一讲就来谈谈这个问题。

在英语中，问题通常都是用一个疑问词开头，而所有的疑问词都由 W 和 H 开头，它们也被称为 WH 单词，包括「是什么」（What），「为什么」（Why），「是谁」（Who），「在什么时候」（When），「在哪里」（Where），「哪一个」（Which），以及「怎么做」（How）。

### 5.1 回答简单问题

前面提到，对计算机来说，除了「为什么」和「怎么做」的问题属于复杂问题，其他问题都属于简单问题。

比如现在 2023 年，你问 ChatGPT 美国总统是谁，它会很快告诉你是拜登。这是有明确答案的。这类问题，计算机根据疑问词和主题词就能理解了，比如这个问题的主题词是「美国总统」，疑问词是「是谁」，再去统计网上关于这个问题的答案，我们就可以建立一个关于美国总统的语义框架，或者说知识框架。

利用语言模型回答问题，不是一个问题对一个答案这样简单的匹配，而是对于问题给出多个答案，然后根据答案的概率排序，返回一个最可能的答案。比如过去的媒体上还有「美国总统特朗普」「美国总统布什」这样的信息，它们也会被统计进去。但是今天的语言模型都很聪明，它们会给最近的内容赋予较高的权重。最后你得到的答案，就会是当前美国总统的名字。当然，如果正好赶在换届时，数据还没有更新，给出的答案就可能出错。

其实都不一定是 ChatGPT，今天用苹果手机的 Siri 功能，或者 Google 的 Google Voice 功能，几乎可以回答所有简单的问题，基本上就是按照这种方法去做的。

讲到苹果的 Siri，很多人觉得它比 ChatGPT 笨多了，即便是很多关于事实的问题，它也给不出答案，直接说不知道。其实这是对美国的商业和法律缺乏了解。苹果作为大的上市公司，是不能随便给别人建议的，否则法律上的麻烦很多。此外，作为一个用户每天依赖的产品，可靠性非常重要，不能时对时错。因此，像 Siri 这类的产品，做不到就不做了。

### 5.2 回答复杂问题

说完简单问题，接下来我们说说复杂问题的回答。对于这类问题，计算机的做法和人有很大的不同。

人遇到这种问题时，有三种途径能够回答。

第一，你知道答案，直接给出。这种做法，计算机也采用。

比如答案就在某个问答网站内。过去计算机采用网页搜索，把那个网页提供出来。今天 ChatGPT 则是把相应的一段话抽取出来。

为什么今天和过去采用的做法不同呢？原因有两个。一个是过去计算机对文本进行摘要的能力不够，二是过去的搜索其实很少分析句子的语义，不确定用户的问题和问答网站上的问题是否一致。

比如你问「为什么最近苹果的股票不涨」，网页给你一个三个月前的网页，分析「为什么苹果的股票在涨」，因为这里面主要的关键词都能匹配上。你遇到这种情况会哭笑不得，觉得搜索质量很差。今天 ChatGPT 足够强大，大部分时候不会犯这样明显的错误。

我们回答复杂问题的第二种情况是，你不知道答案，但是你懂得找到答案的基本知识，于是你利用你的基本知识推出了答案。

比如当有人问你天为什么是蓝色的，你学了中学物理，知道太阳光是由七色光构成的，不同颜色的光折射率不同。同时你还能活学活用，想到阳光进入大气层时因为折射率不同会发生散射，导致天空显得是蓝色的。

但是，计算机其实没有这个能力。硅谷的几家著名公司（这里我就不点出名字了），对 ChatGPT 进行了全面的测试，发现它回答小学常识课（美国叫自然课）的问题，正确率还不到 60%。为什么呢？因为这部分问题很少在网络上被讨论，或者网络上没有靠谱的答案，而 ChatGPT 没有办法像人那样运用知识去寻找答案，只能从现成的答案里归纳总结。

人类解决复杂问题的第三种情况是，你不知道答案，而现有的知识也无法直接推导出答案，需要你做研究工作。

比如这几年疫情，市面上能找到几十种口罩，有些管用，有些不管用，有些虽然管用但是效果有限，到底哪种才靠谱？虽然你能够在互联网上找到一些这方面的内容，其实一大半都是过时的，甚至是想当然的、错误的。过去从来没有人对所有的口罩进行对比研究。这个看似并不复杂的问题，其实并没有好的答案。

2020 年，斯坦福大学材料学专家崔屹教授的团队进行了全面的研究，彻底回答了这个问题。比如他们发现，真正能够有效过滤新冠病毒的口罩，是靠口罩纤维上的静电力把病毒吸附在口罩上，让它无法进入口鼻，而不是用纤维网把病毒挡住。他们还发现，纤维越细的口罩，防护效果越好，纤维粗的反而防护效果差，这和人们的直观感觉有很大的差异。后来诺贝尔奖得主朱棣文教授给了一个合理的解释，就是静电场和纤维直径的平方成反比，纤维粗的口罩的吸附能力弱。这项研究还有很多发现，比如口罩一旦沾染了潮气，防护力基本上就丧失了。基于这项研究的综述论文发表之后，得到了广泛引用。

你看，对于这样的问题，人类是需要通过实验和探索发现新知，而且更正过去认识错误的。这显然也是 ChatGPT 做不到的。

### 5.3 ChatGPT 如何工作

那么 ChatGPT 的答案从哪里来呢？简而言之，回答问题也好、写作短文也好，都基于它对现有事实的抽取和整合，或者说归纳总结。

2012 年，我们在 Google 尝试计算机自动问答时，将当时互联网上全部高质量的英语句子，大约有 1000 亿个，全部做了语法分析。这是一件非常消耗算力的事情，计算量大约是对这些句子做简单统计的上亿倍。完成这项艰巨的任务之后，可以得到两类知识，一个是每一篇文章内自己所包含的知识，另一个则是全网的知识图谱。

在此之前，Google 收购了一家专门构建知识图谱的小公司，那家小公司已经完成了对上百万个知识点的总结，并且构建出它们之间的相互关系。在对上万亿语句进行分析后，这个知识图谱得到了很大的补充。这里必须要提一句，这中间的工作依然需要人工干预，无法全部靠机器完成。

后来，负责 Google 知识图谱的副总裁到了苹果，是今天苹果 Siri 的负责人。苹果的 Siri 其实背后也有一个知识图谱。ChatGPT 背后也一样，虽然它没有公布如何利用各种知识，但是它下载了整个维基百科，而维基百科有一个现成的知识图谱。

当人们在问答系统中提一个问题后，问答系统会首先在知识图谱中寻找可能答案。对于我们前面讲到的简单问题，只要问题靠谱，答案通常都能直接找到。

但是对于复杂问题，有些可以通过知识图谱中的几个相关联的知识点回答，但大部分就不行了，于是需要回到提供知识的原始网页中去寻找答案。这个过程其实是一个逆向思维的过程，和人的思维方式不太一样。

比如我现在问一个复杂问题，计算机就会去找到几十上百个可能包含答案的文章，然后从这些文章中提取一些语句，构成可能的答案。当然，为了让这些句子凑在一起看上去像是一个有逻辑的、连贯的回答，就需要使用语言模型了。

换句话说，这些候选的文章里有了原材料，而语言模型是一个厨师，将它做成一道菜。如果语言模型质量不高，提供的答案就是几个事实的堆砌，让人看了觉得毫无连贯性可言。ChatGPT 之所以比之前的问答系统做得好，并不是它的原材料更多，而是菜做得更精致。

了解了各种问答系统是如何回答问题的，我们再来看看它们如何实现对话与写作。

对话有点像句子中的填空游戏。比如有这样一句话：从去年（ ）开始，考研成了大学生最关注的话题之一。请问中间该填什么词？

对此，语言模型可以给出概率最高的几个词作为候选。比如上半年，下半年，夏天等等，不太可能提示「箱子」、「北京」、「土豆」这些不相关的词。类似地，如果一段话中拿走了一两个句子，今天语言模型也能填回去，只要语言模型足够大、足够好，填进去的内容读起来就通顺。

如果我们从句子层面扩展到段落层面，你说一句话，它说一句话，就是聊天了。它和一段文字中，抽走了一半的话，再用语言模型复原这段话的原理是一样的。由于聊天时你一句我一句，对话是发散的，并不需要下一句话一定是事先想到的，只要前后顺畅，聊天就能进行下去。如果真遇到它填不出来的的句子，它通常会打哈哈，让你提供更多的输入，直到聊天能够继续下去。

至于长篇文章的写作，ChatGPT 所做的实际上是模仿同类的文章，不过会用用户提供的新的信息取代旧的信息。比如我们前面提到的那两位深度使用者，他们就是在不断提供新的信息，引导 ChatGPT 的写作方向。ChatGPT 写的作文其实没有什么营养，内容只是比较巧妙的重复。不过客观地讲，今天大部分中学生写的作文其实质量都不高，也不过是把范文抄来抄去，甚至写得还远不如 ChatGPT。毕竟，模仿和抄袭，人是做不过机器的。

就在我修改这门小课的时候，阿里巴巴也推出了一个类似 ChatGPT 的产品让我试用。于是我发现了一个特点，写一些简单的邮件，ChatGPT 的行文是美国式的描述，像是从英文翻译过去的，而阿里巴巴的是中国式的口吻。它们的语言质量我不做评论，但这个现象说明一个问题：你输入给它什么训练数据，它就给你写出什么样的文章。

下节预告：

你可能马上会想到一句很有名的话，叫「垃圾输入、垃圾输出」，这正是 ChatGPT、甚至今天机器学习方法固有的问题。

下一讲，我们来讨论这个问题。

## 0601. ChatGPT 有哪些固有缺陷？

上一讲，我们了解了 ChatGPT 工作的原理，这一讲，我们来看看它所存在的问题。

我们会看到，ChatGPT 回答问题、写文章，有的时候很靠谱，有的时候又在胡说八道，这些问题能不能解决呢？

我先说结论：有些问题是可以通过改进解决的，有些则是 ChatGPT 固有的问题，甚至是今天机器学习方法固有的问题，要解决是很难的。

### 6.1 哪些问题能够改进

我们先来看看哪些问题是通过改进能够解决的，这是我们将来能够期望的。

首先，有些错误是因为信息量不够所造成的，这部分错误将来 ChatGPT 是可以避免的。人类解决的问题会越来越多，互联网上的优质信息也会越来越多。这一点很容易理解，我们就不展开谈了。

不过，这部分问题解决并不会很快，因为人类创造新知识的速度没有那么快，网络上的文字虽然多，但是每年增加的新知并不多。

ChatGPT 还有一个问题，就是它创作出来的作文段落读起来显得乏味、不生动，用美国一些高中语文老师的话说，就是味同嚼蜡。这些问题也可以解决，毕竟语言模型的技术还在进步。

实事求是地讲，今天 ChatGPT 产生的段落，和十年前我在 Google 生产的段落相比，通顺程度已经好了很多。打个比方，十年前生成的段落相当于初中生的作文，今天有点像初三或者高一学生的水平了。

这样水平的写作已经有了一定的应用价值。比如中国学生申请美国的大学，英语作文一定是写不过美国高中生的，只要你给它规定清楚写什么，用 ChatGPT 写可能比自己写得还通顺些。考虑到自然语言处理技术进步比较快，ChatGPT 的写作可能很快会超过高中生的平均水平。

另一个有希望解决的问题是，今天的 ChatGPT 还离不开后面大量的人工，而且运行的成本特别高。这个问题，随着算法的改进是可以慢慢解决的。举个例子，战胜李世石的那一版 AlphaGo，消耗的能量是一栋十几层办公大楼的耗电量，但是两年后战胜柯洁的那个版本，耗电量就减少了两三个数量级。

### 6.2 垃圾输入，垃圾输出

听到这里，你是不是感觉很乐观？但是，ChatGPT 还有很多问题，它们属于今天完全倚仗数据的机器学习的固有问题。这些问题不是通过增加模型的规模，利用深度学习提高模型的精度，或者挖掘更深的语言知识就能解决的。

对于这一点，很多人不以为然，觉得技术进步了，语言模型和机器学习本身的问题都能解决。不仅外行人这么想，甚至很多从业者也这么想。这让我想起已故的著名物理学家张首晟教授讲的一句话：很多人做了多年研究，结果把物理学的第一性原理都忘了。

在信息领域，信息论所划定的边界，是不可逾越的，就如同光速和绝对零度不可逾越一样。语言模型进化到今天，虽然进步了很多，但依然是一个利用已有的信息预测其他信息的模型，这个性质没有改变。因此，如果你不给它提供足够多的信息，它就无法做事情。这就如同你不给汽车提供燃料，它就无法走一样。

从原理上讲，今天几乎所有的人工智能产品都是复读机。先要有各种知识和信息，ChatGPT 才能工作。你给它提供高质量的数据，它就会产生一个高质量的语言模型，然后给出高质量的答案，写出高质量的文章。相反，你用垃圾数据训练它，它就输出垃圾。

什么叫垃圾数据呢？其中一部分就是噪音。上个世纪 90 年代，美国语音识别的科学家就发现，如果在安静环境下，用麦克风录制语音进行识别，错误率很容易降到 10% 以下。但是，如果是电话录音，由于电话有噪音，错误率会超过 30%。对于语音来讲，噪音就是一种垃圾。

在 2000 年前后，我在约翰霍普金斯大学和 Google 都做过这样一些实验，把训练语言模型的文本混入一些噪音，比如我们在文本里混入一些错别字，或者把一些字的次序交换一下，语言模型的质量会大大下降。

另外，噪音越多，质量下降的速度越快。比如我们用语言模型进行从拼音到汉字的转化，当噪音在 1%、2% 时，不太会影响转换的准确率；噪音到了 5% 时，错误率就会明显上升两三倍；当噪音达到 10% 的时候，错误率就会上升十多倍。如果噪音更多，语言模型就不起作用，产生的结果就是随机的了。

使用过 ChatGPT 的朋友会有这样一个体会，当你和它谈论一些有争议的话题时，它给出的回答可能非常不靠谱，前后自相矛盾，完全没有逻辑，甚至和话题不沾边。为什么会这样？因为在网上关于那个话题的讨论，本身就非常不靠谱，而 ChatGPT 学习了那些内容后，会将不靠谱的表现放大。这就是机器学习中很有名的那句话：垃圾输入，垃圾输出。

有没有什么解法呢？通常只有两个做法。

第一个做法是在噪音不太高时，增加训练的数据量，这个做法是有效的，但是需要多用好几倍的训练数据。比如语料库里混有了 5% 的噪音，将语料库的规模增加十倍，可能可以弥补这个不足。

第二个做法是在能够找到噪音来源时，过滤掉噪音。比如你在开车时使用苹果的 Siri 服务，其实引擎的声音会进入到麦克风，但那是固定频率的噪音，Siri 会将它过滤掉。再比如，我们发现某个网站的内容一直不靠谱，就将相应的内容删除。

但是，相当一部分噪音是随机产生的，我们今天还是无能为力。这是今天机器学习的一大问题。

除了噪音，今天机器学习还有一个问题是：它所依赖的正反馈，有时会将它引向歧途。

什么是正反馈呢？比如你在短视频网站上看了几个 NBA 篮球的短视频，系统就得到一些反馈，觉得你会喜欢类似的视频，然后就调整了特别针对你的推荐模型，多给你推荐 NBA 的节目。这就是系统自适应的正反馈。这种做法通常让使用者觉得非常贴心，越用越好用。很多人对短视频和无厘头的推文上瘾，就是这个原因。

但是，自适应的正反馈是把双刃剑，如果有人刻意引导 ChatGPT 犯错误，这种正反馈机制会导致它错误百出。事实上，很多使用者已经发现，ChatGPT 在回答很多问题时已经被人「教坏了」。比如，有的用户可能是处于开玩笑的目的，给它提供了很多做违法事情的信息，以及很多仇恨的信息，结果它会教人如何杀人，如何做炸弹，并且时不时说出种族歧视的言论。

和过去不同的是，今天我们处在一个信息过载的时代。今天人们发愁的不是无法获得信息，而是信息太多，自己看不过来。很多信息还彼此矛盾，让人们无所适从，更不用说那些毫无营养、让人上瘾的视频和推文了。

面对这样一个信息过载、信息和噪音难以分辨的世界，我们自己判断一条消息真假的成本都很高，ChatGPT 本身更是无能为力。我在前面讲了，它像是一个厨师，你给它有营养的食材，它有可能做出一道既有营养、味道也还不错的菜。但你给它垃圾数据进行训练，它输出的也只能是垃圾。这是今天机器学习普遍的问题。

### 6.3 人工干预的边界

可能有人会想，是否可以进行人工干预，手工过滤掉那些垃圾输入？或者对于敏感的问题干脆不做回答呢？

事实上，ChatGPT 背后是有人工干预的，那些带有仇恨的言论已经被删除了。但是，这个本身有一万亿参数的模型很难进行人工调整。这个巨大的语言模型就像是一个黑盒子，你无法搞清楚里面那些模型参数的含义。

事实上，今天的语言模型和上个世纪早期的语言模型已经有很大的不同了。早期的语言模型比较简单，通常是直接把上下文的概率存在里面。今天的语言模型，存储的是人工神经网络的参数，从那些参数，你完全看不出它们和概率的大小直接的关系。换句话说，你很难通过人为调高或者调低一些参数来控制 ChatGPT 的输出结果。

人工干预还有一个很大的隐患，就是把人主观的好恶加进了一个原本应该客观的语言模型中，这可能导致更大的不公平。

在此之前，推特的人工干预就造成了很坏的影响。在 2020 年美国总统大选期间，推特根据自己的好恶，封掉了它不喜欢的特朗普的账号。这显然是在滥用权力。随后，马斯克认定推特的做法违反了言论自由的原则，收购了推特并且赶走了全部的管理层。在完全控制了推特之后，马斯克来了一个 180 度的大转弯，一方面恢复了特朗普的账号，另一方面封掉很多媒体的账号。

今天的 ChatGPT 已经是一家平台公司了，如果里面的人随意根据自己的好恶选择训练数据，控制结果。这个危害可能比操控推特更大。

下节预告：

如果你是 ChatGPT 的粉丝，听完这一讲可能觉得我在泼凉水。其实我们不是针对这款软件，而是要指出当前这种基于机器学习的智能系统普遍存在的问题，提醒你注意。

在 ChatGPT 的热度过去之后，很多企业和个人都对它进行了全面的测试。大家比较一致的看法是，对于一些有明确答案的问题，它还是能回答得很好的。但是对于缺乏有用信息的问题，它给出的答案相当随意，基本上只能是重复车轱辘话，没有太多的价值。根本原因就是这一讲解释的：垃圾输入，垃圾输出。

下一讲，我们来看看造一个 ChatGPT 需要多少资源，以及它会受到怎样的限制。

## 0701. ChatGPT 需要什么资源？

ChatGPT 走红之后，国内很多媒体和人一下子慌了神。

各种媒体，特别是自媒体上，铺天盖地都是诸如「为什么中国没有诞生 ChatGPT」「美国将要爆发一场新的技术革命」之类的文章。很多研究机构和企业都宣称，我们马上就要推出自己的 ChatGPT。还有很多人觉得，ChatGPT 给自己带来了新机会。

其实，这样全民大炼 ChatGPT，既不必要，也不可能。

这是为什么呢？不必要性，我们后面会慢慢展开讲。这一讲，先来谈谈不可能性。

我们前面讲到，ChatGPT 的核心是语言模型，而语言模型需要用大量的数据来训练，有了数据后，还需要强大的算力支持，然后还需要有足够高水平的并行计算和机器学习的算法支持。数据、算力、算法三道坎，只有极少的互联网大厂能够越过去。

接下来，我们就从这三个方面分别来看一下。

### 7.1 限制一：数据

首先说第一个限制，数据。

早在 2012 年，我在 Google 训练自动问答的模型时，就用到了互联网上能找到的几乎所有高质量的数据。今天的 ChatGPT 所用的数据更多，特别是自媒体上的数据和社交网络上的数据。

这个数据量是极其庞大的。不是每个企业都能得到所有的这些数据。

### 7.2 限制二：算力

而且，就算一家企业今天能得到这些数据，也没有能力存储和处理这些数据，因为处理它们的计算量太大。这就要说到训练语言模型的第二个限制 —— 算力。

前面讲过，早期的算法非常简单，就是对文本做一些统计，然后利用一些简单的数学公式，计算出一个词在不同上下文条件下的概率，事先保存下来。使用的时候，根据这些概率，计算出各种语言现象的概率，比如一句话的概率，或者某个词是名词的概率，或者给定一个问题，某句话是它的答案的概率。

这是截止到上世纪 90 年代的方法。当时，即使做这样简单的统计，所消耗的算力也是惊人的，只有 IBM 这类的大公司才能办到。

但是很快，贾里尼克等人发现，如果只是对文本进行简单的统计，再多的数据也无法覆盖所有的语言现象，而且对于很多语言现象，特别是之前没有见过的语言现象的概率，语言模型估计得都很不准确。于是，人们开始考虑深度挖掘语言中所蕴含的信息，包括语法信息和语义信息。这就需要更大的计算量了。

具体计算量需要增加多少呢？100 万倍到 1 亿倍是一个合理的估计。

很多人觉得，摩尔定律可以让计算的成本不断下降，过去做不成的事情今天能做成了。其实，随着时间的推移，人们对人工智能的要求也在不断提高，需要的算力也在不断增加。算力，永远是一个瓶颈。

上个世纪 90 年代末，贾里尼克为我们申请到了一笔美国国家科学基金会（NSF）的基金，让我们有经费建立复杂的语言模型。与此同时，全世界机器学习领域在算法上也有所突破，能够训练更准确的语言模型。此外，贾里尼克还从美国国防部获得了一笔巨额的经费，让我们能够装备上百台超级服务器。

在这些设备的帮助下，我们得以进行其他研究单位没有条件做的语言模型的改进工作。

1998 年，我参加 IEEE，也就是电气与电子工程师协会的一个重要的学术会议。在我前面做报告的是一个德国著名的科学家，他给出的结论是，更好的机器学习算法似乎对改进语言模型无效；而我接下来的报告则否定了他的结论。我们的研究表明，更好的机器学习的算法很有效，但是需要训练很长的时间，直到模型真正收敛。我们当时能调动的算力大约是他的 100 倍，因此，他做不了的事情我们能做。

但是，即使我们当时有学术界最强的算力，很多想做的事情也还是做不到。当我试图把各种语法信息加入到语言模型中的时候就发现，以我们的算力来做这件事，十年可能也算不完，我也根本无法毕业。

当时，我们看到一位 IBM 的科学家做了类似的事情，并且发表了一篇文章，于是就请他来作报告，想了解一下他是怎么做的。其实，他真正了不起的地方不是在方法上有什么独到之处，而是他写了一个程序，将整个 IBM 沃森实验里闲置的服务器都利用起来，然后计算了整整一年。这个咱们前面也讲到过。

后来，我的导师们希望我超过这位科学家，于是我花了半年的时间从数学上改进算法，将训练的计算量降低了 200 倍，然后在 40 台超级服务器上运行三个月，终于训练出一个当时世界上最好的语言模型。如果没有这 200 倍的改进，我需要计算 50 年。

举这个例子，不是说我有多厉害，而是让你明白，算力对机器学习的重要性。你看，即便当时大幅改进了算法，减少了计算量，仍然需要 40 台超级服务器上运行三个月。

今天 ChatGPT 所采用的语言模型，从参数的数量上讲，是我当时训练的模型的 10 万倍，比我在 Google 构建的模型也大了 1000 倍。开发它所需要的算力，可不只是我在约翰霍普金斯时的 10 万倍，而可能是上百亿倍。

今天，开发 GPT 语言模型的公司 OpenAI 没有讲它使用了多少计算资源，但是它暗示，硬件成本超过一亿美元。如果我们按照 1000 美元一个 GPU 计算，它使用了 10 万个 GPU。以 32 位运算为准，它能提供超 100PFLOPS 的算力，也就是每秒 10 亿亿次运算以上。这是整个特斯拉云计算集群计算能力的两倍，是 Google 最大的数据中心算力的 1/3，是阿里云最大的数据中心算力的 1/4。

这还只是在它开发 GPT-3 时的花费。到了升级为 GPT-4 时，需要的计算资源就更多了，用到了微软云计算大量的计算资源。

即便如此，ChatGPT 最近还关闭了它的付费用户的注册，因为为付费用户提供更优质的服务太消耗计算资源了。

因此，如果你也想做一个 ChatGPT，先要考虑一下自己的算力够不够。很多人一看到出了个新技术，就觉得机会来了，但是绝大部分时候，那些机会和大家无关，因为大家没有基本的能力实现它。

### 7.3 限制三：算法

那么，如果我们举全国之力建几个超级计算中心，是不是很容易超过 ChatGPT 呢？

如果只看算力，似乎可以。但其实，算力也分「聪明的算力」和「笨的算力」，对于人工智能问题，只有前一种 —— 聪明的算力 —— 才有意义。

所谓「聪明的算力」，就是说，光有算力还不够，你还得有配套的算法把算力的作用发挥到极致。如果没有配套的优质算法，算力就只能事倍功半。这就是笨的算力。

这就说到了训练语言模型的第三个限制 —— 算法。

2000 年，我在约翰霍普金斯构建语言模型时，一次能调动 40 台服务器同时工作。当时还没有云计算一说，我靠的是加州大学伯克利分校提供的一个并行计算的工具。这让我不需要手工监控每一台服务器的工作，而且能自动分配任务到空闲的服务器上。后来有了云计算，这件事就容易了很多。

但是今天，训练语言模型用到的机器学习算法要复杂很多。

2010 年的时候，Google 推出了一个基于云计算平台深度学习的工具 ——Google 大脑。采用这个工具，语言模型的效果可以大幅提升，在其它条件不变的情况下，语音识别和机器翻译的相对错误率可以降低 10% 以上。

今天，深度学习的基本算法和支持它的基础架构，已经是智能化的数据中心，有些地方也称之为「智算平台」的标配。

如果只是用一个由处理器，包括 GPU，堆砌起来的数据中心训练语言模型，得到的结果会差很多。

也就是说，除了算力之外，基础的自然语言处理技术，也就是算法，也是实现 ChatGPT 这些产品必不可少的条件。

最直观的例子，你想让计算机回答问题，至少要让它懂得问题。你要想让它从上千亿的文本中知道哪些可能是答案，需要做到问题和答案的匹配。这个工作不是一年半载就能完成的。

今天，很多机器学习的算法已经开源了，有些应用已经有公司和研究机构投入科研力量开发过，比如基本的图形识别和语音识别技术，但是深度的自然语言理解其实还不属于这个范畴。在这些领域所具有的技术积累，其实也是一种资源。

因此，一些长期在这个领域有投入的公司，比如 Google、微软，中国的百度、字节跳动等，其实是有一定的技术积累的，它们可能通过对现有的技术的集成，在较短的时间里，做出类似 ChatGPT 的产品。

但是，对于那些完全不具备应有的技术资源的人来说，跟风 ChatGPT，要么是无知者无畏，要么是炒作。

我们前面讲了，像 GPT 这样的语言模型技术，除了处理自然语言，还可以在很多领域找到应用场景，比如在生物医疗领域、在视频编辑方面，这就需要具有相应的算法，这里我们就不一一列举了。因此，没有足够的算法资源，也不建议大家都去打造 ChatGPT。

下节预告：

好，这一讲，我们说了训练语言模型的三个限制，分别是数据、算力和算法。这三道坎，只有极少的互联网大厂才能越过去。

其实，即便跨过了三道坎，做出了比 ChatGPT 更好的产品，它也不可能像很多人想象的那样无所不能。因为 ChatGPT，乃至人工智能，本身就是有能力边界的，并不能解决所有问题。

## 0801. 今天人工智能的边界在哪里？

2016 年，当 Google 的 AlphaGo 战胜李世石之后，很多人就开始担心，是否世界上所有的事情计算机都能做得比人好。今天，当 ChatGPT 再次引爆人工智能这颗炸弹时，很多人就更恐慌了。

在我看来，这种担心或多或少有点杞人忧天，因为无论是什么样的计算机，都只能解决世界上很小一部分问题。事实也证明，从 2016 年到现在，基本上大家该做什么还是在做什么，只是做事的方式有所改变而已。

更具有讽刺意味的是，智能革命原本应该造就更多的和 IT 相关的工作，让很多所谓的低端劳动消失。但现实情况却是，从 2022 年到现在，全世界最有名的 IT 公司都在裁员，而且裁了不止一轮。

相反，在美国，那些所谓的低端岗位严重缺人。在美国 IT 企业大裁员的情况下，失业率和就业率都达到了历史最好水平，很多行业甚至是两个岗位对应一个就业者。在中国，至少 IT 企业的表现一点也不比美国好。

之所以造成这个结果，原因就在于，在过去的几年里，全世界都在热炒人工智能，以为它无所不能，殊不知，人工智能也是有边界的。

ChatGPT 的边界是人工智能的边界，而人工智能的边界是数学的边界，数学是有边界的。在历史上，已经有三个人从理论上划定了数学和计算机的边界。

这一讲，我们就来谈谈边界的问题，看看 ChatGPT，乃至人工智能的边界在哪里。换句话说，能解决什么问题，不能解决什么问题。

### 8.1 哥德尔

第一个指出数学边界的人是哥德尔。

在 19 世纪末，由于科学技术的突飞猛进，人们普遍对科技的能力感到乐观，觉得它无所不能。在数学界，大家普遍觉得，数学可以是一个既严密又完备的知识体系，甚至认为所有的问题都能用数学的方法解决。

当时，大数学家希尔伯特，就致力于构建一个既完备又一致的数学体系。通俗地讲，所谓「完备」，就是数学的公理可以推导出所有合理的结论；所谓「一致」，就是推导出的结论不矛盾。当然，在数学上，完备性和一致性都有严格的定义，我们就不细究了。

但是在 1931 年，年仅 25 岁的哥德尔证明了，数学体系不可能既完备又一致。也就是说，保证了完备性，结论就会矛盾；保证了一致性，就会有很多结论无法用逻辑推理的方法证明。

这对当时的数学界，特别是年迈的希尔伯特本人，是一个异常巨大的打击。但是，这也唤醒了人们，让人们知道，数学不是万能的，世界上很多问题不是数学问题。

这个认识很重要，让人类少走了很多弯路。

### 8.2 马季亚谢维奇

第二个进一步限制数学或者说算法边界的人，是俄罗斯的数学天才马季亚谢维奇。

早在 1900 年的时候，希尔伯特就提出过一个疑问，也就是著名的希尔伯特第十问题，简单说就是：对于某一类数学问题（不定方程整数解的问题），有没有一个方法，通过有限步，就能判断它有没有解？

注意，希尔伯特关心的只是判断一下它有没有解，还没有考虑如何解决它。当然，判断不清楚是否有解，就不可能解决它。

这个问题困扰了很多数学家一辈子。最终在上个世纪 60 年代末，做大学毕业论文的马季亚谢维奇解决了这个问题，给出了否定的答案。也就是说，即便是对于一些看似很简单的数学问题，我们都可能不仅不知道如何解决它，而且都无法判定它是否有答案。

当时，马季亚谢维奇只有 22 岁。

马季亚谢维奇的结论对搞计算机的人来讲，既是一个坏消息，也是一个好消息：坏消息是，很多问题就不要想着如何编程去解决了。因为刚才说了，我们压根无法判定它是否有答案，更不要说解决了。换句话说，很多数学问题压根无法用计算机一步步解决；好消息则是，避免了很多人在这里浪费时间。

不过，并非所有搞技术的人都知道数学的边界在哪里，依然有无数人在犯错误，试图解决不应该用计算机解决的问题。

### 8.3 图灵

第三个是给出计算机能力边界的人 —— 图灵。

图灵当时知道希尔伯特第十问题，他觉得答案否定的，但是又无法证明。

当时，图灵还受到冯∙诺依曼的启发。他在读了冯∙诺依曼的《量子力学的数学原理》一书后认为，人的意识来自于测不准原理，但是计算则来自于机械的运动。图灵认为，这就确定了什么可以计算、什么不可以计算。

后来，图灵发明了一种叫作图灵机的装置，能够在有限时间内，判断出哪一类问题能够在有限的步骤内计算出来。

简单地说，图灵不仅给出了可计算的问题，也就是有答案的问题的边界，而且告诉了人们实现计算的方法。这个方法就是图灵机，它能够在有限时间内，判断哪一类问题能够在有限的步骤内计算出来。

### 8.4 人工智能的边界

好，讲完了数学的边界和计算机的边界，我们再来看看人工智能的边界。

首先，世界上很多问题都不是数学问题。

比如，你今天在一条单行道上遇到一辆无人驾驶的汽车，你故意挡住道不让它过去。这时候，它就没有办法了，它使尽所有算法都无法把你赶走。当然，如果是人开车遇到这种情况，可以下车把前面这个人拖走（当然这不合适），也可以打电话给警察，让警察把他请走。但是这两种做法，都不是数学的解决办法。

其次，根据希尔伯特和马季亚谢维奇给出的结论，很多数学问题有没有解是不知道的。

在计算机和数学领域，知道一个问题有没有解，叫作「可判定」，这样的问题叫作「可判定问题」。

再次，可判定的问题很多是无解的，只有少部分有答案。

我在文稿里附了一张图，把所有的问题、数学问题、可判定的问题和有答案的问题之间的嵌套关系表示了出来，你可以点开文稿看看。

然后，在有答案的问题中，有一些是可以通过图灵机解决的问题，也就是所谓的可计算的问题。

当然，图灵机是一种理想状态的计算机，它所谓的有限时间，可以是非常长的时间，比如 1 万亿年，超过宇宙的年龄。

再有，现实生活中计算机能解决的问题，又是可计算问题中一个更小的子集，我们称之为「工程上可计算的问题」。

比如，我们前面说了，如果训练一个模型的时间是 20 年，我们今天基本上就认为不可计算了，至少是工程上不可计算。

最后，工程上可计算的问题并非都属于人工智能研究范畴的问题，人工智能可以解决的问题，又只是工程上可计算的问题的更小的子集而已。

文稿区还有一张图，示意出了有答案的问题内各种问题集合之间的嵌套关系。可以看出，人工智能可以解决的问题只是世界上所有问题中极小的问题。

而且，即便是这极少的人工智能可以解决的问题，也还必须找到算法才能计算。

今天，人工智能发展得比较快，解决了不少问题，就是因为过去没找到算法的问题，今天算法找到了。但是，这并不意味着现有的算法已经解决了所有人工智能的问题，更不要说解决全世界的所有问题了。

讲到计算机能力的边界，还有一些人会有一个疑问，就是：量子计算出来之后，是否原来计算机解决不了的问题，今天就能解决了？

这其实是混淆了计算能力和可计算性。计算能力增加，原来可以计算的问题会算得更快，瞬间解决，但是不可算的还是不可算。

我们打个比方，你如果有一台压缩制冷机，可以将温度降低。如果你有一个超大功率的制冷机，温度降低得会快得多。但是，用再多、再大的制冷机也不可能将温度降到绝对零度以下，因为那是物理学的一条边界。

### 8.5 在人工智能的边界内思考

那么，为什么今天很多人会在人工智能的边界外胡思乱想呢？有这种想法的人可以分为两类：

一类是真的无知。

今天有一个怪现象，就是完全没有做过人工智能的研究，甚至不知道人工智能为何物的人，就敢自称专家，对人工智能发表各种看法。这些人就如同当年见证了哥伦布让月亮消失又让月亮出现的牙买加原住民，他们会向其它人宣传，「出现了一个叫作哥伦布的神」。

另一类则是人工智能领域的从业者，他们把自己做的贡献在无意中夸大了。

你可以在脑海中想象两个互相嵌套的圈，大圆是今天能够用计算机解决的问题，小圆是人工智能已经解决的问题。我们可以想象，经过一段时间之后，人工智能能够解决的问题变多了，也就是说，小圆变大了。

但是很多从业者，特别是做出了具体贡献的人，会从自己的角度放大自己的成就。

这些人都只看到问题的局部。事实上，它们看到的工程上能解决的问题和人工智能能解决的问题，根本和现实中不一样，连大小形状都变了。当人工智能发展了一点，小圈变大了，他们就觉得，人工智能似乎已经解决了绝大部分可解决的问题，于是他们或兴奋，或慌张。

不客气地讲，今天很多研究具体问题的人不抬头看路，只看到自己眼前的一点点研究领域，把一点点进步当作很大的进步。这种看法，和庄子所描绘的井底之蛙没有太大的差别。

下节预告：

如果我们生活在与人工智能无关的年代，或许不用关心人工智能的边界，因为它发展得好也罢，坏也罢，和自己无关。但是今天，情况就不同了，因为我们觉得它和我息息相关，我们的决定可能会受到它发展的影响。因此，我们有必要真正了解人工智能的细节。

但事实上，很多人是没有耐心了解原理的，也因为不了解原理，于是只能人云亦云的跟风。

下一讲，我们就来看看，ChatGPT 被热炒，究竟说明了什么？

## 0901. ChatGPT 为什么会被热炒？

2023 年上半年，ChatGPT 有多热，不需要我重复了。你在网上会看到这样的文章，比如：

《ChatGPT 揭开 AI 战幔：杀死黄页一样摧毁 Google？》

《ChatGPT 强势来袭，打工人「瑟瑟发抖」又能做什么？》

有人很焦虑，有人庆贺新技术的到来，你会听到各种声音，这很正常。一个新技术出现，各种消息铺天盖地并不是什么新鲜事，抛开新技术本身，这当中肯定有炒作的成分。

如果翻翻 10 年前的旧闻，你就会发现差不多的文章早就出现过，只不过现在是用 ChatGPT 替换了当时的一些词语。这几年，有差不多待遇的技术就有 VR、量子计算、元宇宙、区块链、Web3，等等。除了 VR 和量子计算在慢慢取得进展，区块链在一些行业开始得到一点应用，剩下的都还没有什么结果。ChatGPT 不是第一个被热炒的话题，也不会是最后一个。

热炒的背后一定有原因，有利益、有恐惧、有哗众取宠，也有无知。我们不妨从投资、从业者、媒体和普通人的角度做一个简单的分析。

### 9.1 投资

我们先从投资的角度看。

当一家企业有了一个别人都没有的技术，特别是这项技术被认为有可能改变世界的时候，它的估值就会暴涨。如果它是一家上市公司，它的股价就会暴涨。

2016 年，当 AlphaGo 打败李世石的时候，Google 的股价就暴涨过一轮。2023 年当微软投资了 OpenAI 一百亿美元，占了它大约 1/3 的股份后，微软的股价在一个月内相比大盘大约上涨了 10%。当然上涨的原因可能有很多，但就算只有一小半来自于 ChatGPT 带来的效应，按 3%-5% 来算，考虑到微软是一家市值 2 万亿美元的公司，它这笔投资带来的收益也是相当可观。

即便国内那些没有开发类似产品，但是有这个技术潜力的公司，在过去的几个月里股价都在蹭蹭地上涨，甚至涨幅远远超过微软。这里我就不列举那些公司了，如果你有兴趣，自己可以做一下研究，对比一下这些公司股价上涨的幅度和微软上涨的幅度。有趣的是，在媒体上鼓吹这项技术最起劲的反而不是已经投资、已经植入产品的微软，而是这些所谓的潜力公司。不难想象，这背后是有巨大的利益的。

我们现在无法判定这些公司将来是否能真的受益于 ChatGPT 热，但是从历史上看，靠炒作获得的股市收益很快会还回去。今天，AlphaGo 对 Google 股价的影响已经荡然无存了，而 Facebook 为了炒作元宇宙改名 Meta 之后，简直成了一个笑话，股价暴跌。当然股价只是外界对它的看法，Facebook 可是真真实实地为此烧掉了上百亿美元，不得不进行两次大规模裁员。

有人问我，为什么苹果公司对此毫无动静？我回答说，这就是苹果公司能够成为今天全球市值最大、利润最高公司的原因。有人觉得我这种回答显得暮气沉沉，但是我们看数据，苹果公司市值一度超过 3 万亿美元（2023 年还有 2.6 万亿），在过去的十年里长期占据世界公司的市值、利润榜首，这不是浪得虚名的。苹果能做到这一点，恰恰是因为它谨慎行事，在一切变化面前淡定，不炒什么概念。

当初苹果的 iPhone 刚出来时，它只有 2G 的服务，而其他电话已经开始支持 3G 了。当很多手机开始宣布支持 5G 时，苹果并没有跟风，过了两年才推出它的 5G 手机。但最终，苹果赢得了手机的市场。今天有人嘲笑苹果的 Siri 落后了，很多问题回答不了，但是我觉得，不给答案要远比给出一个错误的答案好。如果 ChatGPT 这样的服务将来真的很流行，你会看到一个苹果版的，但不是在今天。

### 9.2 从业者

讲完投资者的视角，接下来我们从从业者的角度来看看这个问题。

我们今天能从媒体上听到的从业者声音，大致表达了两种不同的观点。

第一种是兴奋，然后大谈 GPT 的技术细节，认为自己的机会来了。很多人你还批评不得，因为他会说你不懂技术。

在这背后其实也是利益。在过去的几年，试图挤进人工智能领域的技术人员很多，但是真正能做核心工作的不多，受益于人工智能热的更不多。很多转行进入这个领域的人，其实是用自己的业余爱好，和很多接受了七八年训练的专业人士争饭碗，肯定不是明智之举。有些人觉得加入到了人工智能行业，就可以挣别人的钱，其实螳螂捕蝉黄雀在后，真正赚钱的另有其人。

第二种人是恐惧，之前还觉得自己和世界先进水平很接近了，怎么一下距离就被拉开了。生怕自己落伍了，因此一定要做点什么。

我的想法是，如果 ChatGPT 这件事是一个方向，早一天、晚一天开始这方面的工作差别不大，毕竟追赶别人比自己在前面摸黑探索要快得多。在消费电子展（CES）上，从一个概念原型到挣钱的产品，中间要间隔多久呢？通常要七年时间。而七年前展示原型的，和七年后展示挣钱产品的，常常是两家公司。如果一项技术只有一年的窗口期，不是说明它发展得太快，一年就把全部的问题解决了，而是说明它一年后不存在了。

我和美国的不少从业者谈过这事，他们都显得很淡定，原来做什么还做什么。如果你恰好是这方面的从业者，也可以跟我说说你的想法。

### 9.3 媒体

关于 ChatGPT 的热炒，还有一组不能忽略的声音，就是媒体。

中国的媒体和美国的媒体在对待 ChatGPT 上有一个很大的差异。国内的消息你已经看得很多了，我们不妨来看看美国主流媒体是如何评价它的。《大西洋月刊》的报道是，「ChatGPT 远比你想得要笨 —— 它只是一个玩具，还称不上是工具」；《商业内幕》的报道是，「ChatGPT 教人学坏 —— 谋杀，制造炸弹，反犹太……」；美国国家广播公司（NBC）的报道是，「ChatGPT 会产生假新闻和误导信息」。当然，也有赞誉它的报道，但总的来讲大家并没有把它当作神。

国内有些媒体很有意思，它把赞誉 ChatGPT 的报道翻译了过来，然后根据自己有限的体验，增加了一些耸人听闻的报道，甚至对一些专家的评论断章取义。其实大家如果有心把那些媒体过去的十年里对热门技术的报道翻出来，就会发现当初炒 VR 的、区块链的、元宇宙的、自动驾驶汽车的，都是同一批人。当然，那些报道当时是非常吸引眼球的，但事后几乎没有人去深究。

在这里，我有一个提醒，就是不要天天只看媒体报道去投资，因为这样会看不到世界上真正挣钱的地方。

我看到国内有些科技和商业媒体对行业有一种偏见，就是把大部分篇幅都给了互联网和 AI 行业，而忽视其他行业的技术发展。直到今天，全世界互联网的市场规模并不大，还不到 1 万亿美元（像淘宝这样的企业只计算交易收入，不计算商品本身的价值），甚至比不上可再生能源。相反，电信产业的市场规模大约是 4 万亿美元，传统能源的市场大约是 2 万亿美元，医疗健康产业大约是 10 万亿美元，而物流则超过 12 万亿美元 —— 每八块钱 GDP 就有一块钱是物流。但是，物流行业、能源行业甚至电信行业，在国内的科技和商业媒体上的存在感要远远低于互联网行业。

### 9.4 普通人

最后让我们来看看普通人的反应。不客气地讲，很多人把 ChatGPT 看成了一尊新的神，认为它无所不能。

当年的印第安原住民被哥伦布欺骗了，或者说叫做被占了便宜，但是当时他们却把哥伦布当成神。因为以他们的知识储备，无法理解「月食」这个新鲜事物背后的原因。

对于一个新技术，特别是从名称无法理解其内容的新技术，一开始出现和原住民类似的反应是很正常的。但是今天几乎每一个人都接受了至少 12 年的教育，对一件新事物的反应依然如此狂热，这就有点过头了。

在历史上，哥伦布的故事其实还有续集。18 年后，也就是 1521 年，麦哲伦环球航行到达了菲律宾，在那里一开始他享受到了哥伦布的待遇，被当地原住民封神了。但是很快当地人发现他不是神，并且在冲突中杀死了他。在过去的十年里，后来把 VR、区块链和元宇宙贬得一文不值的，也是当初将它们封神的人。

1929 年经济大萧条之前，当纽约街头擦皮鞋的小童向约瑟夫·肯尼迪，也就是肯尼迪总统的父亲兜售股票时，老肯尼迪就知道出问题了。今天，当从来没有从事过计算机行业工作的人开始大谈 AI，觉得自己比专业人士更懂，甚至开始对计算机领域的人指手画脚时，我们就要警惕了。

下节预告：

你可能觉得，这一讲给 ChatGPT 的冷水泼得有点猛。其实我不是说 ChatGPT 这个技术不靠谱，而是通过各方面的分析，让你对它有一个更理性、更全面的看待，不要一起陷入到狂热当中去。

实事求是地讲，ChatGPT 既不像很多人讲的那么神奇，也没有那么可怕，但也不是毫无用途。

下一讲，我们就来分析一下，ChatGPT 到底会不会取代人，或者说抢人的饭碗。
## 1001. ChatGPT 能替代什么人？

上一讲关于 ChatGPT 的热炒，我们算是敲了敲警钟。那么，到底要如何理性看待 ChatGPT 呢？实事求是地讲，ChatGPT 既不像很多人讲得那么神奇，也不可怕，但也并非毫无用途。

关于 ChatGPT，最多人关注的一个问题就是：ChatGPT 能取代人吗，或者说能抢人的饭碗么吗？简单的答案是 ——「能」。但是它只会抢特定人的饭碗，而不是所有的人。接下来我们就来看看什么工作最经不起 ChatGPT 的冲击，更广义地讲，是经不起人工智能的冲击。

### 10.1 什么人会被取代

会被 ChatGPT 取代的人有三个特点：从事不费体力的工作，不动脑子的工作，或者不产出信息的工作。

这里先要纠正很多人的一个错误认知。有人可能会觉得，ChatGPT 能「生成」信息，甚至一些从事自然语言处理工作的人也有这种错误的想法。这种想法其实违背了信息论的基本原理。技术是可以进步的，但是信息论最基本的原理是无法突破的，就如同物理学的规律不能违背一样。今天的语言模型比 50 年前强大得多，但是再强大，它依然不过是一个需要依靠信息的预测模型，这就如同没有翅膀的汽车，再发展也上不了天。

总之，ChatGPT 的特点是利用人喂给它的信息，再去产生信息，这有点像是复读机，或者鹦鹉学舌。

它一旦完成了模型的训练，其实就是用信息武装了自己，然后就可以大量复制信息，从它所知的信息中挑出一段你需要的给你。因此，如果一个人的工作不创造新的知识，只是把现有的信息整理整理，那么一定会被机器替代掉。

我们不妨列举一些这样的职业，按照收入从高到低排个次序：

1、金融分析师

2、大部分财经媒体记者

3、律师助理

4、普通文员

5、短视频制作人

6、大部分公众号写手

……

这些工作都有一个共同的特点，就是本身不产生信息。

金融分析师所做的工作，是把一些公开的数据根据公式整理一下。只要你稍微留意一下就会发现，几乎所有金融机构的研究报告，都是一半对一半错，没有例外。今天大部分财经媒体的记者，创造的信息也很少。

至于，律师助理过去是为律师们提供信息、整理信息、起草文件，这些事情都是把信息从一个文件中搬运到另一个文件中。至于要什么信息、对什么信息作出什么样的判断，是律师的事情，而不是助理的事情。普通文员做的事情也差不多。

还有今天很流行的短视频，没有营养的占大多数，剩下的还有一多半属于剪辑其他视频。很多公众号的文章也是如此，抄袭甚至断章取义别人的内容，再加上一个耸人听闻的标题。如果是这种内容，ChatGPT 产生得更快，甚至比人产生的质量更高。等到 ChatGPT 的创作普及后，这种无脑赚钱的人好日子可能就不长了。当然，对社会来讲，这或许并不是坏事。这个世界并不需要很多在互联网上哗众取宠的人。

另外还有一类工作，在短期内可能收益还不错，但是前景会越来越暗淡，就是低端的 IT 从业者。

今天，从事 IT 开发工作的人，大致可以分为两类。一类是研发新产品和做解决方案的。这一类工作通常比较辛苦，需要接受比较长时间的专业教育，这部分工作不会被取代。危险的是另一类，就是处理日常事务，包括运营、测试、数据处理等等，这些工作只需要经过短期的培训就能胜任。

在就业形势好的时候，两者的收入差不多，因此后一类工作其实投入产出比要高得多。在美国，计算机科学专业和数据科学（Data Science）是两个不同的专业，对教育的要求不同。但是，后一类工作是可以压缩的，因此每次企业想压缩编制，后者就首当其冲，成为了被裁撤的对象。

与之相反的是，真正费脑力的工作当然不会被取代，因为知识和信息就是这些人创造的。

那么，为什么费体力的工作也不容易被替代呢？因为那些工作和 ChatGPT 无关，或许有些机器人能取代一部分体力工作。但事实证明，今天全世界所缺的反而是要费体力的所谓的蓝领工作，比如设备维修、送餐、保洁、酒店服务等等。10 年前我们都觉得这些工作容易被取代，但事实却是，今天全世界这些工作并没有减少，而且招不到人，很多岗位还在增加，薪酬提高的幅度也很大。

### 10.2 什么人不会被取代

说到这里你应该发现了，越是到了各种智能工具不断涌现，做事情越来越便利的时候，从事创造性的工作也就越来越重要。

我们在前面讲了，在智能时代，基础的科研工作是不可取代的，因为无论是人类提供的知识，还是由人工智能转述的知识，都需要有人创造出知识。同样是写一篇文章，它的立意、主题思想、观点和倾向性，还是需要来自于有思想的人，单纯交给机器去做，就失去了个性和思想。

今天很多外国人申请美国大学的时候，让计算机写几篇作文，读起来好像比自己写得还通顺，但其实这对申请好大学是没有帮助的。

很多年前，我在学校里为系里的研究生录取委员会工作，审阅过很多申请材料。我发现中国留学生的推荐信都是从几个模版上复制出来的，整篇推荐信就一句话，这个学生好得不得了。这样的推荐信其实帮助不到申请。我知道，中国大学生的推荐信其实都是学生打印好，请教授签字的，发给每一所大学的推荐信完全相同，同班同学参考的模版也基本相同。相比之下，美国教授给学生写的推荐信就非常个性化，这个学生到底好还是不好会讲得清清楚楚。即便他提到了某些学生的弱点，你也能知道那些弱点是致命的还是无伤大雅的。

因此，对于那些按照模版写作的人，将来还不如让 ChatGPT 去写，至少省时间。

很多人觉得，蓝领的工作具有重复性，白领的工作都是创造性的。这其实是一个误解。一个水管工，一年修的水管还真没有多少完全一样的。相反，今天很多实验室的工作，重复性反而很高。

在学校时，我的导师贾里尼克对我们讲，绝大部分科研论文，除了作者和审稿人，阅读的人不超过十个，引用的人更少，因为那些工作基本上没有价值，只是换一个角度重复一遍热门研究课题的工作。当时我还觉得他有点夸张。后来我到 Google，我的老板尤斯塔斯也跟我表达了同样的观点。他在 Google 管整个的研发和工程部门，在此之前在 DEC 管整个公司的研发，也相当具有发言权。后来我们开发了 Google Scholar，就是为学者提供的论文索引服务，我们发现，绝大部分科学论文真的只有个位数的引用量，而且一大半还是作者自己在引用。

我们总是在说要做具有创造性的工作，但其实大部分人是停留在口头上。世界上有很多未知的问题，需要亲力亲为去解决，它们未必都是大问题，但却很重要。比如我们前面提到的，如何知道哪种口罩防护力最高的问题，需要做研究才能搞清楚，不是坐在办公室里凭空推理能解决的。

2016 年，我在《智能时代》这本书里讲，受益于时代变化的人可能只有 2%。那本书出版才过去 6 年多的时间，从招商银行的年度财富报告来看，前 2% 的优势甚至比我们想象得还要大，今后这个差距还会更大。这倒不是因为前 2% 的人掌握了新技术，而是因为他们掌握了正确的思维和做事的方式。相反，很多人的竞争力则相对下降了，因为思考问题的底层逻辑就是错的，而他们掌握的技能很快会变得毫无价值。

比如过分迷信技术，就是一种错误的思维方式。很多人觉得有了新技术，我将来就省事了，不费脑筋就可以挣大钱。我给你介绍一个美国科学院院报（PNAS）的结论：正是因为全世界很多人都有这种想法，让现在的人变笨了。

在历史上，科技越发达，人们受教育越多，会变得越来越聪明，也就是智商测试结果逐年增加。这也被称为弗林效应。但是，这个趋势在上个世纪 70 年代信息革命开始之后停滞了。随着机器智能越来越先进，智商测试分数反而在逐年降低。美国西北大学和俄勒冈大学科学家从 2006 年到 2018 年做了很多这方面的研究，发现不仅美国人的智商逐年下降，而且年轻的一代，也就是 18~22 岁的人智力已经达不到他们父辈的水平。同时两极分化的现象也非常严重，教育程度较低的人，智商下降幅度更大。不只美国，欧洲多个国家也发现了同样趋势。

也就是说，但凡觉得有了智能技术，自己就不需要努力的人，会首先成为智能时代的牺牲者。人工智能可以解决很多问题，但救不了思想上懒惰的人。相反，有了智能工具，反而更加勤奋的人，不仅不会被 ChatGPT 抢走工作，还会因此有更高的成就。这时，技术就不再是诅咒，而是祝福了。

下节预告：

好，关于 ChatGPT 能不能替代人、会替代哪些人这个很多人关注的问题，我们就说到这里。

下一讲，我们就来分析一下，ChatGPT 里有哪些机会，我们如何理性看待它。

## 1101. 理性看待 ChatGPT

前面课程里，我们花了很大的篇幅讲原理，从 ChatGPT 的本质到语言模型的发展历史，从训练 ChatGPT 需要的资源到人工智能的边界。

但我知道，不少同学还是会有一些困惑，比如，都说 ChatGPT 是大机会，自己不想错过怎么办？还有一些人则充满了恐惧，怎么才能不被 ChatGPT 取代？ChatGPT 会不会出现自我意识？等等。

不同的人对 ChatGPT 的看法当然会不一样，这一讲，我们就说说如何理性地对待它。

### 11.1 投资人的角度

首先，我从投资的角度，说说如何理性看待 ChatGPT 带来的机会。

作为一个投资人，对于新技术，我总是欢迎的，毕竟，没有了技术发展，就没有投资的可能性。但是，当人们对一项技术炒得过热时，其实已经不是投资它的好机会了。

从整体上讲，每一项技术都会带泡沫，泡沫是有好处的，它可以让资金在短时间里流向一个领域，加速它的进步。但是，如果条件尚未成熟，或者技术的影响力被过分夸大，那些投出去的资金就可能永远也回不来。

远的不说，最近十年，在各种热门领域投资的人和机构，包括很多大企业，鲜有获得回报的。VR、人工智能、区块链、元宇宙，甚至 5G 都是如此。

很多人喜欢把巴菲特的一句话放在嘴边，「在别人恐惧时我贪婪，在别人贪婪时我恐惧」，但也仅仅是挂在嘴边而已。

有人可能会说，不抓可惜啊！万一它将来成为趋势了怎么办？如果它真的成为了趋势，等到趋势明显一点再投身进去也来得及。但凡能成为趋势的技术，都会有几十年的发展机会。

就拿电子商务来说，互联网兴起后它就是趋势，但是各种机会持续了 20 多年。最早做电子商务的 8848 公司今天很多人都没有听说过。阿里巴巴只能算是第二批进入这个领域的企业，而且一开始做的是 B2B 平台，还不是卖货给大家。到后来，针对普通消费者的电子商务才起步，那已经是阿里巴巴成立将近十年后的事情了。

这个世纪第一个 10 年的后期，大家都觉得阿里巴巴已经处于垄断地位了，腾讯想挑战它也没有成功，没有人觉得还有机会。结果当时不起眼的京东起来了。

到了第二个 10 年的中后期，大家觉得电子商务已经是二分天下，而且阿里巴巴和京东内部很多人也这么觉得。结果拼多多起来了，而且交易量还超过了京东。

再接下来，大家觉得这回总不会再有机会了吧，结果还是出来了一大堆直播带货达人，头部团队的利润不亚于大部分上市公司。这么算下来，20 多年里，一代人的时间，至少有四拨人挣到了钱。

你看，这个趋势已经持续 20 多年了，中间有很多机会。这个行业，只有做得好和做不好的区别，没有先与后的区别。

相反，如果一项技术，或者一个新领域，只存在几年的时间，错过前两年就没有机会了，那这样的所谓机会不要也罢。毕竟大家用钱投资也好，都是希望有长期回报，而不是仅仅一两年的回报。

### 11.2 从业者的角度

其次，我们再从从业的角度，说说如何理性看待 ChatGPT 带来的变化。

我是这么想的：

2000 年的时候，语音识别非常热门，比今天 ChatGPT 热门多了。当时，很多语音公司不用几年就能上市。这让我的师兄师姐们获得了非常好的工作机会，但是不出 10 年，大家该过什么生活还过什么生活。

2002 年，我在 Google 有一个同事，2000 年的时候他就在做语音识别，幸运地赶上了公司上市，更加幸运的是，将股票抵押给了对冲基金，借了一笔钱出来。但最终，他的收益就是借出来的这一笔钱。因为随后，互联网泡沫破碎，911 事件发生，他手上的股票价值只剩下 5%。这还是赶上了公司上市的好运气。那家公司今天还在，还是行业领头羊，但是几乎没有太多人因此发财。

今天，很多人见到一点点新技术就言必称机会，其实是想多了。大家不妨看看最近十年真正的机会到底有多少：

从上世纪 70 年代起，最大的机会来自于 IT 革命，包括个人电脑、互联网和移动通信，它持续了 50 年。其它的发明创造，本身都算不上是革命，它们大多依附在上面这三个技术之上。

比如卫星通信，只是今天信息产业中较窄的一个领域，卫星电视从起步到今天，订户越来越少。这还是非常革命性的技术呢！剩下的我们所经历的什么团购、P2P，只是 IT 革命浪潮的一个个小涟漪而已。

即便人工智能是一次新的技术革命，ChatGPT 也只能算是其中的一个涟漪。

因此，对于从业者来讲，如果正好身在人工智能领域，恭喜你，把这碗饭好好端十年。如果不在这个领域，也不用刻意挤进来，毕竟拿业余爱好和别人的专业去竞争，很难有胜算。做好本职工作的回报一定是最高的。

如果一定要说哪些从业者有机会，可能是像英伟达、AMD 这些做 GPU 的公司，以及亚马逊和阿里巴巴这些卖算力的公司。毕竟，训练语言模型和提供 ChatGPT 之类的服务是非常消耗算力的，挤到人工智能领域的企业越多，这些公司的生意越好。

这就如同过去加州淘金热时，没有一个淘金者留下姓名，因为他们实际上大都没有挣到钱，只是勉强活下来了而已。但是，卖水、卖工具的人发了财，做牛仔裤的人发了财。

这十几年，我印象很深的有这样两件事：

第一件事是十多年前，中国团购「百团大战」时，我们买了一些广告公司的股票，结果挣了很多钱。原因很简单，那么多公司挤到一个并不算大的市场上，大概率挣不到钱，但是它们为了争夺市场，肯定会拼命做广告。最后，成百上千的团购公司烧完投资者的钱就关门了。那么，钱去了哪里？它们成为了广告公司的收入。

第二件事是在比特币热的时候，一些朋友听了我的分析，买了 AMD 和英伟达的股票，也挣了很多钱。道理也是一样的，挖矿挣不挣钱不知道，但矿机、GPU 是一定要花钱买的。

今天，如果你是做高端处理器或者智能化云计算的，也要恭喜你，你挣钱的机会可能比直接做人工智能的更多。

### 11.3 普通人的角度

当然，大多数人既非投资人，也非从业者，这些人要如何理性地看待 ChatGPT 呢？

对于这些人来讲，可以对新技术和未来有所期待，但不要觉得自己的好运气从此来了，然后放弃自己擅长的事情，去不擅长的领域寻找什么机会。

我对于新技术的态度向来是，把它们当作意外的惊喜，或者叫作意外的奖励（Bonus）。如果它能对我的事业有帮助，固然好；如果没有帮助，也没有关系，因为当初没有给予太高的期望，也不会太失望。

在过去的半年里，很多人问我一个问题：「我在 ChatGPT 中有什么机会？」对于 99% 的人，我的回答都是，「你没有机会，甚至要抱着小亏就是赚的心态对待它，这样才不会失望。」

如果一次技术更新使得一些人的饭碗受到威胁，比如 IT 技术让传统的金融行业萎缩，这些人也只能把自己重新定位，而不应该觉得世界上的坏事都能转化成好事。

当然，其中一些人对我的回答并不满意，甚至觉得我不真诚，或者落伍了。他们会说，「我问你如何把握这次机会，你却告诉我没有机会」。我对他们讲，就算 ChatGPT 有点机会，之前那么多机会哪次你把握住了？事实上，之前把握住某些机会的人，就不会问这个问题，因为他们是先学会了如何把握机会，然后才把握住机会的。

具有把握机会的能力，任何时候都有机会。没有这个能力，试图搭顺风车，这种好事恐怕轮不到大多数人。因为参与的人太多，就不可能有利润了。

文稿里有一张图，是我在某个高铁站的出站口拍的。左边是扶梯，堵得不像样；右边是传统的楼梯，没有人走。扶梯当然快，而且省劲，和楼梯相比也算高技术，但那么多人去挤所谓的「机会」，结果就是没有机会。反而是旁边的楼梯快得多。众争勿往，这个道理对于 ChatGPT 依然成立。

当然，今天还有相当多的人对 ChatGPT 恐惧，觉得天要塌下来了。这就有点像是被哥伦布蒙骗的原住民，完全没有必要。

其实，比恐惧的结果更可怕的是恐惧本身。很多人因为不必要的恐惧把生活搅得一团糟。今年年初，一个在上海的年轻人问我：最近行业的情况不好，公司要裁员，我在想，是否该离开上海回老家？我对他讲，不是还没裁到你吗，真把你裁了，再考虑是否留在上海不迟。今天很多人对 ChatGPT 的恐惧，就是这样一种状态。

对于绝大部分人来讲，有没有 ChatGPT，他今后十年的生活轨迹不会有什么变化。因为在任何社会里，大环境都是最重要的。

今天在 IT 领域获得巨大成功的人，都是因为以 PC 机、互联网和移动通信代表的 IT 革命持续了 50 年，他们获得了 50 年叠加进步的红利。如果只持续 10 年的增长，马云、马化腾就都不是今天的境况了。

今天中国人的财富比祖辈、父辈增加了上百倍，是因为有四十多年和平发展的大环境，而不是大家的本事比父辈、祖辈大，或者把握机会的本事高。

专业做股票交易的人都知道，即使交易非常频繁，而且成功率极高的交易员，80% 以上的回报也来自于大盘，他自己做的那些努力只是微调而已。

因此，大家将来的命运取决于大环境，而不是某一项现在还看不清的技术。如果大环境不好，技术再进步，红利也落不到大家身上。大家看看今天的拉美、俄罗斯和南非就知道了。相反，如果大环境好，任何技术进步都会变成新的就业机会。

面对一个自己看不清楚的事情，不妨多观察一段时间，再做结论。对于自己不懂的事情，要搞清楚，不要和别人一同去造神。今天人们造了太多的神。对很多人来讲，过去乔布斯是神，今天马斯克是神，将来 ChatGPT 是神。但其实，造神是一种思想懒惰的表现，因为不用思考，一切归结为神就可以了。

### 11.4 总结

我知道，当我希望大家冷静看待 ChatGPT 时，很多人会攻击我，因为我阻止了他们割韭菜。但我过去的经验表明，以为在收割别人韭菜的人，自己往往是被别人收割的韭菜。

在过去的十多年里，有无数投资人和创业者，觉得能够从大众身上挣到钱，结果是自己花钱、花费生命给大众提供免费服务，最后钱花完了，故事也就讲完了。因此，越是别人狂热的时候，我们越需要冷静。未来不是给机会主义者的，而是给踏踏实实把本职工作做好的人的。

至于有人担心 ChatGPT 会产生自主意识，提出对它进行道德约束，这更是对意识和人工智能的瞎担心，和有些人怕鬼没有什么差别。

还有人说，ChatGPT 会教人学坏，这是真的，但它只是一个复读机，它是从人那里学坏之后，又把坏想法还给了人类。仅此而已。

下节预告：

这一讲，我们详细说了如何理性看待 ChatGPT。

下一讲，我们把视野扩大，看看除了 ChatGPT，还有哪些值得关注的人工智能技术。

## 1201. 还有哪些人工智能和科技值得关注？

不管 ChatGPT 的结果如何，人工智能依然是未来。

今天，很多人的目光都被 ChatGPT 吸引，殊不知，人工智能的范围很大，远不止 ChatGPT 或者其他自然语言处理工具。

那么在 ChatGPT 之外，还有没有什么值得关注的人工智能技术和应用呢？有的。这一讲，我们就说说这个问题。

### 12.1 多任务人工智能

2019 年，《MIT 科技评论》在介绍 GPT-3 的同时，还介绍了另一项人工智能技术 —— 多任务的人工智能。

这两项技术是作为两种不同的技术路线代表被介绍的：GPT-3 是所谓以数量实现质量的代表，用英语的说法就是「brute force」，翻译成中文就是「简单粗暴」，大数据，大计算量；而多任务的人工智能则是以质量取代数量。人其实就是多任务的智能生物。

今天，几乎所有人工智能的成果都来自于以数量实现质量。数据、计算能力和数学模型，是当今人工智能的基础，ChatGPT 便是如此。

但是，今天的人工智能也有两个大问题：

第一个问题，每个系统只能完成一件特定的事情，你无法让 ChatGPT 下棋，也无法让 AlphaGo 回答问题。因此，虽然这些人工智能系统能把一件事做好，但换一件事就不行了。

第二个问题，由于今天的人工智能是采用以量取胜的方法实现的，当数量达不到时，质量就无从谈起。你要是只让智能摄像头看某个人的一张照片，它通常是无法识别这个人的。但是，人通常只看一眼，就会有印象。

这些问题，在多任务的人工智能系统中是不存在的。那么，怎样才能实现一个多任务的人工智能系统呢？它需要有四个条件：

1、知其然还要知其所以然；

2、具有对身边事物的好奇；

3、具有常识；

4、处理信息的能耗降低几个数量级。

首先说第一个，什么叫「知其然还要知其所以然」。

今天，ChatGPT 能够给出一个问题的答案，AlphaGo 能够给出已知的最好行棋步骤，但是它们并不清楚为什么给出这些答案。这就是知其然而不知其所以然。这样一来，机器在下棋时获得的经验就不能帮助它理解语言，或者积攒其他经验。而人却不同，人在下棋时会悟出很多做事的道理。因此，人的智能是多任务的，而机器不是。

想要做到知其所以然，需要真正的理解知识。还是以自然语言处理为例，不能只是通过大量学习看到了所有的语言学现象，还需要真正对语言有所理解。

今天，包括 ChatGPT 在内的所有自然语言处理系统都有一个问题，就是处理语言二义性的能力很弱。

比如，图灵奖得主辛顿就注意到一个现象，在一段上下文中，代词指代什么，其实是有二义的，人可以根据语法和语义判断清楚，但是计算机做得不是很好。他举了一个例子：

如果有这么一句话：「奖杯放不进包里，因为它太大了。」它的意思很好理解。

但是，如果我们说：「奖杯放不进包里，因为它太小了。」

其实也说得通。因为根据上下文的语义信息，它在这里指的是包，包太小了。

但计算机这时就会糊涂。如果你要是把这两句话从中文或者英文翻译成拉丁语系的语言，比如法语，就会出问题。因为在那些语言中，物品也是有性别的，「奖杯」（coupe）属于阴性，「包」（sac）属于阳性，对应的「它」是两个不同的词，形容词的词性也不一样。

今天，计算机解决这个问题的方法是，必须在不同的语言中看到上面这些语言现象。但是人不需要，因为人在知其所以然之后，会在这里把自己的语言学知识用上。

其次是第二个条件，具有对身边事物的好奇心，这其实是指主动学习。

一个人走到大街上会东张西望，他的好奇心会让他看到很多东西，甚至增长知识。但是今天的人工智能，对于任务以外的事情，它是毫无好奇心的。

举例来讲：当你做一件事非常顺利或者非常耗时的时候，你会去琢磨为什么。做得太顺利或者太不顺利，都可能是做错了，你会主动回去检查。这就是好奇心。但是 ChatGPT 不会，它算了半天没算出来，是不会考虑自己在知识上有欠缺，或者算法上有 Bug 的。

没有好奇心，就无法积攒普遍意义上的经验。

多任务的人工智能系统的第三个实现条件是，它要有常识。

今天，你让人工智能系统画一张「在阳光明媚的春天里，在草地上奔跑的狗的照片」，它能画，但是画得没有意境，非常直接，这是因为它不理解「在阳光明媚的春天里」是什么意思。

当然，更要命的是，如果你让它画一张「在阳光明媚的春天里，在草地上奔跑的海豚的照片」，它很可能会把狗换成海豚交给你，因为它不具有海豚无法上岸的常识。没有常识，就很难完成复杂任务，比如让机器人去给你买菜。

最后是第四个条件，想要实现多任务的人工智能系统，处理信息的能耗必须降低几个数量级。

很多人都有一个问题，既然今天人工智能在很多领域做得比人好，我们把它们集成起来，是否能得到一个多任务的人工智能系统呢？

其实，多任务指的不是几个系统的简单拼接，而是让执行各个任务的算法在内部打通，这件事目前还没有做到。即便是把多个人工智能系统拼接起来，这个系统的运行成本也会极高，因为它每做一件事都极为耗费资源。这个资源既包括计算资源，也包括数据，还包括其他资源，比如传感器等。

作为一种多任务的智能动物，人的学习和思考在能量利用上是非常高效的。人脑有百万亿个神经元连接，而人的寿命一般不超过 90 年，也就是 30 亿秒，在 30 亿秒时间里，对百万亿个神经元连接完成训练，这是非常高效的。

计算机的速度比人快，但是训练一个只有几万个连接的深度人工神经网络就需要几万秒，效率低了很多。如果再把神经网络的规模增加十倍，训练的时间需要增加成百上千倍。

训练人脑这样一个系统，能耗是难以想象的。今天，要想把各种人工智能系统整合成一个，计算的能耗必须几个数量级，甚至几十个数量级地下降。这也是很多人认为，今天的深度学习不可能产生多任务人工智能的原因之一。

今天，研究多任务人工智能的专家，主要还是大学里的教授和工业界不做产品的科学家。我个人认为，这是最值得关注的领域。目前，主要的成果还是在学术期刊上，但是慢慢的，大家会看到关于这方面的技术报道。

### 12.2 生物和医学应用

好，说完了人工智能行业一条值得关注的技术路线，我们再看一个值得关注的应用领域，也就是深度学习在生物和医学上的应用。

迄今为止，这方面最有意义的一项成果是 DeepMind 公司（Google 的子公司）做的 AlphaFold，也就是用深度学习解构蛋白质的结构，研究蛋白质的折叠问题。

我们知道，人的生命是靠新陈代谢完成的，而新陈代谢是由基因控制蛋白质的合成实现的。如果蛋白质的合成出了问题，人就会生病。而几乎所有和衰老有关的疾病，都和蛋白质的合成有关。因此，研究清楚这个问题，是解决衰老的关键。

但是，蛋白质是一种非常复杂的有机分子，由很多氨基酸组成的链构成，而且这个结构还是三维的。今天，人类发现了很多种蛋白质，但是无法搞清楚它们的结构。解决这个问题是极具挑战性的。

每年，全世界都有科学家试图通过 X 射线晶体学、电子显微镜和核磁共振等成像技术分析蛋白质的结构。这些技术既昂贵又费时。

在过去的 60 多年里，人类大约只解构了千分之一的蛋白质。换句话说，现在已知的和生命有关的蛋白质超过 2 亿种，而人类只解构了 17 万种。前面说过，如果能预测蛋白质的结构，将极大地促进生命科学的研究。显然，光靠人力做这件事，我们是等不及的。

从很多年前开始，计算机科学家就同生物学家们合作，用计算机来解决蛋白质结构预测的问题，但只在很小的、简单的蛋白质上获得了成功。

1994 年，全世界设立了一个由计算机预测蛋白质结构的比赛，叫作 CASP。

2018 年，DeepMind 的 AlphaFold 开始用深度学习算法尝试这项工作，并且参加了竞赛。

2019 年，它的第一个版本就夺得了头名。然后，每一年推出一个更好的版本。

2020 年，它的第二个版本，就做到了「违反化学规则的数量接近于 0」的好成绩。这是什么意思呢？

氨基酸在合成蛋白质的时候，需要符合化学规则，不符合化学规则的蛋白质结构是不存在的。而深度学习预测的蛋白质结构可能会有错误，导致一些结构违反了化学规则。因此，通常人们会使用有多少预测违反了化学规则来判断解构蛋白质的准确性。目前，和人已经解构的结果对比，AlphaFold 解构蛋白质的准确率高达 70%。

这项成果的意义非常大，它可以改变生物学和制药学的发展，一旦成功，对人类的福祉有很大的帮助。

因此，在很多从事人工智能研究的学者们看来，这种事情比做一些人工智能的玩具有意义得多。

最后，借这个机会，我来谈谈我们是否需要防范人工智能。

前一阵，盖茨和马斯克都建议，警惕甚至限制开发 ChatGPT 这样的技术。我觉得，这种担心为时尚早。

打一个比方，这就好比我们要造一架飞机，今天我们连空气动力学的原理都还没有完全搞清楚，只能做一个滑翔机，但是却开始担心它掉下来撞死人。事实上，我们今天要担心的，是那些人工智能背后的公司和控制它们的人。

好，我们的课程到这里就结束了。

希望通过这门课，通过对技术原理的学习，你能够不被周围的声音带偏，学会理性、审慎地看待 ChatGPT。我是吴军，我们再会。