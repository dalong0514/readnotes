## 记忆时间

## 0100. 信息的产生

1、信息的定义是消除不确定性（信息的度量单位是比特，一个充满不确定性的系统是个信息源，这个系统里的不确定性是信息熵，而信息就是用来消除这些信息熵的）。

2、如何对信息进行编码（各种编码系统本质上都是在编码复杂性和长度之间寻找平衡，它们数学上是等价的。香农第一定律告诉我们，只要编码设计的足够巧妙，就可以找到最短编码）。

3、如何设计一个好的编码系统（信息的编码要同时具有易辨识和有效性）。

4、理解哈夫曼编码（将最短的编码给出现概率最大的信息，一条信息编码的长度和出现概率的对数成正比），学会把哈夫曼编码运用到生活中，融入血肉里。

5、信息矢量化的原理及其意义。

6、信息冗余度的概念，冗余度有好处（易理解、消歧义和容错性）也有坏处（存储和传输浪费、有噪音时可能导致混淆）。

7、信息是如何被压缩的，信息的压缩方法正好说明了等价信息的重要性。

8、可以利用信息的相关性，对其前后的信息做增量编码，这样可以大幅压缩冗余信息。

9、信息压缩分有损和无损压缩两种。采用何种压缩方式以及压缩到什么程度，是没有标准答案的，得看具体的应用场景。

## 0101信息度量世界上有稳赚不赔的生意吗.md

佐尔格给斯大林的信息作用很大，但是信息量其实不到 1 比特，那到底怎么去衡量信息量的大小呢？在香农之前，人们并不认为信息还能像重量、体积、电流一样可以用什么单位去衡量。

人们过去绞尽脑汁试图从信息的内容出发，通过对比重要性，度量信息。香农说，这条路其实走错了。对于一条信息，重要的是找出其中有多少信息量，要搞清楚「信息量」，就要对信息进行量化的度量。但人们始终没找到量化度量信息的桥梁，也就是缺少一个合适的「衡量单位」，比如你用天平称重，需要在另一边摆放相应重量的砝码，那衡量信息的砝码是什么呢？

香农最大的贡献在于找到了这个「砝码」，也就是将信息的量化度量和不确定性联系起来。他给出一个度量信息量的基本单位，就是我们第一讲所讲的「比特」。

「比特」是这样定义的：如果一个黑盒子中有 A 和 B 两种可能性，它们出现的概率相同，那么要搞清楚到底是 A 还是 B，所需要的信息量就是一比特。如果我们对这个黑盒子有一点知识，知道 A 的概率比 B 大，那么解密它们所需要的信息就不到一比特。

那么如果是多于 A、B 这两种可能性，更复杂的黑盒子，要消除它的不确定性需要多少信息呢？比如我们做选择题四选一，或者猜世界杯足球赛的冠军是谁，他们想知道结果需要多少信息呢？如果我们对选择题的答案一无所知，去向一个知道答案的预言家请教，他每给你一个是非的答案，收取你一块钱。对我们来讲，有效的提问方式不是问他「是否答案是 A，或者是否答案是 B」，而应该先问他，「是否答案在 A、B 中」。如果他回答「是」，我们就圈定答案的范围是 A 或者 B，与 C、D 无关。接下来，再问一个问题就能确定是 A 还是 B 了。反之，当我们知道答案不在 A、B 中，我们也可以用第二个问题确定是 C 还是 D。这样我们一共付 2 块钱就可以了。

1『二分法的思路。』

我们把这样充满不确定性的黑盒子就叫做「信息源」，它里面的不确定性叫做「信息熵」，而「信息」就是用来消除这些不确定性的（信息熵），所以搞清楚黑盒子里是怎么一回事，需要的「信息量」就等于黑盒子里的「信息熵」。

熵其实是一个热力学的概念，表示一个系统的无序状态，或者说随机性。比如把冰水倒进一杯开水中，它们会彼此融合，杯子里的「熵」，也就是混乱程度会增加；在信息系统中也是如此，信息熵则表示一个系统内部的不确定性。

我们都知道，一个系统中的状态数量，也就是可能性，越多，不确定性就越大；在状态数量保持不变时，如果各个状态的可能性相同，不确定性就很大；相反，如果个别状态容易发生，大部分状态都不可能发生，不确定性就小。这段原理其实很简单，你先记住它，接下来我给你详细讲解。

香农把这个原理呢，用公式表示出来了，从此信息不仅可以度量了，信息熵也可以计算了。信息熵的公式：

我们大家不用搞懂公式，但要明白这个公式的原理。我先解释什么叫「一个系统中的状态数量（即可能性）越多，不确定性就越大」。比如，你买彩票，只有两个号，其中一个必中彩，不确定性就小，那么这个问题的信息熵就小。如果有 10000 个号，也是其中有一个必中彩，那不确定性就大了。

接下来，我再解释这半句：「在状态数量保持不变时，如果各个状态的可能性相同，不确定性就很大……」我们现在假定可能性的数量是固定的，比如在只有两种情况时，也就是非 A 即 B 的情况，信息熵的变化图类似一个抛物线：

图中的横轴是 A 发生的概率，它从 0 到 1 分布，纵轴就是熵，也就是确定它发生，你需要的信息量。你会发现，当 A 发生的概率正好是 1/2 时需要的信息熵达到顶峰，是一比特。这就类似抛一枚均质的硬币，谁也猜不好结果，因为正反两种结果发生的概率一样，都是 1/2。但是，如果这枚硬币没造好，一面重，一面轻，那就大概率是重的那面朝下，需要确定它哪面朝下的信息量就小。这告诉我们，永远不要听那些正确率总是 50% 的专家的建议，因为那相当于什么都没说，没有提供能够减少「信息熵」的「信息量」。

最后半句：「相反，如果个别状态容易发生，大部分状态都不可能发生，不确定性就小。」其实是这个意思：如果你买彩票要从 10000 个号里选出一个中奖的，不确定性就大多了。不过，如果其中一个号中彩的可能性是 99%，剩下所有的号加起来的可能性只有 1%，这个问题就比较确定，熵就小。

你知道了信息有单位，还可以通过公式计算，那又有什么用呢？大家都知道赌球的庄家总是稳赚不赔，就觉得里面猫腻很多，这次我带你从信息论的角度来看清这个问题。你会发现其实很多类似的复杂难题都是信息熵的计算问题。

假如，我们能提前确定各个球队获得世界杯冠军的概率，设定它们分别是 P1，P2，……，P32。那么我们套用上面的公式，就可以算出这件事需要多少信息，或者说这个问题的信息熵。我们假定为 3.4 比特，或者说对应于 3.4 块钱。如果有一个人提一次问题支付一块钱，从理论上讲，所有参加赌局的人只要平均支付 3.4 块钱就能得到谁是冠军这个信息。但是如果设定赌局的人将收费标准略微提高，提高到一个人平均 4 元。这里面的盈余就被设赌局的人拿走了。

那你会说，我们不可能提前知道概率，那每个球队得冠军的概率是如何预估的？其实这是我们这些下注的人告诉设赌局的人的。如果大家都往德国队身上下注，结果预测德国获冠军的概率就很高，所以押注的多少其实就是大家给出的概率。而开赌局的，只要收费比信息实际的价值高，都是稳赚不赔的。这里面的细节大家不用太在意，总之记住一点，就是开赌局的从来不是拿自家的钱和你对赌，而是让你们彼此互相赌，他通过变相多收费盈利。

你可能听说过「结构化的投资证券」（Structured Notes），比如说石油的价格上涨到 100 美元以上，每 1 美元高盛就付给你 1.5 美元。但是，如果没有到 100 美元，你需要每个月付给高盛 1 美元。这种投资工具，就被做成一种结构化的投资证券。像航空公司或者运输公司因为害怕油价浮动太高，会购买这样的投资产品。那么你以为是高盛在和石油公司，或者其他人对赌么？不是的，因为高盛转手就将和它完全相反的投资产品，卖给了希望油价波动的人。当然，高盛会包装得很好，让两边都感谢它，其实它才是真正挣钱的一方。

你可能听说过金融数学这个专业，那里面的人天天做的事情就是设计这种不容易为人所看懂的，自己永远不赔钱的金融产品。而所谓的基金经理，很多就是把这样的产品卖给你的人。因此，多了解信息论和基本的数学常识，可以在生活中省下不少冤枉钱。希望你知道，很多交易和产品都是利用了信息的可度量性，知道了这点，就可以看清很多复杂交易背后的原理。

掌握了信息量化度量的原理，你还可以用它来对付当今「信息过载」的问题，比如如何判断一篇报道里到底有多少信息量。信息说到底是用于消除不确定性的。如果讲的事情大部分大家都知道，信息量就很少。这也是为什么那些心灵鸡汤的文章大家不愿意读，并非是它们说的不对，而是没有信息量。

## 0102信息编码数字和文字是如何诞生的.md

在非洲的草原上，食草动物在发现狮子或者狼狗接近它们的时候，会发出预警的声音，然后大家一起逃命。这其实就是使用和传播信息，而那声怪叫，就是一种信息编码。

在没有信息论之前，信息编码的复杂度通常和要传播的信息种类数量有关。早期人类了解和需要传播的信息是很少的，因此他们并不需要语言和数字，只需要发出不同的叫声，或者做些不同的手势和肢体接触即可。

但是随着人类的进步和文明的进展，需要表达的信息也越来越多，不再是几种不同的声音就能完全覆盖，语言就此产生。人们生活的经验，作为一种特定的信息，其实是那个时代最宝贵的财富，他们通过口述的语言传给了后代。同时，由于人类开始拥有一些食物和物件，便有了多和少的概念，因此数字也就产生了。

早期人类对信息的编码，基本上是每一种信息，都有一种相应的编码。要想表达 5 这个数字，就伸五个指头，但是很快人的十个指头就不够用了，于是早期不少文明就把脚给用上了。在历史上一些文明采用 20 进制，比如玛雅文明。另一些文明多少留有了 20 进制的痕迹，比如英语中 20（score）这个词，就是如此。再后来，手脚并用也不够了，于是人类就在石头和骨头上划道道。再往后，当数字多到划道道也无法表达时，就有了对数字的编码，也就是各种文明的数字 —— 用有限数字的组合可以表示更多的数。

我们如果要表达 100 个数字，一个办法是设计 100 个不同的编号，让它们一对一对应，另一种是只设计几种编号，然后相互组合，来表达 100 个数。你可能觉得第二种方法更简洁，但这两种方法，在信息论中是等价的。假定我们有 100 个数，从中挑出一个，不确定性是 100 选 1，用上节课学得信息论的公式表达，它所代表的信息熵为：

	log100=6.65

6.65 比特。也就是说，如果我们有 6.65 比特的信息，就可以确定 100 个数中的一个。接下来，我们看看刚才说的两种编码需要的信息量是否一样。

我们先来试第一种编码，也就是一一对应，比如用 100 种奇形怪状的符号对应这 100 个数字，这种编码所能表示的信息量，其实就是 100 选一的问题，也就是 log100=6.65 比特。由于一个编码正好表示一个数，因此编码的长度为一。第二种编码方法是采用十进制编码，也就是用 10 种符号，每个符号所代表的信息量只有「log10=3.325」比特，但是 10 个符号想表示 100 个数字，就需要两两组合。也就是说，一个符号无法消除 100 个数中的不确定性，这样两个符号的信息量加起来还是 6.65 比特，正好可以消除 100 个数的不确定性。这样的编码系统比较简单，但是编码的长度是前一种的两倍。这个十进制的做法呢，就类似我们现在用到的阿拉伯数字 0~9。

当然，我们还可以用二进制编码，就是只有 0 和 1 这两个符号，它们所包含的信息只有「log2=1」比特，如果我们想用它们来表达 100 个数，则需要 6.65 个码。进位取整以后，也就是 7 位的码长，才能表示 100 个数字。符号越少，意味着码位越长，所以你看到二进制通常是一长串的 0101……由此可见，对数字的各种编码其实是等价的，无非是平衡编码复杂性和编码长度之间的关系。

对于数字，如果采用很多个符号，编码长度就短，但是系统就复杂。比如我们如果采用的是 20 进制，编码长度短了，但这就意味着它的编码系统很复杂，要记住的符号很多，大家学数学就太麻烦了。在历史上即使有这样的文明，在竞争中也会被淘汰。玛雅文明发展不快的一个原因，就和它的计数和书写系统太复杂有关。

相反，如果采用很少的符号编码，比如采用二进制，编码的长度就长。比如 100 在二进制中的编码是 1100100。所幸的是，各种编码系统在数学上是等价的，我们可以为人类找一个自己方便使用的，也可以为计算机找一个它方便使用的。

但是要说明的是，由于它们是等价的，在一个编码系统中解决不了的问题，换一个系统同样解决不了。一些媒体讲，由于量子计算不是二进制的，因此它能解决今天计算机解决不了的问题，这个说法显然缺乏常识，因为任何进制都是等价的。

当然对数字的编码不能有半个，因此如果我们采用二进制对 100 个数编码，刚才计算出来是需要 6.65 个码，那就要取下一个整数，编码的长度也就是 7 了。于是我们就得到了信息论中一个重要的公式：

	编码长度 ≥ 信息熵（信息量）/ 每一个码的信息量

香农对此作出了严格的数学证明，他同时还证明，只要编码设计得足够巧妙，上面的等号是成立的，这就是著名的香农第一定律。至于如何找到最巧妙的编码（或是说最短的编码）。

说完了数字的编码，接下来我们说说文字的诞生的过程。它和数字的诞生也很相似，早期无论是苏美尔人、古埃及人、古中国人，还是印度河文明的古印度人，都采用的是象形文字。一个图画就是一个意思。但是后来要表达的意思实在太多了，总不能无限制地发明文字，于是就出现了用几个文字表达一个复杂的含义。

那么这些原始的编码背后的信息论原理是什么呢？我们还是回到消除不确定性这件事来看待这个问题。假如一个原始人家里有 10 样东西，他给每个东西起一个名字，这就是最简单的编码，而且早期起的那些名字都容易让人联想起东西的特性，就如同把狗叫成汪星人，把猫叫成喵星人一样。

当然，家里的东西多了，要做的动作多了，就做不到把每一件事单独编码，就需要利用一些编码进行组合了。比如说我们有对一些东西的编码，又有了一些对动作的编码，这就形成了可以表达复杂意思的简单的句子。比如说一个原始人让孩子把家里的石斧拿来，他就可以告诉他采用「拿来」这个动作，而要拿的对象是「石斧」。人类使用动词，标志着文明的一大进步，这不仅意味着他们能够把动作进行分类，编码了，而且这样才能表达复杂的意思，才有可能形成知识。

有了象形文字和动词之后，人类就有了书写系统，各种信息就通过文字这种编码记录下来，这才让我们了解到过去的历史。但是，从此人类的不平等也开始加剧，因为能够认识编码的人，就掌握了其他人所没有的信息。信息太重要了。于是，这些能够读写的人就成了精英甚至是统治阶级。在任何历史阶段，谁控制了信息，谁就是世界的主人。

一个最有说服力的例证就是：在马丁·路德之前，关于上帝的信息是由教士和主教们控制的，因此农民们只好受人摆布。在中国虽然大家不信上帝，情况也是类似。过去在农村，不能识文断字的人，哪怕再有钱，也不过是土财主，家业很难长期兴旺；能够读书写字的人，哪怕穷，在宗族里也很有地位。今天，虽然大家都能识文断字，但是有的人掌握的信息多，有的人掌握的少，这就造成了很大的不平等。对于个体来讲，改变自身获取信息的能力，要比改变整个社会的不平等容易得多。

古代文字难以普及的一个重要的原因，就是基于各种象形文字的编码系统太复杂，要记忆的东西太多，学习的成本太高。于是全世界的语言都在沿着简化这条路发展。

我通过讲人类创造数字和文字语言的过程，告诉大家，其实它们都是人类用来消除信息不确定性的编码手段。各种编码系统，其实都是在编码复杂性和编码长度之间作平衡，它们在数学上是等价的；由于它们是等价的，所以，在一个编码系统中解决不了的问题，换一个系统同样解决不了；香农第一定律告诉我们，只要编码设计得足够巧妙，就可以找到最短编码。

## 0103有效编码10个手指能表示多少个数字.md

各种编码系统本身在信息论上是等价的，但是，不同的编码系统可以有好有坏。比如今天使用的阿拉伯数字（其实是印度人发明的）0～9 就是一个很好的编码系统，对于描述数字信息来讲，它们的数量不多不少，形状差异大。如果采用一个小圆点「∙」代表一，两个「∙∙」代表二，三个「∙∙∙」代表三，十个「∙∙∙∙∙∙∙∙∙∙」代表十，就不太好，因为大家容易看花眼。因此好的编码第一个特点就是要便于区分不同的信息。

我国在文革后曾经推行过一版过于简单的简化字，但是很快就停止使用了，这里面主要的原因是将汉字的笔画简化得过少后，使得近形字大量出现，不易辨识，非常容易搞混，因此很快就废止了，从信息论上讲，它违反了好编码要便于信息辨识的原则。

从信息论上讲，它违反了好编码要便于信息辨识的原则。德国著名的营销专家和演说家多米尼克·穆特勒提出的清晰表达的五个原则：明确、诚实、勇气、责任和同理心，前四条就和信息编码要便于识别有关。信息编码的第一个基本原则：「易识别」，应用在我们个人沟通中，也是如此。第二个信息编码的原则：有效性。如何组合信息，保证它高效传递，还能不违背第一条「易辨识」的原则。这就需要我们主动思考了。

这依然不是最有效的编码，如果我们考虑采用二进制，而不是十进制进行编码，则能表示 1024 个不同的数字。具体的做法是这样的，我们把十个指头伸开：从左边的小拇指到大拇指编号为 0～4，再从右边的大拇指到小拇指，编号为 5～9。这十个指头，每一个都有伸出、收起两种状态。每一种状态对应于一位二进制，十个指头能表示 10 位 2 进制，因为 10 个指头，每个指头有两种情况，就是 2 的 10 次方，也就是 1024 种可能性。

1『信息的编码要具有易辨识和有效性，吴军用小老鼠试药的例子说明的有效性的理论上限问题。』

比如在一个产品中，有两种可用的方案，A 和 B，哪种更好呢？过去常常是工程师们和产品经理们拍脑袋想，有些时候某些人的「眼光」很好，正好蒙对了，选了一个用户也喜欢的方案，但是这种「眼光好」是无法复制的，一个公司将自己的商业成功寄托在「眼光好」上早晚要失败。

这时，就可以利用用户大数据评判 A、B 方案的好坏，通常的做法是随机选取 1% 的用户作对比实验。比如 Google 在改进搜索算法或者其它产品体验后，会先做这样不公开的测试，一般会持续一周左右。但是像 Google 这样有好几万工程师的大公司，每天的各种改进是很多的，如果每个项目都用掉 1% 的用户，把全部用户都用上也不够。

这就回到了我们刚才学过的高效编码问题，用少量用户同时进行很多个实验的方法，就类似上面这种让小白鼠试毒药的方法，也就是将各种不会发生冲突的实验用二进制进行编码，几组实验者，就可以同时进行几十个不同的实验。

## 0104最短编码如何利用哈夫曼编码原理投资.md

如何对信息进行编码才最有效？这个问题一直困扰着人们，莫尔斯电码时讲到，他根据常识对经常出现的字母采用较短的编码，对不常见的字母用较长的编码，这样就可以降低编码的整体长度。

如果对英语 26 个字母采用等长度的编码，比如进行二进制编码，需要 log26。 log26 就是约 5 比特信息。而采用莫尔斯的编码方法，平均只需要 3 比特，这个效率就高了很多，这样发报，时间就能节省大约 1/3 左右。

无独有偶，全世界除美国之外，各国在设计长途电话区位码的时候，也充分考虑了每一个城市和地区的电话机数量，比如在中国北京、上海等重要城市就是两位，小城市就使用 3 位，这样做的目的是为了减少平均的编码长度。那么是否能够证明，越常出现的信息采用较短的编码，不常出现的信息采用较长的编码，就能比采用同样码长的信息总体上更合算呢？答案是肯定的。

我们不妨看一个具体的例子。我们假定有 32 条信息，每条信息出现的概率分别为 1/2、1/4、1/8、1/16…… 依次递减，最后 31、32 两个信息出现的概率是 1/2^31、1/2^31（这样 32 个信息的出现概率加起来就是 1 了）。现在需要用二进制数对它们进行编码。等长度和不等长度两种编码方法，我们来对比一下：

方法一：采用等长度编码，码长为 5。因为是 log32=5 比特。

方法二：不等长度编码，如果出现概率高就短一些，概率低就长一些。

我们把第一条信息用 0 编码，第二条用 10 编码，第三条用 110 编码…… 最后 31、32 两条出现概率相同，都很低，码长都是 31。第 31 条信息就用 1111……110（30 个 1 加 1 个 0）编码，第 32 条信息，就用 1111……111（31 个 1）来编码。这样的编码虽然大部分码的长度都超过了 5，但是乘以出现概率后，平均码长只有 2，也就是说节省了 60% 的码长。如果利用这个原理进行数据压缩，可以在不损失任何信息的情况下压缩掉 60%。

事实上，这种最短编码方法等于香农第一定律的继续，它最早是由 MIT 的教授哈夫曼发明的，因此也被称为「哈夫曼编码」。关于哈夫曼编码有三个要点值得一提：

1、如果你还记得第 5 讲的香农第一定律，一定知道编码长度是有个理论最小值的，从数学上可以证明哈夫曼的这种编码方法是最优化的。

2、哈夫曼编码从本质上讲，是将最宝贵的资源（最短的编码）给出现概率最大的信息。至于资源如何分配，哈夫曼给出了一个原则，也就是一条信息编码的长度和出现概率的对数成正比。

注：比如在上面的例子中，第一条消息出现的概率为 1/2，我们知道 1/2（以二为底）的对数等于 - 1，因此它的编码长度就是 1（即码 0）。最后两条消息出现的概率为 1/2^31 次方，取对数后等于 - 31，因此它们的编码长度就是 31。

如果我们回顾一下莫尔斯电码，就会发现它是不自觉地采用了哈夫曼编码的原理。只是它没有严格统计各个字母的频率，没有完全做到最优化。在一个极端的情况下，如果所有的信息出现的概率相同，采用哈夫曼编码，每一条信息的码长都一样，这时哈夫曼编码就变成了等长编码，没有优势了。

3、在现实生活中，很多信息的组合，比单独一条信息，其概率分布差异更大，因此对它们使用哈夫曼编码进行信息压缩，压缩比会更高。比如说，在汉语中，如果对汉字的频率进行统计，然后压缩，一篇文章通常能压缩掉 50% 以上，但是如果按照词进行频率统计，再用哈夫曼编码压缩，可以压缩掉 70% 以上。

哈夫曼编码又是怎么应用到我们的工作生活中呢？其实，但凡需要分配资源的工作，它都有指导意义。我在《浪潮之巅》一书中介绍凯鹏华盈时讲，虽然换了三代掌门人，但它能在四十多年，20 多期基金中，平均每一期基金的回报总是有 40 倍左右，这说明它不是靠一两个人天才的眼光，而是有一整套系统的方法，保证投资的成功率。那么它投资方法中的秘诀是什么呢？其实就是哈夫曼编码的原理，即通过每一次双倍砸钱（double down），把最多的钱投入到最容易成功的项目上。

我们还假设如果投资的公司最后能上市，将获得 50 倍的回报；如果上不了市，只是在下一轮融资被收购，将获得 3～5 倍的回报。在硅谷地区，获得投资的公司最终能上市的概率大约是 1%，大家不要觉得这个比例低，它已经比世界其他地区，包括美国硅谷以外的地区和中国，高很多了。至于被收购的概率，在硅谷地区大约是 20%，比中国要高很多。

如果使用第一种方法，基本上是拿到一个市场的平均回报，也就是一轮基金下来大约是 31% 到 71% 的回报，如果扣除管理费和基金本身拿走的分红，出资人大约能得到 20%～50% 左右的回报。通常一期风险投资基金投资的时间是 2～5 年（持续的时间可以长达 7～10 年），这样年化回报大约是 5%～20% 之间。这是硅谷风险投资的平均水平，大家不要觉得风险投资一定能挣钱，在中国，大部分风险投资基金是赔钱的，而在硅谷赔钱的基金的比例也高达 40%。

第二种方法，只投一家，这其实是赌博，如果碰上这家公司上市，有 50 倍的回报，碰上被收购的有 2～5 倍的回报，但是绝大多数情况则血本无归。如果所有的基金都玩这样的赌博，虽然平均回报率和第一种情况相似，但是投资风险高达 500%。根据投资领域普遍采用的夏普比率（请回到《硅谷来信》查看第 145—148 封信）来衡量，这是极为糟糕的投资方式。

第三种方法是按照哈夫曼编码的原理，可以先把钱分成几部分逐步投入下去，每一次投资的公司呈指数减少，而金额倍增。具体操作方法如下：第一轮，选择 100 家公司，每家投入 25 万美元，这样用掉 2500 万美元；第二轮，假定有 1/3 的公司即 33 家表现较好，每家再投入 75 万美元左右，也用掉 2500 万美元。至于剩下了的 2/3 已经死掉或者不死不活的公司，千万不要救它们，更不要觉得便宜去抄底；第三轮，假定 1/10 的公司，即 10 家表现较好，每家投入 250 万美元，再用掉 2500 万美元；第四轮，假定 3% 的公司，即 3 家表现较好，每家投入 800 万美元左右，用掉最后的 2500 万美元。

当然大部分人不会去参与风险投资，但是这种分配资源的原则在哪儿都适用。我在之前《Google 方法论》中介绍 Google 和 Facebook 等公司的管理方法时讲到，它们内部其实是一个大风投，各个项目一开始都有获得资源（主要是人力和财力）的可能性。但是很快，通常是三个月到半年，类似的项目就要开始整合，资源开始集中到更有希望的项目上去。最后能够变成产品上市的，是少数项目，但是大量的资源投入在其中了。这样既不会失去新的机会，也不会浪费资源。

今天的华为养了一个拥有几万人的庞大的预研部门，很多人觉得这是有了钱之后嘚瑟浪费，但是你可以把它看成是一个内部的大风投，每一个前期研究，都得到一定的发展机会，而投入的资源并不需要太多，最后能够进入到获得巨大资源攻坚阶段的项目，终究是少数。

这个道理对个人来讲也是适用的。美国有名的私立学校哈克学校的前校长尼克诺夫博士讲，在孩子小时候，要让他们尝试各种兴趣爱好，但是最终他们要在一个点上实现突破，他将这比做用圆规画圆，一方面有一个扎得很深的中心，另一方面有足够广的很浅的覆盖面。

一方面我从来不排斥尝试新东西，这样不会失去机会，我尝试过的各种事情远比外界知道的多，只是绝大部分失败了，我没有继续罢了，大家也就无从知晓了；但是，另一方面对于花了一些精力，看样子做不成的事情，我是坚决做减法止损，这样可以把最多的资源投入到我擅长的，有兴趣的，可能也是成功率最高的事情上。

## 0105矢量化象形文字和拼音文字是如何演化的.md

就说英文单词吧，用 5 个字母就能拼出 1200 万个单词，即便扣除掉 iiiii、jjjjj 这种不合理的，也能拼出几十万个看起来很「合理」的英文单词，要是这样的话，所有单词只有 5 个字母，大家不就不需要背那些很长的英文单词了吗？对这个问题简单的回答是，语言和文字是慢慢演化过来的，而不是人为利用信息论的编码原理刻意构造的，因此不可能只照顾易辨识和有效性，而不考虑人类接受它们的难度，以及演化的过程。

相反，人们给计算机识别的单词，比如汇编语言的指令代号，基本上就是很短的、等长的字母组合，因为那是完全利用编码原理人工设计的。当然，在人类文字演化的过程中，也无意间用了一个信息论的原理 —— 信息的矢量数字化（也被称为 VQ），或者简单地讲就是矢量化。我们就从文字和语言的演化过程，来谈谈这个原理以及它们的意义。

人类在进入到文明社会时，活动的范围越来越大，需要记录的信息越来越多，人类就开始通过动词和名词的组合来表达复杂的意思。但是新概念、新事物还是不断地涌现，人类只好造出更多的象形文字，这就如同今天人们不断创造新词一样。信息越多，需要的编码越多，这是文明自然演变不可避免的过程。

太多不同的编码（文字）出现后，就要对编码进行简化，否则大家就没法学习了。而简化的自然过程，就是矢量化的过程。那什么是矢量化呢？你一定有这样的经历，就是把一张图片放大再放大，通常就会模糊，出现马赛克甚至锯齿。学计算机的人知道，计算机中使用的字体有位图（bitmap）和矢量图两种。位图一经放大就会出现锯齿，而矢量图随便放大，都很清晰。这是怎么做到的呢？我们先从信息的矢量化说起。我们假定有一些几何形状，它们具有不同的颜色。比如下面这张图：

这些基本的图形彼此有一些相似性，但是又不完全一样。我只画了十四个不同的形状，当然真实的情况是它们可能有成千上万个。这么多图形，我一个个描述太复杂，于是我们就把这成千上万个彩色的形状，按照颜色和形状两个维度各四种情况，分到了 16 个格中。这样，所有的图形，就被归为了 16 类。当然，其中还有四个格子没有信息，因此可以看成是不存在的。这便是矢量化的原理。

这个用坐标分类，概括多种形状，就是形状的矢量化过程。当然，如果我们分类所概括的是信息，不是图形，道理是一样的。为什么这种特殊的归类过程，我们称之为矢量化呢？因为当我们把杂乱无章的信息投射到两个维度之后，两个维度坐标可以决定平面上的一个矢量。

比如在上面的例子中，要找圆形和椭圆形的蓝色图形，就用下面这个从原点出发到（4，2）坐标的矢量来表述：

当然，通常将信息投射到两个维度是不够的，根据应用场景会投射到多个维度中，这样的过程就被称为矢量化。人类象形文字的演化，实际上就是这样一个矢量化的过程。我们不妨先看看各种象形文字演化的过程。文字演化的第一步是抽象化。下面一张图描述了美索不达米亚文字（上）、古埃及文字（中）和古代中国的文字（下），对「鱼、鸟、戚、矢（「有的放矢」的「矢」，也就是箭）、壶」这五个字抽象化的过程。

你可以看出最初的文字和真实的物体非常相似，但是这些象形文字彼此之间缺乏共性。但是后面逐渐地，它们就被抽象化成一些直线或者弧线了。在中国和美索不达米亚，由于早期的文字是刻在金石、竹木和泥板上，因此，更多被抽象化成点和线的组合，这样便于刻写。而古埃及是写在莎草纸上的，能够使用曲线书写，因此多用曲线进行抽象化。但不管怎么样，抽象化之后，就可以总结出共性了。接下来第二步，我们就以汉字为例来说明矢量化的过程。绝大多数汉字被映射到两个维度上，即一个表意的偏旁维度和一个提示读音的发音维度，有些时候，提示读音的维度本身也表意。再往后，表达含义的偏旁已经和原来的图画不太像了。而这些偏旁就构成了文字的基本单元，而且慢慢固定下来了。以后有新的概念需要创作出新字时，使用那些基本单元，即偏旁部首，重新组合就可以了。

拼音文字的简化主要是围绕读音进行的。在美索不达米亚人发明了楔形文字后，它很快就由象形文字变成了拼音文字。但是那些拼音文字并不简单，每一个表达意思的拼音其实是一堆很复杂的小箭头（很像楔子，所以也被成为楔形文字）。后来楔形文字被当地的闪米特人学会了，他们中间有一支非常善于远洋经商的族群 —— 就是腓尼基人。

腓尼基人将美索不达米亚的文字传播到地中海各岛屿。但是，在经商途中，商人们可没有闲情逸致刻写精美漂亮的楔形文字，于是他们对这种复杂的拼音文字进行了进一步简化，就剩下几十个字母了。可以讲，从复杂的楔形文字，变成简单的几十个字母，是一个巨大的进步，它使得人类学习读写变得很容易。再后来希腊人从腓尼基字母中总结成 24 个希腊字母，而罗马人又将它们变成 22 个拉丁字母。

随着罗马的扩张，征服了很多外国土地，吸纳了很多外国人，有些外国的人名和地名就无法表示了，于是罗马人在字母表中加入了 x，代表所有那些无法表示的音和词，这既是英语里包含 x 的单词特别少的原因，也是后来人们用 x 表示未知数的原因。再后来拉丁文里的 i 被拆成了 i 和 j 两个字母，v 被拆成了 u,v,w 三个字母，最终就形成了今天英语的 26 个字母。

今天欧洲其它的拼音文字大多源于拉丁语，虽然它们字母表的多少略有区别，而且读音不同，但是写法上相似，因为同一种写法表达的是同一条信息。虽然象形文字和拼音文字的形成和进化代表了两种不同的信息编码方式，但是它们都利用了信息论中矢量化的原理。

在欧洲的拼音文字中，虽然没有表达意思的偏旁部首，但是有很多词根，前缀和后缀起到了表达意思的作用，也就是说这些语言实际上将表达信息的基本单元（单词）用一个词根、前缀、后缀这样三维的矢量表示了。于是，稍微有些语言基础的人，可以猜出一些没见过的单词的含义。正因为这个原因，拼音文字比汉语容易学。

在近代史上，曾经有不少学者提出过将汉字改为拼音文字，但其实这是不可行的。比如你把计算机变成 jisuanji 这几个罗马字母，它完全没有词根、前缀和后缀，因此猜不出意思。信息的矢量化这件事应用的场景非常广，前面提到的矢量字体就是一个，它的原理是将字体的轮廓映射到一组曲线上。在显示（和打印）时，经过一系列的数学运算，恢复字体的形状。这一类字库不仅占用空间小，而且从理论上可以被无限地放大，笔划轮廓仍然能保持圆滑，非常美观。

此外，矢量化在生活中也有应用，比如我们通过高考成绩录取大学生，或者通过身高选拔篮球运动员，其实就是利用矢量化的原理，只不过是将所有的人映射到了一维的空间中。这种做法给工作带来了极大的便利性，但是显然没有全面地考察每一个人，或者说有信息的损失。所以，在信息论中，一个更有普遍意义的问题就是，矢量化会带来多大的信息损失，关于这一点，在信息论中有一套理论计算这种损失。而在工程中大家要做的事就是，如何平衡便利性和信息上的损失。人在年轻的时候，总是会想两者兼而有之，学习了各种科学知识后，就知道这种事情在理论上是办不到的。

从文字的演变，介绍了信息的矢量化这个概念，以及它的应用。我们进而讲述了，无论是象形文字还是天然形成的拼音文字，都通过两到三个维度的矢量化兼顾了读音和达意的关系。但是，如果强制将中文拼音化，它将失去达意的功能，这不符合信息论的原则，因此做不下去。世界上人为想做的，但违背规律的事情，做起来总是困难重重。在生活中其实也有很多矢量化的例子，它们让问题变得简单，但是会丢失信息，而平衡便利性和信息的完整性，就成为了艺术。

矢量化通过老师图形的例子和金助教画圆的例子，我觉得矢量化应该是提炼的一种算法吧，尽管不能确保信息的完整性，却能在简洁的算法下，确保能再次按照这样的规则再次重现信息。

## 0106冗余度史记和圣经哪个信息量大.md

介绍了信息冗余度的概念，并且通过冗余度证明了汉语是最简洁的语言，但是同时也说明了因为汉语的冗余度太低，理解起来比较困难，因此难以学习；介绍了冗余度带来的三个好处：易理解、消歧义和容错性。但是信息冗余也带来了问题，一方面它造成信息存储和传输的浪费，另一方面它在有噪音的情况下，可能导致混淆。

《圣经》的英文版有 80 万个英文单词，扣除掉空格和标点符号，存下来接近 4MB。你可以认为将它们用今天计算机常用的 ASCII 编码书写后，长度是四百万字节。然后我又找到《圣经》的中文版本，我用的是比较流行的「和合本」，原文有 93 万多字。和合本虽然是白话文，但是翻译得极为紧凑，一点废话没有。如果用国标汉字存储它，接近 2MB，也就是说大约只有英文文本的一半。

当然学过计算机的同学会说，国标编码本身就比英文的 ASCII 编码紧凑，事实也的确如此，考虑到这个因素，我挤掉国标和 ASCII 编码中的水分，中英文编码长度的差距大约缩小 30%。即便如此，汉语的文本依然可以比英语的短 30% 多，也就是说汉语的文本和英语文本在公平的基础上作对比，长度是 2:3 左右，具体讲是 1.6MB vs 2.5MB。

那么接下来的一个问题是，不同语言的《圣经》是否信息量同样大呢？我采用了课程第 7 讲讲到的哈夫曼编码对《圣经》进行压缩，中文和英文大致都能压缩到 750KB，当然如果我们尽可能地利用上下文的相关性，可以进一步压缩，但这不是我们的目的。我们只是想证实一本经典的信息量不会因为使用不同语言书写而不同，这其实也证实了我们前面讲到的编码的等价性（不同语言可以被看成是不同的编码），即同样的信息采用不同的编码，信息量是不变的。

从这些数据可以看出，英语的压缩比高达 3:1（2.5MB 到 750KB），而中文大约为 2:1（1.6MB 到 750KB），也就是说中文更精炼。

因此，我又用中国自己的经典《史记》做了一次信息压缩的实验。《史记》大约有 53 万字，如果直接按照国标码存储大约要 1.1MB，当然我们说了国标码本身在编码上是有水分的，因此在挤掉水分后，《史记》的编码长度应该是 900KB 左右。如果采用上面同样的哈夫曼编码程序对它压缩，压缩完不到 500K，压缩比大约为 1.8:1，和中文《圣经》的压缩比是差不多的。

那么结论就来了，从上面的例子可以看出，中文的信息比较「密集」，相比之下，英文（和其它欧洲语言）比较「稀疏」。在信息论中，我们采用一种叫做冗余度的概念对信息的这种「密集」和「稀疏」程度进行描述。冗余度是这样定义的：

	（信息的编码长度 - 一条信息的信息量）/ 信息的编码长度

我们知道信息量其实就是按照信息熵计算出来的。

在上述例子中，中文的冗余度大约是 1/2，英文的冗余度为 2/3，如果对其它书籍的双语文本作同样的对比，也能得到类似的结果。因此，中文简洁是完全有科学根据的。接下来可能你会想，简洁不是一件好事么？冗余不是应该尽力去消除吗？如果我们原本需要读 5000 字节的内容，现在只需要读 3000 字节，那不是省时间了么？其实不然，因为如果冗余度太低，会严重影响接收信息的速度。比如我们对一篇经典的论文和一篇小说进行压缩，就会发现小说的冗余度要高得多，但也正是因为如此，小说才容易阅读。

相比之下，像沃森和克里克描述 DNA 双螺旋结构的论文，一共一页纸多一点，几乎每一个单词都不能漏掉，理解起来反而要花一点时间。也就是说，对于同一种语言，不同题材的文章，信息的冗余度差很多。如果对小说进行压缩，压缩比要高不少，也就是说小说的冗余度极高。

除了便于理解，冗余度的第二个好处是，在语言学上它消除了很多歧义性。汉语简洁的一个重要原因，是对比英语，汉语去掉了动词的各种时态、性别、单复数，和语气等信息，名词去掉了数量和阴阳信息，绝大部分名词去掉了正式和非正式的信息，所有这些信息都需要通过上下文来恢复，这其实就花工夫了，如果恢复得不好，在意思的理解上会略有差别，这就造成了误解。

相比之下，极为严谨的拉丁语和法语则没有这个问题。比如在英语中，名词和动词数量的一致性，语句中语气和写法的一致性，都保证了相应的信息不容易漏掉。这都归功于它们的冗余度大。

冗余度的第三个好处是：带来信息的容错性。我们有这样一个经验，如果你的朋友给你发送了一个文本文件，这个文件丢失了一段，你依然能够得到大部分内容，甚至能够通过前面或者后面的内容恢复出一部分丢失的内容。但是，如果他把文件压缩成 Zip 格式了，而压缩后的文件少了一点，你就惨了，完全无法恢复其中的内容。

一块石碑上刻有三种语言，从记录信息的效率上讲是极低的，信息冗余度至少是 2/3，但是正是因为有信息冗余，才能够利用另一种信息找回原来丢失的信息。当然，凡事有一利必有一弊，信息冗余自然有它的问题。

首先，大家能够直接感受到的就是在存储和传递信息时的浪费。可以想象，如果你存储的文件，编码的长度是信息量的好几倍，肯定是浪费。当然有人可能在想，现在存储器便宜得很，网速也很快，冗余就冗余吧，浪费不了太多钱。但是要知道文字的冗余度是在各种信息中非常低的。如果你传输标准的 4K 电视，对于任何信息冗余，一点也不压缩，那你的网速需要每秒钟 12Gbps，也就是采用光纤入户后峰值传输率的大约 10 倍，今天家庭使用的 Wi-Fi 的 200 倍左右。当然，你今天能收看 4K 电视，是因为通常这种视频图像的信息冗余度极高，压缩几十倍也不会损失任何信息，如果允许略微损失一点信息，则可以压缩上千倍。

信息冗余的第二个问题是，如果在信息中混有噪音，过多没用的信息可能会导致错误。我们在前面讲述信息时，都是假设它是准确无误，没有噪音的。但是在真实的世界里，很少有绝对干净的信息，它们总是混有噪音的，这些冗余的信息就可能彼此矛盾，这反而让大家糊涂。

那么这些原理和我们的工作有什么关系呢？善用信息冗余会帮助我们成为沟通的高手，我根据自己的经验，有这样三点体会供你参考：

讲东西时要通过加入一些看似是废话，但是实际上是从侧面诠释你的想法的句子，帮助对方理解你的意思。比如我常说「换句话说」，「比如说」，「从另一方面讲」这样的话，这就是利用信息的冗余便于大家理解。

讲东西要有一致性，不要补充有可能和主要思想相矛盾的例子，或者和想法无关的冗余信息。

在我们脑子存储信息时，要进行压缩，这样脑子才记得住事情。很多人问我，你读那么多书，记那么多事情，怎么记得住的？其实我脑子的记忆力并不好，别人五分钟能背下来的英文单词，我 15 分钟也记不住。但是我无论读书，还是学习，都会做类似于写卡片的工作，也就是说，把这一本厚厚的书的内容，变成薄薄的几页纸的东西，那些冗余的信息，就删除掉了。我有时讲，读书要不求甚解。这不是说不读懂，而是说要读出主线，将一些细节过滤掉。真到了需要寻找细节时，大不了回过头来再看看就好了。

#### 黑板墙

回归到学习习惯，我认为我们要掌握一种能力，就是压缩有用的、冗余的知识，并储备在我们的脑子里。接下来你需要时常的把知识取出来，换一种自己的视角，解读给别人。

万维钢老师的课程里介绍过亚利桑那大学和布朗大学的最新研究结果，神经网络训练的时候，给神经网络学习的数据中采用 85% 的熟悉信息和 15% 的意外信息，神经网络的学习效率是最高的。人脑在本质上也是一个神经网络，那么人接受信息时最好的比例也应该是：85% 的熟悉 + 15% 的意外。作者在写文章的时候应该有对象感，在了解读者真实熟悉的东西的基础上，在文章里给读者提供 15% 的惊喜感。

冗余度的概念，让我想到一个类似的例子，那就是我们熟悉的概念 —— 学习区。人们对外部的认识可以画出一个个同心圆，越往外越生疏、越往内则熟悉。最靠近我们的一圈是舒适区，往外一层是学习区，再往外就是恐慌区。舒适区称不上学习，过于熟悉、毫无新意的素材让人哈欠连连；恐慌区也不适合成长，天书一般、宛如溺水的知识量教人无法吸收。而有点熟悉加上意外，就是令人喜爱的学习区。信息的效率也该找到平衡。过于冗余则浪费存储和运输，过度简洁则容易丢失信息。因此，站在接受信息来讲，怎么写文章才能让大家接受的效率最高呢？那就首先要看「受众」。受众的教育背景、时空环境，都会影响接收程度，你不能奢望对着普罗大众谈核分裂的反应式，也无法期待只懂法文的人会对中文笑话有共鸣。最重要的就是受众，瞄准你要传递信息的对象，剩下才是冗余和简洁的平衡艺术。

香农指出过，任何信息都存在冗余。而此间的冗余大小，与信息中每个符号「例如字母，数字」的出现概率，或其不确定性相关。冗余度是信息交换的桥梁基柱。通信系统中：信源编码是降低信号中的信息冗余度的编码，旨在提高效能；信道编码是提高信息冗余度的编码，旨在提高可靠性。

单纯从接受信息的角度讲：第一，文章需要标注主题转变，告知读者重点概念的过渡。第二，需结合日常语言和叙述修辞。比如：对话，小故事，一定的幽默。偏好故事的人可以利用叙述本身的冗余度，代入己身，进行思考。第三，正反或极端举例，比如：光照下或黑暗里植物会如何；查字典时说明 a 和 z 时应一个从前一个从后找。最后，如果注意整体逻辑框架，信息的传输效率更高。

## 0107等价性信息是如何压缩的.md

当然，这是一桩史学界著名的悬案，目前也没有一个定论，我只是用自己的推导方法为你演绎了一下，面对错综复杂的信息时，如何利用其他信息的等价性为我们理清思路。而信息等价性的应用，在今天尤其广泛，对于指导你处理复杂信息会很有帮助。

在很多时候，我们直接得到一种信息，或者原封不动地保留一条信息并不容易，但是却可以从等价的信息中导出所要的信息。当然，这样倒手一次的操作需要一个桥梁，让原有的信息和等价信息一一对应。在信息科学中，最著名的桥梁就是傅立叶变换了。傅立叶是十九世纪法国的数学家，他发现任何周期性的函数（信号）都等同于一些三角函数的线性组合。下面这张图，就是周期性函数的样子，也就是说它们的波形都是重复的。

一般来讲，我们生活中的各种信号，都是随着时间变化的，比如一年中每一天的温度就是一个信号，它从每一年的第一天到第 365 天会有高有低地变化，如果我们把历史上全部温度的记录画成一条曲线，它大致就是上图那种周期性函数，一个周期就是一年。如果我们要记录 100 年间每天的平均气温，就需要三万多个数据，这个数据量比较大。但是由于它具有周期性，我们就有可能利用这种周期性来进行信息压缩。而对于这一类波动信号，信息压缩的基本原理大致如下：

1. 找到这种周期性信号的等价信息；

2. 对等价信息进行压缩；

3. 如果要使用原来的信号，通过压缩后的等价信息复原原来的信号。

这里面的关键，是找到等价信息。对于周期性的信号，等价信息就是一组正弦（或者余弦）波。正弦波的性质如下，大家可能并不陌生，因为它是最典型的波动曲线的性质。

世界上所有的正弦波曲线形状都差不多，但是振动的幅度可大可小，振动的频率可高可低。比如下面这张图中的正弦波显然振动的频率就比较高，用句俗话讲，它抖动得特别快。

19 世纪初，法国数学家傅立叶发现所有的周期性信号都可以用频率和振幅不同的正弦函数叠加而成，也就是说周期性信号里面所包含的信息和若干正弦函数的频率、振幅信息完全等价，这种变换被称为傅立叶变换。

如果利用傅立叶变换，可以将 100 年里温度变化的信息用大致 20 根频率和振幅不同的正弦曲线叠加而成。也就是说，100 年里 3 万多个温度样点里的信息，基本上就等价于 20 个频率数据和 20 个振幅数据，这样一来信息就被压缩了近百倍。今天音频、图像和视频的压缩，就是利用这个原理。其中的关键就是找出那个等价的信息。

今天我们见到的各种音频信号，包括我们的语音、音乐等等，在较短的时间内，都有相对稳定的周期性，比如下图就是一段语音，你可以看出它有一定的周期性。利用傅立叶变换，可以对语音进行压缩编码，然后传输，这样可以将语音信息压缩 10 倍左右，当然这样可能会有很少的信息损失，这一点我们后面再讲。但不管怎样，这样的信息压缩是非常合算的，比如用微信语音打电话，如果不进行信息压缩，可能要多用十倍的数据流量。

那么图像又是怎么压缩的呢？它们看上去不像是有周期性振动的波形啊。这其实只是我们在宏观上看一幅图，但是如果我们用放大镜把图放得特别大，看到的就是一个个像素，而且相邻的像素之间颜色和灰度的变化会是相对连续的。利用这个特性，人们发明了一种被称为「离散余弦变换」的数学工具，也称为 DCT。DCT 可以被认为是傅立叶变换的延伸，只不过它没有使用正弦波，而是采用了下面图中所示的 64 个基本灰度模板，任何照片都可以用这些模板组合而成。当然，对于彩色图片需要用带有红绿蓝三原色的彩色模板。这样一幅图片，就变成了一组数字，这些数字是模板中相应的模块的权重。我们经常使用的 JPEG 格式的图像，就是这么生成的。

通过上述语音和图像的压缩，我们介绍了信息等价性的应用。很多时候，一种原始的信息，它们虽然里面有很多冗余成分，但是很难直接压缩掉。但我们可以将它们转化为容易压缩的等价的信息，再进行压缩，然后进行存储和传输。在使用和接收到被压缩的等价信息后，我们先解压，再恢复回原来的信息。

不仅每一篇文章，每一段语音，每一个图片可以利用信息的等价性分别压缩，将很多相同形式的内容放到一起，还能进行更有效的压缩。之前有读者问我，在 Google 上什么东西都能够查到，难道它保存了互联网的所有的内容？这听起来难以置信。其实 Google 还真这么做了，只不过它在向大众服务时，把所有网页中的文字顺序打乱了，它按照每一个关键词在网页中出现的位置重新整理了互联网的内容。这样不仅方便查找，而且能够压缩信息，节省存储空间。这样当你查找时，它不仅能够告诉你你要找的内容在哪里，还能够根据每一个词出现的位置，恢复出原来的网页展现给你。这就是等价性在信息处理中的应用。

善用等价信息，是我们这个年代每一个人都必须掌握的工作技巧，这是我们这讲最希望你记住的一个知识点。比如说我们无法看清人体内部的情况，但是我们知道人体内有很多水分，水里有氢原子，它的电子在旋转中形成一个个微小的磁针，我们在人体外面施加磁场，就可以把水分子里的小磁针方向给排顺了，然后我们加入一个能够和水中氢原子共振的脉冲，就可以把人体氢原子振动的信息取出来。由于人体各个部分水的分布不一样，我们通过各个部分氢原子振动的信息，就可以把人的结构画出来。这就是核磁共振的原理。因此核磁共振就是利用了等价信息。

类似地，检测引力波的 LIGO 装置，检测希格斯玻色子的 ATLAS 装置，用的也是等价信息。我们今天在医院里做的大部分血项检查，都是在用等价信息。

#### 黑板墙

在此帮助大家厘清思路：为什么要讲信息压缩与等价性？这是因为我们前面几讲学到的基本编码之后还只是第一个动作，接下来我们要思考如何把信息高效传递出去。

## 0108信息增量信息压缩中的保守主义原则.md

首先，我们讲了善用信息前后的相关性，对于后面的信息做增量编码，达到大幅度压缩信息冗余的目的。其次，我们把这种信息处理的方式，和保守主义的做事方法作了一个对比。所谓保守主义，其实就是坚持总体原则不变，不断作微调，达到渐进改变的目的。这样做，比每一次都推倒重来，或者干脆达不成一致，其实效率反而高，因为我们的世界在绝大多数时候都是渐变的。

说到信息的压缩，大家可能会有这样一个体会，就是视频的压缩比要远比图片的高很多。大家的这个观察是完全正确的，它们通常会相差两个数量级，也就是说 JPEG 图片能压缩 10 倍基本上也看不出损失，而 MPEG 视频能压缩近千倍，肉眼也分辨不出来是压缩过的。

那么能否用视频的压缩方式压缩图片，达到上千倍压缩的效果呢？简单的答案是不能，因为视频压缩时，利用了信息的相关性，能够采用所谓的增量编码，而单一一张图片中，不具有太多的相关性可以利用。所谓利用相关性进行压缩编码，简单来说就是如果两个信息「长得很像」，只要保留一个，对另一个，只要保留它们的差异，然后进行微调就行了。

我们可以利用这个特性进行压缩编码了，具体的做法是这样：

1. 对第一个数字使用 12 比特的编码，我们没有办法做得更精简。

2. 对第二个以后的各个数字，我们将它和上一个数字相比较，发现它相比前一个数字，动态变化的范围在正负 16 以内。因此，我们只需要对差异（也被称为增量）进行增量编码，就可以了。对于这些增量，如果不考虑符号的话，我们用 4 个比特就够了，因为 log 以 2 为底 16 的对数等于 4，也就是 2 的 4 次方等于 16。当然，增量可以是正，也可以是负，我们再加一个比特的信息表示符号，于是从第二个数字开始，我们采用 5 个比特就可以表示它和前一个数字的区别了。

今天对于视频的压缩，用的就是上述原理。我们知道一般的视频一秒钟有 30 帧，高清的是 60 帧，4K 的是 120 帧（甚至 240 帧）。每一帧视频之间的差距其实极小。我们对第一帧视频（也被称为主帧）进行全画面编码，对于这一帧的压缩比，其实不会太高。但是对后面每一帧的视频，只要针对它们和上一帧的差异进行编码即可，这样除了主帧外，后面的每一帧的视频，其实编码的长度非常短，视频文件就显得比较小。

我们上一讲说到的 Google 搜索所用的索引，其实也用到了前后相关性进行压缩。搜索引擎的索引是什么东西呢？它是把每一个单词在全部网页中出现的位置列出来。比如「中国」出现在第 50001，50008，50300 等位置，「科学」出现在 50009，50045 等位置。

由于互联网上的网页数量巨大，单词的位置如果从头到尾排一个序，大约要排到几百亿，这些数字就很大。Google 的做法是每一个网页只保存第一个单词的起始位置，剩下的单词是相对第一个单词的位置。比如某个网页起始的位置是 50000。那么刚才我说出现「中国」这个词出现了三次，它的索引记录就的是 50000，以及位移量 1，8 和 300，「科学」这个词相应的一段索引，记录的是 50000，和位移量 9，45。这样就能有效压缩信息的长度。

在搜索时，如果要找同时包含「中国」和「科学」的网页，只要看看它们是否有共同的网页起始位置即可，比如它们出现在了起始位置为 50000 的网页中。如果非要找「中国科学」（连起来的）这个词，除了保证它们在同一个网页中出现外，还要保证它们的位移量相差正好是 1。因为在这个例子中，「中国」的位移量 8 和「科学」的位移量 9 正好差 1，我们就知道它们是相邻的了。

通过这种方式，网页搜索的索引可以在很大程度上节省空间（大约节省 75%），而且这种信息压缩是无损的。

当然，凡事有一利就有一弊。正如我们前几天所讲，当我们把信息冗余都挤掉后，编码长度非常短时，容错的性能就会下降。你过去看影碟可能有这样的体会，当光盘被划了一道，它就经常跳盘，这就是因为视频的压缩是前后相关的，中间坏了一点，很多帧的视频就都看不了了。为了防止这样编码造成的累积误差，也为了防止中间有一点点信息损失，后面的视频统统打不开，所以，每过若干帧，我们就要重新产生一个主帧，以免错误会传递太远。

信息的前后相关性，其实是信息本身固有的特征。或者说，绝大多数时候，我们这个世界的变化是渐进的，而不是完全随机的。不仅在信息的世界如此，在我们的生活中也是如此。我在过去的专栏里讲了保守主义的做事态度，它的好处其实是由我们这个世界渐变的特征决定的。因此，在绝大多数时候，我们不需要推倒重来，只需要对变化进行一些修补就好了。有些人看不起总在修修补补的做法，觉得缺乏革命性，但是从信息论的角度讲，保守主义的做法成本最低。

在美国生活过的人，一开始会发现两个难以理解的现象。

第一个是美国的税法很复杂，每年报税是一个工作量很大的任务。那么为什么要把税法搞得那么复杂呢？这就是利用增量进行修修补补的结果。每一个群体都有自己的利益，都想要尽可能让自己能够多免税，于是各方博弈，在原有的税法上不断修补，就成了今天的样子。这样经过长时间迭代，总算各方面相对满意。当然，过了很长时间，一些税法跟不上经济和社会发展了，就要作大的调整，这就如同视频压缩时，一旦新的画面出现，就要重新开始一个主帧一样。

第二个现象是学区划分得犬牙交错。这也是为了平衡各方面利益不断修补的结果。

因此，不要根据「保守主义」这四个字就认为英国人和美国人保守，它其实更多地反映在这种渐进的做事方式上。甚至美国的宪法也是通过修正案的方式在作微调，而没有像法国和德国那样几次彻底修改宪法。正是因为渐进，牵扯的利益不会太多，才能够推行得下去，从长期来看才能发展。

如果想一次完成巨大的突变，常常会因为牵扯的利益太多，最后总是搁浅，永远改不了，结果反而是不进步。这就如同我们如果非要对一个 2 小时的电影每一帧都保留全部的信息，那么一部电影的数据会大得在网络上无法传输，我们的计算机播放电影就会不断卡壳，我们看到的画面反而不如压缩的清楚。

## 0109压缩比和失真率如何在信息取舍之间作平衡.md

信息的压缩分为有损的和无损的两种。对于无损的压缩，原先的信息能够完全复原，但是通常压缩比不会太高，因为它存在一个极限，就是香农第一定律给的信息熵的极限。对于有损的压缩，信息复原后，会出现一定程度的失真。通常失真率和压缩比直接相关，压缩比越大，失真率越高。采用什么样的压缩方法，压缩到何种程度，通常要看具体的应用场景。在信息处理这个领域，常常不存在所谓的标准答案和最佳答案，只有针对某个场景的好的答案，而一切都是妥协的结果。信息压缩看似是信息处理专业的问题，但是它的思想可以用到很多地方。我们有时强调要把知识学通，就是这个道理。

我们在前面讲了香农第一定律。香农指出，任何编码的长度都不会小于信息熵，也就是通常会大于等于信息熵，当然最理想的就是能等于。如果编码长度太短，小于信息熵，就会出现损失信息的现象。

因此，信息熵是告诉信息处理的人，做事情的边界，就如同不能试图逾越热力学第二定律发明永动机一样，大家在压缩信息时，如果想要无损，就不能逾越香农给的这个边界。也就是说，如果一张图片里面有 10K 的信息，你再怎么压，也不会比它小，否则就会损失信息。

理解了这一点以后，我们就知道无论是语音，还是图像、视频，都有两类的压缩方式，一类就是无损压缩。比如我们昨天说的通过傅里叶变换和离散余弦变换将音频和图像信息变成频率信息，再用类似哈夫曼编码进行压缩，这是不会丢失信息的。另一类是要丢失一部分信息的，也被称为有损压缩。比如一幅图经过无损压缩已经被压到 10K 了，你还想压到 1K，那就需要有所损失了。因为在很多场合下，有损的压缩还是必要的。

事实上我们今天对于音频、图像和视频的压缩，绝大多数情况都是有损的压缩。而有损压缩最关键的是要清楚如何保证因为压缩而丢失的信息不影响我们对信息的理解呢？这就需要平衡压缩比和信息失真度之间的关系。

所谓失真度，其实通俗来说，就是压缩前、压缩后的两串信息的差的平方。也就是说，如果信息没有任何失真，失真率是 0。如果信息完全消失了，失真率是 100%。有了失真率的概念，我们就可以来讨论有损失的信息压缩了。

其实我们生活中大量应用到的都是有损压缩，只不过我们感受不到，这就说明压缩很好地考虑到了失真率。比如一个数码相机拍的照片，原图有 18MB 的大小。通常 JPEG 算法会将它压缩到 3～5MB，这其实会有信息损失，但是肉眼根本看不出的。你将它印成 10x8 寸的照片，只要你拍得清晰，每一个像素基本上都是清晰的。但是如果你将它压缩到 500K，再印成一样大小的照片，就能看出一个个小色块了，而且一些细节也消失了。同样的道理，如果我们将原始的语音压缩 10 倍，或许有失真，但是语音本身肯定是清晰的，还能听出是谁的声音，但是如果压缩 50 倍，语音内容可以辨认，但是却听不出是谁的声音了。

那么到底该压缩多少倍呢？接下来我就要为你介绍，压缩比和失真率平衡原则。在进行信息压缩之前，明确压缩的目的非常重要，你印一张证件照，图片压缩到 100K 也是可以接受的，你要做一张会展的海报，那就还是用原始文件比较好，不要做有损压缩了。通常，压缩比和失真率的关系会是下面这样一条曲线：

很多时候，我们都是在接受了某个失真度的情况下（也就是在上图中横着切一刀），然后再去尽可能找到好的压缩比。世界上很多时候没有最好的技术方案，只能根据场景找到合适的，因此做事的目的性很重要。这是第一个原则。我们还知道信息的作用是消除不确定性，那么反过来，丢失了一部分信息，一定会增加不确定性。用的信息少，永远不可能做得和原来一样好，这是第二个原则，大家一定要记清楚。

至于如何平衡数据量和效果的关系，就看矛盾的主要方面在哪一方了。如果说过去没有数据，牺牲掉一点性能，也就罢了。今天世界上最不缺的就是数据，在最近的三年里，全世界产生的数据，比三年前到有文字以来人类产生的数据的总和还多。在这种情况下，节省数据是一条错误的努力方向，这家公司不值得投资。

除了要考虑目的，考虑到信息数量之外，第三个原则是，在压缩信息时，有时要看应用场景。还是以语音压缩为例，在语音通话时，牺牲一定的讲话人的口音，问题不大，因为它的目的是传递话音中的信息。但是，在进行声纹识别时，情况就正好相反，那个人说了一句什么话不重要，重要的是知道他是谁。因此在后一种应用中，需要保留的是说话人本身的信息，反倒是他说的内容无关紧要。也正是因为应用的场景不同，才有了各种压缩算法。

接下来大家可能会有一个疑问，那么高比例的信息压缩到底是压缩掉了什么信息？

简单地讲，就是压缩掉了高频信息。进一步说，人通常能够听到 20 赫兹到 2 万赫兹的声音，但是人发音的范围只有 300 赫兹到 4000 赫兹左右，因此任何高于 4000 赫兹的语音信号，就被过滤了。虽然据说世界上音调最高的女性叫起来能到 1.7 万赫兹，这已经远远超过了小提琴的最高音了，但是对不起，为了压缩信息，我们不保留。今天的语音通信就是这么实现的。

对于图像也是如此，你可能注意到这样一个现象，如果你以蓝天为背景拍了一张照片，照片上有只很小的鸟，你如果用 JPEG 算法进行图像压缩，小鸟可能就被过滤掉了，而且蓝天的细节就没有了。这就是因为有损的图像压缩算法都是先过滤高频信息的。

我们在生活中有句谚语，叫做「枪打出头鸟」，其实在信息压缩中，总是遵守这个原则的，任何与众不同的东西，总是先被压缩掉，因为对那些与众不同的东西做编码，占用的空间相对太多。

信息压缩的原理，不仅在计算机存储、通信和信息处理中经常用到，还被用于了生物、金融等很多领域。根据华大基因创始人杨焕明教授的说法，如果把人以及他体内细菌（量是相当大的）的基因都测序，然后存起来，每个人要超过 1PB 的存储空间，也就是 1000 个 1T 的硬盘，这显然是一件成本极高的事情。2012 年，约翰·霍普金斯大学的科学家们发表了一种遗传压缩算法，在不丢失任何信息的情况下，压缩比达到了 1000 倍。这件事对于普及基因测序很有意义。

约翰·霍普金斯大学的科学家们是怎么做的呢？简单地讲，他们的方法和视频压缩的方法很相似 —— 考虑到人的很多基因是相同的，只需要存储有差异的基因即可。






