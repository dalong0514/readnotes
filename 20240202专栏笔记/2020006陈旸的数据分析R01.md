## 记忆时间

## 目录

## 0100数据分析基础篇

1、数据分析基础技能里包含 3 大块，数据采集、数据挖掘和数据可视化。数据分析修炼牢记两条原则：不重复造轮子和工具决定效率。

2、数据挖掘的知识清单，分别是数据挖掘的基本流程、十大算法和数学原理。挖掘的基本流程：理解商业、理解数据、准备数据、建立模型、评估模型和发布上线。数据挖掘的十大算法：分类算法、聚类算法、关联分析和连接分析。数据挖掘的数学原理：概率论与数理统计、线代、图论和最优化方法。

3、Python 基础知识介绍。

4、numpy 的基础知识，比如 ndarray 对象的概念、创建结构数组、ufunc 运算、创建连续数组、算术运算、统计函数以及 NumPy 排序。

5、pandas 的基础知识，比如其数据结构、数据的导入和输出、数据清洗操作（删除元素、重命令行名列名、去重复值、格式的转换）、数据统计函数、数据表的合并以及 SQL 和 pandas 的连接。

### 0101数据分析全景图及修炼指南.md

数据分析分成三个重要的组成部分：数据采集。它是我们的原材料，也是最「接地气」的部分，因为任何分析都要有数据源；数据挖掘。它可以说是最「高大上」的部分，也是整个商业价值所在。之所以要进行数据分析，就是要找到其中的规律，来指导我们的业务。因此数据挖掘的核心是挖掘数据的商业价值，也就是我们所谈的商业智能 BI；数据可视化。它可以说是数据领域中万金油的技能，可以让我们直观地了解到数据分析的结果。

在数据采集部分中，你通常会和数据源打交道，然后使用工具进行采集。在专栏里，我会告诉你都有哪些常用的数据源，以及如何获取它们。另外在工具使用中，你也将掌握「八爪鱼」这个自动抓取的神器，它可以帮你抓取 99% 的页面源。当然我也会教你如何编写 Python 爬虫。掌握 Python 爬虫的乐趣是无穷的。它不仅能让你获取微博上的热点评论，自动下载例如「王祖贤」的海报，还能自动给微博加粉丝，让你掌握自动化的快感。

数据挖掘。知识型的工程，相当于整个专栏中的「算法」部分。首先你要知道它的基本流程、十大算法、以及背后的数学基础。这一部分我们会接触到一些概念，比如关联分析，Adaboost 算法等等，你可能对这些概念还是一知半解，没有关系，我会详细为你介绍这些「朋友」。每讲完一个算法原理，我都会带你做一个项目的实战，我精选了一些典型的、有趣的项目，比如对泰坦尼克号乘客进行生存预测、对文档进行自动分类、以及导演是如何选择演员的等等。掌握了数据挖掘，就好比手握水晶球一样，它会通过历史数据，告诉你未来会发生什么。当然它也会告诉你这件事发生的置信度是怎样的，置信度这个词你先记住就可以了，后面我们来学习它具体代表什么。

数据可视化。这是一个非常重要的步骤，也是我们特别感兴趣的一个步骤。数据往往是隐性的，尤其是当数据量大的时候很难感知，可视化可以帮我们很好地理解这些数据的结构，以及分析结果的呈现。如何进行数据可视化呢？有两种方法。第一种就是使用 Python。在 Python 对数据进行清洗、挖掘的过程中，我们可以使用 Matplotlib、Seaborn 等第三方库进行呈现。第二种就是使用第三方工具。如果你已经生成了 csv 格式文件，想要采用所见即所得的方式进行呈现，可以采用微图、DataV、Data GIF Maker 等第三方工具，它们可以很方便地对数据进行处理，还可以帮你制作呈现的效果。数据采集和数据可视化的原理简单，容易理解。这两个部分注重的是工具的掌握，所以我会把重点放在讲解工具以及应用实战上。虽然这些理论我会给你一一讲解，但纸上得来终觉浅，绝知此事要躬行。手拿地图，我们知道要去哪里，但是怎么去呢？我认为学习数据分析最好的方法是：在工具中灵活运用，在项目中加深理解。

修炼指南。刚才我们讲了数据分析全景图，包括数据采集、数据挖掘、数据可视化这三个部分。你可能觉得东西很多，无从下手，或者感觉数据挖掘涉及好多算法，有点「高深莫测」，掌握起来是不是会吃力。其实这些都是不必要的烦恼。借用傅盛的话来说，人与人最大的差别在于「认知」，所谓成长就是认知的升级。很多人存在对「认知」的误解，认为认知不就是概念么？那么你有没有想过，针对同一个概念，为什么不同的人掌握的程度是不一样的呢？我们只有把知识转化为自己的语言，它才真正变成了我们自己的东西。这个转换的过程，就是认知的过程。

那么如何提升自己的学习吸收能力呢？简单地说，就是要「知行合一」。如果说认知是大脑，那么工具就好比我们的双手，数据工程师和算法科学家每天打交道最多的就是工具。如果你开始做数据分析的项目，你脑海中已经思考好了数据挖掘的算法模型，请牢记下面这两点原则。

不重复造轮子。举个数据采集的例子，我见过很多公司，都有数据采集的需求，他们认为某些工具不能满足他们个性化的需求，因此决定招人专门做这项工作。而结果怎样呢？做了 1 年多的实践，工资投入几十万，结果发现 Bug 一大堆，最后还是选择了第三方工具。耗时耗力，还没什么成效。一个模型是否有相关的类库可以使用 —— 这几乎是每个程序员入行被告知的第一条准则。我也会对新人反复灌输这个概念。大部分情况下你都能找到类库来完成你的想法。

工具决定效率。「不要重复造轮子」意味着首先需要找到一个可以用的轮子，也就是工具。我们该如何选择呢？这取决于你要做的工作，工具没有好坏之分，只有适合与否。除去研究型的工作，大部分情况下，工程师会选择使用者最多的工具。因为：Bug 少、文档全、案例多。比如 Python 在处理数据挖掘上就有很多第三方库，这些库都有大量的用户和帮助文档可以帮助你来上手。选择好工具之后，你要做的就是积累「资产」了。我们很难记住大段的知识点，也背不下来工具的指令，但是我们通常能记住故事、做过的项目、做过的题目。这些题目和项目是你最先行的「资产」。如何快速积累这些「资产」呢？这里我送你三个字：熟练度。把题目完成只是第一步，关键在于训练我们工具使用的「熟练度」。

高中的时候，有一次我做「八皇后」的问题，第一次解答花了一个小时的时间。当时老师明确告诉我必须在 20 分钟内完成，我不敢相信，从解题、思考、动手，最后完成，1 个小时不算慢。但是后来我调整了思考的结构。最后我 6 分钟就可以完成那道题。当熟练度增加的时候，你的思考认知模型也在逐渐提升。所以专栏中，我给你做了一个「专属题库」，在专属题库中你可以进行自我评测，当然我也会对这些练习题进行讲解。在工作篇中，我也会和你一起分享面试技巧、探讨职场上的晋升之路。

认知三步曲，从认知到工具，再到实战。记录下你每天的认知。尤其是每次课程后，对知识点的自我理解。这些认知对应工具的哪些操作。用工具来表达你对知识点的掌握，并用自己的语言记录下这些操作笔记。做更多练习来巩固你的认知。我们学习的内容对于大部分外人来说，就像「开车」一样，很酷。我们学习的内容，对于要掌握的人来说，也像「开车」一样，其实并不难，而且很多人已经上路了。你需要的就是更多的练习。

### 黑板墙

作者回复：知行合一，并不是个先后的顺序过程，就像你刚才说的：实战其实就是很好的学习，能让你理解工具使用和知识点。刚才有个同学分享了：自己写 Demo 是个非常好的体验。

1『Demo 应该指原型，待确认。』

对于软件技术的学习，我认为目标驱动法，或者叫项目驱动法效率比较高。当然这也是建立在具有明确目标的前提下，知道要做什么了。不知道具体目标的时候，能做一个 Demo，实践一下比较好。一个项目对于这个领域基础知识的需求可能并不全面，也就是说你做完一个项目，可能仅仅用了一部分知识，其余知识没有用到。但是有一点比较好，就是项目从启动到结束，对于这个领域的知识，是从浅到深的过程，或者说可以体会到理论如何运用到实践，这个会了以后，就可以举一反三，进而学习这个领域的其余知识，运用到其余项目。我不适合记忆，记不住，最近一段时间在想，如何能更有效的搜索以前的知识点。比如记录到有道云笔记上的知识，但我发现有道云笔记的搜索不好用，还不如 QQ 邮箱的搜索好用。我是「理解」型的学习方法，学习的过程中形成连贯的思维，然后等到需要用的时候，再去搜索。但有的时候脑海中的关键字和以前记的电子笔记不一样，所以如何贯通这个关键字，我正在考虑如何解决这个问题。比如每次记笔记的时候，旁边写上关键字。我还感受到过一种学习方法，就是「内心恐惧」学习法，这个时候会有强大的动力将其学会。

作者回复：掌握分 3 个阶段：第一个阶段，会用就可以了；第二个阶段，你发现还有很多可以调整的地方，开始探索算法的原理，各个参数如何优化；第三个阶段，自己会有新的想法，在原有的基础上进行创新。比如，前人在 ID3 算法基础上提出了 C4.5 算法，就是个很好的例子。所以说呢，第一阶段不需要太多数学基础，而随着阶段的提升，数学的价值也越来越重要。Anyway 慢慢修炼，先用起来。

作者回复：我觉得你的实战经验一定不错。也很感谢你对 MAS 方法的认可，就像你说的：授人予鱼，不如授人予渔。我觉得你可以换个维度，从工作场景和业务需求出发，尤其很多突破性的工作还是在于数据挖掘，或者说人工智能的部分。很明显，AI 这两年的热度要明显高于大数据，也说明这方面的突破比以往更快。所以，我建议你多了解不同行业的业务需求，在工具上可以更关注算法层面。

### 0102学习数据挖掘的最佳路径是什么.md

什么是数据挖掘呢？想象一下，茫茫的大海上，孤零零地屹立着钻井，想要从大海中开采出宝贵的石油。对于普通人来说，大海是很难感知的，就更不用说找到宝藏了。但对于熟练的石油开采人员来说，大海是有坐标的。他们对地质做勘探，分析地质构造，从而发现哪些地方更可能有石油。然后用开采工具，进行深度挖掘，直到打到石油为止。大海、地质信息、石油对开采人员来说就是数据源、地理位置、以及分析得到的结果。而我们要做的数据挖掘工作，就好像这个钻井一样，通过分析这些数据，从庞大的数据中发现规律，找到宝藏。

数据挖掘的知识清单，分别是数据挖掘的基本流程、十大算法和数学原理。

挖掘的基本流程。数据挖掘的过程可以分成以下 6 个步骤。1）理解商业：数据挖掘不是我们的目的，我们的目的是更好地帮助业务，所以第一步我们要从商业的角度理解项目需求，在这个基础上，再对数据挖掘的目标进行定义。2）理解数据：尝试收集部分数据，然后对数据进行探索，包括数据描述、数据质量验证等。这有助于你对收集的数据有个初步的认知。3）准备数据：开始收集数据，并对数据进行清洗、数据集成等操作，完成数据挖掘前的准备工作。4）建立模型：选择和应用各种数据挖掘模型，并进行优化，以便得到更好的分类结果。5）评估模型：对模型进行评价，并检查构建模型的每个步骤，确认模型是否实现了预定的商业目标。6）上线发布：模型的作用是从数据中找到金矿，也就是我们所说的「知识」，获得的知识需要转化成用户可以使用的方式，呈现的形式可以是一份报告，也可以是实现一个比较复杂的、可重复的数据挖掘过程。数据挖掘结果如果是日常运营的一部分，那么后续的监控和维护就会变得重要。

数据挖掘的十大算法。为了进行数据挖掘任务，数据科学家们提出了各种模型，在众多的数据挖掘模型中，国际权威的学术组织 ICDM （the IEEE International Conference on Data Mining）评选出了十大经典的算法。按照不同的目的，我可以将这些算法分成四类，以便你更好的理解。1）分类算法：C4.5，朴素贝叶斯（Naive Bayes），SVM，KNN，Adaboost，CART。2）聚类算法：K-Means，EM。3）关联分析：Apriori。4）连接分析：PageRank。

1）C4.5 算法是得票最高的算法，可以说是十大算法之首。C4.5 是决策树的算法，它创造性地在决策树构造过程中就进行了剪枝，并且可以处理连续的属性，也能对不完整的数据进行处理。它可以说是决策树分类中，具有里程碑式意义的算法。2）朴素贝叶斯模型是基于概率论的原理，它的思想是这样的：对于给出的未知物体想要进行分类，就需要求解在这个未知物体出现的条件下各个类别出现的概率，哪个最大，就认为这个未知物体属于哪个分类。3）SVM 的中文叫支持向量机，英文是 Support Vector Machine，简称 SVM。SVM 在训练中建立了一个超平面的分类模型。如果你对超平面不理解，没有关系，我在后面的算法篇会给你进行介绍。4）KNN 也叫 K 最近邻算法，英文是 K-Nearest Neighbor。所谓 K 近邻，就是每个样本都可以用它最接近的 K 个邻居来代表。如果一个样本，它的 K 个最接近的邻居都属于分类 A，那么这个样本也属于分类 A。

5）Adaboost 在训练中建立了一个联合的分类模型。boost 在英文中代表提升的意思，所以 Adaboost 是个构建分类器的提升算法。它可以让我们多个弱的分类器组成一个强的分类器，所以 Adaboost 也是一个常用的分类算法。6）CART 代表分类和回归树，英文是 Classification and Regression Trees。像英文一样，它构建了两棵树：一棵是分类树，另一个是回归树。和 C4.5 一样，它是一个决策树学习方法。7）Apriori 是一种挖掘关联规则（association rules）的算法，它通过挖掘频繁项集（frequent item sets）来揭示物品之间的关联关系，被广泛应用到商业挖掘和网络安全等领域中。频繁项集是指经常出现在一起的物品的集合，关联规则暗示着两种物品之间可能存在很强的关系。

8）K-Means 算法是一个聚类算法。你可以这么理解，最终我想把物体划分成 K 类。假设每个类别里面，都有个「中心点」，即意见领袖，它是这个类别的核心。现在我有一个新点要归类，这时候就只要计算这个新点与 K 个中心点的距离，距离哪个中心点近，就变成了哪个类别。9）EM 算法也叫最大期望算法，是求参数的最大似然估计的一种方法。原理是这样的：假设我们想要评估参数 A 和参数 B，在开始状态下二者都是未知的，并且知道了 A 的信息就可以得到 B 的信息，反过来知道了 B 也就得到了 A。可以考虑首先赋予 A 某个初值，以此得到 B 的估值，然后从 B 的估值出发，重新估计 A 的取值，这个过程一直持续到收敛为止。EM 算法经常用于聚类和机器学习领域中。10）PageRank 起源于论文影响力的计算方式，如果一篇文论被引入的次数越多，就代表这篇论文的影响力越强。同样 PageRank 被 Google 创造性地应用到了网页权重的计算中：当一个页面链出的页面越多，说明这个页面的「参考文献」越多，当这个页面被链入的频率越高，说明这个页面被引用的次数越高。基于这个原理，我们可以得到网站的权重划分。

算法可以说是数据挖掘的灵魂，也是最精华的部分。这 10 个经典算法在整个数据挖掘领域中的得票最高的，后面的一些其他算法也基本上都是在这个基础上进行改进和创新。今天你先对十大算法有一个初步的了解，你只需要做到心中有数就可以了，具体内容不理解没有关系。

数据挖掘的数学原理。说了这么多数据挖掘中的经典算法，但是如果你不了解概率论和数理统计，还是很难掌握算法的本质；如果你不懂线性代数，就很难理解矩阵和向量运作在数据挖掘中的价值；如果你没有最优化方法的概念，就对迭代收敛理解不深。所以说，想要更深刻地理解数据挖掘的方法，就非常有必要了解它后背的数学原理。

1）概率论与数理统计。概率论在我们上大学的时候，基本上都学过，不过大学里老师教的内容，偏概率的多一些，统计部分讲得比较少。在数据挖掘里使用到概率论的地方就比较多了。比如条件概率、独立性的概念，以及随机变量、多维随机变量的概念。很多算法的本质都与概率论相关，所以说概率论与数理统计是数据挖掘的重要数学基础。2）线性代数。向量和矩阵是线性代数中的重要知识点，它被广泛应用到数据挖掘中，比如我们经常会把对象抽象为矩阵的表示，一幅图像就可以抽象出来是一个矩阵，我们也经常计算特征值和特征向量，用特征向量来近似代表物体的特征。这个是大数据降维的基本思路。基于矩阵的各种运算，以及基于矩阵的理论成熟，可以帮我们解决很多实际问题，比如 PCA 方法、SVD 方法，以及 MF、NMF 方法等在数据挖掘中都有广泛的应用。

3）图论。社交网络的兴起，让图论的应用也越来越广。人与人的关系，可以用图论上的两个节点来进行连接，节点的度可以理解为一个人的朋友数。我们都听说过人脉的六度理论，在 Facebook 上被证明平均一个人与另一个人的连接，只需要 3.57 个人。当然图论对于网络结构的分析非常有效，同时图论也在关系挖掘和图像分割中有重要的作用。4）最优化方法。最优化方法相当于机器学习中自我学习的过程，当机器知道了目标，训练后与结果存在偏差就需要迭代调整，那么最优化就是这个调整的过程。一般来说，这个学习和迭代的过程是漫长、随机的。最优化方法的提出就是用更短的时间得到收敛，取得更好的效果。

### 黑板墙

课后思考：对于思考题，我想到了沃尔玛「啤酒和尿布」经典案例。沃尔玛正是将 Apriori 算法引入到 POS 机数据分析中，从而获得了营销上奇迹。简单说来就是在一个数据集中，找到经常出现的商品组合。当然 Apriori 算法的计算量很大，当商品数据量大时效率低，FP-Tree 算法优化了该算法。

1）数据挖掘学习方法体会：有了知识清单，相当于有了一个系统思维在那，对快速识别问题的确很有帮助。很好的方法方便实践，就像巴菲特和芒格的投资是使用的公司尽调清单一样，MECE 的解决问题。2）基于电商商品的关联进行推荐从而提高销售的话，个人认为是 Apriori 算法，其为了提取频繁项集和一定置信度的关联规则，即用户购买了 X 产品有多大概率去买 Y，根据置信度高的原则推荐。

### 0103Python基础语法.md

最好掌握 Python 语言。首先，在一份关于开发语言的调查中，使用过 Python 的开发者，80% 都会把 Python 作为自己的主要语言。Python 已经成为发展最快的主流编程语言，从众多开发语言中脱颖而出，深受开发者喜爱。其次，在数据分析领域中，使用 Python 的开发者是最多的，远超其他语言之和。最后，Python 语言简洁，有大量的第三方库，功能强大，能解决数据分析的大部分问题，这一点我下面具体来说。

Python 语言最大的优点是简洁，它虽然是 C 语言写的，但是摒弃了 C 语言的指针，这就让代码非常简洁明了。同样的一行 Python 代码，甚至相当于 5 行 Java 代码。我们读 Python 代码就像是读英文一样直观，这就能让程序员更好地专注在问题解决上，而不是在语言本身。当然除了 Python 自身的特点，Python 还有强大的开发者工具。在数据科学领域，Python 有许多非常著名的工具库：比如科学计算工具 NumPy 和 Pandas 库，深度学习工具 Keras 和 TensorFlow，以及机器学习工具 Scikit-learn，使用率都非常高。

2『先学机器学习 Scikit-learn，再学深度学习 TensorFlow。』

大部分 Python 库都同时支持 Python 2.7.x 和 3.x 版本。虽然官方称 Python2.7 只维护到 2020 年，但是我想告诉你的是：千万不要忽视 Python2.7，它的寿命远不止到 2020 年，而且这两年 Python2.7 还是占据着 Python 版本的统治地位。一份调查显示：在 2017 年的商业项目中 2.7 版本依然是主流，占到了 63.7%，即使这两年 Python3.x 版本使用的增速较快，但实际上 Python3.x 在 2008 年就已经有了。这两个版本该如何选择呢？版本选择的标准就是看你的项目是否会依赖于 Python2.7 的包，如果有依赖的就只能使用 Python2.7，否则你可以用 Python 3.x 开始全新的项目。

for 循环是一种迭代循环机制，迭代即重复相同的逻辑操作。如果规定循环的次数，我们可以使用 range 函数，它在 for 循环中比较常用。range (11) 代表从 0 到 10，不包括 11，也相当于 range (0,11)，range 里面还可以增加步长，比如 range (1,11,2) 代表的是 [1,3,5,7,9]。

1『用 for 循环和 range() 实现执行固定次数的代码。』

1 到 10 的求和也可以用 while 循环来写，这里 while 控制了循环的次数。while 循环是条件循环，在 while 循环中对于变量的计算方式更加灵活。因此 while 循环适合循环次数不确定的循环，而 for 循环的条件相对确定，适合固定次数的循环。

注释在 python 中使用 #，如果注释中有中文，一般会在代码前添加 # -\*- coding: utf-8 -*-。如果是多行注释，使用三个单引号，或者三个双引号。

1『原来注释里含有中文的情况下才需要加「# -\*- coding: utf-8 -*-」』

Python 语言中 import 的使用很简单，直接使用 import module_name 语句导入即可。这里 import 的本质是什么呢？import 的本质是路径搜索。import 引用可以是模块 module，或者包 package。针对 module，实际上是引用一个.py 文件。而针对 package，可以采用 from … import … 的方式，这里实际上是从一个目录中引用模块，这时目录结构中必须带有一个 \__init__.py 文件。

上面的讲的这些基础语法，我们可以用 sumlime text 编辑器运行 Python 代码。另外，告诉你一个相当高效的方法，你可以充分利用一个刷题进阶的网址：[ZOJ](https://zoj.pintia.cn/home)，这是浙江大学 ACM 的 OnlineJudge。

2『跟力库一样去刷。』

### 0104Python科学计算用NumPy快速处理数据.md

它不仅是 Python 中使用最多的第三方库，而且还是 SciPy、Pandas 等数据科学的基础库。它所提供的数据结构比 Python 自身的「更高级、更高效」，可以这么说，NumPy 所提供的数据结构是 Python 数据分析的基础。上次讲到了 Python 数组结构中的列表 list，它实际上相当于一个数组的结构。而 NumPy 中一个关键数据类型就是关于数组的，那为什么还存在这样一个第三方的数组结构呢？

实际上，标准的 Python 中，用列表 list 保存数组的数值。由于列表中的元素可以是任意的对象，所以列表中 list 保存的是对象的指针。虽然在 Python 编程中隐去了指针的概念，但是数组有指针，Python 的列表 list 其实就是数组。这样如果我要保存一个简单的数组 [0,1,2]，就需要有 3 个指针和 3 个整数的对象，这样对于 Python 来说是非常不经济的，浪费了内存和计算时间。

为什么要用 NumPy 数组结构而不是 Python 本身的列表 list？这是因为列表 list 的元素在系统内存中是分散存储的，而 NumPy 数组存储在一个均匀连续的内存块中。这样数组计算遍历所有的元素，不像列表 list 还需要对内存地址进行查找，从而节省了计算资源。另外在内存访问模式中，缓存会直接把字节块从 RAM 加载到 CPU 寄存器中。因为数据连续的存储在内存中，NumPy 直接利用现代 CPU 的矢量化指令计算，加载寄存器中的多个连续浮点数。另外 NumPy 中的矩阵计算可以采用多线程的方式，充分利用多核 CPU 计算资源，大大提升了计算效率。

当然除了使用 NumPy 外，你还需要一些技巧来提升内存和提高计算资源的利用率。一个重要的规则就是：避免采用隐式拷贝，而是采用就地操作的方式。举个例子，如果我想让一个数值 x 是原来的两倍，可以直接写成 x\*=2，而不要写成 y=x*2。这样速度能快到 2 倍甚至更多。既然 NumPy 这么厉害，你该从哪儿入手学习呢？在 NumPy 里有两个重要的对象：ndarray（N-dimensional array object）解决了多维数组问题，而 ufunc（universal function object）则是解决对数组进行处理的函数。下面，我就带你一一来看。

1『注意，x*=2 表达式之间不能有空格。』

ndarray 实际上是多维数组的含义。在 NumPy 数组中，维数称为秩（rank），一维数组的秩为 1，二维数组的秩为 2，以此类推。在 NumPy 中，每一个线性的数组称为一个轴（axes），其实秩就是描述轴的数量。

创建数组前，你需要引用 NumPy 库，可以直接通过 array 函数创建数组，如果是多重数组，比如示例里的 b，那么该怎么做呢？你可以先把一个数组作为一个元素，然后嵌套起来，比如示例 b 中的 [1,2,3] 就是一个元素，然后 [4,5,6][7,8,9] 也是作为元素，然后把三个元素再放到 [] 数组里，赋值给变量 b。

当然数组也是有属性的，比如你可以通过函数 shape 属性获得数组的大小，通过 dtype 获得元素的属性。如果你想对数组里的数值进行修改的话，直接赋值即可，注意下标是从 0 开始计的，所以如果你想对 b 数组，九宫格里的中间元素进行修改的话，下标应该是 [1,1]。

你看下这个例子，首先在 NumPy 中是用 dtype 定义的结构类型，然后在定义数组的时候，用 array 中指定了结构数组的类型 dtype=persontype，这样你就可以自由地使用自定义的 persontype 了。比如想知道每个人的语文成绩，就可以用 chineses = peoples [:][‘chinese’]，当然 NumPy 中还有一些自带的数学运算，比如计算平均值使用 np.mean。

ufunc 是 universal function 的缩写，是不是听起来就感觉功能非常强大？确如其名，它能对数组中每个元素进行函数操作。NumPy 中很多 ufunc 函数计算速度非常快，因为都是采用 C 语言实现的。

NumPy 可以很方便地创建连续数组，比如我使用 arange 或 linspace 函数进行创建。np.arange 和 np.linspace 起到的作用是一样的，都是创建等差数组。这两个数组的结果 x1,x2 都是 [1 3 5 7 9]。结果相同，但是你能看出来创建的方式是不同的。arange () 类似内置函数 range ()，通过指定初始值、终值、步长来创建等差数列的一维数组，默认是不包括终值的。linspace 是 linear space 的缩写，代表线性等分向量的含义。linspace () 通过指定初始值、终值、元素个数来创建等差数列的一维数组，默认是包括终值的。

1『发现用 x1 = np.arange(1, 11, 2) 创建的是整数型数据，而用 x2 = np.linspace(1, 9, 5) 创建的是浮点数据。』

通过 NumPy 可以自由地创建等差数组，同时也可以进行加、减、乘、除、求 n 次方和取余数。我还以 x1, x2 数组为例，求这两个数组之间的加、减、乘、除、求 n 次方和取余数。在 n 次方中，x2 数组中的元素实际上是次方的次数，x1 数组的元素为基数。在取余函数里，你既可以用 np.remainder (x1, x2)，也可以用 np.mod (x1, x2)，结果是一样的。

统计函数。如果你想要对一堆数据有更清晰的认识，就需要对这些数据进行描述性的统计分析，比如了解这些数据中的最大值、最小值、平均值，是否符合正态分布，方差、标准差多少等等。它们可以让你更清楚地对这组数据有认知。

计数组 / 矩阵中的最大值函数 amax ()，最小值函数 amin ()。amin () 用于计算数组中的元素沿指定轴的最小值。对于一个二维数组 a，amin (a) 指的是数组中全部元素的最小值，amin (a,0) 是延着 axis=0 轴的最小值，axis=0 轴是把元素看成了 [1,4,7], [2,5,8], [3,6,9] 三个元素，所以最小值为 [1,2,3]，amin (a,1) 是延着 axis=1 轴的最小值，axis=1 轴是把元素看成了 [1,2,3], [4,5,6], [7,8,9] 三个元素，所以最小值为 [1,4,7]。同理 amax () 是计算数组中元素沿指定轴的最大值。

1『axis=0 是跨行，表现的结果一列一列的，axis=1 是跨行，表现的结果一行一行的。axis 即为轴或者秩。』

统计最大值与最小值之差 ptp ()。对于相同的数组 a，np.ptp (a) 可以统计数组中最大值与最小值的差，即 9-1=8。同样 ptp (a,0) 统计的是沿着 axis=0 轴的最大值与最小值之差，即 7-1=6（当然 8-2=6,9-3=6，第三行减去第一行的 ptp 差均为 6），ptp (a,1) 统计的是沿着 axis=1 轴的最大值与最小值之差，即 3-1=2（当然 6-4=2, 9-7=2，即第三列与第一列的 ptp 差均为 2）。

统计数组的百分位数 percentile()。percentile () 代表着第 p 个百分位数，这里 p 的取值范围是 0-100，如果 p=0，那么就是求最小值，如果 p=50 就是求平均值，如果 p=100 就是求最大值。同样你也可以求得在 axis=0 和 axis=1 两个轴上的 p% 的百分位数。你可以用 median () 和 mean () 求数组的中位数、平均值，同样也可以求得在 axis=0 和 1 两个轴上的中位数、平均值。

average () 函数可以求加权平均，加权平均的意思就是每个元素可以设置个权重，默认情况下每个元素的权重是相同的，所以 np.average (a)=(1+2+3+4)/4=2.5，你也可以指定权重数组 wts=[1,2,3,4]，这样加权平均 np.average (a,weights=wts)=(1\*1+2\*2+3\*3+4*4)/(1+2+3+4)=3.0。

统计数组中的标准差 std ()、方差 var ()。方差的计算是指每个数值与平均值之差的平方求和的平均值，即 mean ((x - x.mean ())** 2)。标准差是方差的算术平方根。在数学意义上，代表的是一组数据离平均值的分散程度。所以 np.var (a)=1.25, np.std (a)=1.118033988749895。

NumPy 排序。排序是算法中使用频率最高的一种，也是在数据分析工作中常用的方法，计算机专业的同学会在大学期间的算法课中学习。那么这些排序算法在 NumPy 中实现起来其实非常简单，一条语句就可以搞定。这里你可以使用 sort 函数，sort (a, axis=-1, kind=‘quicksort’, order=None)，默认情况下使用的是快速排序；在 kind 里，可以指定 quicksort、mergesort、heapsort 分别表示快速排序、合并排序、堆排序。同样 axis 默认是 -1，即沿着数组的最后一个轴进行排序，也可以取不同的 axis 轴，或者 axis=None 代表采用扁平化的方式作为一个向量进行排序。另外 order 字段，对于结构化的数组可以指定按照某个字段进行排序。

在 NumPy 学习中，你重点要掌握的就是对数组的使用，因为这是 NumPy 和标准 Python 最大的区别。在 NumPy 中重新对数组进行了定义，同时提供了算术和统计运算，你也可以使用 NumPy 自带的排序功能，一句话就搞定各种排序算法。当然要理解 NumPy 提供的数据结构为什么比 Python 自身的「更高级、更高效」，要从对数据指针的引用角度进行理解。

### 0105Python科学计算Pandas.md

和 NumPy 一样，Pandas 有两个非常重要的数据结构：Series 和 DataFrame。使用 Pandas 可以直接从 csv 或 xlsx 等文件中导入数据，以及最终输出到 excel 表中。重点介绍了数据清洗中的操作，当然 Pandas 中同样提供了多种数据统计的函数。最后我们介绍了如何将数据表进行合并，以及在 Pandas 中使用 SQL 对数据表更方便地进行操作。Pandas 包与 NumPy 工具库配合使用可以发挥巨大的威力，正是有了 Pandas 工具，Python 做数据挖掘才具有优势。

在数据分析工作中，Pandas 的使用频率是很高的，一方面是因为 Pandas 提供的基础数据结构 DataFrame 与 json 的契合度很高，转换起来就很方便。另一方面，如果我们日常的数据清理工作不是很复杂的话，你通常用几句 Pandas 代码就可以对数据进行规整。Pandas 可以说是基于 NumPy 构建的含有更高级数据结构和分析能力的工具包。在 NumPy 中数据结构是围绕 ndarray 展开的，那么在 Pandas 中的核心数据结构是什么呢？Series 和 DataFrame 这两个核心数据结构，分别代表着一维的序列和二维的表结构。基于这两种数据结构，Pandas 可以对数据进行导入、清洗、处理、统计和输出。

1『 DataFrame 与 json 的契合度很高，那么爬虫输出的 json 数据可以直接先转成 DataFrame 对象，然后进行清洗处理。』

Series 是个定长的字典序列。说是定长是因为在存储的时候，相当于两个 ndarray，这也是和字典结构最大的不同。因为在字典的结构里，元素的个数是不固定的。Series 有两个基本属性：index 和 values。在 Series 结构中，index 默认是 0,1,2,…… 递增的整数序列，当然我们也可以自己来指定索引，比如 index=[‘a’, ‘b’, ‘c’, ‘d’]。

这个例子中，x1 中的 index 采用的是默认值，x2 中 index 进行了指定。我们也可以采用字典的方式来创建 Series，比如；DataFrame 类型数据结构类似数据库表。它包括了行索引和列索引，我们可以将 DataFrame 看成是由相同索引的 Series 组成的字典类型。

1『Series 只有行索引，DataFrame 有行索引和列索引。』
 
我们虚构一个王者荣耀考试的场景，想要输出几位英雄的考试成绩；在后面的案例中，我一般会用 df, df1, df2 这些作为 DataFrame 数据类型的变量名，我们以例子中的 df2 为例，列索引是 [‘English’, ‘Math’, ‘Chinese’]，行索引是 [‘ZhangFei’, ‘GuanYu’, ‘ZhaoYun’, ‘HuangZhong’, ‘DianWei’]，所以 df2 的输出是；在了解了 Series 和 DataFrame 这两个数据结构后，我们就从数据处理的流程角度，来看下他们的使用方法。

1『

```
Out[63]:
            English  Math  Chinese
ZhangFei         65    30       66
GuanYu           85    98       95
ZhaoYun          92    96       93
HuangZhong       88    77       90
DianWei          90    90       80

In [68]: df2.iloc[0]
Out[68]:
English    65
Math       30
Chinese    66
Name: ZhangFei, dtype: int64

In [69]: df2.iloc[:2]
Out[69]:
          English  Math  Chinese
ZhangFei       65    30       66
GuanYu         85    98       95

In [71]: df2.iloc[:, 1:3]
In [76]: df2.loc[:, 'Math']

```

』

数据导入和输出。Pandas 允许直接从 xlsx，csv 等文件中导入数据，也可以输出到 xlsx, csv 等文件，非常方便。需要说明的是，在运行的过程可能会存在缺少 xlrd 和 openpyxl 包的情况，到时候如果缺少了，可以在命令行模式下使用「pip install」命令来进行安装。

数据清洗。数据清洗是数据准备过程中必不可少的环节，Pandas 也为我们提供了数据清洗的工具，在后面数据清洗的章节中会给你做详细的介绍，这里简单介绍下 Pandas 在数据清洗中的使用方法。删除 DataFrame 中的不必要的列或行。Pandas 提供了一个便捷的方法 drop () 函数来删除我们不想要的列或行。比如我们想把「语文」这列删掉；想把「张飞」这行删掉；重命名列名 columns，让列表名更容易识别。如果你想对 DataFrame 中的 columns 进行重命名，可以直接使用 rename (columns=new_names, inplace=True) 函数，比如我把列名 Chinese 改成 YuWen，English 改成 YingYu；去重复的行。数据采集可能存在重复的行，这时只要使用 drop\_duplicates () 就会自动把重复的行去掉。

```
df2 = df2.drop(columns=['Chinese'])
df2 = df2.drop(index=['ZhangFei'])

df2.rename(columns={'Chinese': 'YuWen', 'English': 'Yingyu'}, inplace = True)

df = df.drop\_duplicates () #去除重复行
```

更改数据格式。这是个比较常用的操作，因为很多时候数据格式不规范，我们可以使用 astype 函数来规范数据格式，比如我们把 Chinese 字段的值改成 str 类型，或者 int64 可以这么写；数据间的空格。有时候我们先把格式转成了 str 类型，是为了方便对数据进行操作，这时想要删除数据间的空格，我们就可以使用 strip 函数；如果数据里有某个特殊的符号，我们想要删除怎么办？同样可以使用 strip 函数，比如 Chinese 字段里有美元符号，我们想把这个删掉，可以这么写。

```
df2['Chinese'].astype('str') 
df2['Chinese'].astype(np.int64) 

#删除左右两边空格
df2['Chinese']=df2['Chinese'].map(str.strip)
#删除左边空格
df2['Chinese']=df2['Chinese'].map(str.lstrip)
#删除右边空格
df2['Chinese']=df2['Chinese'].map(str.rstrip)

df2['Chinese']=df2['Chinese'].str.strip('$')
```

1『更改数据格式的操作，直觉上会有大用。』

大小写转换。大小写是个比较常见的操作，比如人名、城市名等的统一都可能用到大小写的转换，在 Python 里直接使用 upper (), lower (), title () 函数，方法如下；查找空值。数据量大的情况下，有些字段存在空值 NaN 的可能，这时就需要使用 Pandas 中的 isnull 函数进行查找。比如，我们输入一个数据表如下；如果我们想看下哪个地方存在空值 NaN，可以针对数据表 df 进行 df.isnull ()，结果如下；如果我想知道哪列存在空值，可以使用 df.isnull ().any ()，结果如下：

```
#全部大写
df2.columns = df2.columns.str.upper()
#全部小写
df2.columns = df2.columns.str.lower()
#首字母大写
df2.columns = df2.columns.str.title()
```

使用 apply 函数对数据进行清洗。apply 函数是 Pandas 中自由度非常高的函数，使用频率也非常高。比如我们想对 name 列的数值都进行大写转化可以用；我们也可以定义个函数，在 apply 中进行使用。比如定义 double_df 函数是将原来的数值 \*2 进行返回。然后对 df1 中的「语文」列的数值进行 *2 处理，可以写成；我们也可以定义更复杂的函数，比如对于 DataFrame，我们新增两列，其中 ’new1’ 列是「语文」和「英语」成绩之和的 m 倍，'new2’ 列是「语文」和「英语」成绩之和的 n 倍，我们可以这样写；其中 axis=1 代表按照列为轴进行操作，axis=0 代表按照行为轴进行操作，args 是传递的两个参数，即 n=2, m=3，在 plus 函数中使用到了 n 和 m，从而生成新的 df。

```
df['name'] = df['name'].apply(str.upper)

def double_df(x):
           return 2*x
df1[u'语文'] = df1[u'语文'].apply(double_df)

def plus(df,n,m):
    df['new1'] = (df[u'语文']+df[u'英语']) * m
    df['new2'] = (df[u'语文']+df[u'英语']) * n
    return df
df1 = df1.apply(plus,axis=1,args=(2,3,))
```

1『apply() 函数厉害，函数可以做参数。』

数据统计。在数据清洗后，我们就要对数据进行统计了。Pandas 和 NumPy 一样，都有常用的统计函数，如果遇到空值 NaN，会自动排除。常用的统计函数包括；表格中有一个 describe () 函数，统计函数千千万，describe () 函数最简便。它是个统计大礼包，可以快速让我们对数据有个全面的了解。下面我直接使用 df1.descirbe () 输出结果为：

```
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
print df1.describe()
```

数据表合并。有时候我们需要将多个渠道源的多个数据表进行合并，一个 DataFrame 相当于一个数据库的数据表，那么多个 DataFrame 数据表的合并就相当于多个数据库的表合并。比如我要创建两个 DataFrame；两个 DataFrame 数据表的合并使用的是 merge () 函数，有下面 5 种形式：1）比如我们可以基于 name 这列进行连接。2）inner 内链接是 merge 合并的默认情况，inner 内连接其实也就是键的交集，在这里 df1, df2 相同的键是 name，所以是基于 name 字段做的连接。3）左连接是以第一个 DataFrame 为主进行的连接，第二个 DataFrame 作为补充。4）右连接是以第二个 DataFrame 为主进行的连接，第一个 DataFrame 作为补充。5）外连接相当于求两个 DataFrame 的并集。

```
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
df2 = DataFrame({'name':['ZhangFei', 'GuanYu', 'A', 'B', 'C'], 'data2':range(5)})

df3 = pd.merge(df1, df2, on='name')
df3 = pd.merge(df1, df2, how='inner')
df3 = pd.merge(df1, df2, how='left')
df3 = pd.merge(df1, df2, how='right')
df3 = pd.merge(df1, df2, how='outer')
```

2『做设备表时，CAD 导出的数据库与外面通用设备数据库之间的关联，应该是使用 inner 内连，待验证。经试验，用的是外连接，然后通过删除设备名称列里是空值的行。』

如何用 SQL 方式打开 Pandas。Pandas 的 DataFrame 数据类型可以让我们像处理数据表一样进行操作，比如数据表的增删改查，都可以用 Pandas 工具来完成。不过也会有很多人记不住这些 Pandas 的命令，相比之下还是用 SQL 语句更熟练，用 SQL 对数据表进行操作是最方便的，它的语句描述形式更接近我们的自然语言。事实上，在 Python 里可以直接使用 SQL 语句来操作 Pandas。

这里给你介绍个工具：pandasql。pandasql 中的主要函数是 sqldf，它接收两个参数：一个 SQL 查询语句，还有一组环境变量 globals () 或 locals ()。这样我们就可以在 Python 里，直接用 SQL 语句中对 DataFrame 进行操作，举个例子：

```
import pandas as pd
from pandas import DataFrame
from pandasql import sqldf, load_meat, load_births
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
pysqldf = lambda sql: sqldf(sql, globals())
sql = "select * from df1 where name ='ZhangFei'"
print pysqldf(sql)
```

1『pandasql 这个库实在太方便。SQL 筛选语句里的 name 指代列名。』

上面这个例子中，我们是对「name='ZhangFei」的行进行了输出。当然你会看到我们用到了 lambda，lambda 在 python 中算是使用频率很高的，那 lambda 是用来做什么的呢？它实际上是用来定义一个匿名函数的，具体的使用形式为：

     lambda argument_list: expression

这里 argument_list 是参数列表，expression 是关于参数的表达式，会根据 expression 表达式计算结果进行输出返回。在上面的代码中，我们定义了：

    pysqldf = lambda sql: sqldf(sql, globals())

在这个例子里，输入的参数是 sql，返回的结果是 sqldf 对 sql 的运行结果，当然 sqldf 中也输入了 globals 全局参数，因为在 sql 中有对全局参数 df1 的使用。

## 01数据分析基础篇2

### 1. 逻辑脉络

1、百货商店利用数据预测用户购物行为属于商业智能，他们积累的顾客的消费行为习惯会存储在数据仓库中，通过对个体进行消费行为分析总结出来的规律属于数据挖掘。一本图书的信息包括了书名、作者、出版社、ISBN、出版时间、页数和定价等多个属性的信息，我们就可以把这些属性定义成一套图书的元数据。在图书这个元数据中，书名、作者、出版社就是数据元。数据挖掘的流程：分类、聚类、预测和关联分析。数据挖掘前要对数据进行预处理，包括数据清洗，数据集成，以及数据变换。

2、如何一步步做数据分析。首先可以做用户画像，1）用户从哪里来，即统一化，2）用户是谁，即标签化，3）用户要到哪里去，即业务化。标签化的流程是通过数据层的「事实标签」，在算法层进行计算，打上「模型标签」的分类结果，最后指导业务层，得出「预测标签」。锻炼自己的抽象能力，将繁杂的事务简单化。

3、数据挖掘的原料来自数据采集。一个数据的走势，是由多个维度影响的。我们需要通过多源的数据采集，收集到尽可能多的数据维度，同时保证数据的质量，这样才能得到高质量的数据挖掘结果。数据源分为四类：开放数据源、爬虫抓取、传感器和日志采集。

4、八爪鱼的基本操作，包括八爪鱼的任务建立、流程设计，还有一个实操的案例。

5、爬虫的整个过程包括三个阶段：打开网页、提取数据和保存数据。了解 XPath 定位，JSON 对象解析。如何使用 lxml 库，进行 XPath 的提取。如何在 Python 中使用 Selenium 库来帮助你模拟浏览器，获取完整的 HTML。其中，Python + Selenium + 第三方浏览器可以让我们处理多种复杂场景，包括网页动态加载、JS 响应、Post 表单等。因为 Selenium 模拟的就是一个真实的用户的操作行为，就不用担心 cookie 追踪和隐藏字段的干扰了。

### 2. 摘录及评论

### 0106学数据分析要掌握哪些基本概念.md

美国明尼苏达州一家 Target 百货被客户投诉，这名客户指控 Target 将婴儿产品优惠券寄给他的女儿，而他女儿还是一名高中生。但没多久这名客户就来电道歉，因为女儿经他逼问后坦承自己真的怀孕了。Target 百货寄送婴儿产品优惠券绝非偶然之举，他们发现妇女在怀孕的情况下，购买的物品会发生变化，比如护手霜会从有香味的改成无味的，此外还会购买大量维生素等保健品。通过类似的关联分析，Target 构建了一个「怀孕预测指数」，通过这个指数预测到了顾客已经怀孕的情况，并把优惠券寄送给她。

2『精英日课里「大数据下的真实你我」里也有这个案例，做一张论据卡片。』

那么顾客怀孕与商品之间的关联关系是如何被发现的呢？实际上他们都是用的 Apriori 算法，该算法是由美国学者 Agrawal 在 1994 年提出的。他通过分析购物篮中的商品集合，找出商品之间的关联关系。利用这种隐性关联关系，商家就可以强化这类购买行为，从而提升销售额。

这就是数据分析的力量，人们总是从数据分析中得到有价值的信息，啤酒和尿布的故事也是个经典的案例。如今在超市中，我们还能看到不少组合的套装打包在一起卖，比如宝洁的产品：飘柔洗发水 + 玉兰油沐浴露、海飞丝洗发水 + 舒肤佳沐浴露等等。商品的捆绑销售是个很有用的营销方式，背后都是数据分析在发挥作用。

商业智能 BI、数据仓库 DW、数据挖掘 DM 三者之间的关系。开头中的百货商店利用数据预测用户购物行为属于商业智能，他们积累的顾客的消费行为习惯会存储在数据仓库中，通过对个体进行消费行为分析总结出来的规律属于数据挖掘。所以我们能在这个场景里看到三个重要的概念：商业智能、数据仓库和数据挖掘。

1）商业智能的英文是 Business Intelligence，缩写是 BI。相比于数据仓库、数据挖掘，它是一个更大的概念。商业智能可以说是基于数据仓库，经过了数据挖掘后，得到了商业价值的过程。所以说数据仓库是个金矿，数据挖掘是炼金术，而商业报告则是黄金。2）数据仓库的英文是 Data Warehouse，缩写是 DW。它可以说是 BI 这个房子的地基，搭建好 DW 这个地基之后，才能进行分析使用，最后产生价值。数据仓库可以说是数据库的升级概念。从逻辑上理解，数据库和数据仓库没有什么区别，都是通过数据库技术来存储数据的。不过从数量上来讲，数据仓库的量更庞大，适用于数据挖掘和数据分析。数据库可以理解是一项技术。数据仓库将原有的多个数据来源中的数据进行汇总、整理而得。数据进入数据仓库前，必须消除数据中的不一致性，方便后续进行数据分析和挖掘。3）数据挖掘的英文是 Data Mining，缩写是 DM。在商业智能 BI 中经常会使用到数据挖掘技术。数据挖掘的核心包括分类、聚类、预测、关联分析等任务，通过这些炼金术，我们可以从数据仓库中得到宝藏，比如商业报告。很多时候，企业老板总是以结果为导向，他们认为商业报告才是他们想要的，但是这也是需要经过地基 DW、搬运工 ETL、科学家 DM 等共同的努力才得到的。

元数据 VS 数据元。我们前面提到了数据仓库，在数据仓库中，还有一类重要的数据是元数据，那么它和数据元有什么区别呢？1）元数据（MetaData）：描述其它数据的数据，也称为「中介数据」。2）数据元（Data Element）：就是最小数据单元。

在生活中，只要有一类事物，就可以定义一套元数据。举个例子，比如一本图书的信息包括了书名、作者、出版社、ISBN、出版时间、页数和定价等多个属性的信息，我们就可以把这些属性定义成一套图书的元数据。在图书这个元数据中，书名、作者、出版社就是数据元。你可以理解是最小的数据单元。元数据最大的好处是使信息的描述和分类实现了结构化，让机器处理起来很方便。

元数据可以很方便地应用于数据仓库。比如数据仓库中有数据和数据之间的各种复杂关系，为了描述这些关系，元数据可以对数据仓库的数据进行定义，刻画数据的抽取和转换规则，存储与数据仓库主题有关的各种信息。而且整个数据仓库的运行都是基于元数据的，比如抽取调度数据、获取历史数据等。通过元数据，可以很方便地帮助我们管理数据仓库。

数据挖掘的流程。聊完了数据仓库，我们再来谈谈数据挖掘。数据挖掘不是凭空产生的，它与数据库技术的发展分不开。数据挖掘的一个英文解释叫 Knowledge Discovery in Database，简称 KDD，也就是数据库中的知识发现。在数据挖掘中，有几个非常重要的任务，就是分类、聚类、预测和关联分析。

1）分类。通过训练集得到一个分类模型，然后用这个模型可以对其他数据进行分类。这里需要说明下训练集和测试集的概念。一般来说数据可以划分为训练集和测试集。训练集是用来给机器做训练的，通常是人们整理好训练数据，以及这些数据对应的分类标识。通过训练，机器就产生了自我分类的模型，然后机器就可以拿着这个分类模型，对测试集中的数据进行分类预测。同样如果测试集中，人们已经给出了测试结果，我们就可以用测试结果来做验证，从而了解分类器在测试环境下的表现。2）聚类。人以群分，物以类聚。聚类就是将数据自动聚类成几个类别，聚到一起的相似度大，不在一起的差异性大。我们往往利用聚类来做数据划分。3）预测。顾名思义，就是通过当前和历史数据来预测未来趋势，它可以更好地帮助我们识别机遇和风险。4）关联分析。就是发现数据中的关联规则，它被广泛应用在购物篮分析，或事务数据分析中。比如我们开头提到的那个案例。

数据挖掘要怎么完成这些任务呢？它需要将数据库中的数据经过一系列的加工计算，最终得出有用的信息。这个过程可以用以下步骤来描述。

首先，输入我们收集到的数据，然后对数据进行预处理。预处理通常是将数据转化成我们想要的格式，然后我们再对数据进行挖掘，最后通过后处理得到我们想要的信息。那你可能想问，为什么不直接进行数据挖掘，还要进行数据预处理呢？因为在这个过程中，输入的数据通常是从不同渠道采集而来的，所以数据的格式以及质量是参差不齐的，所以我们需要对数据进行预处理。数据预处理中，我们会对数据进行几个处理步骤：数据清洗，数据集成，以及数据变换。

1）数据清洗。主要是为了去除重复数据，去噪声（即干扰数据）以及填充缺失值；2）数据集成。是将多个数据源中的数据存放在一个统一的数据存储中。3）数据变换。就是将数据转换成适合数据挖掘的形式。比如，通过归一化将属性数据按照比例缩放，这样就可以将数值落入一个特定的区间内，比如 0-1 之间。我会在后面的几节课给你讲解如何对数据进行预处理。数据后处理是将模型预测的结果进一步处理后，再导出。比如在二分类问题中，一般能得到的是 0-1 之间的概率值，此时把数据以 0.5 为界限进行四舍五入就可以实现后处理。

2『在「2020017集体智慧编程R00」第 2 章里提到的「欧几里德距离」模型，可以通过归一化做预处理提升模型的精准性。』

来打个比喻。比如你认识了两个漂亮的女孩。商业智能会告诉你要追哪个？成功概率有多大？数据仓库会说，我这里存储了这两个女孩的相关信息，你要吗？其中每个女孩的数据都有单独的文件夹，里面有她们各自的姓名、生日、喜好和联系方式等，这些具体的信息就是数据元，加起来叫作元数据。数据挖掘会帮助你确定追哪个女孩，并且整理好数据仓库，这里就可以使用到各种算法，帮你做决策了。你可能会用到分类算法。御姐、萝莉、女王，她到底属于哪个分类？如果认识的女孩太多了，多到你已经数不过来了，比如说 5 万人！你就可以使用聚类算法了，它帮你把这些女孩分成多个群组，比如 5 个组。然后再对每个群组的特性进行了解，进行决策。这样就把 5 万人的决策，转化成了 5 个组的决策。成功实现降维，大大提升了效率。如果你想知道这个女孩的闺蜜是谁，那么关联分析算法可以告诉你。

如果你的数据来源比较多，比如有很多朋友给你介绍女朋友，很多人都推荐了同一个，你就需要去重，这叫数据清洗；为了方便记忆，你把不同朋友推荐的女孩信息合成一个，这叫数据集成；有些数据渠道统计的体重的单位是公斤，有些是斤，你就需要将它们转换成同一个单位，这叫数据变换。最后你可以进行数据可视化了，它会直观地把你想要的结果呈现出来。

上帝不会告诉我们规律，而是展示给我们数据。还记得高中物理课上我们计算自由落体运动加速度的实验吗？我们将重物连上纸带，通过电火花打点计时器，在纸带上会出现多个打点。然后我们通过纸带上打点的个数，以及点之间的间距，来计算自由落体运动的加速度。通过多组实验，取平均值的方式将误差降到最小。在我们的高中时代，许多定律都是通过实验得出的。参加工作以后，很多数据是业务数据，比如电商的客户购物数据等，这些数据依然有着某些规律。这就需要我们通过数据挖掘的力量来帮我们揭示规律，通过利用这些规律，可以帮我们创造更大的价值。

### 黑板墙

项目经历：毕业半年，个人是从计算机转数据分析岗位，之前对数学知识所知甚少，接触的第一个项目是用户画像，所谓用户画像就是标签的汇总，从用户不同维度的信息当中提取有价值的特征从而构建标签库，最后从标签库探索信息，从而构建用户画像。也算是走了一个完整的数据分析流程，从最开始的数据理解、数据预处理、特征选择、以及构建画像时运用到的聚类算法，实现不同人群的划分，使每类用户都具有一些鲜明的特征，从而提高产品服务或者是提升利润。

作者回复：不一样的。分类是已知了类别，然后看样本属于哪个分类。聚类是不知道有哪些类别，按照样本的属性来进行聚类，实际上是一种降维方式。比如你追个女生，你知道女生有御姐，萝莉，两种类别，你可以判断下追的这个女生属于御姐还是萝莉，这个属于分类。比如你追 5000 个女生，你也不知道女生都有哪些类别，为了方便，你将 5000 个女生，按照属性的相似度划分成了 5 个组，这个属于聚类。先聚成了 5 类，然后再看每个组的特点，给不同的组取名，比如「大小姐组」，「小家碧玉组」等等，都是先聚类，然后再判断。

回答问题：数据挖掘的价值，其实这是个很大的话题。正因为我做的是数据挖掘的行业，深刻理解一个互联网产品是怎样的一个演变流程，数据如何驱动运营，如何影响决策，最终决定产品走向。这些都是数据挖掘在做支撑。对于电商行业，可以通过数据挖掘引导采购，识别爆款等等，数据挖掘不止赋能于业务部门，更是从公司战略层面给与重大决策支持，以及评估业务上的重大策略效果。

个人所理解的数据挖掘的价值，是通过人 / 事物所表现出来的行为数据，抽象出某种直观上难以总结出的通用规律，便于我们在不确定的世界（尤商业世界）中摸索出较确定的努力方向，并通过分析这个确定的规律去认识世界，认识人性。比如前阵子参加了个比赛，选题是识别上市公司的财务造假，这是一个典型的不平衡分类问题，而且数据量还不大。这其实就可以看作是通过数据挖掘的手段，探索上市公司财务人员在造假时会表现出怎样的手段；即使是再精妙的设计，由于经过了粉饰扭曲，也难免会在细节之处暴露出差异。

之前做过的一个两周小项目，领导提供的公司近三年直销平台的交易数据，包括客户名称，购买日期，以及购买数量，型号等信息。数据清洗，去除重复值，空值，还有多余的属性，然后在进行数据集成，将各个平台的数据汇总导入到 spss 中进行 RFM 建模分析 (spss 中有模块可以进行交易数据的的建模分析)，得到变换后的客户数据。然后将其中三种属性进行规约，聚类分析得出八种客户类型。这个是第一阶段的分析。第二阶段是针对于第一类客户，结合企查查上爬取的公司，行业，规模，资质等信息，把数据统一放在 excel 中进行分析，画气泡图，饼图，柱状图。其实我理解的这个过程就是打标签。老师怎么看，以及后续分析的话有没有可以提供的思路。传统企业，老总说明年要分析直销，渠道所有的数据。

### 0107用户画像标签化就是数据的抽象能力.md

王兴说过，我们已经进入到互联网的下半场。在上半场，也就是早期的互联网时代，你永远不知道在对面坐的是什么样的人。那个年代大部分人还是 QQ 的早期用户。在下半场，互联网公司已经不新鲜了，大部分公司已经互联网化。他们已经在用网络进行产品宣传，使用电商销售自己的商品。

这两年引领下半场发展的是那些在讲「大数据」「赋能」的企业，他们有数据，有用户。通过大数据告诉政府该如何智慧地管理交通，做城市规划。通过消费数据分析，告诉企业该在什么时间生产什么产品，以最大化地满足用户的需求。通过生活大数据告诉我们餐饮企业，甚至房地产企业该如何选址。如果说互联网的上半场是粗狂运营，因为有流量红利不需要考虑细节。那么在下半场，精细化运营将是长久的主题。有数据，有数据分析能力才能让用户得到更好的体验。所以，用户是根本，也是数据分析的出发点。

假如你进入到一家卖羊肉串的餐饮公司，老板说现在竞争越来越激烈，要想做得好就要明白顾客喜欢什么。于是上班第一天，老板问你：「你能不能分析下用户数据，给咱们公司的业务做个赋能啊？」听到这，你会怎么想？你说：「老板啊，咱们是卖羊肉串的，做数据挖掘没用啊。」估计老板听后，晚上就把你给开了。那该怎么办呢？

用户画像的准则。首先就是将自己企业的用户画像做个白描，告诉他这些用户「都是谁」「从哪来」「要去哪」。你可以这么和老板说：「老板啊，用户画像建模是个系统的工程，我们要解决三个问题。第一呢，就是用户从哪里来，这里我们需要统一标识用户 ID，方便我们对用户后续行为进行跟踪。我们要了解这些羊肉串的用户从哪里来，他们是为了聚餐，还是自己吃宵夜，这些场景我们都要做统计分析。第二呢，这些用户是谁？我们需要对这些用户进行标签化，方便我们对用户行为进行理解。第三呢，就是用户要到哪里去？我们要将这些用户画像与我们的业务相关联，提升我们的转化率，或者降低我们的流失率。」

首先，为什么要设计唯一标识？用户唯一标识是整个用户画像的核心。我们以一个 App 为例，它把「从用户开始使用 APP 到下单到售后整个所有的用户行为」进行串联，这样就可以更好地去跟踪和分析一个用户的特征。设计唯一标识可以从这些项中选择：用户名、注册手机号、联系人手机号、邮箱、设备号、CookieID 等。

其次，给用户打标签。你可能会想，标签有很多，且不同的产品，标签的选择范围也不同，这么多的标签，怎样划分才能既方便记忆，又能保证用户画像的全面性呢？这里总结了八个字，叫「用户消费行为分析」。我们可以从这 4 个维度来进行标签划分。1）用户标签：它包括了性别、年龄、地域、收入、学历、职业等。这些包括了用户的基础属性。2）消费标签：消费习惯、购买意向、是否对促销敏感。这些统计分析用户的消费习惯。3）行为标签：时间段、频次、时长、访问路径。这些是通过分析用户行为，来得到他们使用 App 的习惯。4）内容分析：对用户平时浏览的内容，尤其是停留时间长、浏览次数多的内容进行分析，分析出用户对哪些内容感兴趣，比如，金融、娱乐、教育、体育、时尚、科技等。

可以说，用户画像是现实世界中的用户的数学建模，我们正是将海量数据进行标签化，来得到精准的用户画像，从而为企业更精准地解决问题。最后，当你有了用户画像，可以为企业带来什么业务价值呢？我们可以从用户生命周期的三个阶段来划分业务价值，包括：获客、粘客和留客。1）获客：如何进行拉新，通过更精准的营销获取客户。2）粘客：个性化推荐，搜索排序，场景运营等。3）留客：流失率预测，分析关键节点降低流失率。

如果按照数据流处理的阶段来划分用户画像建模的过程，可以分为数据层、算法层和业务层。你会发现在不同的层，都需要打上不同的标签。1）数据层指的是用户消费行为里的标签。我们可以打上「事实标签」，作为数据客观的记录。2）算法层指的是透过这些行为算出的用户建模。我们可以打上「模型标签」，作为用户画像的分类标识。3）业务层指的是获客、粘客、留客的手段。我们可以打上「预测标签」，作为业务关联的结果。所以这个标签化的流程，就是通过数据层的「事实标签」，在算法层进行计算，打上「模型标签」的分类结果，最后指导业务层，得出「预测标签」。

美团外卖的用户画像该如何设计？刚才讲的是用户画像的三个阶段，以及每个阶段的准则。如果你是美团外卖的数据分析师，你该如何制定用户标识 ID，制定用户画像，以及基于用户画像可以做哪些业务关联？

首先，我们先回顾下美团外卖的产品背景。美团已经和大众点评进行了合并，因此在大众点评和美团外卖上都可以进行外卖下单。另外美团外卖针对的是高频 O2O 的场景，美团外卖是美团的核心产品，基本上有一半的市值都是由外卖撑起来的。基于用户画像实施的三个阶段，我们首先需要统一用户的唯一标识，那么究竟哪个字段可以作为用户标识呢？

我们先看下美团和大众点评都是通过哪些方式登录的。我们看到，美团采用的是手机号、微信、微博、美团账号的登录方式。大众点评采用的是手机号、微信、QQ、微博的登录方式。这里面两个 APP 共同的登录方式都是手机号、微信和微博。那么究竟哪个可以作为用户的唯一标识呢？当然主要是以用户的注册手机号为标准。这样美团和大众点评的账号体系就可以相通。当然，大家知道在集团内部，各部门之间的协作，尤其是用户数据打通是非常困难的，所以这里建议，如果希望大数据对各个部门都能赋能，一定要在集团的战略高度上，尽早就在最开始的顶层架构上，将用户标识进行统一，这样在后续过程中才能实现用户数据的打通。

然后我们思考下，有了用户，用户画像都可以统计到哪些标签。我们按照「用户消费行为分析」的准则来进行设计。1）用户标签：性别、年龄、家乡、居住地、收货地址、婚姻、宝宝信息、通过何种渠道进行的注册。2）消费标签：餐饮口味、消费均价、团购等级、预定使用等级、排队使用等级、外卖等级。3）行为标签：点外卖时间段、使用频次、平均点餐用时、访问路径。4）内容分析：基于用户平时浏览的内容进行统计，包括餐饮口味、优惠敏感度等。

当你有了「用户消费行为分析」的标签之后，你就可以更好地理解业务了。比如一个经常买沙拉的人，一般很少吃夜宵。同样，一个经常吃夜宵的人，吃小龙虾的概率可能远高于其他人。这些结果都是通过数据挖掘中的关联分析得出的。有了这些数据，我们就可以预测用户的行为。比如一个用户购买了「月子餐」后，更有可能购买婴儿水，同样婴儿相关的产品比如婴儿湿巾等的购买概率也会增大。

具体在业务层上，我们都可以基于标签产生哪些业务价值呢？1）在获客上，我们可以找到优势的宣传渠道，如何通过个性化的宣传手段，吸引有潜在需求的用户，并刺激其转化。2）在粘客上，如何提升用户的单价和消费频次，方法可以包括购买后的个性化推荐、针对优质用户进行优质高价商品的推荐、以及重复购买，比如通过红包、优惠等方式激励对优惠敏感的人群，提升购买频次。3）在留客上，预测用户是否可能会从平台上流失。在营销领域，关于用户留存有一个观点 —— 如果将顾客流失率降低 5%，公司利润将提升 25%-85%。可以看出留存率是多么的重要。用户流失可能会包括多种情况，比如用户体验、竞争对手、需求变化等，通过预测用户的流失率可以大幅降低用户留存的运营成本。

锻炼自己的抽象能力，将繁杂的事务简单化。上面我们讲到的「用户消费行为标签」都是基于一般情况考虑的，除此之外，用户的行为也会随着营销的节奏产生异常值，比如双十一的时候，如果商家都在促销就会产生突发的大量订单。因此在做用户画像的时候，还要考虑到异常值的处理。总之，数据量是庞大的，会存在各种各样的使用情况。光是分析 EB 级别的大数据，我们就要花很长的时间。但我们的最终目的不是处理这些数据，而是理解、使用这些数据挖掘的结果。对数据的标签化能让我们快速理解一个用户，一个商品，乃至一个视频内容的特征，从而方便我们去理解和使用数据。

对数据的标签化其实考验的是我们的抽象能力，在日常工作中，我们也要锻炼自己的抽象能力，它可以让我们很快地将一个繁杂的事物简单化，不仅方便理解，还有益后续的使用。我们今天讲了用户画像的流程，其中很重要的一个步骤就是给用户打标签，那么你不妨想想，如果给羊肉串连锁店进行用户画像分析，都可以从哪些角度进行标签化？最后，我们从现实生活中出发，打开你的手机，翻翻看你的微信通讯录，分析下你的朋友圈，都有哪些用户画像？如果你来给它设计标签，都有哪些种类需要统计呢。为了方便后续使用，你是如何将他们归类分组的？

### 黑板墙

羊肉串店的用户画像。1）唯一 ID 确认：可以根据付款人、付款账号等信息确认。2）用户标签：性别、年龄、家乡。3）消费标签：餐饮口味、消费均价、预定使用等级、排队使用等级。4）行为标签：光顾时间、光顾频率、光顾的地理位置（连锁店）、平均点餐时间、对优惠券的敏感程度。5）内容标签：菜品种类、菜品数量、餐饮口味。

朋友圈画像。标签有：家人、朋友、同学、同事、陌生人。需统计的标签：1）用户标签：性别、年龄、地区、通过何种方式添加。2）消费标签：点赞内容、参与的活动等。3）行为标签：点赞频率，点赞时间、聊天时间、聊天频率、聊天时长。4）内容标签：点赞内容，聊天内容、发布朋友圈内容、点赞的文章，关注的公众号。

### 0108数据采集如何自动化采集数据.md

数据采集是数据分析的关键，很多时候我们会想到 Python 网络爬虫，实际上数据采集的方法、渠道很广，有些可以直接使用开放的数据源，比如想获取比特币历史的价格及交易数据，可以直接从 Kaggle 上下载，不需要自己爬取。另一方面根据我们的需求，需要采集的数据也不同，比如交通行业，数据采集会和摄像头或者测速仪有关。对于运维人员，日志采集和分析则是关键。所以我们需要针对特定的业务场景，选择适合的采集工具。

上一节中我们讲了如何对用户画像建模，而建模之前我们都要进行数据采集。数据采集是数据挖掘的基础，没有数据，挖掘也没有意义。很多时候，我们拥有多少数据源，多少数据量，以及数据质量如何，将决定我们挖掘产出的成果会怎样。举个例子，你做量化投资，基于大数据预测未来股票的波动，根据这个预测结果进行买卖。你当前能够拿到以往股票的所有历史数据，是否可以根据这些数据做出一个预测率高的数据分析系统呢？实际上，如果你只有股票历史数据，你仍然无法理解股票为什么会产生大幅的波动。比如，当时可能是爆发了 SARS 疫情，或者某地区发生了战争等。这些重大的社会事件对股票的影响也是巨大的。因此我们需要考虑到，一个数据的走势，是由多个维度影响的。我们需要通过多源的数据采集，收集到尽可能多的数据维度，同时保证数据的质量，这样才能得到高质量的数据挖掘结果。

那么，从数据采集角度来说，都有哪些数据源呢？我将数据源分成了以下的四类：开放数据源、爬虫抓取、传感器和日志采集。它们各有特点。

1）开放数据源一般是针对行业的数据库。比如美国人口调查局开放了美国的人口信息、地区分布和教育情况数据。除了政府外，企业和高校也会开放相应的大数据，这方面北美相对来说做得好一些。国内，贵州做了不少大胆尝试，搭建了云平台，逐年开放了旅游、交通、商务等领域的数据量。要知道很多研究都是基于开放数据源进行的，否则每年不会有那么多论文发表，大家需要相同的数据集才能对比出算法的好坏。2）爬虫抓取，一般是针对特定的网站或 App。如果我们想要抓取指定的网站数据，比如购物网站上的购物评价等，就需要我们做特定的爬虫抓取。3）传感器，它基本上采集的是物理信息。比如图像、视频、或者某个物体的速度、热度、压强等。4）日志采集，这个是统计用户的操作。我们可以在前端进行埋点，在后端进行脚本收集、统计，来分析网站的访问情况，以及使用瓶颈等。知道了有四类数据源，那如何采集到这些数据呢？

如何使用开放数据源。我们先来看下开放数据源，教你个方法，开放数据源可以从两个维度来考虑，一个是单位的维度，比如政府、企业、高校；一个就是行业维度，比如交通、金融、能源等领域。这方面，国外的开放数据源比国内做得好一些，当然近些年国内的政府和高校做开放数据源的也越来越多。一方面服务社会，另一方面自己的影响力也会越来越大。比如，下面这张表格列举的就是单位维度的数据源。所以如果你想找某个领域的数据源，比如金融领域，你基本上可以看下政府、高校、企业是否有开放的数据源。当然你也可以直接搜索金融开放数据源。

如何使用爬虫做抓取。爬虫抓取应该属于最常见的需求，比如你想要餐厅的评价数据。当然这里要注重版权问题，而且很多网站也是有反爬机制的。最直接的方法就是使用 Python 编写爬虫代码，当然前提是你需要会 Python 的基本语法。除此之外，PHP 也可以做爬虫，只是功能不如 Python 完善，尤其是涉及到多线程的操作。在 Python 爬虫中，基本上会经历三个过程。1）使用 Requests 爬取内容。我们可以使用 Requests 库来抓取网页信息。Requests 库可以说是 Python 爬虫的利器，也就是 Python 的 HTTP 库，通过这个库爬取网页中的数据，非常方便，可以帮我们节约大量的时间。2）使用 XPath 解析内容。XPath 是 XML Path 的缩写，也就是 XML 路径语言。它是一种用来确定 XML 文档中某部分位置的语言，在开发中经常用来当作小型查询语言。XPath 可以通过元素和属性进行位置索引。3）使用 Pandas 保存数据。Pandas 是让数据分析工作变得更加简单的高级数据结构，我们可以用 Pandas 保存爬取的数据。最后通过 Pandas 再写入到 XLS 或者 MySQL 等数据库中。Requests、XPath、Pandas 是 Python 的三个利器。当然做 Python 爬虫还有很多利器，比如 Selenium，PhantomJS，或者用 Puppteteer 这种无头模式。另外我们也可以不编程就抓取到网页信息，这里介绍三款常用的抓取工具。

1、火车采集器。火车采集器已经有 13 年历史了，是老牌的采集工具。它不仅可以做抓取工具，也可以做数据清洗、数据分析、数据挖掘和可视化等工作。数据源适用于绝大部分的网页，网页中能看到的内容都可以通过采集规则进行抓取。

2、八爪鱼。八爪鱼也是知名的采集工具，它有两个版本，一个就是免费的采集模板，还有一个就是云采集（付费）。免费的采集模板实际上就是内容采集规则，包括了电商类、生活服务类、社交媒体类和论坛类的网站都可以采集，用起来非常方便。当然你也可以自己来自定义任务。那什么是云采集呢？就是当你配置好采集任务，就可以交给八爪鱼的云端进行采集。八爪鱼一共有 5000 台服务器，通过云端多节点并发采集，采集速度远远超过本地采集。此外还可以自动切换多个 IP，避免 IP 被封，影响采集。做过工程项目的同学应该能体会到，云采集这个功能太方便了，很多时候自动切换 IP 以及云采集才是自动化采集的关键。

3、集搜客。这个工具的特点是完全可视化操作，无需编程。整个采集过程也是所见即所得，抓取结果信息、错误信息等都反应在软件中。相比于八爪鱼来说，集搜客没有流程的概念，用户只需要关注抓取什么数据，而流程细节完全交给集搜客来处理。但是集搜客的缺点是没有云采集功能，所有爬虫都是在用户自己电脑上跑的。

如何使用日志采集工具。传感器采集基本上是基于特定的设备，将设备采集的信息进行收集即可，这里我们就不重点讲解了。来看日志采集。为什么要做日志采集呢？日志采集最大的作用，就是通过分析用户访问情况，提升系统的性能，从而提高系统承载量。及时发现系统承载瓶颈，也可以方便技术人员基于用户实际的访问情况进行优化。日志采集也是运维人员的重要工作之一，那么日志都包括哪些呢，又该如何对日志进行采集呢？

日志就是日记的意思，它记录了用户访问网站的全过程：哪些人在什么时间，通过什么渠道（比如搜索引擎、网址输入）来过，都执行了哪些操作；系统是否产生了错误；甚至包括用户的 IP、HTTP 请求的时间，用户代理等。这些日志数据可以被写在一个日志文件中，也可以分成不同的日志文件，比如访问日志、错误日志等。

日志采集可以分两种形式。1）通过 Web 服务器采集，例如 httpd、Nginx、Tomcat 都自带日志记录功能。同时很多互联网企业都有自己的海量数据采集工具，多用于系统日志采集，如 Hadoop 的 Chukwa、Cloudera 的 Flume、Facebook 的 Scribe 等，这些工具均采用分布式架构，能够满足每秒数百 MB 的日志数据采集和传输需求。2）自定义采集用户行为，例如用 JavaScript 代码监听用户的行为、AJAX 异步请求后台记录日志等。

埋点是什么。埋点是日志采集的关键步骤，那什么是埋点呢？埋点就是在有需要的位置采集相应的信息，进行上报。比如某页面的访问情况，包括用户信息、设备信息；或者用户在页面上的操作行为，包括时间长短等。这就是埋点，每一个埋点就像一台摄像头，采集用户行为数据，将数据进行多维度的交叉分析，可真实还原出用户使用场景，和用户使用需求。

那我们要如何进行埋点呢？埋点就是在你需要统计数据的地方植入统计代码，当然植入代码可以自己写，也可以使用第三方统计工具。我之前讲到「不重复造轮子」的原则，一般来说需要自己写的代码，一般是主营核心业务，对于埋点这类监测性的工具，市场上已经比较成熟，这里推荐你使用第三方的工具，比如友盟、Google Analysis、Talkingdata 等。他们都是采用前端埋点的方式，然后在第三方工具里就可以看到用户的行为数据。但如果我们想要看到更深层的用户操作行为，就需要进行自定义埋点。

总结一下，日志采集有助于我们了解用户的操作数据，适用于运维监控、安全审计、业务数据分析等场景。一般 Web 服务器会自带日志功能，也可以使用 Flume 从不同的服务器集群中采集、汇总和传输大容量的日志数据。当然我们也可以使用第三方的统计工具或自定义埋点得到自己想要的统计内容。

### 黑板墙

据我所知，今年区块链、比特币是在年初时突然间出现了大众的视野，随之比特币高涨，吸引了不少人进场，但是也出现了不少劣币，这些劣币收割了不少韭菜，人们炒币的热度下降了不少，比特币价格大跌，而且政府也有意控制比特币。所以，我觉得需要以下的数据来判断比特币的价格走势。所需维度：1）比特币的价格走势（历史价格和交易数据）；2）区块链、比特币的热度指数（分析下降原因，人 们对比特币的使用程度）；3）相关的政策（政府的对比特币的态度）。如何搜集：Kaggle、百度指数、各大新闻网站。作者回复：同意，这道题没有标准答案，不过政策确实是个很大的因素，还有人们对于比特币的信心。

预测比特币的未来走势，可以从以下维度抓取数据：1）认知度：社会对比特币的认可，抓取百度指数、谷歌搜索量、微博数据等。2）比特币依赖的技术：作为数字货币，核心技术的完善和认可度占比重较大，可以从区块链相关技术网站爬取数据，也可以从微博爬取（微博也是技术人活跃的交流平台）。3）供给平衡：比特币虽说是一种数字货币，但仍逃脱不掉是一种商品的本质，商品必然受市场平衡调节影响，所以爬取买入量、抛售量还有历史价格也是一种预测维度。4）政府政策：政府政策的影响占很大比重，若国家出台政策强制打压或者支持，那么对价格的影响起了根本性作用，所以需要爬取相关的新闻。5）竞争数字货币：作为一种商品，必然要考虑竞争品的相关情况，需要抓取其它数字货币相关信息如其它货币的价格、交易量。资本在流入其它市场的时候，与之对应的竞品必然会受影响。抓取数据途径：其它货币交易平台。

爬虫不止 php，很多编程语言都是可以写的，java、node.js、.net、go 等等编程都可以写，R 语言也可以写爬虫。只是 python 上手比较简单，网上几乎都是 python 的教程，导致有些人认为只有 python 能写。作者回复：同意，同样数据分析也不止是 Python 语言，只是用的人多，资料文档还有第三方工具相比于其他语言更完善。

埋点是怎么一种操作？作者回复：比如使用前端埋点，你可以通过 JavaScript 获取一些信息，包括页面标题，访问的 URL，浏览器的语言，显示的颜色深度，分辨率等。同时你还可以通过埋点获取想要监测的业务数据。

影响比特币的几大因素：1）政府的调控，政府可以通过定价控制各种资产，通过交易来控制国际市场，政府有能力通过降低加比特币的热情，限制沉迷加密货币的总体成本，进行第三方监控机制，使得比特币透明化，会对比特币的价格有一定的影响。2）媒体的影响力：针对媒体的一些负面影响，从而导致价格下跌，3）区块链的稳定性和网络的稳定性，针对网络安全，安全系数的增加，会伴随价格的上涨，同时和人民币不同的是，这种比特币充满商业泡沫，本身不存在价值，要和其他货币交换才有价值，也会导致比特币价格下跌。4）比特币的需求和供应，比特币的总数不能超过 2100 万，买和卖的供应需求不对等的情况下，也会导致价格上涨。5）交易量和交易频率：交易数量表明比特币网络的使用增加或者减少，这是衡量比特币涨跌最佳方式之一。6）更广泛的接受：越来越多人接受的事物，也就成了一种交易手段，比特币在是市场的一个接受程度，也在影响它的一个价值提现。7）采矿难度：犹如黄金和钻石一样，开采的成本也会影响价格，8）技术改革：是一个非常动态的环境，技术会影响比特币的定价，因为会使它交易更加容易，技术越发苏武成熟。

### 0109数据采集如何用八爪鱼采集微博上的评论.md

讲下我的一些心得体会。我们的工作流程通常很长，所以更应该专注工作的核心，比如说数据分析这块，所有的辅助都可以采用第三方工具来做。如果老板让你统计微博上的评论，实际上老板最想知道的不是采集的过程，而是整体的概况，比如说影响了多少人，评论如何，是否有 KOL 关注等等。如果你之前没有数据采集的经验，那么第三方工具，以及采用可视化的方式来进行采集应该是你的首选。可视化的方式可以让你迅速上手，了解到整个数据采集的过程。我们应该从基础步骤开始，遇到特定需求的时候再学习了解高级步骤。这篇文章只介绍了基本的流程，但你可以上手操作了。在实际操作中，你可能会遇到各种问题，这个时候再对高级步骤进行学习，如果要进阶的话，还需要你掌握 XPath 的使用。

八爪鱼的基本操作。在开始操作前，我先来介绍下今天要讲的主角「八爪鱼」工具。相比使用 Python 进行爬虫，八爪鱼的使用更加简便，因为是所见即所得的方式，基本上不需要编写代码，除了在正则表达式匹配的时候会用到 XPath。这里简单介绍下 XPath，XPath 的英文是 XML Path Language，也就是 XML 的路径语言，用来在 XML 文件中寻找我们想要的元素。所以八爪鱼可以使用 XPath 帮我们更灵活地定位我们想要找的元素。

自定义任务 VS 简易采集。如果你想要采集数据就需要新建一个任务，在建任务的时候，八爪鱼会给你一个提示，是使用八爪鱼自带的「简易采集」，还是自定义一个任务。简易采集集成了一些热门的模板，也就是我们经常访问的一些网站。它可以帮助我们轻松地实现采集，只需要我们告诉工具两个信息即可，一个是需要采集的网址，另一个是登录网站的账号和密码。虽然简易采集比较方便快捷，但通常还是推荐使用自定义任务的方式，这样可以更灵活地帮我们提取想要的信息，比如你只想采集关于「D&G」的微博评论。

八爪鱼的采集共分三步：1）输入网页：每个采集需要输入你想要采集的网页。在新建任务的时候，这里是必填项。2）设计流程：这个步骤最为关键，你需要告诉八爪鱼，你是如何操作页面的、想要提取页面上的哪些信息等。因为数据条数比较多，通常你还需要翻页，所以要进行循环翻页的设置。在设计流程中，你可以使用简易采集方式，也就是八爪鱼自带的模板，也可以采用自定义的方式。3）启动采集：当你设计好采集流程后，就可以启动采集任务了，任务结束后，八爪鱼会提示你保存采集好的数据，通常是 xlsx 或 csv 格式。

如果你使用的是自定义采集，就需要自己来设计采集流程，也就是采集流程中的第二步。八爪鱼的流程步骤有两类，可以划分为基本步骤和高级步骤。基本步骤就是最常用的步骤，每次采集都会用到，一共分为 4 步，分别是打开网页、点击元素、循环翻页、提取数据。高级步骤是辅助步骤，可以帮我们更好地对数据进行提取，比如我们想要某个关键词的数据，就需要在网页输入框中输入对应的文字。有时候源网页的系统会提示需要输入验证码，我们就可以采用验证码识别的模块帮我们解决。有时候我们需要用下拉选项帮我们筛选想要的数据，或者某些判断条件下（比如存在某个关键词）才触发的采集等。这些操作可以更精细化地提取想要的数据。

下面我来介绍下基本步骤：1）打开网页。所有的采集默认第一项都是打开网页。所以在新建任务之后，系统会提示你输入网址。当你输入之后，八爪鱼就会自动建立一个「打开网页」的流程。2）点击元素。这里元素的定义比较广泛，它可以是某个按钮，或者某个链接，也或者是某个图片或文字。使用这个步骤是你在搜索或者提交某个请求。当你点击元素后，八爪鱼会提示你想要达到的目的：点击该按钮、采集该元素文本、还是鼠标移到该链接上。然后再选择「点击该按钮」进行确认即可。如果我们点击某个元素的目的是循环翻页，或者提取数据，那么在点击之后，八爪鱼会确认你的目的，你只要点击相关的按钮即可。3）循环翻页。很多数据都存在翻页的情况，通常你需要找到翻页的位置，比如网页底部的「下一页」按钮，点击它，会提示你「循环点击下一页」、「采集该链接文本」还是「点击该链接」。你需要确认这里是进行的「循环点击下一页」。4）提取数据。在网页上选择你想要提取的页面范围，鼠标移动到页面上会呈现蓝色的阴影面积，它表明了你想提取的数据范围。然后点击鼠标后，在右侧选择「采集数据」即可。

这 4 个基本操作就像它们的名称一样简单直接，这里我给你一些使用的建议：1）尽量使用用户操作视角进行模拟的方式进行操作，而不是在「流程视图」中手动创建相应的步骤。因为八爪鱼最大的特点就是所见即所得，所以一切就按照用户使用的流程进行操作即可。2）使用「流程视图」方便管理和调整。右侧有「流程视图」的按钮，点击之后进入到流程视图，会把你之前的操作以流程图的方式展示出来。我会在文章下面详细介绍一下。

为什么要这么做呢？这样的话每个步骤流程清晰可见，而且你还可以调整每个步骤的参数，比如你之前的网址写错了想要修改，或者之前输入的文字需要调整等。另外很多时候需要账号登录后才能采集数据，我们可以提前在八爪鱼工具里登录，这样再进行抓取的时候就是登录的状态，直接进行采集就可以了。

采集微博上的「Dolce&Gabbana」评论。在了解基本步骤之后，我们就可以自己动手采集内容了。比如说我想要采集微博上关于「D&G」的评论，那么我可以先在浏览器上，人工操作下整个流程，梳理出来以下的步骤。这几个流程具体是怎么做的呢？我来给你一一梳理一下。

1）输入网页。对应基本步骤「打开网页」，我们输入微博搜索的地址：[微博搜索](https://s.weibo.com/)。2）输入关键词。对应「输入文本」，我把鼠标移动到输入框中，点击后会在右侧进行操作目的的确认，选择「输入文本」即可，然后输入我们想要搜索的内容「D&G」。3）点击搜索。对应「点击元素」，我们点击「搜索按钮」，然后确认操作目的是「点击元素」。4）设置翻页。因为我们想要采集全量数据，因此需要先设置翻页。这里特别注意下，翻页的操作要在数据提取之前，因为翻页是个循环的命令，就像我们平时写 for 语句一样，一定是先设置 for 循环，然后在循环中进行数据提取。

5）提取数据。提取数据的时候，我们需要提取多个字段，比如，用户、微博内容、发表时间、该微博网址。而且一个页面上会有多个微博，都需要进行采集。所以你需要先选择单条内容的最大的目标区域，在确认目的时，会发现里面有子元素，这里目的选择为「选中子元素」。因为我们要对子元素内容进行采集，方便把内容按照字段进行划分。这时会提示页面中还有 20 个相同元素时，选择「选中全部」即可。6）启动采集。都选择好之后，系统会给出三个提示，分别是「启动本地采集」、「启动云采集」和「设置定时采集」。数据量不大的时候，我们选择「启动本地采集」即可。你可以看出，这整个过程比较简便，但中间有一些细节你可能会出错，比如说你忘记了先翻页，再选取你想提取的元素。这样如果遇到了问题，有两个重要的工具一定要用好：流程视图和 XPath。

流程视图。流程视图我在上面提到过，这里详细介绍一下。流程视图应该是在可视化中应用最多的场景，我们可以使用流程视图查看创建流程，调整顺序，或者删掉不想要的步骤。另外我们还能在视图中查看数据提取的字段。选中「提取数据」步骤，可以看到该步骤提取的字段都有哪些。一般都会出现很多冗余的字段，因为 HTML 代码段中有很多隐藏的内容也会被提取到，这里你可以删掉没用的字段，把保留的字段名称进行备注修改。

1『微博搜索里输入的关键词是「dolce&gabbana」。使用后发现还是插件「Web Scraper」更方便。』

流程视图我在上面提到过，这里详细介绍一下。流程视图应该是在可视化中应用最多的场景，我们可以使用流程视图查看创建流程，调整顺序，或者删掉不想要的步骤。另外我们还能在视图中查看数据提取的字段。选中「提取数据」步骤，可以看到该步骤提取的字段都有哪些。一般都会出现很多冗余的字段，因为 HTML 代码段中有很多隐藏的内容也会被提取到，这里你可以删掉没用的字段，把保留的字段名称进行备注修改。这里有张图，是我通过八爪鱼可视化操作采集微博评论时，自动生成的流程视图。

介绍完流程视图之后，我们再来说一下 XPath。在八爪鱼工具中内置了 XPath 引擎，所以在我们用可视化方式选择元素的时候，会自动生成相应的 XPath 路径。当然我们也可以查看这些元素的 XPath，方便对它们进行精细地控制。为什么有了可视化操作，还需要自己来定义 XPath 呢？这是因为有时候我们采集的网站页面是不规律的，比如你可以看到微博搜索结果页中，第一页和第二页的 HTML 排版是不同的，这样的话，可视化操作得到的 XPath 可能不具备通用性。这种情况下，如果你用搜索结果第一页提取数据得到的 XPath，就无法匹配搜索结果页第二页的数据。在八爪鱼工具中，很多控件都有 XPath，最常用的还是循环和提取数据中的 XPath，下面我来一一简单介绍下。

循环中的 XPath。在微博采集这个例子中，我们用到了两种循环方式，一种是「循环翻页」，一种是「循环列表」。在「循环翻页」中，你可以在「流程视图」中点击「循环翻页」的控件，看到右侧的「高级选项」中的 XPath。在微博采集这个例子中，循环翻页的 XPath 是 //A [@class=‘next’]。在「循环列表」中，我在提取数据的时候，出现了页面提示「还有 20 个相同元素」，这时我选择「选中全部」。相当于出现了 20 个元素的循环列表。所以你在流程视图中，可以会看到提取数据外层嵌套了个循环列表。同样我们可以看到循环列表的 XPath 是 //DIV [@class=‘card-feed’]。

提取数据的 XPath。当我们点击流程中的「提取数据」，可以看到有很多字段名称，XPath 实际上定位到了这些要采集的字段。所以你需要选中一个字段，才能看到它的 XPath。现在你知道了，八爪鱼的匹配是基于 XPath 的，那么你也可以自己来调整 XPath 参数。这样当匹配不到想要的数据的时候，可以检查下是不是 XPath 参数设置的问题，可以手动调整，从而抓取到正确的元素。

### 0110Python爬虫如何自动化下载王祖贤海报.md

1）Python 爬虫的流程；2）了解 XPath 定位，JSON 对象解析；3）如何使用 lxml 库，进行 XPath 的提取；4）如何在 Python 中使用 Selenium 库来帮助你模拟浏览器，获取完整的 HTML。其中，Python + Selenium + 第三方浏览器可以让我们处理多种复杂场景，包括网页动态加载、JS 响应、Post 表单等。因为 Selenium 模拟的就是一个真实的用户的操作行为，就不用担心 cookie 追踪和隐藏字段的干扰了。当然，Python 还给我们提供了数据处理工具，比如 lxml 库和 JSON 库，这样就可以提取想要的内容了。

像八爪鱼这种可视化的采集是一种非常好的方式。它最大的优点就是上手速度快，当然也存在一些问题，比如运行速度慢、可控性差等。相比之下，爬虫可以很好地避免这些问题，今天我来分享下如何通过编写爬虫抓取数据。

爬虫的流程。相信你对「爬虫」这个词已经非常熟悉了，爬虫实际上是用浏览器访问的方式模拟了访问网站的过程，整个过程包括三个阶段：打开网页、提取数据和保存数据。在 Python 中，这三个阶段都有对应的工具可以使用。1）在「打开网页」这一步骤中，可以使用 Requests 访问页面，得到服务器返回给我们的数据，这里包括 HTML 页面以及 JSON 数据。2）在「提取数据」这一步骤中，主要用到了两个工具。针对 HTML 页面，可以使用 XPath 进行元素定位，提取数据；针对 JSON 数据，可以使用 JSON 进行解析。3）在最后一步「保存数据」中，我们可以使用 Pandas 保存数据，最后导出 CSV 文件。

Requests 访问页面。Requests 是 Python HTTP 的客户端库，编写爬虫的时候都会用到，编写起来也很简单。它有两种访问方式：Get 和 Post。这两者最直观的区别就是：Get 把参数包含在 url 中，而 Post 通过 request body 来传递参数。假设我们想访问豆瓣，那么用 Get 访问的话，代码可以写成下面这样的：

    r = requests.get('http://www.douban.com')

代码里的「r」就是 Get 请求后的访问结果，然后我们可以使用 r.text 或 r.content 来获取 HTML 的正文。如果我们想要使用 Post 进行表单传递，代码就可以这样写：

    r = requests.post('http://xxx.com', data = {'key':'value'})

这里 data 就是传递的表单参数，data 的数据类型是个字典的结构，采用 key 和 value 的方式进行存储。

XPath 是 XML 的路径语言，实际上是通过元素和属性进行导航，帮我们定位位置。它有几种常用的路径表达方式。我来给你简单举一些例子：

xpath (‘node’) 选取了 node 节点的所有子节点；xpath (’/div’) 从根节点上选取 div 节点；xpath (’//div’) 选取所有的 div 节点；xpath (’./div’) 选取当前节点下的 div 节点；xpath (’…’) 回到上一个节点；xpath (’//@id’) 选取所有的 id 属性；xpath (’//book [@id]’) 选取所有拥有名为 id 的属性的 book 元素；xpath (’//book [@id="abc"]’) 选取所有 book 元素，且这些 book 元素拥有 id= "abc" 的属性；xpath (’//book/title | //book/price’) 选取 book 元素的所有 title 和 price 元素。

上面我只是列举了 XPath 的部分应用，XPath 的选择功能非常强大，它可以提供超过 100 个内建函数，来做匹配。我们想要定位的节点，几乎都可以使用 XPath 来选择。使用 XPath 定位，你会用到 Python 的一个解析库 lxml。这个库的解析效率非常高，使用起来也很简便，只需要调用 HTML 解析命令即可，然后再对 HTML 进行 XPath 函数的调用。比如我们想要定位到 HTML 中的所有列表项目，可以采用下面这段代码。

```
from lxml import etree
html = etree.HTML(html)
result = html.xpath('//li')
```

JSON 是一种轻量级的交互方式，在 Python 中有 JSON 库，可以让我们将 Python 对象和 JSON 对象进行转换。为什么要转换呢？原因也很简单。将 JSON 对象转换成为 Python 对象，我们对数据进行解析就更方便了。这是一段将 JSON 格式转换成 Python 对象的代码，你可以自己运行下这个程序的结果。接下来，我们就要进行实战了，我会从两个角度给你讲解如何使用 Python 爬取海报，一个是通过 JSON 数据爬取，一个是通过 XPath 定位爬取。

```
import json
jsonData = '{"a":1,"b":2,"c":3,"d":4,"e":5}';
input = json.loads(jsonData)
print input
```

1『json.dumps() 是将 Python 对象转换为 JSON 对象；json.load() 是将 JSON 对象转换为 Python 对象。』

我在上面讲了 Python 爬虫的基本原理和实现的工具，下面我们来实战一下。如果想要从豆瓣图片中下载王祖贤的海报，你应该先把我们日常的操作步骤整理下来：打开网页；输入关键词「王祖贤」；在搜索结果页中选择「图片」；下载图片页中的所有海报。

这里你需要注意的是，如果爬取的页面是动态页面，就需要关注 XHR 数据。因为动态页面的原理就是通过原生的 XHR 数据对象发出 HTTP 请求，得到服务器返回的数据后，再进行处理。XHR 会用于在后台与服务器交换数据。你需要使用浏览器的插件查看 XHR 数据，比如在 Chrome 浏览器中使用开发者工具。

在豆瓣搜索中，我们对「王祖贤」进行了模拟，发现 XHR 数据中有一个请求是这样的：

    https://www.douban.com/j/search_photo?q=%E7%8E%8B%E7%A5%96%E8%B4%A4&limit=20&start=0

1『这里正常打开的 URL 是「https://www.douban.com/search?cat=1025&q=%E7%8E%8B%E7%A5%96%E8%B4%A4」，需要进 Chrome 的开发者工具（快捷键 option+command+I），在 Network 里，在过滤器 Filter 里选 XHR，刷新网页。Name 里会出现结果，点第一个进去，可以看到 XHR 数据对象发出 HTTP 请求时的 URL。』

url 中的乱码正是中文的 url 编码，打开后，我们看到了很清爽的 JSON 格式对象，展示的形式是这样的：

```
{"images":
       [{"src": …, "author": …, "url":…, "id": …, "title": …, "width":…, "height":…},
    …
   {"src": …, "author": …, "url":…, "id": …, "title": …, "width":…, "height":…}],
 "total":22471,"limit":20,"more":true}
```
 
从这个 JSON 对象中，我们能看到，王祖贤的图片一共有 22471 张，其中一次只返回了 20 张，还有更多的数据可以请求。数据被放到了 images 对象里，它是个数组的结构，每个数组的元素是个字典的类型，分别告诉了 src、author、url、id、title、width 和 height 字段，这些字段代表的含义分别是原图片的地址、作者、发布地址、图片 ID、标题、图片宽度、图片高度等信息。有了这个 JSON 信息，你很容易就可以把图片下载下来。当然你还需要寻找 XHR 请求的 url 规律。如何查看呢，我们再来重新看下这个网址本身。

[https://www.douban.com/j/search_photo?q=王祖贤&limit=20&start=0](https://www.douban.com/j/search_photo?q=%E7%8E%8B%E7%A5%96%E8%B4%A4&limit=20&start=0)

你会发现，网址中有三个参数：q、limit 和 start。start 实际上是请求的起始 ID，这里我们注意到它对图片的顺序标识是从 0 开始计算的。所以如果你想要从第 21 个图片进行下载，你可以将 start 设置为 20。王祖贤的图片一共有 22471 张，你可以写个 for 循环来跑完所有的请求，具体的代码如下：

```
# coding:utf-8
import requests
import json
query = '王祖贤'
''' 下载图片 '''
def download(src, id):
  dir = './' + str(id) + '.jpg'
  try:
    pic = requests.get(src, timeout=10)
    fp = open(dir, 'wb')
    fp.write(pic.content)
    fp.close()
  except requests.exceptions.ConnectionError:
    print('图片无法下载')
            
''' for 循环 请求全部的 url '''
for i in range(0, 22471, 20):
  url = 'https://www.douban.com/j/search_photo?q='+query+'&limit=20&start='+str(i)
  html = requests.get(url).text    # 得到返回结果
  response = json.loads(html,encoding='utf-8') # 将 JSON 格式转换成 Python 对象
  for image in response['images']:
    print(image['src']) # 查看当前下载的图片网址
    download(image['src'], image['id']) # 下载一张图片
```

如何使用 XPath 自动下载王祖贤的电影海报封面。如果你遇到 JSON 的数据格式，那么恭喜你，数据结构很清爽，通过 Python 的 JSON 库就可以解析。但有时候，网页会用 JS 请求数据，那么只有 JS 都加载完之后，我们才能获取完整的 HTML 文件。XPath 可以不受加载的限制，帮我们定位想要的元素。比如，我们想要从豆瓣电影上下载王祖贤的电影封面，需要先梳理下人工的操作流程：1）打开网页[豆瓣电影](https://movie.douban.com/)；2）输入关键词「王祖贤」；3）下载图片页中的所有电影封面。

这里你需要用 XPath 定位图片的网址，以及电影的名称。一个快速定位 XPath 的方法就是采用浏览器的 XPath Helper 插件，使用 Ctrl+Shift+X 快捷键的时候，用鼠标选中你想要定位的元素，就会得到类似下面的结果。XPath Helper 插件中有两个参数，一个是 Query，另一个是 Results。Query 其实就是让你来输入 XPath 语法，然后在 Results 里看到匹配的元素的结果。

1『在 Chrome 商店里下载 XPath Helper，注意要重启 Chrome 才会生效，在浏览器里输入命令「chrome://restart」重启。』

我们看到，这里选中的是一个元素，我们要匹配上所有的电影海报，就需要缩减 XPath 表达式。你可以在 Query 中进行 XPath 表达式的缩减，尝试去掉 XPath 表达式中的一些内容，在 Results 中会自动出现匹配的结果。经过缩减之后，你可以得到电影海报的 XPath（假设为变量 src_xpath）：

    //div[@class='item-root']/a[@class='cover-link']/img[@class='cover']/@src

以及电影名称的 XPath（假设为变量 title_xpath）：

    //div[@class='item-root']/div[@class='detail']/div[@class='title']/a[@class='title-text']

但有时候当我们直接用 Requests 获取 HTML 的时候，发现想要的 XPath 并不存在。这是因为 HTML 还没有加载完，因此你需要一个工具，来进行网页加载的模拟，直到完成加载后再给你完整的 HTML。在 Python 中，这个工具就是 Selenium 库，使用方法如下：

```
from selenium import webdriver
driver = webdriver.Chrome()
driver.get(request_url)
```

Selenium 是 Web 应用的测试工具，可以直接运行在浏览器中，它的原理是模拟用户在进行操作，支持当前多种主流的浏览器。这里我们模拟 Chrome 浏览器的页面访问。你需要先引用 Selenium 中的 WebDriver 库。WebDriver 实际上就是 Selenium 2，是一种用于 Web 应用程序的自动测试工具，提供了一套友好的 API，方便我们进行操作。然后通过 WebDriver 创建一个 Chrome 浏览器的 drive，再通过 drive 获取访问页面的完整 HTML。

当你获取到完整的 HTML 时，就可以对 HTML 中的 XPath 进行提取，在这里我们需要找到图片地址 srcs 和电影名称 titles。这里通过 XPath 语法匹配到了多个元素，因为是多个元素，所以我们需要用 for 循环来对每个元素进行提取。然后使用上面我编写好的 download 函数进行图片下载。

```
srcs = html.xpath(src_xpath)
titles = html.xpath(title_path)
for src, title in zip(srcs, titles):
  download(src, title.text)
```  

### 黑板墙

如果是需要用户登陆后才能爬取的数据该怎么用 python 来实现呢？作者回复：你可以使用 python+selenium 的方式完成账户的自动登录，因为 selenium 是个自动化测试的框架，使用 selenium 的 webdriver 就可以模拟浏览器的行为。找到输入用户名密码的地方，输入相应的值，然后模拟点击即可完成登录（没有验证码的情况下）。另外你也可以使用 cookie 来登录网站，方法是你登录网站时，先保存网站的 cookie，然后用下次访问的时候，加载之前保存的 cookie，放到 request headers 中，这样就不需要再登录网站了。

说明两点问题：1）留言里有人评论说用 XPath 下载的图片打不开，其原因是定义的下载函数保存路径后缀名为 '.jpg'，但是用 XPath 下载获得的图片 url 为 'https://img3.doubanio.com/view/celebrity/s_ratio_celebrity/public/p616.webp'，本身图片为 webp 格式，所以若保存为 jpg 格式，肯定是打不开的。2）老师在文章内讲的用 XPath 下载代码只能下载第一页的内容，并不是全部的数据，不知道大家有没有查看用 xpath 函数获得的数组，大家留言里的代码似乎和老师的一样，只能得到首页的内容，所以也是需要模拟翻页操作才能获得完整的数据。

那些用 ChromeDriver 的出现报错的可能是没有安装 ChromeDriver，或者是没给出 ChromeDriver 的路径，具体可以看看下面这篇文章：[Python 用 ChromeDriver 实现登录和签到](https://mp.weixin.qq.com/s/UL0bcLr3KOb-qpI9oegaIQ)。

## 01数据分析基础篇3

### 1. 逻辑脉络

1、在分析前，要投入大量的时间和精力把数据「整理裁剪」成自己想要或需要的样子。好的数据分析师必定是一名数据清洗高手，要知道在整个数据分析过程中，不论是在时间还是功夫上，数据清洗大概都占到了 80%。数据清洗规则有 4 个关键点，「完全合一」：完整性、完全性、合法性和唯一性。

2、数据集成有更广泛的意义，包括了数据清洗、数据抽取、数据集成和数据变换等操作。数据集成的两种架构分别为 ETL 和 ELT，实现 ETL 的典型工具如 Kettle。ETL 是数据挖掘前重要的工作，包括了抽取各种数据、完成转化和加载三个步骤。

3、数据变换，是让不同渠道的数据统一到一个目标数据库里。数据变换是数据准备的重要环节，它通过数据平滑、数据聚集、数据概化和规范化等方式将数据转换成适用于数据挖掘的形式。有时候数据变换比算法选择更重要，数据错了，算法再正确也是错的。数据规范化有 3 种常用的方法：Min-max 规范化、Z-Score 规范化和小数定标规范化。Python 的 sklearn 库里有实现数据规范化的函数。

4、数据可视化的学习有三个路径：重点推荐 Tableau；使用微图、DataV；Python 可视化。Excel：体量小且需要马上出结果；Python：灵活可复用，数据量大；Tableau：清洗数据后使用，一般不知道如何分析时，会用 Tableau 随意分析，找寻灵感。

5、Matplotlib 和 Seaborn 工具包的使用。视图的分类，以及可以从哪些维度对它们进行分类，从数据间关系的维度看有比较、联系、构成和分布 4 大类，从变量个数的维度看有单变量和多变量的 2 大类。十种常见视图的概念，以及如何在 Python 中进行使用，都需要用到哪些函数。

### 2. 摘录及评论

### 0111数据科学家花费在了这些清洗任务上的时间.md

上一节中讲了数据采集，以及相关的工具使用，但做完数据采集就可以直接进行挖掘了吗？肯定不是的。就拿做饭打个比方吧，对于很多人来说，热油下锅、掌勺翻炒一定是做饭中最过瘾的环节，但实际上炒菜这个过程只占做饭时间的 20%，剩下 80% 的时间都是在做准备，比如买菜、择菜、洗菜等等。在数据挖掘中，数据清洗就是这样的前期准备工作。对于数据科学家来说，我们会遇到各种各样的数据，在分析前，要投入大量的时间和精力把数据「整理裁剪」成自己想要或需要的样子。

为什么呢？因为我们采集到的数据往往有很多问题。我们先看一个例子，假设老板给你以下的数据，让你做数据分析，你看到这个数据后有什么感觉呢？你刚看到这些数据可能会比较懵，因为这些数据缺少标注。

我们在收集整理数据的时候，一定要对数据做标注，数据表头很重要。比如这份数据表，就缺少列名的标注，这样一来我们就不知道每列数据所代表的含义，无法从业务中理解这些数值的作用，以及这些数值是否正确。但在实际工作中，也可能像这个案例一样，数据是缺少标注的。

我简单解释下这些数据代表的含义。这是一家服装店统计的会员数据。最上面的一行是列坐标，最左侧一列是行坐标。列坐标中，第 0 列代表的是序号，第 1 列代表的会员的姓名，第 2 列代表年龄，第 3 列代表体重，第 4-6 列代表男性会员的三围尺寸，第 7-9 列代表女性会员的三围尺寸。了解含义以后，我们再看下中间部分具体的数据，你可能会想，这些数据怎么这么「脏乱差」啊，有很多值是空的（NaN），还有空行的情况。

是的，这还仅仅是一家商店的部分会员数据，我们一眼看过去就能发现一些问题。日常工作中的数据业务会复杂很多，通常我们要统计更多的数据维度，比如 100 个指标，数据量通常都是超过 TB、EB 级别的，所以整个数据分析的处理难度是呈指数级增加的。这个时候，仅仅通过肉眼就很难找到问题所在了。

我举了这样一个简单的例子，带你理解在数据分析之前为什么要有数据清洗这个重要的准备工作。有经验的数据分析师都知道，好的数据分析师必定是一名数据清洗高手，要知道在整个数据分析过程中，不论是在时间还是功夫上，数据清洗大概都占到了 80%。

数据质量的准则。在上面这个服装店会员数据的案例中，一看到这些数据，你肯定能发现几个问题。你是不是想知道，有没有一些准则来规范这些数据的质量呢？准则肯定是有的。不过如果数据存在七八种甚至更多的问题，我们很难将这些规则都记住。而数据清洗要解决的问题，远不止 7 条，我们万一漏掉一项该怎么办呢？有没有一种方法，我们既可以很方便地记住，又能保证我们的数据得到很好的清洗，提升数据质量呢？在这里，我将数据清洗规则总结为以下 4 个关键点，统一起来叫「完全合一」，下面我来解释下。

1）完整性：单条数据是否存在空值，统计的字段是否完善。2）全面性：观察某一列的全部数值，比如在 Excel 表中，我们选中一列，可以看到该列的平均值、最大值、最小值。我们可以通过常识来判断该列是否有问题，比如：数据定义、单位标识、数值本身。3）合法性：数据的类型、内容、大小的合法性。比如数据中存在非 ASCII 字符，性别存在了未知，年龄超过了 150 岁等。4）唯一性：数据是否存在重复记录，因为数据通常来自不同渠道的汇总，重复的情况是常见的。行数据、列数据都需要是唯一的，比如一个人不能重复记录多次，且一个人的体重也不能在列指标中重复记录多次。

在很多数据挖掘的教学中，数据准则通常会列出来 7-8 项，在这里我们归类成了「完全合一」4 项准则，按照以上的原则，我们能解决数据清理中遇到的大部分问题，使得数据标准、干净、连续，为后续数据统计、数据挖掘做好准备。如果想要进一步优化数据质量，还需要在实际案例中灵活使用。

清洗数据，一一击破。了解了数据质量准则之后，我们针对上面服装店会员数据案例中的问题进行一一击破。这里你就需要 Python 的 Pandas 工具了。这个工具我们之前介绍过。它是基于 NumPy 的工具，专门为解决数据分析任务而创建。Pandas 纳入了大量库，我们可以利用这些库高效地进行数据清理工作。

这里我补充说明一下，如果你对 Python 还不是很熟悉，但是很想从事数据挖掘、数据分析相关的工作，那么花一些时间和精力来学习一下 Python 是很有必要的。Python 拥有丰富的库，堪称数据挖掘利器。当然了，数据清洗的工具也还有很多，这里我们只是以 Pandas 为例，帮你应用数据清洗准则，带你更加直观地了解数据清洗到底是怎么回事儿。下面，我们就依照「完全合一」的准则，使用 Pandas 来进行清洗。

1、完整性。问题 1：缺失值。在数据中有些年龄、体重数值是缺失的，这往往是因为数据量较大，在过程中，有些数值没有采集到。通常我们可以采用以下三种方法：1）删除：删除数据缺失的记录；2）均值：使用当前列的均值；3）高频：使用当前列出现频率最高的数据。比如我们想对 df [‘Age’] 中缺失的数值用平均年龄进行填充，可以这样写；如果我们用最高频的数据进行填充，可以先通过 value_counts 获取 Age 字段最高频次 age_maxf，然后再对 Age 字段中缺失的数据用 age_maxf 进行填充；问题 2：空行。我们发现数据中有一个空行，除了 index 之外，全部的值都是 NaN。Pandas 的 read_csv () 并没有可选参数来忽略空行，这样，我们就需要在数据被读入之后再使用 dropna () 进行处理，删除空行。

```
df['Age'].fillna(df['Age'].mean(), inplace=True)

age_maxf = train_features['Age'].value_counts().index[0]
train_features['Age'].fillna(age_maxf, inplace=True)

# 删除全空的行
df.dropna(how='all',inplace=True) 
```

2、全面性。问题：列数据的单位不统一。观察 weight 列的数值，我们能发现 weight 列的单位不统一。有的单位是千克（kgs），有的单位是磅（lbs）。这里我使用千克作为统一的度量单位，将磅（lbs）转化为千克（kgs）：

```
# 获取 weight 数据列中单位为 lbs 的数据
rows_with_lbs = df['weight'].str.contains('lbs').fillna(False)
print df[rows_with_lbs]
# 将 lbs转换为 kgs, 2.2lbs=1kgs
for i,lbs_row in df[rows_with_lbs].iterrows():
  # 截取从头开始到倒数第三个字符之前，即去掉lbs。
  weight = int(float(lbs_row['weight'][:-3])/2.2)
  df.at[i,'weight'] = '{}kgs'.format(weight) 
```

3、合理性。问题：非 ASCII 字符。我们可以看到在数据集中 Firstname 和 Lastname 有一些非 ASCII 的字符。我们可以采用删除或者替换的方式来解决非 ASCII 问题，这里我们使用删除方法：

```
# 删除非 ASCII 字符
df['first_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
df['last_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
```

1『又见正则表达式，太重要了，好好看那两本书。已下载书籍「2019082Python核心编程3Ed/2019082Core_Python_Applications_Programming3Ed」和「2020059精通正则表达式/2020059Mastering_Regular_Expressions」。』

4、唯一性。问题 1：一列有多个参数。

在数据中不难发现，姓名列（Name）包含了两个参数 Firstname 和 Lastname。为了达到数据整洁目的，我们将 Name 列拆分成 Firstname 和 Lastname 两个字段。我们使用 Python 的 split 方法，str.split (expand=True)，将列表拆成新的列，再将原来的 Name 列删除。问题 2：重复数据。我们校验一下数据中是否存在重复记录。如果存在重复记录，就使用 Pandas 提供的 drop\_duplicates () 来删除重复数据。这样，我们就将上面案例中的会员数据进行了清理，来看看清理之后的数据结果。怎么样？是不是又干净又标准？

```
# 切分名字，删除源数据列
df[['first_name','last_name']] = df['name'].str.split(expand=True)
df.drop('name', axis=1, inplace=True)

# 删除重复数据行
df.drop\_duplicates(['first_name','last_name'],inplace=True)
```

养成数据审核的习惯。现在，你是不是能感受到数据问题不是小事，上面这个简单的例子里都有 6 处错误。所以我们常说，现实世界的数据是「肮脏的」，需要清洗。第三方的数据要清洗，自有产品的数据，也需要数据清洗。比如美团自身做数据挖掘的时候，也需要去除爬虫抓取，作弊数据等。可以说没有高质量的数据，就没有高质量的数据挖掘，而数据清洗是高质量数据的一道保障。当你从事这方面工作的时候，你会发现养成数据审核的习惯非常重要。而且越是优秀的数据挖掘人员，越会有「数据审核」的「职业病」。这就好比编辑非常在意文章中的错别字、语法一样。

数据的规范性，就像是你的作品一样，通过清洗之后，会变得非常干净、标准。当然了，这也是一门需要不断修炼的功夫。终有一天，你会进入这样一种境界：看一眼数据，差不多 7 秒钟的时间，就能知道这个数据是否存在问题。为了这一眼的功力，我们要做很多练习。刚开始接触数据科学工作的时候，一定会觉得数据挖掘是件很酷、很有价值的事。确实如此，不过今天我还要告诉你，再酷炫的事也离不开基础性的工作，就像我们今天讲的数据清洗工作。对于这些基础性的工作，我们需要耐下性子，一个坑一个坑地去解决。

### 黑板墙

觉得完全合一原则挺好，不过有些操作顺序是不是得更改一下，比如数值补全要在删除全空行之后，否则在补全的时候全空行也会补全。接下来总结在清洗过程中的问题：1）不知道 Python2 执行情况如何，在用 Python3 进行数据清理的时候，对于女性三围数据补全的时候因为列中有空字符的存在，会提示 ‘must str not int’，需要自己过滤含有数值的有效数据进行 mean () 计算。2）生成的新列一般会自动补到后面，但 first_name，last_name 需要在第一列和第二列，所以要进行列移动或列交换。3）在删除数据之后默认加载的索引会出现问题，需要自己更新索引。

首先按照所讲的数据质量准则，数据存在的问题有：1）「完整性」问题：数据有缺失，在 ounces 列的第三行存在缺失值。处理办法：可以用该列的平均值来填充此缺失值。2）「全面性」问题：food 列的值大小写不统一。处理办法：统一改为小写。3）「合理性」问题：某一行的 ounces 值出现负值。处理办法：将该条数据记录删除4）「唯一性」问题：food 列大小写统一后会出现同名现象。处理办法：需要将 food 列和 animal 列值均相同的数据记录进行合并到同一天记录中国。

另外感觉这一讲有好多点都没讲深入，下面代码是对课程中的样例进行清洗，感觉只能做到几小点了。特别是在填充 nan 值的时候，一开始想着遍历每一个 nan 值，然后再特判列的类型进行填充的。但是发现三围那里有个大问题，按理说三围应该是 int 类型，但是因为有 - 这个东西的存在，搞的三围是 object 类型，一开始赋值的时候报错提示需要 str 后来想把列的类型转换成 int 也失败了，还有好多地方都卡着了。

### 0112数据集成这些大号一共20亿粉丝.md

介绍了数据集成的两种架构方式，以及 Kettle 工具的基本操作。不要小看了 ETL，虽然它不直接交付数据挖掘的结果，但是却是数据挖掘前重要的工作，它包括了抽取各种数据、完成转化和加载这三个步骤。因此除了数据科学家外，还有个工作职位叫 ETL 工程师，这份工作正是我们今天介绍的从事 ETL 这种架构工作的人。如果你以后有机会从事这份工作，你不仅要对今天介绍的数据集成概念有所了解，还要掌握至少一种 ETL 开发工具，如 Kettle、DataX、 Sqoop 等；此外还需要熟悉主流数据库技术，比如 SQL Server、PostgeSQL、Oracle 等。

我们采集的数据经常会有冗余重复的情况。举个简单的例子，假设你是一个网络综艺节目的制片人，一共有 12 期节目，你一共打算邀请 30 位明星作为节目的嘉宾。你知道这些明星影响力都很大，具体在微博上的粉丝数都有标记。于是你想统计下，这些明星一共能直接影响到微博上的多少粉丝，能产生多大的影响力。然后你突然发现，这些明星的粉丝数总和超过了 20 亿。那么他们一共会影响到中国 20 亿人口么？显然不是的，我们都知道中国人口一共是 14 亿，这 30 位明星的影响力总和不会覆盖中国所有人口。那么如何统计这 30 位明星真实的影响力总和呢？这里就需要用到数据集成的概念了。

数据集成就是将多个数据源合并存放在一个数据存储中（如数据仓库），从而方便后续的数据挖掘工作。据统计，大数据项目中 80% 的工作都和数据集成有关，这里的数据集成有更广泛的意义，包括了数据清洗、数据抽取、数据集成和数据变换等操作。这是因为数据挖掘前，我们需要的数据往往分布在不同的数据源中，需要考虑字段表达是否一样，以及属性是否冗余。

数据集成是数据工程师要做的工作之一。一般来说，数据工程师的工作包括了数据的 ETL 和数据挖掘算法的实现。算法实现可以理解，就是通过数据挖掘算法，从数据仓库中找到「金子」。什么是 ETL 呢？ETL 是英文 Extract、Transform 和 Load 的缩写，顾名思义它包括了数据抽取、转换、加载三个过程。ETL 可以说是进行数据挖掘这项工作前的「备菜」过程。

1）抽取是将数据从已有的数据源中提取出来。2）转换是对原始数据进行处理，例如将表输入 1 和 表输入 2 进行连接形成一张新的表。如果是三张表连接的话，可以怎么操作呢？先将表输入 1 和表输入 2 进行连接形成表输入 1-2，然后将表输入 1-2 和表输入 3 进行连接形成新的表。3）然后再将生成的新表写入目的地。

根据转换发生的顺序和位置，数据集成可以分为 ETL 和 ELT 两种架构。ETL 的过程为提取 (Extract)—— 转换 (Transform)—— 加载 (Load)，在数据源抽取后首先进行转换，然后将转换的结果写入目的地。ELT 的过程则是提取 (Extract)—— 加载 (Load）—— 变换 (Transform)，在抽取后将结果先写入目的地，然后利用数据库的聚合分析能力或者外部计算框架，如 Spark 来完成转换的步骤。

目前数据集成的主流架构是 ETL，但未来使用 ELT 作为数据集成架构的将越来越多。这样做会带来多种好处：1）ELT 和 ETL 相比，最大的区别是「重抽取和加载，轻转换」，从而可以用更轻量的方案搭建起一个数据集成平台。使用 ELT 方法，在提取完成之后，数据加载会立即开始。一方面更省时，另一方面 ELT 允许 BI 分析人员无限制地访问整个原始数据，为分析师提供了更大的灵活性，使之能更好地支持业务。2）在 ELT 架构中，数据变换这个过程根据后续使用的情况，需要在 SQL 中进行，而不是在加载阶段进行。这样做的好处是你可以从数据源中提取数据，经过少量预处理后进行加载。这样的架构更简单，使分析人员更好地了解原始数据的变换过程。

典型的 ETL 工具有：1）商业软件：Informatica PowerCenter、IBM InfoSphere DataStage、Oracle Data Integrator、Microsoft SQL Server Integration Services 等。2）开源软件：Kettle、Talend、Apatar、Scriptella、DataX、Sqoop 等。相对于传统的商业软件，Kettle 是一个易于使用的，低成本的解决方案。国内很多公司都在使用 Kettle 用来做数据集成。所以我重点给你讲解下 Kettle 工具的使用。

Kettle 是一款国外开源的 ETL 工具，纯 Java 编写，可以在 Window 和 Linux 上运行，不需要安装就可以使用。Kettle 中文名称叫水壶，该项目的目标是将各种数据放到一个壶里，然后以一种指定的格式流出。Kettle 在 2006 年并入了开源的商业智能公司 Pentaho，正式命名为 Pentaho Data Integeration，简称「PDI」。因此 Kettle 现在是 Pentaho 的一个组件，下载地址：[Article Detail](https://community.hitachivantara.com/s/article/data-integration-kettle)。

在使用 Kettle 之前还需要安装数据库软件和 Java 运行环境（JRE）。Kettle 采用可视化的方式进行操作，来对数据库间的数据进行迁移。它包括了两种脚本：Transformation 转换和 Job 作业。1）Transformation（转换）：相当于一个容器，对数据操作进行了定义。数据操作就是数据从输入到输出的一个过程。你可以把转换理解成为是比作业粒度更小的容器。在通常的工作中，我们会把任务分解成为不同的作业，然后再把作业分解成多个转换。2）Job（作业）：相比于转换是个更大的容器，它负责将转换组织起来完成某项作业。接下来分别讲下这两个脚本的创建过程。

如何创建 Transformation（转换）。Transformation 可以分成三个步骤，它包括了输入、中间转换以及输出。在 Transformation 中包括两个主要概念：Step 和 Hop。Step 的意思就是步骤，Hop 就是跳跃线的意思。1）Step（步骤）：Step 是转换的最小单元，每一个 Step 完成一个特定的功能。在上面这个转换中，就包括了表输入、值映射、去除重复记录、表输出这 4 个步骤；2）Hop（跳跃线）：用来在转换中连接 Step。它代表了数据的流向。

如何创建 Job（作业）。完整的任务，实际上是将创建好的转换和作业串联起来。在这里 Job 包括两个概念：Job Entry、Hop。如何理解这两个概念呢？1）Job Entry（工作实体）：Job Entry 是 Job 内部的执行单元，每一个 Job Entry 都是用来执行具体的任务，比如调用转换，发送邮件等。2）Hop：指连接 Job Entry 的线。并且它可以指定是否有条件地执行。在 Kettle 中，你可以使用 Spoon，它是一种一种图形化的方式，来让你设计 Job 和 Transformation，并且可以保存为文件或者保存在数据库中。下面我来带你做一个简单的例子。

案例 1：如何将文本文件的内容转化到 MySQL 数据库中。Step1：创建转换，右键「转换→新建」；Step2：在左侧「核心对象」栏目中选择「文本文件输入」控件，拖拽到右侧的工作区中；Step3：从左侧选择「表输出」控件，拖拽到右侧工作区；Step4：鼠标在「文本文件输入」控件上停留，在弹窗中选择图标，鼠标拖拽到「表输出」控件，将一条连线连接到两个控件上。这时我们已经将转换的流程设计好了，现在是要对输入和输出两个控件进行设置。Step5：双击「文本文件输入」控件，导入已经准备好的文本文件；Step6：双击「表输出」控件，这里你需要配置下 MySQL 数据库的连接，同时数据库中需要有一个数据表，字段的设置与文本文件的字段设置一致（这里我设置了一个 wucai 数据库，以及 score 数据表。字段包括了 name、create_time、Chinese、English、Math，与文本文件的字段一致）。

具体操作可以看下面的演示：Step7：创建数据库字段的对应关系，这个需要双击「表输出」，找到数据库字段，进行字段映射的编辑；Step8：点击左上角的执行图标，如下图；这样我们就完成了从文本文件到 MySQL 数据库的转换。Kettle 的控件比较多，内容无法在一节课内容中完整呈现，我只给你做个入门了解。

另外给你推荐一个 Kettle 的开源社区：http://www.ukettle.org 。在社区里，你可以和大家进行交流。因为 Kettle 相比其他工具上手简单，而且是开源工具，有问题可以在社群里咨询。因此我推荐你使用 Kettle 作为你的第一个 ETL 工具。当然除了 Kettle 工具，实际工作中，你可能也会接触到其他的 ETL 工具，这里我给你简单介绍下阿里巴巴的开源工具 DataX 和 Apache 的 Sqoop。

阿里开源软件：DataX。在以往的数据库中，数据库都是两两之间进行的转换，没有统一的标准，转换形式是这样的；但 DataX 可以实现跨平台、跨数据库、不同系统之间的数据同步及交互，它将自己作为标准，连接了不同的数据源，以完成它们之间的转换。DataX 的模式是基于框架 + 插件完成的，DataX 的框架如下图；在这个框架里，Job 作业被 Splitter 分割器分成了许多小作业 Sub-Job。在 DataX 里，通过两个线程缓冲池来完成读和写的操作，读和写都是通过 Storage 完成数据的交换。比如在「读」模块，切分后的小作业，将数据从源头装载到 DataXStorage，然后在「写」模块，数据从 DataXStorage 导入到目的地。这样的好处就是，在整体的框架下，我们可以对 Reader 和 Writer 进行插件扩充，比如我想从 MySQL 导入到 Oracle，就可以使用 MySQLReader 和 OracleWriter 插件，装在框架上使用即可。

Apache 开源软件：Sqoop。Sqoop 是一款开源的工具，是由 Apache 基金会所开发的分布式系统基础架构。Sqoop 在 Hadoop 生态系统中是占据一席之地的，它主要用来在 Hadoop 和关系型数据库中传递数据。通过 Sqoop，我们可以方便地将数据从关系型数据库导入到 HDFS 中，或者将数据从 HDFS 导出到关系型数据库中。Hadoop 实现了一个分布式文件系统，即 HDFS。Hadoop 的框架最核心的设计就是 HDFS 和 MapReduce。HDFS 为海量的数据提供了存储，而 MapReduce 则为海量的数据提供了计算。

### 黑板墙

数据集成理解：数据集成是对各种业务数据进行清洗、按数据仓库模型转换后形成一套跨业务的集成数据，这套集成数据可以从各个维度、全方位地展示业务数据，为 BI 分析和数据挖掘提供可靠、丰富的数据来源。我曾经用过的 ETL 工具是 IBM 的 DataStage，DataStage 的数据抽取效率还是挺高的，它是在抽取过程中对数据进行分区，各个分区并行抽取和转换，最后再对各个分区进行收集和合并，这种机制极大提高了 ETL 的效率。DataStage 采用图形化界面进行流程设计，不需要编程，上手比较容易，但各个组件需要进行大量配置，配置比较繁琐，另外，调试起来也不是很直观方便，但总体来说还是一款不错的 ETL 工具。

大约三年大数据工程师工作，从最开始的数据集成（sqoop、代码、商用软件 ETL 工具等），将数据汇聚到数据仓库，理解业务，清洗应用需要的数据。数据集成是将多源（多系统）、多样（结构化、非结构化、半结构化）、多维度数据整合进数据仓库，形成数据海洋，更好的提供业务分析系统的数据服务，通过数仓的数据集成，达到数据共享的效果，降低对原始业务系统的影响，同时加快数据分析工作者的数据准备周期。数据集成最开始就是原始系统的数据，照样搬到数据仓库，这种类型工作长期实施，容易疲劳失去兴趣，理解业务需求，通过自己的数据集成、清洗、数据分析，提供有意思的数据，就是挖金子过程，应该也是一件有趣的事情。

### 0113数据变换考试成绩要求正态分布合理么.md

上一讲是数据集成，这讲是数据变换。如果一个人在百分制的考试中得了 95 分，你肯定会认为他学习成绩很好，如果得了 65 分，就会觉得他成绩不好。如果得了 80 分呢？你会觉得他成绩中等，因为在班级里这属于大部分人的情况。

为什么会有这样的认知呢？这是因为我们从小到大的考试成绩基本上都会满足正态分布的情况。什么是正态分布呢？正态分布也叫作常态分布，就是正常的状态下，呈现的分布情况。比如你可能会问班里的考试成绩是怎样的？这里其实指的是大部分同学的成绩如何。以下图为例，在正态分布中，大部分人的成绩会集中在中间的区域，少部分人处于两头的位置。正态分布的另一个好处就是，如果你知道了自己的成绩，和整体的正态分布情况，就可以知道自己的成绩在全班中的位置。

另一个典型的例子就是，美国 SAT 考试成绩也符合正态分布。而且美国本科的申请，需要中国高中生的 GPA 在 80 分以上（百分制的成绩），背后的理由也是默认考试成绩属于正态分布的情况。为了让成绩符合正态分布，出题老师是怎么做的呢？他们通常可以把考题分成三类：第一类：基础题，占总分 70%，基本上属于送分题；第二类：灵活题，基础范围内 + 一定的灵活性，占 20%；第三类：难题，涉及知识面较广的难题，占 10%；

那么，你想下，如果一个出题老师没有按照上面的标准来出题，而是将第三类难题比重占到了 70%，也就是我们说的「超纲」，结果会是怎样呢？你会发现，大部分人成绩都「不及格」，最后在大家激烈的讨论声中，老师会将考试成绩做规范化处理，从而让成绩满足正态分布的情况。因为只有这样，成绩才更具有比较性。所以正态分布的成绩，不仅可以让你了解全班整体的情况，还能了解每个人的成绩在全班中的位置。

数据变换在数据分析中的角色。我们再来举个例子，假设 A 考了 80 分，B 也考了 80 分，但前者是百分制，后者 500 分是满分，如果我们把从这两个渠道收集上来的数据进行集成、挖掘，就算使用效率再高的算法，结果也不是正确的。因为这两个渠道的分数代表的含义完全不同。所以说，有时候数据变换比算法选择更重要，数据错了，算法再正确也是错的。你现在可以理解为什么 80% 的工作时间会花在前期的数据准备上了吧。

那么如何让不同渠道的数据统一到一个目标数据库里呢？这样就用到了数据变换。在数据变换前，我们需要先对字段进行筛选，然后对数据进行探索和相关性分析，接着是选择算法模型（这里暂时不需要进行模型计算），然后针对算法模型对数据的需求进行数据变换，从而完成数据挖掘前的准备工作。所以你从整个流程中可以看出，数据变换是数据准备的重要环节，它通过数据平滑、数据聚集、数据概化和规范化等方式将数据转换成适用于数据挖掘的形式。

我来介绍下这些常见的变换方法：1）数据平滑：去除数据中的噪声，将连续数据离散化。这里可以采用分箱、聚类和回归的方式进行数据平滑，我会在后面给你讲解聚类和回归这两个算法；2）数据聚集：对数据进行汇总，在 SQL 中有一些聚集函数可以供我们操作，比如 Max () 反馈某个字段的数值最大值，Sum () 返回某个字段的数值总和；3）数据概化：将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如说上海、杭州、深圳、北京可以概化为中国。4）数据规范化：使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小 — 最大规范化、Z—score 规范化、按小数定标规范化等，我会在后面给你讲到这些方法的使用；5）属性构造：构造出新的属性并添加到属性集中。这里会用到特征工程的知识，因为通过属性与属性的连接构造新的属性，其实就是特征工程。比如说，数据表中统计每个人的英语、语文和数学成绩，你可以构造一个「总和」这个属性，来作为新属性。这样「总和」这个属性就可以用到后续的数据挖掘计算中。

在这些变换方法中，最简单易用的就是对数据进行规范化处理。数据规范化的几种常用方法如下。

1、 Min-max 规范化。Min-max 规范化方法是将原始数据变换到 [0,1] 的空间中。用公式表示就是：新数值 =（原数值 - 极小值）/（极大值 - 极小值）。

2、Z-Score 规范化。假设 A 与 B 的考试成绩都为 80 分，A 的考卷满分是 100 分（及格 60 分），B 的考卷满分是 500 分（及格 300 分）。虽然两个人都考了 80 分，但是 A 的 80 分与 B 的 80 分代表完全不同的含义。那么如何用相同的标准来比较 A 与 B 的成绩呢？Z-Score 就是用来可以解决这一问题的。我们定义：新数值 =（原数值 - 均值）/ 标准差。假设 A 所在的班级平均分为 80，标准差为 10。B 所在的班级平均分为 400，标准差为 100。那么 A 的新数值 =(80-80)/10=0，B 的新数值 =(80-400)/100=-3.2。那么在 Z-Score 标准下，A 的成绩会比 B 的成绩好。我们能看到 Z-Score 的优点是算法简单，不受数据量级影响，结果易于比较。不足在于，它需要数据整体的平均值和方差，而且结果没有实际意义，只是用于比较。

3、小数定标规范化。小数定标规范化就是通过移动小数点的位置来进行规范化。小数点移动多少位取决于属性 A 的取值中的最大绝对值。举个例子，比如属性 A 的取值范围是 -999 到 88，那么最大绝对值为 999，小数点就会移动 3 位，即新数值 = 原数值 /1000。那么 A 的取值范围就被规范化为 -0.999 到 0.088。

Python 的 SciKit-Learn 库使用。SciKit-Learn 是 Python 的重要机器学习库，它帮我们封装了大量的机器学习算法，比如分类、聚类、回归、降维等。此外，它还包括了数据变换模块。我现在来讲下如何使用 SciKit-Learn 进行数据规范化。

1、Min-max 规范化。我们可以让原始数据投射到指定的空间 [min, max]，在 SciKit-Learn 里有个函数 MinMaxScaler 是专门做这个的，它允许我们给定一个最大值与最小值，然后将原数据投射到 [min, max] 中。默认情况下 [min,max] 是 [0,1]，也就是把原始数据投放到 [0,1] 范围内。

```
# coding:utf-8
from sklearn import preprocessing
import numpy as np
# 初始化数据，每一行表示一个样本，每一列表示一个特征
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行[0,1]规范化
min_max_scaler = preprocessing.MinMaxScaler()
minmax_x = min_max_scaler.fit_transform(x)
print minmax_x
```

2、Z-Score 规范化。在 SciKit-Learn 库中使用 preprocessing.scale () 函数，可以直接将给定数据进行 Z-Score 规范化。这个结果实际上就是将每行每列的值减去了平均值，再除以方差的结果。我们看到 Z-Score 规范化将数据集进行了规范化，数值都符合均值为 0，方差为 1 的正态分布。

```
from sklearn import preprocessing
import numpy as np
# 初始化数据
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行Z-Score规范化
scaled_x = preprocessing.scale(x)
print scaled_x
```

3、小数定标规范化。我们需要用 NumPy 库来计算小数点的位数。NumPy 库我们之前提到过。

数据挖掘中数据变换比算法选择更重要。在考试成绩中，我们都需要让数据满足一定的规律，达到规范性的要求，便于进行挖掘。这就是数据变换的作用。如果不进行变换的话，要不就是维数过多，增加了计算的成本，要不就是数据过于集中，很难找到数据之间的特征。在数据变换中，重点是如何将数值进行规范化，有三种常用的规范方法，分别是 Min-Max 规范化、Z-Score 规范化、小数定标规范化。其中 Z-Score 规范化可以直接将数据转化为正态分布的情况，当然不是所有自然界的数据都需要正态分布，我们也可以根据实际的情况进行设计，比如取对数 log，或者神经网络里采用的激励函数等。

在最后我给大家推荐了 Python 的 sklearn 库，它和 NumPy，Pandas 都是非常有名的 Python 库，在数据统计工作中起了很大的作用。SciKit-Learn 不仅可以用于数据变换，它还提供了分类、聚类、预测等数据挖掘算法的 API 封装。后面我会详细给你讲解这些算法，也会教你如何使用 SciKit-Learn 工具来完成数据挖掘算法的工作。

### 黑板墙

留道思考题吧，假设属性 income 的最小值和最大值分别是 5000 元和 58000 元。利用 Min-Max 规范化的方法将属性的值映射到 0 至 1 的范围内，那么属性 income 的 16000 元将被转化为多少？另外数据规范化都有哪些方式，他们是如何进行规范化的？

ceil () 本身就是 Python 的一个向正无穷取整的函数，np.ceil (np.log10 (np.max (abs (x)))) 得到的是需要将数据整体移动的小数位数。

老师您好，Z-Score 规范化的分数转化这块，由于我目前在一家公司做产品经理，现在刚好在负责教育行业成绩分析业务，想跟你探讨下。将学生的原始分数成绩进行转化成 Z 分可以进行比较一个学生历次考试之间的波动情况进步程度，或者是同一次考试的不同科目直接进行比较来判断学生的各科均衡度。但是，这个「Z-Score」分的计算方式，目前我查到其它资料，老师文章中的列出的计算公式是属于「Z 分的线性计算公式」，将整批学生的成绩都转化成 Z 分后，其 Z 分不一定能完全呈现标准的正态分布，在某些情况下做比较可信度是不高的（比如：一次考试本身的原始分数呈现负偏态，另一次考试原始成绩呈现正偏态，转化后的 Z 分并不一定能让偏态变成标准正态，在两次考试都处于偏态情况下，同一学生的两次考试成绩做波动进步程度分析就不大可信。

我刚查到另一种计算非线性 Z 分的方式：1）先按该公式计算出百级等级分：百分等级分数（年级）=100﹣（100× 年级名次 - 50）÷ 年级有效参考总人数；该百级等级分便是每个学生在该批学生的相对位置，其百分等级分数对应便是标准正态分布图的所占面积比例。2）再按该百分等级数去标准正态分布表里去查出 Z-Score ，这样最终得出的 Z 分便是标准的正态分布，能将偏态转化成标准正态，这样就能比较同一学生的各次考试。

### 0114数据可视化掌握数据领域的万金油技能.md

如果你想做一名数据分析师，那么掌握可视化技能是必不可少的，因为在大部分情况下，老板更关心呈现的结果。另外当这些可视化的结果呈现在你眼前时，你才能直观地体会到「数据之美」。图片在内容表达上，要远胜于文字，它不仅能体现数据真实性，还能给人很大的想象空间。

在数据可视化产品中，一般都包括哪些视图？我们常用的可视化视图超过 20 种，分别包括：文本表、热力图、地图、符号地图、饼图、水平条、堆叠条、并排条、树状图、圆视图、并排圆、线、双线、面积图、双组合、散点图、直方图、盒须图、甘特图、靶心图、气泡图等。你不用记住这些视图名称都是什么，因为在可视化工具中，操作面板上都会有这些图形可控选择。你看一眼就知道它是不是你想要的。当然，你不仅要掌握这些视图的使用，更要了解使用它们背后的目的是什么，这里我整理了下，可以分为以下的 9 种情况：分布（distribution）、时间相关（change over time）、局部/整体（part to whole）；偏差（deviation）、相关性（correlation）、排名（ranking）；量级（magnitude）、地图（spatial）、流动（flow）。

在上面的这几种情况里，你也许想要看某个数据的分布情况，或者它随着时间的趋势，或者是局部与整体之间的关系等等。所以在设计之前，你需要思考的是，你的用户是谁，想给他们呈现什么，需要突出数据怎样的特点，以及采用哪种视图来进行呈现。比如说，你想呈现某个变量的分布情况，就可以通过直方图的形式来呈现。如果你想要看两个变量之间的相关性及分布情况，可以采用散点图的形式呈现。一个视图可能会有多种表达的目的，比如散点图既可以表明两个变量之间的关系，也可以体现它们的分布情况。同样，如果我想看变量的分布情况，既可以采用散点图的形式，也可以采用直方图的形式。所以说，具体要采用哪种视图，取决于你想要数据可视化呈现什么样的目的。

1、商业智能分析。首先在商业智能分析软件中，最著名的当属 Tableau 和 PowerBI 了，另外中国帆软出品的 FineBI 也受到国内很多企业的青睐。Tableau 是国外的商业软件，收费不低。它适合 BI 工程师、数据分析分析师。如果可以熟练掌握 Tableau，那么找到一份数据分析的工作是不难的。PowerBI 是微软出品的，可以和 Excel 搭配使用，你可以通过 PowerBI 来呈现 Excel 的可视化内容。在 BI 行业中，Tableau 和 PowerBI 远超其他 BI 产品，处于行业的领导者的位置。另外 FineBI 是中国的帆软出品，针对国内使用更加友好，同时也倾向于企业级应用的 BI。

2、可视化大屏类。大屏作为一种视觉效果强、科技感强的技术，被企业老板所青睐，可以很好地展示公司的数据化能力。这里给你介绍两款可视化大屏的软件 DataV 和 FineReport。1）DataV 是一款可视化的工具，天猫双十一大屏就是用它呈现的。你要做的就是选择相应的控件，配置控件的样式、数据传输和交互效果等。当然 DataV 本身有一些免费的模板，你可以直接通过模板来创建。不过一些特殊的控件和交互效果还是需要购买企业版才行。2）FineReport 是帆软出品的工具，你可以看出他家的产品基本上都是 Fine 开头的，包括刚才给你介绍的 FineBI。FineReport 可以做数据大屏，也可以做可视化报表，在很多行业都有解决方案，操作起来也很方便。可以实时连接业务数据，对数据进行展示。

3、前端可视化组件。如果你想要成为一名前端数据可视化工程师的话，至少熟练掌握一种前端可视化组件是必不可少的，不少公司招聘「高级前端工程师」的时候，都要求熟悉几个开源数据可视化组件。可视化组件都是基于 Web 渲染的技术的。所以你需要了解一下几个典型的 Web 渲染技术：Canvas、SVG 和 WebGL。简单来说，Canvas 和 SVG 是 HTML5 中主要的 2D 图形技术，WebGL 是 3D 框架。

1）Canvas 适用于位图，也就是给了你一张白板，需要你自己来画点。Canvas 技术可以绘制比较复杂的动画。不过它是 HTML5 自带的，所以低版本浏览器不支持 Canvas。ECharts 这个可视化组件就是基于 Canvas 实现的。2）SVG 的中文是可缩放矢量图形，它是使用 XML 格式来定义图形的。相当于用点和线来描绘了图形，相比于位图来说文件比较小，而且任意缩放都不会失真。SVG 经常用于图标和图表上。它最大的特点就是支持大部分浏览器，动态交互性实现起来也很方便，比如在 SVG 中插入动画元素等。3）WebGL 是一种 3D 绘图协议，能在网页浏览器中呈现 3D 画面技术，并且可以和用户进行交互。你在网页上看到的很多酷炫的 3D 效果，基本上都是用 WebGL 来渲染的。下面介绍的 Three.js 就是基于 WebGL 框架的。

在了解这些 Web 渲染协议之后，我再来带你看下这些常用的可视化组件： Echarts、D3、Three.js 和 AntV。

1）ECharts 是基于 H5 canvas 的 Javascript 图表库，是百度的开源项目，一直都有更新，使用的人也比较多。它作为一个组件，可以和 DataV、Python 进行组合使用。你可以在 DataV 企业版中接入 ECharts 图表组件。也可以使用 Python 的 Web 框架（比如 Django、Flask）+ECharts 的解决方案。这样可以让你的项目更加灵活地使用到 ECharts 的图表库，不论你是用 Python 语言，还是用 DataV 的工具，都可以享受到 ECharts 丰富的图表库样式。2）D3 的全称是 Data-Driven Documents，简单来说，是一个 JavaScript 的函数库，因为文件的后缀名通常为「.js」，所以 D3 也常使用 D3.js 来称呼。它提供了各种简单易用的函数，大大简化了 JavaScript 操作数据的难度。你只需要输入几个简单的数据，就能够转换为各种绚丽的图形。由于它本质上是 JavaScript，所以用 JavaScript 也是可以实现所有功能的。

3）Three.js，顾名思义，就是 Three+JS 的意思。「Three」表示 3D 的意思，「Three.js」就是使用 JavaScript 来实现 3D 效果。Three.js 是一款 WebGL 框架，封装了大量 WebGL 接口，因为直接用 WebGL API 写 3D 程序太麻烦了。4）AntV 是蚂蚁金服出品的一套数据可视化组件，包括了 G2、G6、F2 和 L7 一共 4 个组件。其中 G2 应该是最知名的，它的意思是 The grammar Of Graphics，也就是一套图形语法。它集成了大量的统计工具，而且可以让用户通过简单的语法搭建出多种图表。G6 是一套流程图和关系分析的图表库。F2 适用于移动端的可视化方案。L7 提供了地理空间的数据可视化框架。

4、编程语言。使用数据分析工具，你一定离不开 Python 语言，当然也有人使用 R 语言。在用 Python 和 R 做数据分析的时候，一定少不了用到可视化的部分。下面简单介绍下，如何使用 Python 和 R 进行数据可视化。在 Python 里包括了众多可视化库，比如 Matplotlib、Seaborn、Bokeh、Plotly、Pyecharts、Mapbox 和 Geoplotlib。其中使用频率最高，最需要掌握的就是 Matplotlib 和 Seaborn。1）Matplotlib 是 Python 的可视化基础库，作图风格和 MATLAB 类似，所以称为 Matplotlib。一般学习 Python 数据可视化，都会从 Matplotlib 入手，然后再学习其他的 Python 可视化库。2）Seaborn 是一个基于 Matplotlib 的高级可视化效果库，针对 Matplotlib 做了更高级的封装，让作图变得更加容易。你可以用短小的代码绘制更多维度数据的可视化效果图，比如下面这个例子。

在 R 中也有很多可视化库可供选择。其中包括了 R 自带的绘图包 Graphics 以及工具包 ggplot2、ggmap、timevis 和 plotly 等。其中 ggplot2 是 R 语言中重要的绘图包，这个工具包将数据与绘图操作进行了分离，所以使用起来清晰明了，画出的图也漂亮。其实在 Python 里后来也引入了 ggplot 库，这样在 Python 中也可以很方便地使用到 ggplot，而且和 R 语言中的 ggplot2 代码差别不大，稍作修改，就能直接在 Python 中运行了。

如何开始数据可视化的学习。其实很多企业都有在用商业分析软件，Tableau 算是使用率很高的。如果你想做相关的数据分析研究，掌握一门语言尤其是 Python 还是很有必要的。如果你想要全面的学习数据可视化，你可以有以下的 3 个路径：1）重点推荐 Tableau。Tableau 在可视化灵活分析上功能强大，主要目标用户更多是较专业的数据分析师。同时在工作场景中使用率高，因此掌握 Tableau 对于晋升和求职都很有帮助。上手起来有一些门槛，需要一定数据基础。2）使用微图、DataV。微图免费，当你用八爪鱼采集数据之后，就直接可以用微图进行数据可视化。DataV 是阿里推出的数字大屏技术，收费的产品。它最大的好处，就是可以分享链接，让别人可以在线浏览，不需要像 Tableau 一样安装客户端才能看到数据可视化的结果。另外 DataV 有一些模板，你直接可以使用。3）Python 可视化。Python 是数据分析的首选语言，如果你不进行编程，可以使用我在上文中提到的数据可视化的工具。如果你的目标是个数据挖掘工程师，或者算法工程师，那么最重要的就是要了解，并且熟练掌握 Python 的数据可视化。

### 黑板墙

Excel：体量小且需要马上出结果；Python：灵活可复用，数据量大；Tableau：清洗数据后使用，一般不知道如何分析时，会用 Tableau 随意分析，找寻灵感。个人比较喜欢 excel 和 tableau，python 一般用于搭建可复用的框架（编程能力并不强）。

传统企业中的数据分析师目前仅局限在报表制作与展示上，更多的是为了让业务方有数据，所以用数据可视化的产品也仅仅局限在基础功能。像我们工作中基本帆软会解决所有问题，帆软的灵活度较高，对于没有前端经验的分析师来说容易上手，且也能够很灵活的应对业务方百变的需求。大屏展示也是一部分，这部分我们之前用过 DataV，同样没有前端经验的分析师来说很容易上手，但个人感觉该产品还不够完善，部分常见的可视化效果无法满足，故后面我们废弃了，专用前端工程师进行大屏页面的开发。

我上一份工作是作为房地产商业软件的产品助理，其中一个功能板块是展示房地产销售的营业数据，其中用到的就是 Echarts 插件，他提供了许多模板，很好地为客户展示数据。在这个过程中，我么团队大部分的时间是用来校对数据上，即清洗数据。因为存在大量的错误的，重复的数据，虽然低效，但至关重要。

### 黑板墙

Seaborn 数据集中自带了 car_crashes 数据集，这是一个国外车祸的数据集，你要如何对这个数据集进行成对关系的探索呢？第二个问题就是，请你用 Seaborn 画二元变量分布图，如果想要画散点图，核密度图，Hexbin 图，函数该怎样写？


### 0115一次学会Python数据可视化的10种技能.md

今天讲了 Python 可视化工具包 Matplotlib 和 Seaborn 工具包的使用。他们两者之间的关系就相当于 NumPy 和 Pandas 的关系。Seaborn 是基于 Matplotlib 更加高级的可视化库。针对这 10 种可视化视图，可以按照变量之间的关系对它们进行分类，这些关系分别是比较、联系、构成和分布。当然我们也可以按照随机变量的个数来进行划分，比如单变量分析和多变量分析。在数据探索中，成对关系 pairplot() 的使用，相好比 Pandas 中的 describe() 使用一样方便，常用于项目初期的数据可视化探索。在 Matplotlib 和 Seaborn 的函数中，只列了最基础的使用，当然如果你也可以设置修改颜色、宽度等视图属性。你可以自己查看相关的函数帮助文档。

关于本次 Python 可视化的学习，我希望你能掌握：1）视图的分类，以及可以从哪些维度对它们进行分类；2）十种常见视图的概念，以及如何在 Python 中进行使用，都需要用到哪些函数；3）需要自己动手跑一遍案例中的代码，体验下 Python 数据可视化的过程。

如果你想要用 Python 进行数据分析，就需要在项目初期开始进行探索性的数据分析，这样方便你对数据有一定的了解。其中最直观的就是采用数据可视化技术，这样，数据不仅一目了然，而且更容易被解读。同样在数据分析得到结果之后，我们还需要用到可视化技术，把最终的结果呈现出来。可视化视图都有哪些？

按照数据之间的关系，我们可以把可视化视图划分为 4 类，它们分别是比较、联系、构成和分布。四种关系的特点：1）比较：比较数据间各类别的关系，或者是它们随着时间的变化趋势，比如折线图；2）联系：查看两个或两个以上变量之间的关系，比如散点图；3）构成：每个部分占整体的百分比，或者是随着时间的百分比变化，比如饼图；4）分布：关注单个变量，或者多个变量的分布情况，比如直方图。

同样，按照变量的个数，我们可以把可视化视图划分为单变量分析和多变量分析。1）单变量分析指的是一次只关注一个变量。比如我们只关注「身高」这个变量，来看身高的取值分布，而暂时忽略其他变量。2）多变量分析可以让你在一张图上可以查看两个以上变量的关系。比如「身高」和「年龄」，你可以理解是同一个人的两个参数，这样在同一张图中可以看到每个人的「身高」和「年龄」的取值，从而分析出来这两个变量之间是否存在某种联系。

可视化的视图可以说是分门别类，多种多样，今天我主要介绍常用的 10 种视图，这些视图包括了散点图、折线图、直方图、条形图、箱线图、饼图、热力图、蜘蛛图、二元变量分布和成对关系。

1、散点图。散点图的英文叫做 scatter plot，它将两个变量的值显示在二维坐标中，非常适合展示两个变量之间的关系。当然，除了二维的散点图，我们还有三维的散点图。在 Matplotlib 中，经常会用到 pyplot 这个工具包，它包括了很多绘图函数，类似 Matlab 的绘图框架。在使用前你需要进行引用：import matplotlib.pyplot as plt。在工具包引用后，画散点图，需要使用 plt.scatter (x, y, marker=None) 函数。x、y 是坐标，marker 代表了标记的符号。比如「x」、「>」或者「o」。选择不同的 marker，呈现出来的符号样式也会不同，你可以自己试一下。

除了 Matplotlib 外，也可以使用 Seaborn 进行散点图的绘制。在使用 Seaborn 前，也需要进行包引用：import seaborn as sns。在引用 seaborn 工具包之后，就可以使用 seaborn 工具包的函数了。如果想要做散点图，可以直接使用 sns.jointplot (x, y, data=None, kind=‘scatter’) 函数。其中 x、y 是 data 中的下标。data 就是我们要传入的数据，一般是 DataFrame 类型。kind 这类我们取 scatter，代表散点的意思。当然 kind 还可以取其他值，这个我在后面的视图中会讲到，不同的 kind 代表不同的视图绘制方式。让我们来模拟下，假设我们的数据是随机的 1000 个点。

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
N = 1000
x = np.random.randn(N)
y = np.random.randn(N)
# 用Matplotlib画散点图
plt.scatter(x, y,marker='x')
plt.show()
# 用Seaborn画散点图
df = pd.DataFrame({'x': x, 'y': y})
sns.jointplot(x="x", y="y", data=df, kind='scatter');
plt.show()
```

其实你能看到 Matplotlib 和 Seaborn 的视图呈现还是有差别的。Matplotlib 默认情况下呈现出来的是个长方形。而 Seaborn 呈现的是个正方形，而且不仅显示出了散点图，还给了这两个变量的分布情况。

2、折线图。折线图可以用来表示数据随着时间变化的趋势。在 Matplotlib 中，可以直接使用 plt.plot () 函数，当然需要提前把数据按照 x 轴的大小进行排序，要不画出来的折线图就无法按照 x 轴递增的顺序展示。在 Seaborn 中，使用 sns.lineplot (x, y, data=None) 函数。其中 x、y 是 data 中的下标。data 就是我们要传入的数据，一般是 DataFrame 类型。这里我们设置了 x、y 的数组。x 数组代表时间（年），y 数组我们随便设置几个取值。然后我们分别用 Matplotlib 和 Seaborn 进行画图，可以得到下面的图示。你可以看出这两个图示的结果是完全一样的，只是在 seaborn 中标记了 x 和 y 轴的含义。

```
# 数据准备
x = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]
y = [5, 3, 6, 20, 17, 16, 19, 30, 32, 35]
# 使用Matplotlib画折线图
plt.plot(x, y)
plt.show()
# 使用Seaborn画折线图
df = pd.DataFrame({'x': x, 'y': y})
sns.lineplot(x="x", y="y", data=df)
plt.show()
```

3、直方图。直方图是比较常见的视图，它是把横坐标等分成了一定数量的小区间，这个小区间也叫作「箱子」，然后在每个「箱子」内用矩形条（bars）展示该箱子的箱子数（也就是 y 值），这样就完成了对数据集的直方图分布的可视化。在 Matplotlib 中，使用 plt.hist (x, bins=10) 函数，其中参数 x 是一维数组，bins 代表直方图中的箱子数量，默认是 10。在 Seaborn 中，使用 sns.distplot (x, bins=10, kde=True) 函数。其中参数 x 是一维数组，bins 代表直方图中的箱子数量，kde 代表显示核密度估计，默认是 True，我们也可以把 kde 设置为 False，不进行显示。核密度估计是通过核函数帮我们来估计概率密度的方法。这是一段绘制直方图的代码。我们创建一个随机的一维数组，然后分别用 Matplotlib 和 Seaborn 进行直方图的显示，结果如下，你可以看出，没有任何差别，其中最后一张图就是 kde 默认为 Ture 时的显示情况。

```
# 数据准备
a = np.random.randn(100)
s = pd.Series(a) 
# 用Matplotlib画直方图
plt.hist(s)
plt.show()
# 用Seaborn画直方图
sns.distplot(s, kde=False)
plt.show()
sns.distplot(s, kde=True)
plt.show()
```

4、条形图。如果说通过直方图可以看到变量的数值分布，那么条形图可以帮我们查看类别的特征。在条形图中，长条形的长度表示类别的频数，宽度表示类别。在 Matplotlib 中，使用 plt.bar (x, height) 函数，其中参数 x 代表 x 轴的位置序列，height 是 y 轴的数值序列，也就是柱子的高度。在 Seaborn 中，使用 sns.barplot (x=None, y=None, data=None) 函数。其中参数 data 为 DataFrame 类型，x、y 是 data 中的变量。我们创建了 x、y 两个数组，分别代表类别和类别的频数，然后用 Matplotlib 和 Seaborn 进行条形图的显示，结果如下：

```
# 数据准备
x = ['Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5']
y = [5, 4, 8, 12, 7]
# 用Matplotlib画条形图
plt.bar(x, y)
plt.show()
# 用Seaborn画条形图
sns.barplot(x, y)
plt.show()
```

5、箱线图。箱线图，又称盒式图，它是在 1977 年提出的，由五个数值点组成：最大值 (max)、最小值 (min)、中位数 (median) 和上下四分位数 (Q3, Q1)。它可以帮我们分析出数据的差异性、离散程度和异常值等。在 Matplotlib 中，使用 plt.boxplot (x, labels=None) 函数，其中参数 x 代表要绘制箱线图的数据，labels 是缺省值，可以为箱线图添加标签。在 Seaborn 中，使用 sns.boxplot (x=None, y=None, data=None) 函数。其中参数 data 为 DataFrame 类型，x、y 是 data 中的变量。这段代码中，我生成 0-1 之间的 10*4 维度数据，然后分别用 Matplotlib 和 Seaborn 进行箱线图的展示，结果如下。

```
# 数据准备
# 生成0-1之间的10*4维度数据
data=np.random.normal(size=(10,4)) 
lables = ['A','B','C','D']
# 用Matplotlib画箱线图
plt.boxplot(data,labels=lables)
plt.show()
# 用Seaborn画箱线图
df = pd.DataFrame(data, columns=lables)
sns.boxplot(data=df)
plt.show()
```

6、饼图。饼图是常用的统计学模块，可以显示每个部分大小与总和之间的比例。在 Python 数据可视化中，它用的不算多。我们主要采用 Matplotlib 的 pie 函数实现它。在 Matplotlib 中，我们使用 plt.pie (x, labels=None) 函数，其中参数 x 代表要绘制饼图的数据，labels 是缺省值，可以为饼图添加标签。这里我设置了 lables 数组，分别代表高中、本科、硕士、博士和其他几种学历的分类标签。nums 代表这些学历对应的人数。通过 Matplotlib 的 pie 函数，我们可以得出下面的饼图。

```
import matplotlib.pyplot as plt
# 数据准备
nums = [25, 37, 33, 37, 6]
labels = ['High-school','Bachelor','Master','Ph.d', 'Others']
# 用Matplotlib画饼图
plt.pie(x = nums, labels=labels)
plt.show()
```

7、热力图。英文叫 heat map，是一种矩阵表示方法，其中矩阵中的元素值用颜色来代表，不同的颜色代表不同大小的值。通过颜色就能直观地知道某个位置上数值的大小。另外你也可以将这个位置上的颜色，与数据集中的其他位置颜色进行比较。热力图是一种非常直观的多元变量分析方法。我们一般使用 Seaborn 中的 sns.heatmap (data) 函数，其中 data 代表需要绘制的热力图数据。这里我们使用 Seaborn 中自带的数据集 flights，该数据集记录了 1949 年到 1960 年期间，每个月的航班乘客的数量。通过 seaborn 的 heatmap 函数，我们可以观察到不同年份，不同月份的乘客数量变化情况，其中颜色越浅的代表乘客数量越多，如下图所示。

```
# 数据准备
flights = sns.load_dataset("flights")
data=flights.pivot('year','month','passengers')
# 用Seaborn画热力图
sns.heatmap(data)
plt.show()
```

8、蜘蛛图。蜘蛛图是一种显示一对多关系的方法。在蜘蛛图中，一个变量相对于另一个变量的显著性是清晰可见的。假设我们想要给王者荣耀的玩家做一个战力图，指标一共包括推进、KDA、生存、团战、发育和输出。那该如何做呢？这里我们需要使用 Matplotlib 来进行画图，首先设置两个数组：labels 和 stats。他们分别保存了这些属性的名称和属性值。因为蜘蛛图是一个圆形，你需要计算每个坐标的角度，然后对这些数值进行设置。当画完最后一个点后，需要与第一个点进行连线。因为需要计算角度，所以我们要准备 angles 数组；又因为需要设定统计结果的数值，所以我们要设定 stats 数组。并且需要在原有 angles 和 stats 数组上增加一位，也就是添加数组的第一个元素。

```
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.font_manager import FontProperties  
# 数据准备
labels=np.array([u"推进","KDA",u"生存",u"团战",u"发育",u"输出"])
stats=[83, 61, 95, 67, 76, 88]
# 画图数据准备，角度、状态值
angles=np.linspace(0, 2*np.pi, len(labels), endpoint=False)
stats=np.concatenate((stats,[stats[0]]))
angles=np.concatenate((angles,[angles[0]]))
# 用Matplotlib画蜘蛛图
fig = plt.figure()
ax = fig.add_subplot(111, polar=True)   
ax.plot(angles, stats, 'o-', linewidth=2)
ax.fill(angles, stats, alpha=0.25)
# 设置中文字体
font = FontProperties(fname=r"C:\Windows\Fonts\simhei.ttf", size=14)  
ax.set_thetagrids(angles * 180/np.pi, labels, FontProperties=font)
plt.show()
```

代码中 flt.figure 是创建一个空白的 figure 对象，这样做的目的相当于画画前先准备一个空白的画板。然后 add_subplot (111) 可以把画板划分成 1 行 1 列。再用 ax.plot 和 ax.fill 进行连线以及给图形上色。最后我们在相应的位置上显示出属性名。这里需要用到中文，Matplotlib 对中文的显示不是很友好，因此我设置了中文的字体 font，这个需要在调用前进行定义。最后我们可以得到下面的蜘蛛图，看起来是不是很酷？

9、二元变量分布。如果我们想要看两个变量之间的关系，就需要用到二元变量分布。当然二元变量分布有多种呈现方式，开头给你介绍的散点图就是一种二元变量分布。在 Seaborn 里，使用二元变量分布是非常方便的，直接使用 sns.jointplot (x, y, data=None, kind) 函数即可。其中用 kind 表示不同的视图类型：「kind=‘scatter’」代表散点图，「kind=‘kde’」代表核密度图，「kind=‘hex’」代表 Hexbin 图，它代表的是直方图的二维模拟。这里我们使用 Seaborn 中自带的数据集 tips，这个数据集记录了不同顾客在餐厅的消费账单及小费情况。代码中 total_bill 保存了客户的账单金额，tip 是该客户给出的小费金额。我们可以用 Seaborn 中的 jointplot 来探索这两个变量之间的关系。代码中我用 kind 分别显示了他们的散点图、核密度图和 Hexbin 图，如下图所示。

```
# 数据准备
tips = sns.load_dataset("tips")
print(tips.head(10))
# 用Seaborn画二元变量分布图（散点图，核密度图，Hexbin图）
sns.jointplot(x="total_bill", y="tip", data=tips, kind='scatter')
sns.jointplot(x="total_bill", y="tip", data=tips, kind='kde')
sns.jointplot(x="total_bill", y="tip", data=tips, kind='hex')
plt.show()
```

10、成对关系。如果想要探索数据集中的多个成对双变量的分布，可以直接采用 sns.pairplot () 函数。它会同时展示出 DataFrame 中每对变量的关系，另外在对角线上，你能看到每个变量自身作为单变量的分布情况。它可以说是探索性分析中的常用函数，可以很快帮我们理解变量对之间的关系。pairplot 函数的使用，就像在 DataFrame 中使用 describe () 函数一样方便，是数据探索中的常用函数。这里我们使用 Seaborn 中自带的 iris 数据集，这个数据集也叫鸢尾花数据集。鸢尾花可以分成 Setosa、Versicolour 和 Virginica 三个品种，在这个数据集中，针对每一个品种，都有 50 个数据，每个数据中包括了 4 个属性，分别是花萼长度、花萼宽度、花瓣长度和花瓣宽度。通过这些数据，需要你来预测鸢尾花卉属于三个品种中的哪一种。

这里我们用 Seaborn 中的 pairplot 函数来对数据集中的多个双变量的关系进行探索，如下图所示。从图上你能看出，一共有 sepal_length、sepal_width、petal_length 和 petal_width4 个变量，它们分别是花萼长度、花萼宽度、花瓣长度和花瓣宽度。下面这张图相当于这 4 个变量两两之间的关系。比如矩阵中的第一张图代表的就是花萼长度自身的分布图，它右侧的这张图代表的是花萼长度与花萼宽度这两个变量之间的关系。

```
# 数据准备
iris = sns.load_dataset('iris')
# 用Seaborn画成对关系
sns.pairplot(iris)
plt.show()
```