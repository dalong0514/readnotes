## 记忆时间

## 卡片

## 目录

## 0201. 数据分析算法篇

1、决策树的工作原理，包括构造原理和剪枝原理。两种决策树分类算法 ID3 和 C4.5，以及它们的数学原理、优缺点。ID3：以信息增益作为判断标准，计算每个特征的信息增益，选取信息增益最大的特征，但是容易选取到取值较多的特征。C4.5：以信息增益比作为判断标准，计算每个特征的信息增益比，选取信息增益比最大的特征。CART：分类树以基尼系数为标准，选取基尼系数小的的特征。回归树以均方误差或绝对值误差为标准，选取均方误差或绝对值误差最小的特征。

2、CART 决策树，它是一棵决策二叉树，既可以做分类树，也可以做回归树。作为分类树，CART 采用基尼系数作为节点划分的依据，得到的是离散的结果，也就是分类结果；作为回归树，CART 可以采用最小绝对偏差（LAD），或者最小二乘偏差（LSD）作为节点划分的依据，得到的是连续值，即回归预测结果。

3、itanic 乘客生存预测。1）问题描述。2）生存预测的关键流程。注意点：1）特征选择是分类模型好坏的关键。选择什么样的特征，以及对应的特征值矩阵，决定了分类模型的好坏。通常情况下，特征值不都是数值类型，可以使用 DictVectorizer 类进行转化；2）模型准确率需要考虑是否有测试集的实际结果可以做对比，当测试集没有真实结果可以对比时，需要使用 K 折交叉验证 cross_val_score；3）Graphviz 可视化工具可以很方便地将决策模型呈现出来，帮助你更好理解决策树的构建。

14、Apriori 算法是在「购物篮分析」中常用的关联规则挖掘算法，在 Apriori 算法中最需要明白支持度、置信度、提升度这几个概念，以及 Apriori 迭代计算频繁项集的工作流程。Apriori 算法在实际工作中需要对数据集扫描多次，会消耗大量的计算时间，所以在 2000 年 FP-Growth 算法被提出来，它只需要扫描两次数据集即可以完成关联规则的挖掘。FP-Growth 算法最主要的贡献就是提出了 FP 树和项头表，通过 FP 树减少了频繁项集的存储以及计算时间。

15、Apriori 算法的核心就是理解频繁项集和关联规则。在算法运算的过程中，还要重点掌握对支持度、置信度和提升度的理解。在工具使用上，你可以使用 efficient-apriori 这个工具包，它会把每一条数据中的项（item）放到一个集合（篮子）里来处理，不考虑项（item）之间的先后顺序。虽然工具的使用会很方便，但重要的还是数据挖掘前的准备过程，也就是获取某个导演的电影数据集。

## 0201决策树1要不要去打篮球决策树来告诉你.md

前面我们讲了两种决策树分类算法 ID3 和 C4.5，了解了它们的数学原理。你可能会问，公式这么多，在实际使用中该怎么办呢？实际上，我们可以使用一些数据挖掘工具使用它们，比如 Python 的 sklearn，或者是 Weka（一个免费的数据挖掘工作平台），它们已经集成了这两种算法。只是我们在了解了这两种算法之后，才能更加清楚这两种算法的优缺点。

首先我们采用决策树分类，需要了解它的原理，包括它的构造原理、剪枝原理。另外在信息度量上，我们需要了解信息度量中的纯度和信息熵的概念。在决策树的构造中，一个决策树包括根节点、子节点、叶子节点。在属性选择的标准上，度量方法包括了信息增益和信息增益率。在算法上，我讲解了两种算法：ID3 和 C4.5，其中 ID3 是基础的决策树算法，C4.5 在它的基础上进行了改进，也是目前决策树中应用广泛的算法。然后在了解这些概念和原理后，强烈推荐你使用工具。

决策树的工作原理。决策树基本上就是把我们以前的经验总结出来。我给你准备了一个打篮球的训练集。如果我们要出门打篮球，一般会根据「天气」、「温度」、「湿度」、「刮风」这几个条件来判断，最后得到结果：去打篮球？还是不去？上面这个图就是一棵典型的决策树。我们在做决策树的时候，会经历两个阶段：构造和剪枝。

1、构造。什么是构造呢？构造就是生成一棵完整的决策树。简单来说，构造的过程就是选择什么属性作为节点的过程，那么在构造过程中，会存在三种节点：1）根节点：就是树的最顶端，最开始的那个节点。在上图中，「天气」就是一个根节点；2）内部节点：就是树中间的那些节点，比如说「温度」、「湿度」、「刮风」；3）叶节点：就是树最底部的节点，也就是决策结果。

节点之间存在父子关系。比如根节点会有子节点，子节点会有子子节点，但是到了叶节点就停止了，叶节点不存在子节点。那么在构造过程中，你要解决三个重要的问题：1）选择哪个属性作为根节点；2）选择哪些属性作为子节点；3）什么时候停止并得到目标状态，即叶节点。

2、剪枝。决策树构造出来之后是不是就万事大吉了呢？也不尽然，我们可能还需要对决策树进行剪枝。剪枝就是给决策树瘦身，这一步想实现的目标就是，不需要太多的判断，同样可以得到不错的结果。之所以这么做，是为了防止「过拟合」（Overfitting）现象的发生。「过拟合」这个概念你一定要理解，它指的就是模型的训练结果「太好了」，以至于在实际应用的过程中，会存在「死板」的情况，导致分类错误。欠拟合，和过拟合就好比是下面这张图中的第一个和第三个情况一样，训练的结果「太好」，反而在实际应用过程中会导致分类错误。

3『精英日课「指导生活算法」中的过渡拟合；「魔鬼数学」里也有过度拟合的信息。』

造成过拟合的原因之一就是因为训练集中样本量较小。如果决策树选择的属性过多，构造出来的决策树一定能够「完美」地把训练集中的样本分类，但是这样就会把训练集中一些数据的特点当成所有数据的特点，但这个特点不一定是全部数据的特点，这就使得这个决策树在真实的数据分类中出现错误，也就是模型的「泛化能力」差。泛化能力指的分类器是通过训练集抽象出来的分类能力，你也可以理解是举一反三的能力。如果我们太依赖于训练集的数据，那么得到的决策树容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。

既然要对决策树进行剪枝，具体有哪些方法呢？一般来说，剪枝可以分为「预剪枝」（Pre-Pruning）和「后剪枝」（Post-Pruning）。1）预剪枝是在决策树构造时就进行剪枝。方法是在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。2）后剪枝就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。

如何判断要不要去打篮球？我给你准备了打篮球的数据集，训练数据如下；我们该如何构造一个判断是否去打篮球的决策树呢？再回顾一下决策树的构造原理，在决策过程中有三个重要的问题：将哪个属性作为根节点？选择哪些属性作为后继节点？什么时候停止并得到目标值？显然将哪个属性（天气、温度、湿度、刮风）作为根节点是个关键问题，在这里我们先介绍两个指标：纯度和信息熵。

1『纯度与信息熵相对。』

1、纯度。你可以把决策树的构造过程理解成为寻找纯净划分的过程。数学上，我们可以用纯度来表示，纯度换一种方式来解释就是让目标变量的分歧最小。我在这里举个例子，假设有 3 个集合：集合 1：6 次都去打篮球；集合 2：4 次去打篮球，2 次不去打篮球；集合 3：3 次去打篮球，3 次不去打篮球。按照纯度指标来说，集合 1> 集合 2> 集合 3。因为集合 1 的分歧最小，集合 3 的分歧最大。

2、然后我们再来介绍信息熵（entropy）的概念，它表示了信息的不确定度。在信息论中，随机离散事件出现的概率存在着不确定性。为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式。p (i|t) 代表了节点 t 为分类 i 的概率，其中 log2 为取以 2 为底的对数。这里我们不是来介绍公式的，而是说存在一种度量，它能帮我们反映出来这个信息的不确定度。当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。我举个简单的例子，假设有 2 个集合：在集合 1 中，有 6 次决策，其中打篮球是 5 次，不打篮球是 1 次。那么假设：类别 1 为「打篮球」，即次数为 5；类别 2 为「不打篮球」，即次数为 1。那么节点划分为类别 1 的概率是 5/6，为类别 2 的概率是 1/6，带入上述信息熵公式可以计算得出（信息熵为 0.65）；同样，集合 2 中，也是一共 6 次决策，其中类别 1 中「打篮球」的次数是 3，类别 2「不打篮球」的次数也是 3，那么信息熵为多少呢？我们可以计算得出（信息熵为 1）。

1『信息熵为 1 即为 1bit，香农大神的信息论。』

从上面的计算结果中可以看出，信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。我们在构造决策树的时候，会基于纯度来构建。而经典的「不纯度」的指标有三种，分别是信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。

1、我们先看下 ID3 算法。ID3 算法计算的是信息增益，信息增益指的就是划分可以带来纯度的提高，信息熵的下降。它的计算公式，是父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中，我们会计算每个子节点的归一化信息熵，即按照每个子节点在父节点中出现的概率，来计算这些子节点的信息熵。所以信息增益的公式可以表示为。

公式中 D 是父亲节点，Di 是子节点，Gain (D,a) 中的 a 作为 D 节点的属性选择。假设「天气 = 晴」的时候，会有 5 次去打篮球，5 次不打篮球。其中 D1 刮风 = 是，有 2 次打篮球，1 次不打篮球。D2 刮风 = 否，有 3 次打篮球，4 次不打篮球。那么 a 代表节点的属性，即天气 = 晴。你可以在下面的图例中直观地了解这几个概念。

比如针对图上这个例子，D 作为节点的信息增益为；也就是 D 节点的信息熵 - 2 个子节点的归一化信息熵。2 个子节点归一化信息熵 = 3/10 的 D1 信息熵 +7/10 的 D2 信息熵。我们基于 ID3 的算法规则，完整地计算下我们的训练集，训练集中一共有 7 条数据，3 个打篮球，4 个不打篮球，所以根节点的信息熵是；如果你将天气作为属性的划分，会有三个叶子节点 D1、D2 和 D3，分别对应的是晴天、阴天和小雨。我们用 + 代表去打篮球，- 代表不去打篮球。那么第一条记录，晴天不去打篮球，可以记为 1-，于是我们可以用下面的方式来记录 D1，D2，D3；我们先分别计算三个叶子节点的信息熵。

因为 D1 有 3 个记录，D2 有 2 个记录，D3 有 2 个记录，所以 D 中的记录一共是 3+2+2=7，即总数为 7。所以 D1 在 D（父节点）中的概率是 3/7，D2 在父节点的概率是 2/7，D3 在父节点的概率是 2/7。那么作为子节点的归一化信息熵 = 3/7\*0.918+2/7\*1.0+2/7*1.0=0.965。

因为我们用 ID3 中的信息增益来构造决策树，所以要计算每个节点的信息增益。天气作为属性节点的信息增益为，Gain (D , 天气)=0.985-0.965=0.020。同理我们可以计算出其他属性作为根节点的信息增益，它们分别为；我们能看出来温度作为属性的信息增益最大。因为 ID3 就是要将信息增益最大的节点作为父节点，这样可以得到纯度高的决策树，所以我们将温度作为根节点。其决策树状图分裂为下图所示：

然后我们要将上图中第一个叶节点，也就是 D1={1-,2-,3+,4+} 进一步进行分裂，往下划分，计算其不同属性（天气、湿度、刮风）作为节点的信息增益，可以得到；我们能看到湿度，或者天气为 D1 的节点都可以得到最大的信息增益，这里我们选取湿度作为节点的属性划分。同理，我们可以按照上面的计算步骤得到完整的决策树，结果如下。

于是我们通过 ID3 算法得到了一棵决策树。ID3 的算法规则相对简单，可解释性强。同样也存在缺陷，比如我们会发现 ID3 算法倾向于选择取值比较多的属性。这样，如果我们把「编号」作为一个属性（一般情况下不会这么做，这里只是举个例子），那么「编号」将会被选为最优属性 。但实际上「编号」是无关属性的，它对「打篮球」的分类并没有太大作用。所以 ID3 有一个缺陷就是，有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性。这种缺陷不是每次都会发生，只是存在一定的概率。在大部分情况下，ID3 都能生成不错的决策树分类。针对可能发生的缺陷，后人提出了新的算法进行改进。

在 ID3 算法上进行改进的 C4.5 算法。C4.5 都在哪些方面改进了 ID3 呢？

1、采用信息增益率。因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。信息增益率 = 信息增益 / 属性熵，具体的计算公式这里省略。当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5 来说，属性熵也会变大，所以整体的信息增益率并不大。

2、采用悲观剪枝。ID3 构造决策树的时候，容易产生过拟合的情况。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。

3、离散化处理连续属性。C4.5 可以处理连续属性的情况，对连续的属性进行离散化的处理。比如打篮球存在的「湿度」属性，不按照「高、中」划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢，C4.5 选择具有最高信息增益的划分所对应的阈值。

4、处理缺失值。针对数据集不完整的情况，C4.5 也可以进行处理。假如我们得到的是如下的数据，你会发现这个数据中存在两点问题。第一个问题是，数据集中存在数值缺失的情况，如何进行属性选择？第二个问题是，假设已经做了属性划分，但是样本在这个属性上有缺失值，该如何对样本进行划分？

我们不考虑缺失的数值，可以得到温度 D={2-,3+,4+,5-,6+,7-}。温度 = 高：D1={2-,3+,4+} ；温度 = 中：D2={6+,7-}；温度 = 低：D3={5-} 。这里 + 号代表打篮球，- 号代表不打篮球。比如 ID=2 时，决策是不打篮球，我们可以记录为 2-。针对将属性选择为温度的信息增为：Gain (D′, 温度)=Ent (D′)-0.792=1.0-0.792=0.208；属性熵 =1.459, 信息增益率 Gain_ratio (D′, 温度)=0.208/1.459=0.1426。

D′的样本个数为 6，而 D 的样本个数为 7，所以所占权重比例为 6/7，所以 Gain (D′，温度) 所占权重比例为 6/7，所以：Gain_ratio (D, 温度)=6/7*0.1426=0.122。这样即使在温度属性的数值有缺失的情况下，我们依然可以计算信息增益，并对属性进行选择。

Cart 算法在这里不做介绍，我会在下一讲给你讲解这个算法。现在我们总结下 ID3 和 C4.5 算法。首先 ID3 算法的优点是方法简单，缺点是对噪声敏感。训练数据如果有少量错误，可能会产生决策树分类错误。C4.5 在 ID3 的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝、处理连续数值以及数值缺失等情况，但是由于 C4.5 需要对数据集进行多次扫描，算法效率相对较低。

## 0202决策树2CART一棵是回归树另一棵是分类树.md

给你讲了 CART 决策树，它是一棵决策二叉树，既可以做分类树，也可以做回归树。你需要记住的是，作为分类树，CART 采用基尼系数作为节点划分的依据，得到的是离散的结果，也就是分类结果；作为回归树，CART 可以采用最小绝对偏差（LAD），或者最小二乘偏差（LSD）作为节点划分的依据，得到的是连续值，即回归预测结果。

最后我们来整理下三种决策树之间在属性选择标准上的差异：1）ID3 算法，基于信息增益做判断；2）C4.5 算法，基于信息增益率做判断；3）CART 算法，分类树是基于基尼系数做判断。回归树是基于偏差做判断。实际上这三个指标也是计算「不纯度」的三种计算方式。在工具使用上，我们可以使用 sklearn 中的 DecisionTreeClassifier 创建 CART 分类树，通过 DecisionTreeRegressor 创建 CART 回归树。

上节课我们讲了决策树，基于信息度量的不同方式，我们可以把决策树分为 ID3 算法、C4.5 算法和 CART 算法。CART 算法，英文全称叫做 Classification And Regression Tree，中文叫做分类回归树。ID3 和 C4.5 算法可以生成二叉树或多叉树，而 CART 只支持二叉树。同时 CART 决策树比较特殊，既可以作分类树，又可以作回归树。

那么你首先需要了解的是，什么是分类树，什么是回归树呢？我用下面的训练数据举个例子，你能看到不同职业的人，他们的年龄不同，学习时间也不同。如果我构造了一棵决策树，想要基于数据判断这个人的职业身份，这个就属于分类树，因为是从几个分类中来做选择。如果是给定了数据，想要预测这个人的年龄，那就属于回归树。分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本的类别，而回归树可以对连续型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。

CART 分类树的工作流程。通过上一讲，我们知道决策树的核心就是寻找纯净的划分，因此引入了纯度的概念。在属性选择上，我们是通过统计「不纯度」来做判断的，ID3 是基于信息增益做判断，C4.5 在 ID3 的基础上做了改进，提出了信息增益率的概念。实际上 CART 分类树与 C4.5 算法类似，只是属性选择的指标采用的是基尼系数。

你可能在经济学中听过说基尼系数，它是用来衡量一个国家收入差距的常用指标。当基尼系数大于 0.4 的时候，说明财富差异悬殊。基尼系数在 0.2-0.4 之间说明分配合理，财富差距不大。基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。分类的过程本身是一个不确定度降低的过程，即纯度的提升过程。所以 CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。

我们接下来详解了解一下基尼系数。基尼系数不好懂，你最好跟着例子一起手动计算下。假设 t 为节点，那么该节点的 GINI 系数的计算公式为；这里 p (Ck|t) 表示节点 t 属于类别 Ck 的概率，节点 t 的基尼系数为 1 减去各类别 Ck 概率平方和。通过下面这个例子，我们计算一下两个集合的基尼系数分别为多少：集合 1：6 个都去打篮球；集合 2：3 个去打篮球，3 个不去打篮球。

针对集合 1，所有人都去打篮球，所以 p (Ck|t)=1，因此 GINI (t)=1-1=0。针对集合 2，有一半人去打篮球，而另一半不去打篮球，所以，p (C1|t)=0.5，p (C2|t)=0.5，GINI (t)=1-(0.5\*0.5+0.5*0.5)=0.5。通过两个基尼系数你可以看出，集合 1 的基尼系数最小，也证明样本最稳定，而集合 2 的样本不稳定性更大。

在 CART 算法中，基于基尼系数对特征属性进行二元分裂，假设属性 A 将节点 D 划分成了 D1 和 D2，如下图所示；节点 D 的基尼系数等于子节点 D1 和 D2 的归一化基尼系数之和，用公式表示为；归一化基尼系数代表的是每个子节点的基尼系数乘以该节点占整体父亲节点 D 中的比例。上面我们已经计算了集合 D1 和集合 D2 的 GINI 系数，得到；所以在属性 A 的划分下，节点 D 的基尼系数为；节点 D 被属性 A 划分后的基尼系数越大，样本集合的不确定性越大，也就是不纯度越高。

如何使用 CART 算法来创建分类树。通过上面的讲解你可以知道，CART 分类树实际上是基于基尼系数来做属性划分的。在 Python 的 sklearn 中，如果我们想要创建 CART 分类树，可以直接使用 DecisionTreeClassifier 这个类。创建这个类的时候，默认情况下 criterion 这个参数等于 gini，也就是按照基尼系数来选择属性划分，即默认采用的是 CART 分类树。下面，我们来用 CART 分类树，给 iris 数据集构造一棵分类决策树。iris 这个数据集，我在 Python 可视化中讲到过，实际上在 sklearn 中也自带了这个数据集。基于 iris 数据集，构造 CART 分类树的代码如下：

如果我们把决策树画出来，可以得到下面的图示：1）首先 train_test_split 可以帮助我们把数据集抽取一部分作为测试集，这样我们就可以得到训练集和测试集。2）使用 clf = DecisionTreeClassifier (criterion=‘gini’) 初始化一棵 CART 分类树。这样你就可以对 CART 分类树进行训练。3）使用 clf.fit (train_features, train_labels) 函数，将训练集的特征值和分类标识作为参数进行拟合，得到 CART 分类树。4）使用 clf.predict (test_features) 函数进行预测，传入测试集的特征值，可以得到测试结果 test_predict。5）最后使用 accuracy_score (test_labels, test_predict) 函数，传入测试集的预测结果与实际的结果作为参数，得到准确率 score。我们能看到 sklearn 帮我们做了 CART 分类树的使用封装，使用起来还是很方便的。

CART 回归树的工作流程。CART 回归树划分数据集的过程和分类树的过程是一样的，只是回归树得到的预测结果是连续值，而且评判「不纯度」的指标不同。在 CART 分类树中采用的是基尼系数作为标准，那么在 CART 回归树中，如何评价「不纯度」呢？实际上我们要根据样本的混乱程度，也就是样本的离散程度来评价「不纯度」。

样本的离散程度具体的计算方式是，先计算所有样本的均值，然后计算每个样本值到均值的差值。我们假设 x 为样本的个体，均值为 u。为了统计样本的离散程度，我们可以取差值的绝对值，或者方差。其中差值的绝对值为样本值减去样本均值的绝对值：|x-µ|。方差为每个样本值减去样本均值的平方和除以样本个数：s=(1/n)∑(x-µ)^2。所以这两种节点划分的标准，分别对应着两种目标函数最优化的标准，即用最小绝对偏差（LAD），或者使用最小二乘偏差（LSD）。这两种方式都可以让我们找到节点划分的方法，通常使用最小二乘偏差的情况更常见一些。我们可以通过一个例子来看下如何创建一棵 CART 回归树来做预测。

如何使用 CART 回归树做预测。这里我们使用到 sklearn 自带的波士顿房价数据集，该数据集给出了影响房价的一些指标，比如犯罪率，房产税等，最后给出了房价。根据这些指标，我们使用 CART 回归树对波士顿房价进行预测，代码如下；运行结果（每次运行结果可能会有不同）。

如果把回归树画出来，可以得到下面的图示（波士顿房价数据集的指标有些多，所以树比较大）。我们来看下这个例子，首先加载了波士顿房价数据集，得到特征集和房价。然后通过 train_test_split 帮助我们把数据集抽取一部分作为测试集，其余作为训练集。1）使用 dtr=DecisionTreeRegressor () 初始化一棵 CART 回归树。2）使用 dtr.fit (train_features, train_price) 函数，将训练集的特征值和结果作为参数进行拟合，得到 CART 回归树。3）使用 dtr.predict (test_features) 函数进行预测，传入测试集的特征值，可以得到预测结果 predict_price。4）最后我们可以求得这棵回归树的二乘偏差均值，以及绝对值偏差均值。我们能看到 CART 回归树的使用和分类树类似，只是最后求得的预测值是个连续值。

CART 决策树的剪枝。CART 决策树的剪枝主要采用的是 CCP 方法，它是一种后剪枝的方法，英文全称叫做 cost-complexity prune，中文叫做代价复杂度。这种剪枝方式用到一个指标叫做节点的表面误差率增益值，以此作为剪枝前后误差的定义。用公式表示则是；其中 Tt 代表以 t 为根节点的子树，C (Tt) 表示节点 t 的子树没被裁剪时子树 Tt 的误差，C (t) 表示节点 t 的子树被剪枝后节点 t 的误差，|Tt | 代子树 Tt 的叶子数，剪枝后，T 的叶子数减少了 |Tt|-1。

所以节点的表面误差率增益值等于节点 t 的子树被剪枝后的误差变化除以剪掉的叶子数量。因为我们希望剪枝前后误差最小，所以我们要寻找的就是最小 α 值对应的节点，把它剪掉。这时候生成了第一个子树。重复上面的过程，继续剪枝，直到最后只剩下根节点，即为最后一个子树。得到了剪枝后的子树集合后，我们需要用验证集对所有子树的误差计算一遍。可以通过计算每个子树的基尼指数或者平方误差，取误差最小的那个树，得到我们想要的结果。

### 黑板墙

给你留两道思考题吧，你能说下 ID3，C4.5，以及 CART 分类树在做节点划分时的区别吗？第二个问题是，sklearn 中有个手写数字数据集，调用的方法是 load_digits ()，你能否创建一个 CART 分类树，对手写数字数据集做分类？另外选取一部分测试集，统计下分类树的准确率？

ID3：以信息增益作为判断标准，计算每个特征的信息增益，选取信息增益最大的特征，但是容易选取到取值较多的特征。C4.5：以信息增益比作为判断标准，计算每个特征的信息增益比，选取信息增益比最大的特征。CART：分类树以基尼系数为标准，选取基尼系数小的的特征。回归树以均方误差或绝对值误差为标准，选取均方误差或绝对值误差最小的特征。

## 0203决策树3泰坦尼克乘客生存预测.md

用泰坦尼克乘客生存预测案例把决策树模型的流程跑了一遍。在实战中，你需要注意一下几点：1）特征选择是分类模型好坏的关键。选择什么样的特征，以及对应的特征值矩阵，决定了分类模型的好坏。通常情况下，特征值不都是数值类型，可以使用 DictVectorizer 类进行转化；2）模型准确率需要考虑是否有测试集的实际结果可以做对比，当测试集没有真实结果可以对比时，需要使用 K 折交叉验证 cross_val_score；3）Graphviz 可视化工具可以很方便地将决策模型呈现出来，帮助你更好理解决策树的构建。

在前面的文章中讲了决策树算法。决策树算法是经常使用的数据挖掘算法，这是因为决策树就像一个人脑中的决策模型一样，呈现出来非常直观。基于决策树还诞生了很多数据挖掘算法，比如随机森林（Random forest）。决策树分类的应用场景非常广泛，在各行各业都有应用，比如在金融行业可以用决策树做贷款风险评估，医疗行业可以用决策树生成辅助诊断，电商行业可以用决策树对销售额进行预测等。在了解决策树的原理后，我们用 sklearn 工具解决一个实际的问题：泰坦尼克号乘客的生存预测。

sklearn 中的决策树模型。首先，我们需要掌握 sklearn 中自带的决策树分类器 DecisionTreeClassifier，方法如下：clf = DecisionTreeClassifier(criterion='entropy')；到目前为止，sklearn 中只实现了 ID3 与 CART 决策树，所以我们暂时只能使用这两种决策树，在构造 DecisionTreeClassifier 类时，其中有一个参数是 criterion，意为标准。它决定了构造的分类树是采用 ID3 分类树，还是 CART 分类树，对应的取值分别是 entropy 或者 gini：1）entropy: 基于信息熵，也就是 ID3 算法，实际结果与 C4.5 相差不大；2）gini：默认参数，基于基尼系数。CART 算法是基于基尼系数做属性划分的，所以 criterion=gini 时，实际上执行的是 CART 算法。

我们通过设置 criterion='entropy’ 可以创建一个 ID3 决策树分类器，然后打印下 clf，看下决策树在 sklearn 中是个什么东西？这里我们看到了很多参数，除了设置 criterion 采用不同的决策树算法外，一般建议使用默认的参数，默认参数不会限制决策树的最大深度，不限制叶子节点数，认为所有分类的权重都相等等。当然你也可以调整这些参数，来创建不同的决策树模型。我整理了这些参数代表的含义。

在构造决策树分类器后，我们可以使用 fit 方法让分类器进行拟合，使用 predict 方法对新数据进行预测，得到预测的分类结果，也可以使用 score 方法得到分类器的准确率。下面这个表格是 fit 方法、predict 方法和 score 方法的作用。

itanic 乘客生存预测。

1、问题描述。泰坦尼克海难是著名的十大灾难之一，究竟多少人遇难，各方统计的结果不一。现在我们可以得到部分的数据，具体数据你可以从 GitHub 上下载。其中数据集格式为 csv，一共有两个文件：1）train.csv 是训练数据集，包含特征信息和存活与否的标签；2）test.csv: 测试数据集，只包含特征信息。现在我们需要用决策树分类对训练集进行训练，针对测试集中的乘客进行生存预测，并告知分类器的准确率。在训练集中，包括了以下字段，它们具体为：

2、生存预测的关键流程。我们要对训练集中乘客的生存进行预测，这个过程可以划分为两个重要的阶段：1）准备阶段：我们首先需要对训练集、测试集的数据进行探索，分析数据质量，并对数据进行清洗，然后通过特征选择对数据进行降维，方便后续分类运算；2）分类阶段：首先通过训练集的特征矩阵、分类结果得到决策树分类器，然后将分类器应用于测试集。然后我们对决策树分类器的准确性进行分析，并对决策树模型进行可视化。下面，我分别对这些模块进行介绍。

模块 1：数据探索。数据探索这部分虽然对分类器没有实质作用，但是不可忽略。我们只有足够了解这些数据的特性，才能帮助我们做数据清洗、特征选择。那么如何进行数据探索呢？这里有一些函数你需要了解：1）使用 info () 了解数据表的基本情况：行数、列数、每列的数据类型、数据完整度；2）使用 describe () 了解数据表的统计情况：总数、平均值、标准差、最小值、最大值等；3）使用 describe (include=[‘O’]) 查看字符串类型（非数字）的整体情况；4）使用 head 查看前几行数据（默认是前 5 行）；5）使用 tail 查看后几行数据（默认是最后 5 行）。我们可以使用 Pandas 便捷地处理这些问题：

模块 2：数据清洗。通过数据探索，我们发现 Age、Fare 和 Cabin 这三个字段的数据有所缺失。其中 Age 为年龄字段，是数值型，我们可以通过平均值进行补齐；Fare 为船票价格，是数值型，我们也可以通过其他人购买船票的平均值进行补齐。具体实现的代码如下；Cabin 为船舱，有大量的缺失值。在训练集和测试集中的缺失率分别为 77% 和 78%，无法补齐；Embarked 为登陆港口，有少量的缺失值，我们可以把缺失值补齐。

首先观察下 Embarked 字段的取值，方法如下：print(train_data['Embarked'].value_counts())。结果如下；我们发现一共就 3 个登陆港口，其中 S 港口人数最多，占到了 72%，因此我们将其余缺失的 Embarked 数值均设置为 S：

模块 3：特征选择。特征选择是分类器的关键。特征选择不同，得到的分类器也不同。那么我们该选择哪些特征做生存的预测呢？通过数据探索我们发现，PassengerId 为乘客编号，对分类没有作用，可以放弃；Name 为乘客姓名，对分类没有作用，可以放弃；Cabin 字段缺失值太多，可以放弃；Ticket 字段为船票号码，杂乱无章且无规律，可以放弃。其余的字段包括：Pclass、Sex、Age、SibSp、Parch 和 Fare，这些属性分别表示了乘客的船票等级、性别、年龄、亲戚数量以及船票价格，可能会和乘客的生存预测分类有关系。具体是什么关系，我们可以交给分类器来处理。

因此我们先将 Pclass、Sex、Age 等这些其余的字段作特征，放到特征向量 features 里。特征值里有一些是字符串，这样不方便后续的运算，需要转成数值类型，比如 Sex 字段，有 male 和 female 两种取值。我们可以把它变成 Sex=male 和 Sex=female 两个字段，数值用 0 或 1 来表示。同理 Embarked 有 S、C、Q 三种可能，我们也可以改成 Embarked=S、Embarked=C 和 Embarked=Q 三个字段，数值用 0 或 1 来表示。

那该如何操作呢，我们可以使用 sklearn 特征选择中的 DictVectorizer 类，用它将可以处理符号化的对象，将符号转成数字 0/1 进行表示。具体方法如下；你会看到代码中使用了 fit_transform 这个函数，它可以将特征向量转化为特征值矩阵。然后我们看下 dvec 在转化后的特征属性是怎样的，即查看 dvec 的 feature_names_ 属性值，方法如下：print(dvec.feature_names_)。运行结果：

    ['Age', 'Embarked=C', 'Embarked=Q', 'Embarked=S', 'Fare', 'Parch', 'Pclass', 'Sex=female', 'Sex=male', 'SibSp']

你可以看到原本是一列的 Embarked，变成了「Embarked=C」「Embarked=Q」「Embarked=S」三列。Sex 列变成了「Sex=female」「Sex=male」两列。这样 train_features 特征矩阵就包括 10 个特征值（列），以及 891 个样本（行），即 891 行，10 列的特征矩阵。

模块 4：决策树模型。刚才我们已经讲了如何使用 sklearn 中的决策树模型。现在我们使用 ID3 算法，即在创建 DecisionTreeClassifier 时，设置 criterion=‘entropy’，然后使用 fit 进行训练，将特征值矩阵和分类标识结果作为参数传入，得到决策树分类器。

模块 5：模型预测 & 评估。在预测中，我们首先需要得到测试集的特征值矩阵，然后使用训练好的决策树 clf 进行预测，得到预测结果 pred_labels；在模型评估中，决策树提供了 score 函数可以直接得到准确率，但是我们并不知道真实的预测结果，所以无法用预测值和真实的预测结果做比较。我们只能使用训练集中的数据进行模型评估，可以使用决策树自带的 score 函数计算下得到的结果：

你会发现你刚用训练集做训练，再用训练集自身做准确率评估自然会很高。但这样得出的准确率并不能代表决策树分类器的准确率。这是为什么呢？因为我们没有测试集的实际结果，因此无法用测试集的预测结果与实际结果做对比。如果我们使用 score 函数对训练集的准确率进行统计，正确率会接近于 100%（如上结果为 98.2%），无法对分类器的在实际环境下做准确率的评估。

那么有什么办法，来统计决策树分类器的准确率呢？这里可以使用 K 折交叉验证的方式，交叉验证是一种常用的验证分类准确率的方法，原理是拿出大部分样本进行训练，少量的用于分类器的验证。K 折交叉验证，就是做 K 次交叉验证，每次选取 K 分之一的数据作为验证，其余作为训练。轮流 K 次，取平均值。

K 折交叉验证的原理是这样的：1）将数据集平均分割成 K 个等份；2）使用 1 份数据作为测试数据，其余作为训练数据；3）计算测试准确率；4）使用不同的测试集，重复 2、3 步骤。

在 sklearn 的 model_selection 模型选择中提供了 cross_val_score 函数。cross_val_score 函数中的参数 cv 代表对原始数据划分成多少份，也就是我们的 K 值，一般建议 K 值取 10，因此我们可以设置 CV=10，我们可以对比下 score 和 cross_val_score 两种函数的正确率的评估结果；运行结果；你可以看到，score 函数的准确率为 0.9820，cross_val_score 准确率为 0.7835。这里很明显，对于不知道测试集实际结果的，要使用 K 折交叉验证才能知道模型的准确率。

模块 6：决策树可视化。sklearn 的决策树模型对我们来说，还是比较抽象的。我们可以使用 Graphviz 可视化工具帮我们把决策树呈现出来。安装 Graphviz 库需要下面的几步：1）安装 graphviz 工具，这里是它的下载地址；2）将 Graphviz 添加到环境变量 PATH 中；3）需要 Graphviz 库，如果没有可以使用 pip install graphviz 进行安装。这样你就可以在程序里面使用 Graphviz 对决策树模型进行呈现，最后得到一个决策树可视化的 PDF 文件，可视化结果文件 Source.gv.pdf 你可以在 GitHub 上下载。

### 黑板墙

最后给你留一个思考题吧，我在构造特征向量时使用了 DictVectorizer 类，使用 fit_transform 函数将特征向量转化为特征值矩阵。DictVectorizer 类同时也提供 transform 函数，那么这两个函数有什么区别？

1）fit 从一个训练集中学习模型参数，其中就包括了归一化时用到的均值，标准偏差等，可以理解为一个训练过程。2）transform: 在fit的基础上，对数据进行标准化，降维，归一化等数据转换操作。3）fit_transform: 将模型训练和转化合并到一起，训练样本先做fit，得到mean，standard deviation，然后将这些参数用于transform（归一化训练数据），使得到的训练数据是归一化的，而测试数据只需要在原先fit得到的mean，std上来做归一化就行了，所以用transform就行了。

编辑回复: 总结的很好。需要注意的是，transform 和fit_transform 虽然结果相同，但是不能互换。因为 fit_transform 只是 fit+transform 两个步骤合并的简写。而各种分类算法都需要先 fit，然后再进行 transform。所以如果把 fit_transform 替换为 transform 可能会报错。

作者回复：你可以看下输出的决策树的图形，有几个数值你需要了解：1）比如类似 X[7]<=0.5 这种就是告诉你这个节点，选择的属性是 X[7]，阈值是 0.5。当<=0.5的时候，决策进入到左子树，当>0.5的时候，决策进入到右子树。2）entropy 实际上代表了信息不纯度，这个数值越大，代表纯度越低。3）samples 代表的是这个节点的样本数，比如 samples=891，就代表这个节点一般有 891 个样本。然后 value 这个数组会告诉你这个样本集是如何分布的，比如 value=[549,342]，即 891 个样本，有 549 个为 True，也就是 X[7]<=0.5，还有 342 个样本为 False，即这些样本的 X[7]>0.5。4）好了，然后继续上面的分裂过程，直到叶子节点，纯度越来越高，最终归为同一个类别时，纯度最高，entropy=0，此时样本都为同一个类别，也就是按照这条线路可以得到的最终分类结果。所以你能看到：决策树的使用，就是从根节点开始，然后属性划分，当<=阈值时走左子树，>阈值时走右子树，最终在叶子节点可以得到分类的结果。你指的每个方框里的 entropy, samples, vale 都是中间的计算结果。

## 0214关联规则挖掘1如何用Apriori发现用户购物规则.md

Apriori 算法是在「购物篮分析」中常用的关联规则挖掘算法，在 Apriori 算法中你最主要是需要明白支持度、置信度、提升度这几个概念，以及 Apriori 迭代计算频繁项集的工作流程。Apriori 算法在实际工作中需要对数据集扫描多次，会消耗大量的计算时间，所以在 2000 年 FP-Growth 算法被提出来，它只需要扫描两次数据集即可以完成关联规则的挖掘。FP-Growth 算法最主要的贡献就是提出了 FP 树和项头表，通过 FP 树减少了频繁项集的存储以及计算时间。当然 Apriori 的改进算法除了 FP-Growth 算法以外，还有 CBA 算法、GSP 算法，这里就不进行介绍。

你能发现一种新理论的提出，往往是先从最原始的概念出发，提出一种新的方法。原始概念最接近人们模拟的过程，但往往会存在空间和时间复杂度过高的情况。所以后面其他人会对这个方法做改进型的创新，重点是在空间和时间复杂度上进行降维，比如采用新型的数据结构。你能看出树在存储和检索中是一个非常好用的数据结构。

关联规则这个概念，最早是由 Agrawal 等人在 1993 年提出的。在 1994 年 Agrawal 等人又提出了基于关联规则的 Apriori 算法，至今 Apriori 仍是关联规则挖掘的重要算法。关联规则挖掘可以让我们从数据集中发现项与项（item 与 item）之间的关系，它在我们的生活中有很多应用场景，「购物篮分析」就是一个常见的场景，这个场景可以从消费者交易记录中发掘商品与商品之间的关联关系，进而通过商品捆绑销售或者相关推荐的方式带来更多的销售量。所以说，关联规则挖掘是个非常有用的技术。

在今天的内容中，希望你能带着问题，和我一起来搞懂以下几个知识点：1）搞懂关联规则中的几个重要概念：支持度、置信度、提升度；2）Apriori 算法的工作原理；3）在实际工作中，我们该如何进行关联规则挖掘。

1、搞懂关联规则中的几个概念。我举一个超市购物的例子，下面是几名客户购买的商品列表：

什么是支持度呢？支持度是个百分比，它指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的频率越大。在这个例子中，我们能看到「牛奶」出现了 4 次，那么这 5 笔订单中「牛奶」的支持度就是 4/5=0.8。同样「牛奶 + 面包」出现了 3 次，那么这 5 笔订单中「牛奶 + 面包」的支持度就是 3/5=0.6。

什么是置信度呢？它指的就是当你购买了商品 A，会有多大的概率购买商品 B，在上面这个例子中：1）置信度（牛奶→啤酒）=2/4=0.5，代表如果你购买了牛奶，有多大的概率会购买啤酒？2）置信度（啤酒→牛奶）=2/3=0.67，代表如果你购买了啤酒，有多大的概率会购买牛奶？我们能看到，在 4 次购买了牛奶的情况下，有 2 次购买了啤酒，所以置信度 (牛奶→啤酒)=0.5，而在 3 次购买啤酒的情况下，有 2 次购买了牛奶，所以置信度（啤酒→牛奶）=0.67。所以说置信度是个条件概念，就是说在 A 发生的情况下，B 发生的概率是多少。

什么是提升度呢？我们在做商品推荐的时候，重点考虑的是提升度，因为提升度代表的是「商品 A 的出现，对商品 B 的出现概率提升的」程度。还是看上面的例子，如果我们单纯看置信度 (可乐→尿布)=1，也就是说可乐出现的时候，用户都会购买尿布，那么当用户购买可乐的时候，我们就需要推荐尿布么？实际上，就算用户不购买可乐，也会直接购买尿布的，所以用户是否购买可乐，对尿布的提升作用并不大。我们可以用下面的公式来计算商品 A 对商品 B 的提升度：

    提升度 (A→B)= 置信度 (A→B)/ 支持度 (B)

这个公式是用来衡量 A 出现的情况下，是否会对 B 出现的概率有所提升。所以提升度有三种可能：1）提升度 (A→B)>1：代表有提升；2）提升度 (A→B)=1：代表有没有提升，也没有下降；3）提升度 (A→B)<1：代表有下降。

2、Apriori 的工作原理。明白了关联规则中支持度、置信度和提升度这几个重要概念，我们来看下 Apriori 算法是如何工作的。首先我们把上面案例中的商品用 ID 来代表，牛奶、面包、尿布、可乐、啤酒、鸡蛋的商品 ID 分别设置为 1-6，上面的数据表可以变为：

Apriori 算法其实就是查找频繁项集 (frequent itemset) 的过程，所以首先我们需要定义什么是频繁项集。频繁项集就是支持度大于等于最小支持度 (Min Support) 阈值的项集，所以小于最小值支持度的项目就是非频繁项集，而大于等于最小支持度的项集就是频繁项集。项集这个概念，英文叫做 itemset，它可以是单个的商品，也可以是商品的组合。我们再来看下这个例子，假设我随机指定最小支持度是 50%，也就是 0.5。

我们来看下 Apriori 算法是如何运算的。首先，我们先计算单个商品的支持度，也就是得到 K=1 项的支持度；因为最小支持度是 0.5，所以你能看到商品 4、6 是不符合最小支持度的，不属于频繁项集，于是经过筛选商品的频繁项集就变成。

在这个基础上，我们将商品两两组合，得到 k=2 项的支持度；我们再筛掉小于最小值支持度的商品组合，可以得到；我们再将商品进行 K=3 项的商品组合，可以得到；再筛掉小于最小值支持度的商品组合，可以得到：

通过上面这个过程，我们可以得到 K=3 项的频繁项集 {1,2,3}，也就是 {牛奶、面包、尿布} 的组合。到这里，你已经和我模拟了一遍整个 Apriori 算法的流程，下面我来给你总结下 Apriori 算法的递归流程：1）K=1，计算 K 项集的支持度；2）筛选掉小于最小支持度的项集；3）如果项集为空，则对应 K-1 项集的结果为最终结果。否则 K=K+1，重复 1-3 步。

3、Apriori 的改进算法：FP-Growth 算法。我们刚完成了 Apriori 算法的模拟，你能看到 Apriori 在计算的过程中有以下几个缺点：1）可能产生大量的候选集。因为采用排列组合的方式，把可能的项集都组合出来了；2）每次计算都需要重新扫描数据集，来计算每个项集的支持度。

所以 Apriori 算法会浪费很多计算空间和计算时间，为此人们提出了 FP-Growth 算法，它的特点是：1）创建了一棵 FP 树来存储频繁项集。在创建前对不满足最小支持度的项进行删除，减少了存储空间。我稍后会讲解如何构造一棵 FP 树；2）整个生成过程只遍历数据集 2 次，大大减少了计算量。所以在实际工作中，我们常用 FP-Growth 来做频繁项集的挖掘，下面我给你简述下 FP-Growth 的原理。

创建项头表（item header table）。创建项头表的作用是为 FP 构建及频繁项集挖掘提供索引。这一步的流程是先扫描一遍数据集，对于满足最小支持度的单个项（K=1 项集）按照支持度从高到低进行排序，这个过程中删除了不满足最小支持度的项。项头表包括了项目、支持度，以及该项在 FP 树中的链表。初始的时候链表为空。

构造 FP 树。FP 树的根节点记为 NULL 节点。整个流程是需要再次扫描数据集，对于每一条数据，按照支持度从高到低的顺序进行创建节点（也就是第一步中项头表中的排序结果），节点如果存在就将计数 count+1，如果不存在就进行创建。同时在创建的过程中，需要更新项头表的链表。

通过 FP 树挖掘频繁项集。到这里，我们就得到了一个存储频繁项集的 FP 树，以及一个项头表。我们可以通过项头表来挖掘出每个频繁项集。具体的操作会用到一个概念，叫「条件模式基」，它指的是以要挖掘的节点为叶子节点，自底向上求出 FP 子树，然后将 FP 子树的祖先节点设置为叶子节点之和。我以「啤酒」的节点为例，从 FP 树中可以得到一棵 FP 子树，将祖先节点的支持度记为叶子节点之和，得到：

你能看出来，相比于原来的 FP 树，尿布和牛奶的频繁项集数减少了。这是因为我们求得的是以「啤酒」为节点的 FP 子树，也就是说，在频繁项集中一定要含有「啤酒」这个项。你可以再看下原始的数据，其中订单 1 {牛奶、面包、尿布} 和订单 5 {牛奶、面包、尿布、可乐} 并不存在「啤酒」这个项，所以针对订单 1，尿布→牛奶→面包这个项集就会从 FP 树中去掉，针对订单 5 也包括了尿布→牛奶→面包这个项集也会从 FP 树中去掉，所以你能看到以「啤酒」为节点的 FP 子树，尿布、牛奶、面包项集上的计数比原来少了 2。

条件模式基不包括「啤酒」节点，而且祖先节点如果小于最小支持度就会被剪枝，所以「啤酒」的条件模式基为空。同理，我们可以求得「面包」的条件模式基为；所以可以求得面包的频繁项集为 {尿布，面包}，{尿布，牛奶，面包}。同样，我们还可以求得牛奶，尿布的频繁项集，这里就不再计算展示。

### 黑板墙

你能说一说 Apriori 的工作原理吗？相比于 Apriori，FP-Growth 算法都有哪些改进？

构建子树：1）假设已经完成创建项头表的工作，省略 count+1；2）扫描数据集，按照项头表排列好的结果，一次创建节点；3）因为尿布出现在所有订单中，没有例外情况，所以这只有一个子节点；4）因为牛奶出现在尿布中的所有订单里，所以只有一个子节点；5）由表中数据可得，在出现牛奶的订单中，面包出现的情况，分为两种，a）出现 3 次面包，出现在有牛奶的订单中。b）出现一次面包，出现在没有牛奶的订单中，故，生成两个子节点；6）后续内容属于迭代内容，自行体会。

创建条件模式集是一个减掉子树过程。将祖先节点的支持度，记为叶子节点之和，减少频繁项集。简单理解，就是有几个叶子，说明最开始的节点，怀了几个孩子，怀几个生几个。理解：1）创建含有啤酒的 FP 树，只有订单中含有啤酒的频繁项集才存在；2）去掉啤酒节点，品酒节点为空，得到，两个频繁项集。

Apriori 挖掘频繁项集，那么置信度和提升度是对得出的频繁项集进行验证的是吧？如得出了啤酒的频繁项集后是对每个结果计算提升度，怎么选择最优的组合呢？是否会出现提升度大而置信度下降的情况？编辑回复: 置信度和提升度是对频繁项集的一种验证，在筛选最优组合的时候，一般会设置最小支持度，最小置信度，这样频繁项集和关联关系都要满足这个条件。提升度 (A→B)= 置信度 (A→B)/ 支持度 (B)，所以提升度是对满足前两者条件的另一种验证方式，这样避免一种情况：置信度(A->B)很高，是因为本身支持度(B)很高，实际上和A的出现关系不大。

构造FP树的过程这里看不懂，面包，啤酒为什么会拆分呢？编辑回复: FP -Growth中有一个概念叫：条件模式基。它在FP树创建的时候还用不上，创建的时候主要是通过扫描整个数据，和项头表来构造 FP 树。条件模式基用于挖掘频繁项的过程。通过数找到每个项（item）的条件模式基，递归挖掘频繁项集。

FP 还是这里说的清楚：[机器学习之手把手实现，第 2 部分: 频繁项集与关联规则 FP-growth 的原理和实现](https://www.ibm.com/developerworks/cn/analytics/library/machine-learning-hands-on2-fp-growth/index.html)

2『已下载原文「0601频繁项集与关联规则FP-growth的原理和实现」作为专栏附件。』

## 0215关联规则挖掘2导演如何选择演员.md

Apriori 算法的核心就是理解频繁项集和关联规则。在算法运算的过程中，还要重点掌握对支持度、置信度和提升度的理解。在工具使用上，你可以使用 efficient-apriori 这个工具包，它会把每一条数据中的项（item）放到一个集合（篮子）里来处理，不考虑项（item）之间的先后顺序。在实际运用中你还需要灵活处理，比如导演如何选择演员这个案例，虽然工具的使用会很方便，但重要的还是数据挖掘前的准备过程，也就是获取某个导演的电影数据集。

关联规则挖掘在生活中有很多使用场景，不仅是商品的捆绑销售，甚至在挑选演员决策上，你也能通过关联规则挖掘看出来某个导演选择演员的倾向。带你用 Apriori 算法做一个项目实战。你需要掌握的是以下几点：1）熟悉上节课讲到的几个重要概念：支持度、置信度和提升度；2）熟悉与掌握 Apriori 工具包的使用；3）在实际问题中，灵活运用。包括数据集的准备等。

1、如何使用 Apriori 工具包。Apriori 虽然是十大算法之一，不过在 sklearn 工具包中并没有它，也没有 FP-Growth 算法。这里教你个方法，来选择 Python 中可以使用的工具包，你可以通过 [PyPI · The Python Package Index](https://pypi.org/) 搜索工具包。这个网站提供的工具包都是 Python 语言的，你能找到 8 个 Python 语言的 Apriori 工具包，具体选择哪个呢？建议你使用第二个工具包，即 efficient-apriori。后面我会讲到为什么推荐这个工具包。

首先你需要通过 pip install efficient-apriori 安装这个工具包。然后看下如何使用它，核心的代码就是这一行：

    itemsets, rules = apriori(data, min_support,  min_confidence)

其中 data 是我们要提供的数据集，它是一个 list 数组类型。min_support 参数为最小支持度，在 efficient-apriori 工具包中用 0 到 1 的数值代表百分比，比如 0.5 代表最小支持度为 50%。min_confidence 是最小置信度，数值也代表百分比，比如 1 代表 100%。

关于支持度、置信度和提升度，我们再来简单回忆下。支持度指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的概率越大。置信度是一个条件概念，就是在 A 发生的情况下，B 发生的概率是多少。提升度代表的是「商品 A 的出现，对商品 B 的出现概率提升了多少」。接下来我们用这个工具包，跑一下上节课中讲到的超市购物的例子。下面是客户购买的商品列表；具体实现的代码如下：

```
from efficient_apriori import apriori
# 设置数据集
data = [('牛奶','面包','尿布'),
           ('可乐','面包', '尿布', '啤酒'),
           ('牛奶','尿布', '啤酒', '鸡蛋'),
           ('面包', '牛奶', '尿布', '啤酒'),
           ('面包', '牛奶', '尿布', '可乐')]
# 挖掘频繁项集和频繁规则
itemsets, rules = apriori(data, min_support=0.5,  min_confidence=1)
print(itemsets)
print(rules)
```

你能从代码中看出来，data 是个 List 数组类型，其中每个值都可以是一个集合。实际上你也可以把 data 数组中的每个值设置为 List 数组类型，比如：

两者的运行结果是一样的，efficient-apriori 工具包把每一条数据集里的项式都放到了一个集合中进行运算，并没有考虑它们之间的先后顺序。因为实际情况下，同一个购物篮中的物品也不需要考虑购买的先后顺序。而其他的 Apriori 算法可能会因为考虑了先后顺序，出现计算频繁项集结果不对的情况。所以这里采用的是 efficient-apriori 这个工具包。

2、挖掘导演是如何选择演员的。在实际工作中，数据集是需要自己来准备的，比如今天我们要挖掘导演是如何选择演员的数据情况，但是并没有公开的数据集可以直接使用。因此我们需要使用之前讲到的 Python 爬虫进行数据采集。不同导演选择演员的规则是不同的，因此我们需要先指定导演。数据源我们选用豆瓣电影。

先来梳理下采集的工作流程。首先我们先在[豆瓣电影](https://movie.douban.com/)搜索框中输入导演姓名，比如「宁浩」。页面会呈现出来导演之前的所有电影，然后对页面进行观察，你能观察到以下几个现象：1）页面默认是 15 条数据反馈，第一页会返回 16 条。因为第一条数据实际上这个导演的概览，你可以理解为是一条广告的插入，下面才是真正的返回结果。2）每条数据的最后一行是电影的演出人员的信息，第一个人员是导演，其余为演员姓名。姓名之间用「/」分割。

有了这些观察之后，我们就可以编写抓取程序了。在代码讲解中你能看出这两点观察的作用。抓取程序的目的是为了生成宁浩导演（你也可以抓取其他导演）的数据集，结果会保存在 csv 文件中。完整的抓取代码如下：

代码中涉及到了几个模块，我简单讲解下这几个模块。在引用包这一段，我们使用 csv 工具包读写 CSV 文件，用 efficient_apriori 完成 Apriori 算法，用 lxml 进行 XPath 解析，time 工具包可以让我们在模拟后有个适当停留，代码中我设置为 1 秒钟，等 HTML 数据完全返回后再进行 HTML 内容的获取。使用 selenium 的 webdriver 来模拟浏览器的行为。在读写文件这一块，我们需要事先告诉 python 的 open 函数，文件的编码是 utf-8-sig（对应代码：encoding=‘utf-8-sig’），这是因为我们会用到中文，为了避免编码混乱。

编写 download 函数，参数传入我们要采集的页面地址（request_url）。针对返回的 HTML，我们需要用到之前讲到的 Chrome 浏览器的 XPath Helper 工具，来获取电影名称以及演出人员的 XPath。我用页面返回的数据个数来判断当前所处的页面序号。如果数据个数 >15，也就是第一页，第一页的第一条数据是广告，我们需要忽略。如果数据个数 =15，代表是中间页，需要点击「下一页」，也就是翻页。如果数据个数 <15，代表最后一页，没有下一页。在程序主体部分，我们设置 start 代表抓取的 ID，从 0 开始最多抓取 1 万部电影的数据（一个导演不会超过 1 万部电影），每次翻页 start 自动增加 15，直到 flag=False 为止，也就是不存在下一页的情况。

1『记得把程序「chromedriver」放在脚本所在的目录里。成功爬取了，之前只是熟悉 scrapy 框架爬，原始爬取不太熟悉，好好研究一下上面的代码；Apriori 算法的代码跑失败了，把支持度降为 0.35 后才跑出来跟作者一样的结果。』

你可以模拟下抓取的流程，获得指定导演的数据，比如我上面抓取的宁浩的数据。这里需要注意的是，豆瓣的电影数据可能是不全的，但基本上够我们用。有了数据之后，我们就可以用 Apriori 算法来挖掘频繁项集和关联规则，代码如下：

代码中使用的 apriori 方法和开头中用 Apriori 获取购物篮规律的方法类似，比如代码中都设定了最小支持度和最小置信系数，这样我们可以找到支持度大于 50%，置信系数为 1 的频繁项集和关联规则。这是最后的运行结果：

```
{1: {(' 徐峥 ',): 5, (' 黄渤 ',): 6}, 2: {(' 徐峥 ', ' 黄渤 '): 5}}
[{徐峥} -> {黄渤}]
```

你能看出来，宁浩导演喜欢用徐峥和黄渤，并且有徐峥的情况下，一般都会用黄渤。你也可以用上面的代码来挖掘下其他导演选择演员的规律。

2『汇总了一下目前收集的统计学书籍「2018099女士品茶」、「2020019统计学习方法」、「2019278统计与真理」、「2020060统计数据会说谎」、「2019038赤裸裸的统计学」、「2019238统计学关我什么事」、「2019103统计思维」、「2020057看穿一切数字的统计学」、「2020058写给所有人的极简统计学」、「2019240统计学」。（2020-03-01）』

### 黑板墙

个人的直觉感觉，这个应该跟数据集的大小和特点有关。编辑回复：对，和数据集特点有关系，不过数据集大的情况下，不好观察特征。我们可以通过设置最小值支持度和最小置信度来观察关联规则的结果。一般来说最小支持度常见的取值有 0.5，0.1, 0.05。最小置信度常见的取值有 1.0, 0.9, 0.8。可以通过尝试一些取值，然后观察关联结果的方式来调整最小值尺度和最小置信度的取值。

老师，FP-growth 在 python 有集成吗，，想用 fp-growth 试试。作者回复：有一个工具包 import fptools as fp 你可以试试。