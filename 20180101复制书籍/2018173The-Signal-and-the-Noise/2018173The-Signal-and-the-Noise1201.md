Nate Silver.(2012).2018173The-Signal-and-the-Noise.Penguin Group => 0101





## 1201A CLIMATE OF HEALTHY SKEPTICISM

June 23, 1988, was an unusually hot day on Capitol Hill. The previous afternoon, temperatures hit 100 degrees at Washington's National Airport, the first time in decades they reached triple-digits so early in the summer.1 The NASA climatologist James Hansen wiped his brow—the air-conditioning had inconveniently* ceased to function in the Senate Energy Committee's hearing room—and told the American people they should prepare for more of the same.

The greenhouse effect had long been accepted theory, predicted by scientists to warm the planet.2 But for the first time, Hansen said, it had begun to produce an unmistakable signal in the temperature record: global temperatures had increased by about 0.4°C since the 1950s, and this couldn't be accounted for by natural variations. "The probability of a chance warming of that magnitude is about 1 percent," Hansen told Congress. "So with 99 percent confidence we can state that the warming trend during this time period is a real warming trend." 3

Hansen predicted more frequent heat waves in Washington and in other cities like Omaha—already the change was "large enough to be noticeable to the average person." The models needed to be refined, he advised, but both the temperature trend and the reasons for it were clear. "It is time to stop waffling so much," Hansen said. "The evidence is pretty strong that the greenhouse effect is here."4

With nearly a quarter century having passed since Hansen's hearing, it is time to ask some of the same questions about global warming that we have of other fields in this book. How right, or how wrong, have the predictions about it been so far? What are scientists really agreed upon and where is there more debate? How much uncertainty is there in the forecasts, and how should we respond to it? Can something as complex as the climate system really be modeled well at all? Are climate scientists prone to the same problems, like overconfidence, that befall forecasters in other fields? How much have politics and other perverse incentives undermined the search for scientific truth? And can Bayesian reasoning be of any help in adjudicating the debate?

We should examine the evidence and articulate what might be thought of as healthy skepticism toward climate predictions. As you will see, this kind of skepticism does not resemble the type that is common in blogs or in political arguments over global warming.

The Noise and the Signal

Many of the examples in this book concern cases where forecasters mistake correlation for causation and noise for a signal. Up until about 1997, the conference of the winning Super Bowl team had been very strongly correlated with the direction of the stock market over the course of the next year. However, there was no credible causal mechanism behind the relationship, and if you had made investments on that basis you would have lost your shirt. The Super Bowl indicator was a false positive.

The reverse can sometimes also be true. Noisy data can obscure the signal, even when there is essentially no doubt that the signal exists. Take a relationship that few of us would dispute: if you consume more calories, you are more likely to become fat. Surely such a basic relationship would show up clearly in the statistical record?

I downloaded data from eighty-four countries for which estimates of both obesity rates and daily caloric consumption are publicly available.5 Looked at in this way, the relationship seems surprisingly tenuous. The daily consumption in South Korea, which has a fairly meat-heavy diet, is about 3,070 calories per person per day, slightly above the world average. However, the obesity rate there is only about 3 percent. The Pacific island nation of Nauru, by contrast, consumes about as many calories as South Korea per day,6 but the obsesity rate there is 79 percent. If you plot the eighty-four countries on a graph (figure 12-1) there seems to be only limited evidence of a connection between obesity and calorie consumption; it would not qualify as "statistically significant" by standard tests.*

FIGURE 12-1: CALORIE CONSUMPTION AND OBESITY RATES IN 84 COUNTRIES

There are, of course, many conflating factors that obscure the relationship. Certain countries have better genetics, or better exercise habits. And the data is rough: estimating how many calories an adult consumes in a day is challenging.7 A researcher who took this statistical evidence too literally might incorrectly reject the connection between calorie consumption and obesity, a false negative.

It would be nice if we could just plug data into a statistical model, crunch the numbers, and take for granted that it was a good representation of the real world. Under some conditions, especially in data-rich fields like baseball, that assumption is fairly close to being correct. In many other cases, a failure to think carefully about causality will lead us up blind alleys.

There would be much reason to doubt claims about global warming were it not for their grounding in causality. The earth's climate goes through various warm and cold phases that play out over periods of years or decades or centuries. These cycles long predate the dawn of industrial civilization.

However, predictions are potentially much stronger when backed up by a sound understanding of the root causes behind a phenomenon. We do have a good understanding of the cause of global warming: it is the greenhouse effect.

The Greenhouse Effect Is Here

In 1990, two years after Hansen's hearing, the United Nations' International Panel on Climate Change (IPCC) released more than a thousand pages of findings about the science of climate change in its First Assessment Report. Produced over several years by a team of hundreds of scientists from around the globe, the report went into voluminous detail on the potential changes in temperatures and ecosystems, and outlined a variety of strategies to mitigate these effects.

The IPCC's scientists classified just two findings as being absolutely certain, however. These findings did not rely on complex models, and they did not make highly specific predictions about the climate. Instead, they were based on relatively simple science that had been well-understood for more than 150 years and which is rarely debated even by self-described climate skeptics. They remain the most important scientific conclusions about climate change today.

The IPCC's first conclusion was simply that the greenhouse effect exists:

There is a natural greenhouse effect that keeps the Earth warmer than it otherwise would be.8

The greenhouse effect is the process by which certain atmospheric gases—principally water vapor, carbon dioxide (CO2), methane, and ozone—absorb solar energy that has been reflected from the earth's surface. Were it not for this process, about 30 percent9 of the sun's energy would be reflected back out into space in the form of infrared radiation. That would leave the earth's temperatures much colder than they actually are: about 0° Fahrenheit or –18° Celsius10 on average, or the same as a warm day on Mars.11

Conversely, if these gases become more plentiful in the atmosphere, a higher fraction of the sun's energy will be trapped and reflected back onto the surface, making temperatures much warmer. On Venus, which has a much thicker atmosphere consisting almost entirely of carbon dioxide, the average temperature is 460°C.12 Some of that heat comes from Venus's proximity to the sun, but much of it is because of the greenhouse effect.13

There is no scenario in the foreseeable future under which the earth's climate will come to resemble that of Venus. However, the climate is fairly sensitive to changes in atmospheric composition, and human civilization thrives within a relatively narrow band of temperatures. The coldest world capital is Ulan Bator, Mongolia, where temperatures average about –1°C (or +30°F) over the course of the year;14 the warmest is probably Kuwait City, Kuwait, where they average +27°C (+81°F).15 Temperatures can be hotter or cooler during winter or summer or in sparsely populated areas,16 but the temperature extremes are modest on an interplanetary scale. On Mercury, by contrast, which has little atmosphere to protect it, temperatures often vary between about –200°C and +400°C over the course of a single day.17

The IPCC's second conclusion made an elementary prediction based on the greenhouse effect: as the concentration of greenhouse gases increased in the atmosphere, the greenhouse effect and global temperatures would increase along with them:

Emissions resulting from human activities are substantially increasing the atmospheric concentrations of the greenhouse gases carbon dioxide, methane, chlorofluorocarbons (CFCs) and nitrous oxide. These increases will enhance the greenhouse effect, resulting on average in additional warming of the Earth's surface. The main greenhouse gas, water vapor, will increase in response to global warming and further enhance it.

This IPCC finding makes several different assertions, each of which is worth considering in turn.

First, it claims that atmospheric concentrations of greenhouse gases like CO2 are increasing, and as a result of human activity. This is a matter of simple observation. Many industrial processes, particularly the use of fossil fuels, produce CO2 as a by-product.18 Because CO2 remains in the atmosphere for a long time, its concentrations have been rising: from about 315 parts per million (ppm) when CO2 levels were first directly monitored at the Mauna Loa Observatory in Hawaii in 1959 to about 390 PPM as of 2011.19

The second claim, "these increases will enhance the greenhouse effect, resulting on average in additional warming of the Earth's surface," is essentially just a restatement of the IPCC's first conclusion that the greenhouse effect exists, phrased in the form of a prediction. The prediction relies on relatively simple chemical reactions that were identified in laboratory experiments many years ago. The greenhouse effect was first proposed by the French physicist Joseph Fourier in 1824 and is usually regarded as having been proved by the Irish physicist John Tyndall in 1859,20 the same year that Charles Darwin published On the Origin of Species.

The third claim—that water vapor will also increase along with gases like CO2, thereby enhancing the greenhouse effect—is modestly bolder. Water vapor, not CO2, is the largest contributor to the greenhouse effect.21 If there were an increase in CO2 alone, there would still be some warming, but not as much as has been observed to date or as much as scientists predict going forward. But a basic thermodynamic principle known as the Clausius–Clapeyron relation, which was proposed and proved in the nineteenth century, holds that the atmosphere can retain more water vapor at warmer temperatures. Thus, as CO2 and other long-lived greenhouse gases increase in concentration and warm the atmosphere, the amount of water vapor will increase as well, multiplying the effects of CO2 and enhancing warming.

This Isn't Rocket Science

Scientists require a high burden of proof before they are willing to conclude that a hypothesis is incontrovertible. The greenhouse hypothesis has met this standard, which is why the original IPCC report singled it out from among hundreds of findings as the only thing that scientists were absolutely certain about. The science behind the greenhouse effect was simple enough to have been widely understood by the mid- to late nineteenth century, when the lightbulb and the telephone and the automobile were being invented—and not the atomic bomb or the iPhone or the Space Shuttle. The greenhouse effect isn't rocket science.

Indeed, predictions that industrial activity would eventually trigger global warming were made long before the IPCC—as early as 189722 by the Swedish chemist Svante Arrhenius, and at many other times23 before the warming signal produced by the greenhouse signal had become clear enough to be distinguished from natural causes.

It now seems almost quaint to refer to the greenhouse effect. In the mid-1980s, the term greenhouse effect was about five times more common in English-language books24 than the phrase global warming. But usage of greenhouse effect peaked in the early 1990s and has been in steady decline since. It is now used only about one-sixth as often as the term global warming, and one-tenth as often as the broader term climate change.25

This change has largely been initiated by climate scientists26 as they seek to expand the predictive implications of the theory. However, the pullback from speaking about the causes of the change—the greenhouse effect—yields predictably misinformed beliefs about it.*

In January 2012, for instance, the Wall Street Journal published an editorial27 entitled "No Need to Panic About Global Warming," which was signed by a set of sixteen scientists and advocates who might be considered global warming skeptics. Accompanying the editorial was a video produced by the Wall Street Journal that was captioned with the following phrase:

A large number of scientists don't believe that carbon dioxide is causing global warming.

In fact, very few scientists doubt this—there is essentially no debate that greenhouse gases cause global warming. Among the "believers" in the theory was the physics professor William Happer of Princeton, who cosigned the editorial and who was interviewed for the video. "Most people like me believe that industrial emissions will cause warming," Happer said about two minutes into the video. Happer takes issue with some of the predictions of global warming's effects, but not with its cause.

I do not mean to suggest that you should just blindly accept a theory in the face of contradictory evidence. A theory is tested by means of its predictions, and the predictions made by climate scientists have gotten some things right and some things wrong. Temperature data is quite noisy. A warming trend might validate the greenhouse hypothesis or it might be caused by cyclical factors. A cessation in warming could undermine the theory or it might represent a case where the noise in the data had obscured the signal.

But even if you believe, as Bayesian reasoning would have it, that almost all scientific hypotheses should be thought of probabilistically, we should have a greater degree of confidence in a hypothesis backed up by strong and clear causal relationships. Newly discovered evidence that seems to militate against the theory should nevertheless lower our estimate of its likelihood, but it should be weighed in the context of the other things we know (or think we do) about the planet and its climate.

Healthy skepticism needs to proceed from this basis. It needs to weigh the strength of new evidence against the overall strength of the theory, rather than rummaging through fact and theory alike for argumentative and ideological convenience, as is the cynical practice when debates become partisan and politicized.

Three Types of Climate Skepticism

It is hard to imagine a worse time and place to hold a global climate conference than Copenhagen in December, as the United Nations did in in 2009. During the winter solstice there, the days are short and dark—perhaps four hours of decent sunlight—and the temperatures are cold, with the wind whipping off the Øresund, the narrow strait that separates Denmark from Sweden.

Worse yet, the beer is expensive: the high taxes on alcohol and pretty much everything else in Denmark help to pay for a green-technology infrastructure that rivals almost anywhere in the world. Denmark consumes no more energy today than it did in the late 1960s,28 in part because it is environmentally friendly and in part because of its low population growth. (By contrast, the United States' energy consumption has roughly doubled over the same period.29) The implicit message seemed to be that an energy-efficient future would be cold, dark, and expensive.

It is little wonder, then, that the mood at Copenhagen's Bella Center ranged far beyond skepticism and toward outright cynicism. I had gone to the conference, somewhat naively, seeking a rigorous scientific debate about global warming. What I found instead was politics, and the differences seemed irreconcilable.

Delegates from Tuvalu, a tiny, low-lying Pacific island nation that would be among the most vulnerable to rising sea levels, roamed the halls, loudly protesting what they thought to be woefully inadequate targets for greenhouse-gas reduction. Meanwhile, the large nations that account for the vast majority of greenhouse-gas emissions were nowhere near agreement.

President Obama had arrived at the conference empty-handed, having burned much of his political capital on his health-care bill and his stimulus package. Countries like China, India, and Brazil, which are more vulnerable than the United States to climate change impacts because of their geography but are reluctant to adopt commitments that might impair their economic growth, weren't quite sure where to stand. Russia, with its cold climate and its abundance of fossil-fuel resources, was a wild card. Canada, also cold and energy-abundant, was another, unlikely to push for any deal that the United States lacked the willpower to enact.30 There was some semblance of a coalition among some of the wealthier nations in Europe, along with Australia, Japan, and many of the world's poorer countries in Africa and the Pacific.31 But global warming is a problem wherein even if the politics are local, the science is not. CO2 quickly circulates around the planet: emissions from a diesel truck in Qingdao will eventually affect the climate in Quito. Emissions-reductions targets therefore require near-unanimity, and not mere coalition-building, in order to be enacted successfully. That agreement seemed years if not decades away.

I was able to speak with a few scientists at the conference. One of them was Richard Rood, a soft-spoken North Carolinian who once led teams of scientists at NASA and who now teaches a course on climate policy to students at the University of Michigan.

"At NASA, I finally realized that the definition of rocket science is using relatively simple psychics to solve complex problems," Rood told me. "The science part is relatively easy. The other parts—how do you develop policy, how do you respond in terms of public health—these are all relatively difficult problems because they don't have as well defined a cause-and-effect mechanism."

As I was speaking with Rood, we were periodically interrupted by announcements from the Bella Center's loudspeaker. "No consensus was found. Therefore I suspend this agenda item," said a French-sounding woman, mustering her best English. But Rood articulated the three types of skepticism that are pervasive in the debate about the future of climate.

One type of skepticism flows from self-interest. In 2011 alone, the fossil fuel industry spent about $300 million on lobbying activities (roughly double what they'd spent just five years earlier).32, * Some climate scientists I later spoke with for this chapter used conspiratorial language to describe their activities. But there is no reason to allege a conspiracy when an explanation based on rational self-interest will suffice: these companies have a financial incentive to preserve their position in the status quo, and they are within their First Amendment rights to defend it. What they say should not be mistaken for an attempt to make accurate predictions, however.

A second type of skepticism falls into the category of contrarianism. In any contentious debate, some people will find it advantageous to align themselves with the crowd, while a smaller number will come to see themselves as persecuted outsiders. This may especially hold in a field like climate science, where the data is noisy and the predictions are hard to experience in a visceral way. And it may be especially common in the United States, which is admirably independent-minded. "If you look at climate, if you look at ozone, if you look at cigarette smoking, there is always a community of people who are skeptical of the science-driven results," Rood told me.

Most importantly, there is scientific skepticism. "You'll find that some in the scientific community have valid concerns about one aspect of the science or the other," Rood said. "At some level, if you really want to move forward, we need to respect some of their points of view."

A Forecaster's Critique of Global Warming Forecasts

In climate science, this healthy skepticism is generally directed at the reliability of computer models used to forecast the climate's course. Scott Armstrong, a professor at the Wharton School at the University of Pennsylvania, is such a skeptic. He is also among the small group of people who have devoted their lives to studying forecasting. His book, Principles of Forecasting, should be considered canonical to anybody who is seriously interested in the field. I met with Armstrong in his office at Huntsman Hall in Philadelphia. He is seventy-four years old but has a healthy goatee and looks perhaps fifteen years younger.

In 2007, Armstrong challenged Al Gore to a bet. Armstrong posited that what he calls his "no-change" forecast—global temperatures would remain at their 2007 levels—would beat the IPCC's forecast, which predicted continued warming. Gore never accepted the bet, but Armstrong proceeded to publish the results without him. The bet was to be resolved monthly—whichever forecast was closer to the actual temperatures for that month won the round. Through January 2012, Armstrong's no-change forecast had prevailed over the IPCC's forecast of slow-but-steady warming in twenty-nine months out of forty-seven.33

FIGURE 12-2: ARMSTRONG–GORE BET

Armstrong told me that he does not doubt the science behind the greenhouse effect per se. "I mean there has been a little bit of warming," he told me. "But nobody is arguing about that over the last 150 years."

But Armstrong does have some grievances with the majority view on global warming.* In 2007, at about the same time he proposed his bet to Gore, Armstrong and his colleague Kesten Green subjected global warming forecasts to what they called an "audit."34 The idea was to see how well global warming forecasts, especially those produced by the IPCC, abided by his forecasting principles.

The Armstrong and Green paper claimed to find the IPCC forecasts wanting; it suggested that they had failed to abide by seventy-two of eighty-nine forecasting principles. Eighty-nine forecasting principles35 is probably too many.36 Nevertheless, most of Armstrong's principles are good rules of thumb for forecasters, and when applied to global warming forecasts they can be simplified into what is essentially a three-pronged critique.

First, Armstrong and Green contend that agreement among forecasters is not related to accuracy—and may reflect bias as much as anything else. "You don't vote," Armstrong told me. "That's not the way science progresses."

Next, they say the complexity of the global warming problem makes forecasting a fool's errand. "There's been no case in history where we've had a complex thing with lots of variables and lots of uncertainty, where people have been able to make econometric models or any complex models work," Armstrong told me. "The more complex you make the model the worse the forecast gets."

Finally, Armstrong and Green write that the forecasts do not adequately account for the uncertainty intrinsic to the global warming problem. In other words, they are potentially overconfident.

Complexity, uncertainty, and the value (or lack thereof) of consensus views are core themes of this book. Each claim deserves a full hearing.

All the Climate Scientists Agree on Some of the Findings

There is an unhealthy obsession with the term consensus as it is applied to global warming. Some who dissent from what they see as the consensus view are proud to acknowledge it and label themselves as heretics.37 Others, however, have sought strength in numbers, sometimes resorting to dubious techniques like circulating online petitions in an effort to demonstrate how much doubt there is about the theory.* Meanwhile, whenever any climate scientist publicly disagrees with any finding about global warming, they may claim that this demonstrates a lack of consensus about the theory.

Many of these debates turn on a misunderstanding of the term. In formal usage, consensus is not synonymous with unanimity—nor with having achieved a simple majority. Instead, consensus connotes broad agreement after a process of deliberation, during which time most members of a group coalesce around a particular idea or alternative. (Such as in: "We reached a consensus to get Chinese food for lunch, but Horatio decided to get pizza instead.")

A consensus-driven process, in fact, often represents an alternative to voting. Sometimes when a political party is trying to pick a presidential nominee, one candidate will perform so strongly in early-voting states like Iowa and New Hampshire that all the others drop out. Even though the candidate is far from having clinched the nomination mathematically, there may be no need for the other states to hold a meaningful vote if the candidate has demonstrated that he is acceptable to most key coalitions within the party. Such a candidate can be described as having won the nomination by consensus.

Science, at least ideally, is exactly this sort of deliberative process. Articles are published and conferences are held. Hypotheses are tested, findings are argued over; some survive the scrutiny better than others.

The IPCC is potentially a very good example of a consensus process. Their reports take years to produce and every finding is subject to a thorough—if somewhat byzantine and bureaucratic—review process. "By convention, every review remark has to be addressed," Rood told me. "If your drunk cousin wants to make a remark, it will be addressed."

The extent to which a process like the IPCC's can be expected to produce better predictions is more debatable, however. There is almost certainly some value in the idea that different members of a group can learn from one another's expertise. But this introduces the possibility of groupthink and herding. Some members of a group may be more influential because of their charisma or status and not necessarily because they have the better idea. Empirical studies of consensus-driven predictions have found mixed results, in contrast to a process wherein individual members of a group submit independent forecasts and those are averaged or aggregated together, which can almost always be counted on to improve predictive accuracy.38

The IPCC process may reduce the independence of climate forecasters. Although there are nominally about twenty different climate models used in the IPCC's forecast, they make many of the same assumptions and use some of the same computer code; the degree of overlap is significant enough that they represent the equivalent of just five or six independent models.39 And however many models there are, the IPCC settles on just one forecast that is endorsed by the entire group.

Climate Scientists Are Skeptical About Computer Models

"It's critical to have a diversity of models," I was told by Kerry Emanuel, an MIT meteorologist who is one of the world's foremost theorists about hurricanes. "You do not want to put all your eggs in one basket."

One of the reasons this is so critical, Emanuel told me, is that in addition to the different assumptions these models employ, they also contain different bugs. "That's something nobody likes to talk about," he said. "Different models have different coding errors. You cannot assume that a model with millions and millions of lines of code, literally millions of instructions, that there isn't a mistake in there."

If you're used to thinking about the global warming debate as series of arguments between "skeptics" and "believers," you might presume that this argument emanates from a scientist on the skeptical side of the aisle. In fact, although Emanuel has described himself as conservative and Republican40—which is brave enough at MIT—he would probably not think of himself as a global warming skeptic. Instead, he is a member in good standing of the scientific establishment, having been elected to the National Academy of Sciences. His 2006 book41 presented a basically "consensus" (and extremely thoughtful and well-written) view on climate science.

Emanuel's concerns are actually quite common among the scientific community: climate scientists are in much broader agreement about some parts of the debate than others. A survey of climate scientists conducted in 200842 found that almost all (94 percent) were agreed that climate change is occurring now, and 84 percent were persuaded that it was the result of human activity. But there was much less agreement about the accuracy of climate computer models. The scientists held mixed views about the ability of these models to predict global temperatures, and generally skeptical ones about their capacity to model other potential effects of climate change. Just 19 percent, for instance, thought they did a good job of modeling what sea-rise levels will look like fifty years hence.

Results like these ought to be challenging to anyone who takes a caricatured view of climate science. They should cut against the notion that scientists are injudiciously applying models to make fantastical predictions about the climate; instead, the scientists have as much doubt about the models as many of their critics.43 However, cinematographic representations of climate change, like Al Gore's An Inconvenient Truth, have sometimes been less cautious, portraying a polar bear clinging to life in the Arctic, or South Florida and Lower Manhattan flooding over.44 Films like these are not necessarily a good representation of the scientific consensus. The issues that climate scientists actively debate are much more banal: for instance, how do we develop computer code to make a good representation of a cloud?

Climate Science and Complexity

Weather forecasters and climatologists often find themselves at odds;45 a large number of meteorologists are either implicitly or explicitly critical of climate science.

Weather forecasters have endured decades of struggle to improve their forecasts, and they can still expect to receive angry e-mails whenever they get one wrong. It is challenging enough to predict the weather twenty-four hours in advance. So how can climate forecasters, who are applying somewhat analogous techniques, expect to predict what the climate will look like decades from now?

Some of the distinction, as in the case of the term consensus, is semantic. Climate refers to the long-term equilibriums that the planet achieves; weather describes short-term deviations from it.46 Climate forecasters are not attempting to predict whether it will rain in Tulsa on November 22, 2062, although they are perhaps interested in whether it will be rainier on average throughout the Northern Hemisphere.

Meteorologists, nevertheless, have to wrestle with complexity:* the entire discipline of chaos theory developed out of what were essentially frustrated attempts to make weather forecasts. Climatologists have to deal with complexity as well: clouds, for instance, are small-scale phenomena that require a lot of computer power to model accurately, but they can have potentially profound effects on the feedback loops intrinsic to climate forecasts.47

The irony is that weather forecasting is one of the success stories in this book. Through hard work, and a melding of computer power with human judgment, weather forecasts have become much better than they were even a decade or two ago. Given that forecasters in most domains are prone to overconfidence, it is admirable that weather forecasters are hard on themselves and their forecasting peers. But the improvements they have made refute the idea that progress is hopeless in the face of complexity.

The improvements in weather forecasts are a result of two features of their discipline. First meteorologists get a lot of feedback—weather predictions play out daily, a reality check that helps keep them well-calibrated. This advantage is not available to climate forecasters and is one of the best reasons to be skeptical about their predictions, since they are made at scales that stretch out to as many as eighty or one hundred years in advance.

Meteorologists also benefit, however, from a strong understanding of the physics of the weather system, which is governed by relatively simple and easily observable laws. Climate forecasters potentially have the same advantage. We can observe clouds and we have a pretty good idea of how they behave; the challenge is more in translating that into mathematical terms.

One favorable example for climate forecasting comes from the success at forecasting the trajectories of some particularly big and important clouds—those that form hurricanes. Emanuel's office at MIT, designated as room 54-1814, is something of a challenge to find (I was assisted by an exceptional janitor who may as well have been the inspiration for Good Will Hunting). But it offers a clear view of the Charles River. It was easy to imagine a hurricane out in the distance: Would it careen toward Cambridge or blow out into the North Atlantic?

Emanuel articulated a distinction between two types of hurricane forecasts. One is purely statistical. "You have a long record of the phenomenon you're interested in. And you have a long record of what you consider to be viable predictors—like the wind in a large-scale flow of the atmosphere, or the temperature of the ocean, what have you," he said. "And without being particularly physical about it, you just use statistics to relate to what you're trying to predict to those predictors."

Imagine that a hurricane is sitting in the Gulf of Mexico. You could build a database of past hurricanes and look at their wind speed and their latitude and longitude and the ocean temperature and so forth, and identify those hurricanes that were most similar to this new storm. How had those other hurricanes behaved? What fraction struck populous areas like New Orleans and what fraction dissipated? You would not really need all that much meteorological knowledge to make such a forecast, just a good database.

Techniques like these can provide for crude but usable forecasts. In fact, up until about thirty years ago, purely statistical models were the primary way that the weather service forecasted hurricane trajectories.

Such techniques, however, are subject to diminishing returns. Hurricanes are not exactly rare, but severe storms hit the United States perhaps once every year on average. Whenever you have a large number of candidate variables applied to a rarely occurring phenomenon, there is the risk of overfitting your model and mistaking the noise in the past data for a signal.

There is an alternative, however, when you have some knowledge of the structure behind the system. This second type of model essentially creates a simulation of the physical mechanics of some portion of the universe. It takes much more work to build than a purely statistical method and requires a more solid understanding of the root causes of the phenomenon. But it is potentially more accurate. Models like these are now used to forecast hurricane tracks and they have been highly successful. As I reported in chapter 4, there has been roughly a threefold increase in the accuracy of hurricane track projections since the 1980s, and the location near New Orleans where Hurricane Katrina made landfall had been pinpointed well more than forty-eight hours in advance48 (though not everyone chose to listen to the forecast). Statistically driven systems are now used as little more than the baseline to measure these more accurate forecasts against.

Beyond a Cookbook Approach to Forecasting

The criticisms that Armstrong and Green make about climate forecasts derive from their empirical study of disciplines like economics in which there are few such physical models available49 and the causal relationships are poorly understood. Overly ambitious approaches toward forecasting have often failed in these fields, and so Armstrong and Green infer that they will fail in climate forecasting as well.

The goal of any predictive model is to capture as much signal as possible and as little noise as possible. Striking the right balance is not always so easy, and our ability to do so will be dictated by the strength of the theory and the quality and quantity of the data. In economic forecasting, the data is very poor and the theory is weak, hence Armstrong's argument that "the more complex you make the model the worse the forecast gets."

In climate forecasting, the situation is more equivocal: the theory about the greenhouse effect is strong, which supports more complicated models. However, temperature data is very noisy, which argues against them. Which consideration wins out? We can address this question empirically, by evaluating the success and failure of different predictive approaches in climate science. What matters most, as always, is how well the predictions do in the real world.

I would urge caution against reducing the forecasting process to a series of bumper-sticker slogans. Heuristics like Occam's razor ("other things being equal, a simpler explanation is better than a more complex one"50) sound sexy, but they are hard to apply. We have seen cases, as in the SIR models used to forecast disease outbreaks, where the assumptions of a model are simple and elegant—but where they are much too naïve to provide for very skillful forecasts. We have also seen cases, as in earthquake prediction, where unbelievably convoluted forecasting schemes that look great in the software package fail miserably in practice.

An admonition like "The more complex you make the model the worse the forecast gets" is equivalent to saying "Never add too much salt to the recipe." How much complexity—how much salt—did you begin with? If you want to get good at forecasting, you'll need to immerse yourself in the craft and trust your own taste buds.

Uncertainty in Climate Forecasts

Knowing the limitations of forecasting is half the battle, and on that score the climate forecasters do reasonably well. Climate scientists are keenly aware of uncertainty: variations on the term uncertain or uncertainty were used 159 times in just one of the three IPCC 1990 reports.51 And there is a whole nomenclature that the IPCC authors have developed to convey how much agreement or certainty there is about a finding. For instance, the phrase "likely" taken alone is meant to imply at least a 66 percent chance of a prediction occurring when it appears in an IPCC report, while the phrase "virtually certain" implies 99 percent confidence or more.52

Still, it is one thing to be alert to uncertainty and another to actually estimate it properly. When it comes to something like political polling, we can rely on a robust database of historical evidence: if a candidate is ten points ahead in the polls with a month to go until an election, how often will she wind up winning? We can look through dozens of past elections to get an empirical answer to that.

The models that climate forecasters build cannot rely on that sort of technique. There is only one planet and forecasts about how its climate will evolve are made at intervals that leap decades into the future. Although climatologists might think carefully about uncertainty, there is uncertainty about how much uncertainty there is. Problems like these are challenging for forecasters in any discipline.

Nevertheless, it is possible to analyze the uncertainty in climate forecasts as having three component parts. I met with Gavin Schmidt, a NASA colleague of Hansen's and a somewhat sarcastic Londoner who is a co-author of the blog RealClimate.org, at a pub near his office in Morningside Heights in New York.

Schmidt took out a cocktail napkin and drew a graph that looked something like what you see in figure 12-3, which illustrates the three distinct problems that climate scientists face. These different types of uncertainty become more or less prevalent over the course of a climate forecast.

FIGURE 12-3: SCHEMATIC OF UNCERTAINTY IN GLOBAL WARMING FORECASTS

First, there is what Schmidt calls initial condition uncertainty—the short-term factors that compete with the greenhouse signal and impact the way we experience the climate. The greenhouse effect is a long-term phenomenon, and it may be obscured by all types of events on a day-to-day or year-to-year basis.

The most obvious type of initial condition uncertainty is simply the weather; when it comes to forecasting the climate, it represents noise rather than signal. The current IPCC forecasts predict that temperatures might increase by 2°C (or about 4°F) over the course of the next century. That translates into an increase of just 0.2°C per decade, or 0.02°C per year. Such a signal is hard to perceive when temperatures can easily fluctuate by 15°C from day to night and perhaps 30°C from season to season in temperate latitudes.

In fact, just a few days before I met with Schmidt in 2011, there had been a freakish October snowstorm in New York and other parts of the Northeast. The snowfall, 1.3 inches in Central Park, set an October record there,53 and was more severe in Connecticut, New Jersey, and Massachusetts, leaving millions of residents without power.54

Central Park happens to have a particularly good temperature record;55 it dates back to 1869.56 In figure 12-4, I have plotted the monthly average temperature for Central Park in the century encompassing 1912 through 2011. You will observe the seasons in the graphic; the temperature fluctuates substantially (but predictably enough) from warm to cool and back again—a little more so in some years than others. In comparison to the weather, the climate signal is barely noticeable. But it does exist: temperatures have increased by perhaps 4°F on average over the course of this one-hundred-year period in Central Park.

FIGURE 12-4: CENTRAL PARK (NEW YORK CITY) MONTHLY AVERAGE TEMPERATURES, 1912–2011, IN °F

There are also periodic fluctuations that take hold at periods of a year to a decade at a time. One is dictated by what is called the ENSO cycle (the El Niño–Southern Oscillation). This cycle, which evolves over intervals of about three years at a time,57 is instigated by temperature shifts in the waters of the tropical Pacific. El Niño years, when the cycle is in full force, produce warmer weather in much of the Northern Hemisphere, and probably reduce hurricane activity in the Gulf of Mexico.58 La Niña years, when the Pacific is cool, do just the opposite. Beyond that, relatively little is understood about the ENSO cycle.

Another such medium-term process is the solar cycle. The sun gives off slightly more and slightly less radiation over cycles that last for about eleven years on average. (This is often measured through sunspots, the presence of which correlate with higher levels of solar activity.) But these cycles are somewhat irregular: Solar Cycle 24, for instance, which was expected to produce a maximum of solar activity (and therefore warmer temperatures) in 2012 or 2013, turned out to be somewhat delayed.59 Occasionally, in fact, the sun can remain dormant for decades at a time; the Maunder Minimum, a period of about seventy years during the late seventeenth and early eighteenth centuries when there was very little sunspot activity, may have triggered cooler temperatures in Europe and North America.60

Finally, there are periodic interruptions from volcanoes, which blast sulfur—a gas that has an anti-greenhouse effect and tends to cool the planet— into the atmosphere. The eruption of Mount Pinatubo in 1991 reduced global temperatures by about 0.2°C for a period of two years, equivalent to a decade's worth of greenhouse warming.

The longer your time horizon, the less concern you might have about these medium-term effects. They can dominate the greenhouse signal over periods of a year to a decade at a time, but they tend to even out at periods beyond that.

Another type of uncertainty, however—what Schmidt calls scenario uncertainty—increases with time. This concerns the level of CO2 and other greenhouse gases in the atmosphere. At near time horizons, atmospheric composition is quite predictable. The level of industrial activity is fairly constant, but CO2 circulates quickly into the atmosphere and remains there for a long time. (Its chemical half-life has been estimated at about thirty years.61) Even if major industrialized countries agreed to immediate and substantial reductions in CO2 emissions, it would take years to reduce the growth rate of CO2 in the atmosphere, let alone to actually reverse it. "Neither you nor I will ever see a year in which carbon dioxide concentrations have gone down, not ever," Schmidt told me. "And not your children either."

Still, since climate models rely on specific assumptions about the amount of atmospheric CO2, this can significantly complicate forecasts made for fifty or one hundred years out and affect them at the margin in the nearer term, depending on how political and economic decisions influence CO2 emissions.

Last, there is the structural uncertainty in the models. This is the type of uncertainty that both climate scientists and their critics are rightly most worried about, because it is the most challenging to quantify. It concerns how well we understand the dynamics of the climate system and how well we can represent them mathematically. Structural uncertainty might increase slightly over time, and errors can be self-reinforcing in a model of a dynamic system like the climate.

Taken together, Schmidt told me, these three types of uncertainty tend to be at a minimum at a period of about twenty or twenty-five years in advance of a climate forecast. This is close enough that we know with reasonable certainty how much CO2 there will be in the atmosphere—but far enough away that the effects of ENSO and volcanoes and the solar cycle should have evened out.

As it happens, the first IPCC report, published in 1990, falls right into this twenty-year sweet spot. So do some of the early forecasts made by James Hansen in the 1980s. It is time, in other words, to assess the accuracy of the forecasts. So how well did they do?

A Note on the Temperature Record

To measure the accuracy of a prediction, you first need a measuring stick—and climate scientists have quite a few choices. There are four major organizations that build estimates of global temperatures from thermometer readings at land and sea stations around the globe. These organizations include NASA62 (which maintains its GISS63 temperature record), NOAA64 (the National Oceanic and Atmospheric Administration, which manages the National Weather Service), and the meteorological offices of the United Kingdom65 and Japan.66

A more recent entrant into the temperature sweepstakes are observations from satellites. The most commonly used satellite records are from the University of Alabama at Huntsville and from a private company called Remote Sensing Systems.67 The satellites these records rely on do not take the temperature directly—instead, they infer it by measuring microwave radiation. But the satellites' estimates of temperatures in the lower atmosphere68 provide a reasonably good proxy for surface temperatures.69

FIGURE 12-5: GLOBAL TEMPERATURE ANOMALY RELATIVE TO 1951–80 BASELINE: SIX TEMPERATURE RECORDS

The temperature records also differ in how far they track the climate backward; the oldest are the observations from the UK's Met Office, which date back to 1850; the satellite records are the youngest and date from 1979. And the records are measured relative to different baselines—the NASA/GISS record is taken relative to average temperatures from 1951 through 1980, for instance, while NOAA's temperatures are measured relative to the average throughout the twentieth century. But this is easy to correct for,70 and the goal of each system is to measure how much temperatures are rising or falling rather than what they are in any absolute sense.

Reassuringly, the differences between the various records are fairly modest71 (figure 12-5). All six show both 1998 and 2010 as having been among the three warmest years on record, and all six show a clear long-term warming trend, especially since the 1950s when atmospheric CO2 concentrations began to increase at a faster rate. For purposes of evaluating the climate forecasts, I've simply averaged the six temperate records together.

James Hansen's Predictions

One of the more forthright early efforts to forecast temperature rise came in 1981, when Hansen and six other scientists published a paper in the esteemed journal Science.72 These predictions, which were based on relatively simple statistical estimates of the effects of CO2 and other atmospheric gases rather than a fully fledged simulation model, have done quite well. In fact, they very slightly underestimated the amount of global warming observed through 2011.73

Hansen is better known, however, for his 1988 congressional testimony as well as a related 1988 paper74 that he published in the Journal of Geophysical Research. This set of predictions did rely on a three-dimensional physical model of the atmosphere.

Hansen told Congress that Washington could expect to experience more frequent "hot summers." In his paper, he defined a hot summer as one in which average temperatures in Washington were in the top one-third of the summers observed from 1950 through 1980. He said that by the 1990s, Washington could expect to experience these summers 55 to 70 percent of the time, or roughly twice their 33 percent baseline rate.

In fact, Hansen's prediction proved to be highly prescient for Washington, DC. In the 1990s, six of the ten summers75 qualified as hot (figure 12-6), right in line with his prediction. About the same rate persisted in the 2000s and Washington experienced a record heat wave in 2012.

In his paper, Hansen had also made these predictions for three other cities: Omaha, Memphis, and New York. These results were more mixed and go to illustrate the regional variability of the climate. Just 1 out of 10 summers in Omaha in the 1990s qualified as "hot" by Hansen's standard, well below the historical average rate of 33 percent. But 8 out of 10 summers in New York did, according to observations at LaGuardia Airport.

Overall, the predictions for the four cities were reasonably good, but were toward the lower end of Hansen's range. His global temperature predictions are harder to evaluate because they articulated a plethora of scenarios that relied on different assumptions, but they were also somewhat too high.76 Even the most conservative scenario somewhat overestimated the warming experienced through 2011.

The IPCC's 1990 Predictions

The IPCC's 1990 forecasts represented the first true effort at international consensus predictions in the field and therefore received an especially large amount of attention. These predictions were less specific than Hansen's, although when they did go into detail they tended to get things mostly right. For instance, they predicted that land surfaces would warm more quickly than water surfaces, especially in the winter, and that there would be an especially substantial increase in temperature in the Arctic and other northerly latitudes. Both of these predictions have turned out to be correct.

The headline forecast, however, was that of the global temperature rise. Here, the IPCC's prediction left more to be desired.

The IPCC's temperature forecast, unlike Hansen's, took the form of a range of possible outcomes. At the high end of the range was a catastrophic temperature increase of 5°C over the course of the next one hundred years. At the low end was a more modest increase of 2°C per century, with a 3°C increase representing the most likely case.77

In fact, the actual temperature increase has been on a slower pace since the report was published (figure 12-7). Temperatures increased by an average of 0.015°C per year from the time the IPCC forecast was issued in 1990 through 2011, or at a rate of 1.5°C per century. This is about half the IPCC's most likely case, of 3°C warming per century, and also slightly less than the low end of their range at 2°C. The IPCC's 1990 forecast also overestimated the amount of sea-level rise.78

This represents a strike against the IPCC's forecasts, although we should consider one important qualification.

The IPCC forecasts were predicated on a "business-as-usual" case that assumed that there would be no success at all in mitigating carbon emissions.79 This scenario implied that the amount of atmospheric CO2 would increase to about four hundred parts per million (ppm) by 2010.80 In fact, some limited efforts to reduce carbon emissions were made, especially in the European Union,81 and this projection was somewhat too pessimistic; CO2 levels had risen to about 390 ppm as of 2010.82 In other words, the error in the forecast in part reflected scenario uncertainty—which turns more on political and economic questions than on scientific ones—and the IPCC's deliberately pessimistic assumptions about carbon mitigation efforts.*

Nevertheless, the IPCC later acknowledged their predictions had been too aggressive. When they issued their next forecast, in 1995, the range attached to their business-as-usual case had been revised considerably lower: warming at a rate of about 1.8°C per century.83 This version of the forecasts has done quite well relative to the actual temperature trend.84 Still, that represents a fairly dramatic shift. It is right to correct a forecast when you think it might be wrong rather than persist in a quixotic fight to the death for it. But this is evidence of the uncertainties inherent in predicting the climate.

The score you assign to these early forecasting efforts overall might depend on whether you are grading on a curve. The IPCC's forecast miss in 1990 is partly explained by scenario uncertainty. But this defense would be more persuasive if the IPCC had not substantially changed its forecast just five years later. On the other hand, their 1995 temperature forecasts have gotten things about right, and the relatively few specific predictions they made beyond global temperature rise (such as ice shrinkage in the Arctic85) have done quite well. If you hold forecasters to a high standard, the IPCC might deserve a low but not failing grade. If instead you have come to understand that the history of prediction is fraught with failure, they look more decent by comparison.

Uncertainty in forecasts is not necessarily a reason not to act—the Yale economist William Nordhaus has argued instead that it is precisely the uncertainty in climate forecasts that compels action,86 since the high-warming scenarios could be quite bad. Meanwhile, our government spends hundreds of billions toward economic stimulus programs, or initiates wars in the Middle East, under the pretense of what are probably far more speculative forecasts than are pertinent in climate science.87

The Lessons of "Global Cooling"

Still, climate scientists put their credibility on the line every time they make a prediction. And in contrast to other fields in which poor predictions are quickly forgotten about, errors in forecasts about the climate are remembered for decades.

One common claim among climate critics is that there once had been predictions of global cooling and possibly a new ice age. Indeed, there were a few published articles that projected a cooling trend in the 1970s. They rested on a reasonable-enough theory: that the cooling trend produced by sulfur emissions would outweigh the warming trend produced by carbon emissions.

These predictions were refuted in the majority of the scientific literature.88 This was less true in the news media. A Newsweek story in 1975 imagined that the River Thames and the Hudson River might freeze over and stated that there would be a "drastic decline" in food production89—implications drawn by the writer of the piece but not any of the scientists he spoke with.

If the media can draw false equivalences between "skeptics" and "believers" in the climate science debate, it can also sometimes cherry-pick the most outlandish climate change claims even when they have been repudiated by the bulk of a scientist's peers.

"The thing is, many people are going around talking as if they looked at the data. I guarantee that nobody ever has," Schmidt told me after New York's October 2011 snowstorm, which various media outlets portrayed as evidence either for or against global warming.

Schmidt received numerous calls from reporters asking him what October blizzards in New York implied about global warming. He told them he wasn't sure; the models didn't go into that kind of detail. But some of his colleagues were less cautious, and the more dramatic their claims, the more likely they were to be quoted in the newspaper.

The question of sulfur emissions, the basis for those global cooling forecasts in the 1970s, may help to explain why the IPCC's 1990 forecast went awry and why the panel substantially lowered their range of temperature predictions in 1995. The Mount Pinatubo eruption in 1991 burped sulfur into the atmosphere, and its effects were consistent with climate models.90 But it nevertheless underscored that the interactions between different greenhouse gases can be challenging to model and can introduce error into the system.

Sulfur emissions from manmade sources peaked in the early 1970s before declining91 (figure 12-8), partly because of policy like the Clean Air Act signed into law by President Nixon in 1970 to combat acid rain and air pollution. Some of the warming trend during the 1980s and 1990s probably reflected this decrease in sulfur, since SO2 emissions counteract the greenhouse effect.

Since about 2000, however, sulfur emissions have increased again, largely as the result of increased industrial activity in China,92 which has little environmental regulation and a lot of dirty coal-fired power plants. Although the negative contribution of sulfur emissions on global warming is not as strong as the positive contribution from carbon—otherwise those global cooling theories might have proved to be true!—this may have provided for something of a brake on warming.

A Simple Climate Forecast

So suppose that you have good reason to be skeptical of a forecast—for instance, because it purports to make fairly precise predictions about a very complex process like the climate, or because it would take years to verify the forecast's accuracy.

Sophomoric forecasters sometimes make the mistake of assuming that just because something is hard to model they may as well ignore it. Good forecasters always have a backup plan—a reasonable baseline case that they can default to if they have reason to worry their model is failing. (In a presidential election, your default prediction might be that the incumbent will win—that will do quite a bit better than just picking between the candidates at random.)

What is the baseline in the case of the climate? If the critique of global warming forecasts is that they are unrealistically complex, the alternative would be a simpler forecast, one grounded in strong theoretical assumptions but with fewer bells and whistles.

Suppose, for instance, that you had attempted to make a climate forecast based on an extremely simple statistical model: one that looked solely at CO2 levels and temperatures, and extrapolated a prediction from these variables alone, ignoring sulfur and ENSO and sunspots and everything else. This wouldn't require a supercomputer; it could be calculated in a few microseconds on a laptop. How accurate would such a prediction have been?

In fact, it would have been very accurate—quite a bit better, actually, than the IPCC's forecast. If you had placed the temperature record from 1850 through 1989 into a simple linear regression equation, along with the level of CO2 as measured in Antarctic ice cores93 and at the Mauna Loa Observatory in Hawaii, it would have predicted a global temperature increase at the rate of 1.5°C per century from 1990 through today, exactly in line with the actual figure (figure 12-9).

Another technique, only slightly more complicated, would be to use estimates that were widely available at the time about the overall relationship between CO2 and temperatures. The common currency of any global warming forecast is a value that represents the effect on temperatures from a doubling (that is, a 100 percent increase) in atmospheric CO2. There has long been some agreement about this doubling value.94 From forecasts like those made by the British engineer G. S. Callendar in 193895 that relied on simple chemical equations, to those produced by today's supercomputers, estimates have congregated96 between 2°C and 3°C of warming from a doubling of CO2.

Given the actual rate of increase in atmospheric CO2, that simple conversion would have implied temperature rise at a rate of between 1.1°C and 1.7°C per century from 1990 through the present day. The actual warming pace of 0.015°C per year or 1.5°C per century fits snugly within that interval.

James Hansen's 1981 forecasts, which relied on an approach much like this, did quite a bit better at predicting current tempertaures than his 1988 forecast, which relied on simulated models of the climate.

The Armstrong and Green critique of model complexity thus looks pretty good here. But the success of the more basic forecasting methods suggests that Armstrong's critique may have won the battle but not the war. He is asking some good questions about model complexity, and the fact that the simple models do pretty well in predicting the climate is one piece of evidence in favor of his position that simpler models are preferable. However, since the simple methods correctly predicted a temperature increase in line with the rise in CO2, they are also evidence in favor of the greenhouse-effect hypothesis.

Armstrong's no-change forecast, by contrast, leaves some of the most basic scientific questions unanswered. The forecast used 2007 temperatures as its baseline, a year that was not exceptionally warm but which was nevertheless warmer than all but one year in the twentieth century. Is there a plausible hypothesis that explains why 2007 was warmer than 1987 or 1947 or 1907—other than through changes in atmospheric composition? One of the most tangible contributions of climate models, in fact, is that they find it impossible to replicate the current climate unless they account for the increased atmospheric concentration of CO2 and other greenhouse gases.97

Armstrong told me he made the no-change forecast because he did not think there were good Bayesian priors for any alternative assumption; the no-change forecast, he has found, has been a good default in the other areas that he has studied. This would be a more persuasive case if he had applied the same rigor to climate forecasting that he had to other areas he has studied. Instead, as Armstrong told a congressional panel in 2011,98 "I actually try not to learn a lot about climate change. I am the forecasting guy."

This book advises you to be wary of forecasters who say that the science is not very important to their jobs, or scientists who say that forecasting is not very important to their jobs! These activities are essentially and intimately related. A forecaster who says he doesn't care about the science is like the cook who says he doesn't care about food. What distinguishes science, and what makes a forecast scientific, is that it is concerned with the objective world. What makes forecasts fail is when our concern only extends as far as the method, maxim, or model.

An Inconvenient Truth About the Temperature Record

But if Armstrong's critique is so off the mark, what should we make of his proposed bet with Gore? It has not been a failed forecast at all; on the contrary, it has gone quite successfully. Since Armstrong made the bet in 2007, temperatures have varied considerably from month to month but not in any consistent pattern; 2011 was a slightly cooler year than 2007, for instance.

And this has been true for longer than four years: one inconvenient truth is that global temperatures did not increase at all in the decade between 2001 and 2011 (figure 12-10). In fact they declined, although imperceptibly.99

This type of framing can sometimes be made in bad faith. For instance, if you set the year 1998 as your starting point, which had record-high temperatures associated with the ENSO cycle, it will be easier to identify a cooling "trend." Conversely, the decadal "trend" from 2008 through 2018 will very probably be toward warming once it is calculated, since 2008 was a relatively cool year. Statistics of this sort are akin to when the stadium scoreboard optimistically mentions that the shortstop has eight hits in his last nineteen at-bats against left-handed relief pitchers—ignoring the fact that he is batting .190 for the season.100

Yet global warming does not progress at a steady pace. Instead, the history of temperature rise is one of a clear long-term increase punctuated by periods of sideways or even negative trends. In addition to the decade between 2001 and 2011, for instance, there would have been little sign of warming between 1894 and 1913, or 1937 and 1956, or 1966 and 1977 (figure 12-11)—even though CO2 concentrations were increasing all the while. This problem bears some resemblance to that faced by financial analysts: over the very long run, the stock market essentially always moves upward. But this tells you almost nothing about how it will behave in the next day, week, or year.

It might be possible to explain some of the recent sideways trend directly from the science; increased sulfur emissions in China might have played some role, for instance. And it might be remembered that although temperatures did not rise from 2001 through 2011, they were still much warmer than in any prior decade.

Nevertheless, this book encourages readers to think carefully about the signal and the noise and to seek out forecasts that couch their predictions in percentage or probabilistic terms. They are a more honest representation of the limits of our predictive abilities. When a prediction about a complex phenomenon is expressed with a great deal of confidence, it may be a sign that the forecaster has not thought through the problem carefully, has overfit his statistical model, or is more interested in making a name for himself than in getting at the truth.

Neither Armstrong nor Schmidt was willing to hedge very much on their predictions about the temperature trend. "We did some simulations from 1850 up to 2007," Armstrong told me. "When we looked one hundred years ahead it was virtually certain that I would win that bet."101 Schmidt, meanwhile, was willing to offer attractive odds to anyone betting against his position that temperatures would continue to increase. "I could easily give you odds on the next decade being warmer than this decade," he told me. "You want 100-to-1 odds, I'd give it to you."

The statistical forecasting methods that I outlined earlier can be used to resolve the dispute—and they suggest that neither Armstrong nor Schmidt has it quite right. If you measure the temperature trend one decade at a time, it registers a warming trend about 75 percent of the time since 1900, but a cooling trend the other 25 percent of the time. As the growth rate of atmospheric CO2 increases, creating a stronger greenhouse signal, periods of flat or cooling temperatures should become less frequent. Nevertheless, they are not impossible, nor are the odds anything like 100-to-1 against them. Instead, if you assume that CO2 levels will increase at the current pace of about 2 ppm per year, the chance that there would be no net warming over the course of a given decade would be about 15 percent102 according to this method.

Yet Another Reason Why Estimating Uncertainty Is Essential

Uncertainty is an essential and nonnegotiable part of a forecast. As we have found, sometimes an honest and accurate expression of the uncertainty is what has the potential to save property and lives. In other cases, as when trading stock options or wagering on an NBA team, you may be able to place bets on your ability to forecast the uncertainty accurately.

However, there is another reason to quantify the uncertainty carefully and explicitly. It is essential to scientific progress, especially under Bayes's theorem.

Suppose that in 2001, you had started out with a strong prior belief in the hypothesis that industrial carbon emissions would continue to cause a temperature rise. (In my view, such a belief would have been appropriate because of our strong causal understanding of the greenhouse effect and the empirical evidence for it up to that point.) Say you had attributed the chance of the global warming hypothesis's being true at 95 percent.

But then you observe some new evidence: over the next decade, from 2001 through 2011, global temperatures do not rise. In fact, they fall, although very slightly. Under Bayes's theorem, you should revise your estimate of the probability of the global warming hypothesis downward; the question is by how much.

If you had come to a proper estimate of the uncertainty in near-term temperature patterns, the downward revision would not be terribly steep. As we found, there is about a 15 percent chance that there will be no net warming over a decade even if the global warming hypothesis is true because of the variability in the climate. Conversely, if temperature changes are purely random and unpredictable, the chance of a cooling decade would be 50 percent since an increase and a decrease in temperatures are equally likely. Under Bayes's theorem (figure 12-12), a no-net-warming decade would cause you to revise downward your estimate of the global warming hypothesis's likelihood to 85 percent from 95 percent.

On the other hand, if you had asserted that there was just a 1 percent chance that temperatures would fail to increase over the decade, your theory is now in much worse shape because you are claiming that this was a more definitive test. Under Bayes's theorem, the probability you would attach to the global warming hypothesis has now dropped to just 28 percent.

When we advance more confident claims and they fail to come to fruition, this constitutes much more powerful evidence against our hypothesis. We can't really blame anyone for losing faith in our forecasts when this occurs; they are making the correct inference under Bayesian logic.

So what is the incentive to make more confident claims to begin with, especially when they are not really justified by the statistical evidence? There are all sorts of reasons that people may do this in practice. In the climate debate, it may be because these more confident claims can seem more persuasive—and they may be, but only if they are right. Attributing every weather anomaly to manmade climate change—other than the higher temperatures the global warming phenomenon is named for—is a high-stakes gamble, rooted more in politics than in science. There is little consensus about the ways that climate change might manifest itself other than through temperature increases and probably rising sea levels. Obviously, charging that every snowfall is evidence against the theory is just as ridiculous.

"We're in a Street Fight with These People"

The fundamental dilemma faced by climatologists is that global warming is a long-term problem that might require a near-term solution. Because carbon dioxide remains in the atmosphere for so long, decisions that we make about it today will affect the lives of future generations.

In a perfectly rational and benevolent world, this might not be so worrying. But our political and cultural institutions are not so well-devised to handle these problems—not when the United States Congress faces reelection every two years and when businesses are under pressure to meet earnings forecasts every quarter. Climate scientists have reacted to this challenge in a variety of ways, some involving themselves more in the political debate and others keeping it at arm's length.

Michael Mann, who is director of the Earth System Science Center at Penn State University, was once at the center of a controversy. "Climategate" concerned the hacking of a server at the Climatic Research Unit (CRU) at the University of East Anglia,103 which produces the temperature record that the UK's Met Office uses. Skeptics alleged that Mann and other scientists had conspired to manipulate the CRU's temperature record.

The pertinent facts are that the scientists were cleared of wrongdoing by a panel of their peers,104 and that the CRU's temperature record is quite consistent with the others105—but Mann and other scientists in the hacked e-mails demonstrated a clear concern with the public relations elements of how the science would be perceived. Mann is happy to tell you as much. I met with him on a crisp fall afternoon at his office at Penn State, where we chatted for about two hours.

Mann is exceptionally thoughtful about the science behind global warming. Like most other climatologists, he has little doubt about the theoretical mechanisms behind climate change, but he takes a skeptical view toward the predictions rendered by climate models.

"Any honest assessment of the science is going to recognize that there are things we understand pretty darn well and things that we sort of know," he told me. "But there are things that are uncertain and there are things we just have no idea about whatsoever."

"In my mind, one of the unfortunate consequences of this bad-faith public conversation we've been having is that we're wasting our time debating a proposition that is very much accepted within the scientific community, when we could be having a good-faith discussion about the uncertainties that do exist."

But Mann, who blogs along with Schmidt at RealClimate.org, sees himself as engaged in trench warfare against groups like the Heartland Institute. "We're in a street fight with these people," he told me, referring to a Nature editorial106 that employed the phrase. The long-term goal of the street fight is to persuade the public and policy makers about the urgency (or lack thereof) of action to combat climate change. In a society accustomed to overconfident forecasters who mistake the confidence they express in a forecast for its veracity, expressions of uncertainty are not seen as a winning strategy by either side.

"Where you have to draw the line is to be very clear about where the uncertainties are, but to not have our statements be so laden in uncertainty that no one even listens to what we're saying," Mann told me. "It would be irresponsible for us as a community to not be speaking out. There are others who are happy to fill the void. And they're going to fill the void with disinformation."

The Difference Between Science and Politics

In practice, Mann's street fight is between "consensus" Web sites like RealClimate.org and "skeptical" ones like Watts Up With That,107 and revolves around day-to-day scuffles about the latest journal article or weather pattern or political controversy. Both sides almost invariably stick up for others in their circle and refuse to yield ground. When you're a Jet, you're a Jet all the way.

I do not mean to suggest that the territory occupied by the two sides is symmetrical. In the scientific argument over global warming, the truth seems to be mostly on one side: the greenhouse effect almost certainly exists and will be exacerbated by manmade CO2 emissions. This is very likely to make the planet warmer. The impacts of this are uncertain, but are weighted toward unfavorable outcomes.108

The street-fighter mentality, nevertheless, seems to be predicated on the notion that we are just on the verge of resolving our political problems, if only a few more people could be persuaded about the science. In fact, we are probably many years away. "There's a point when I come to the conclusion that we're going to have to figure out how to take the carbon out," Richard Rood told me in Copenhagen, anticipating that there was almost no way the 193 members of the United Nations would agree to mutually acceptable terms.

Meanwhile, the American public's confidence that global warming is occurring has decreased somewhat over the past several years.109 And even if there were 100 percent agreement on the effects of climate change, some states and some countries would make out better than others in any plan to mitigate carbon emissions. "We have some very progressive Democratic governors in coal states," I was told by the governor of Washington, Christine Gregoire. "Boy, are they nervous about all this."

I don't know how to resolve these problems, which are not unique to the climate debate.110 What I do know is that there is a fundamental difference between science and politics. In fact, I've come to view them more and more as opposites.

In science, progress is possible. In fact, if one believes in Bayes's theorem, scientific progress is inevitable as predictions are made and as beliefs are tested and refined.* The march toward scientific progress is not always straightforward, and some well-regarded (even "consensus") theories are later proved wrong—but either way science tends to move toward the truth.

In politics, by contrast, we seem to be growing ever further away from consensus. The amount of polarization between the two parties in the United States House, which had narrowed from the New Deal through the 1970s, had grown by 2011 to be the worst that it had been in at least a century.111 Republicans have moved especially far away from the center,112 although Democrats have to some extent too.

In science, one rarely sees all the data point toward one precise conclusion. Real data is noisy—even if the theory is perfect, the strength of the signal will vary. And under Bayes's theorem, no theory is perfect. Rather, it is a work in progress, always subject to further refinement and testing. This is what scientific skepticism is all about.

In politics, one is expected to give no quarter to his opponents. It is seen as a gaffe when one says something inconvenient—and true.113 Partisans are expected to show equal conviction about a set of beliefs on a range of economic, social, and foreign policy issues that have little intrinsic relation to one another. As far as approximations of the world go, the platforms of the Democratic and Republican parties are about as crude as it gets.

It is precisely because the debate may continue for decades that climate scientists might do better to withdraw from the street fight and avoid crossing the Rubicon from science into politics. In science, dubious forecasts are more likely to be exposed—and the truth is more likely to prevail. In politics, a domain in which the truth enjoys no privileged status, it's anybody's guess.

The dysfunctional state of the American political system is the best reason to be pessimistic about our country's future. Our scientific and technological prowess is the best reason to be optimistic. We are an inventive people. The United States produces ridiculous numbers of patents,114 has many of the world's best universities and research institutions, and our companies lead the market in fields ranging from pharmaceuticals to information technology. If I had a choice between a tournament of ideas and a political cage match, I know which fight I'd rather be engaging in—especially if I thought I had the right forecast.