Leopold Aschenbrenner.(2024).2024109Situational-Awareness-The-Decade-Ahead.Dwarkesh podcast => IV. The Project

## IV. The Project

[IV. The Project - SITUATIONAL AWARENESS](https://situational-awareness.ai/the-project/)

As the race to AGI intensifies, the national security state will get involved. The USG will wake from its slumber, and by 27/28 we'll get some form of government AGI project. No startup can handle superintelligence. Somewhere in a SCIF, the endgame will be on.

In this piece:

The path to The Project

Why The Project is the only way

Superintelligence will be the United States' most important national defense project

A sane chain of command for superintelligence

The civilian uses of superintelligence

Security

Safety

Stabilizing the international situation

The Project is inevitable; whether it's good is not

The endgame

"We must be curious to learn how such a set of objects—hundreds of power plants, thousands of bombs, tens of thousands of people massed in national establishments—can be traced back to a few people sitting at laboratory benches discussing the peculiar behavior of one type of atom."

Spencer R. Weart

Many plans for "AI governance" are put forth these days, from licensing frontier AI systems to safety standards to a public cloud with a few hundred million in compute for academics. These seem well-intentioned—but to me, it seems like they are making a category error.

I find it an insane proposition that the US government will let a random SF startup develop superintelligence. Imagine if we had developed atomic bombs by letting Uber just improvise.

Superintelligence—AI systems much smarter than humans—will have vast power, from developing novel weaponry to driving an explosion in economic growth. Superintelligence will be the locus of international competition; a lead of months potentially decisive in military conflict.

It is a delusion of those who have unconsciously internalized our brief respite from history that this will not summon more primordial forces. Like many scientists before us, the great minds of San Francisco hope that they can control the destiny of the demon they are birthing. Right now, they still can; for they are among the few with situational awareness, who understand what they are building. But in the next few years, the world will wake up. So too will the national security state. History will make a triumphant return.

As in many times before—Covid, WWII—it will seem as though the United States is asleep at the wheel—before, all at once, the government shifts into gear in the most extraordinary fashion. There will be a moment—in just a few years, just a couple more "2023-level" leaps in model capabilities and AI discourse—where it will be clear: we are on the cusp of AGI, and superintelligence shortly thereafter. While there's a lot of flux within the exact mechanics, one way or another, the USG will be at the helm; the leading labs will ("voluntarily") merge; Congress will appropriate trillions for chips and power; a coalition of democracies formed.

Startups are great for many things—but a startup on its own is simply not equipped for being in charge of the United States' most important national defense project. We will need government involvement to have even a hope of defending against the all-out espionage threat we will face; the private AI efforts might as well be directly delivering superintelligence to the CCP. We will need the government to ensure even a semblance of a sane chain of command; you can't have random CEOs (or random nonprofit boards) with the nuclear button. We will need the government to manage the severe safety challenges of superintelligence, to manage the fog of war of the intelligence explosion. We will need the government to deploy superintelligence to defend against whatever extreme threats unfold, to make it through the extraordinarily volatile and destabilized international situation that will follow. We will need the government to mobilize a democratic coalition to win the race with authoritarian powers, and forge (and enforce) a nonproliferation regime for the rest of the world. I wish it weren't this way—but we will need the government. (Yes, regardless of the Administration.)

In any case, my main claim is not normative, but descriptive. In a few years, The Project will be on.

国家项目

随着通用人工智能（AGI）竞赛的白热化，国家安全部门将不得不介入。美国政府将从沉睡中觉醒，到 2027/2028 年，我们将看到某种形式的政府 AGI 项目诞生。没有任何初创公司能够掌控超级智能（Superintelligence）。在某个绝密设施中，人类命运的终局将徐徐展开。

本文讨论：

通往这一国家项目的道路

为什么这一国家项目是唯一可行的方案

超级智能将成为美国最重要的国防项目

为超级智能构建合理的指挥体系

超级智能在民用领域的应用

安保工作

安全保障

稳定国际形势

这一国家项目必将到来，但其利弊尚难预料

最终命运

令人惊叹的是，几个坐在实验室工作台前、仅仅在讨论某种原子特殊性质的科学家，却引发了如此巨大的连锁反应 —— 数百座发电站、数千枚炸弹、数万名聚集在国家机构中的工作人员。我们必须带着好奇心去探究这其中的奥秘。

—— Spencer R. Weart

当下有许多关于「AI 治理」的提案，包括对前沿 AI 系统进行许可管理、制定安全标准、为学术界提供价值数亿美元的公共计算资源等。这些提案看似用心良苦 —— 但在我看来，它们都犯了一个根本性的认知错误。

让一家随机的旧金山初创公司来开发超级智能，这种想法简直是疯狂的。这就好比让 Uber 随意发挥来研制原子弹一样荒谬。

这让我们想起历史上的许多时刻 —— 新冠疫情、第二次世界大战 —— 美国似乎总是一开始毫无作为，但随后却能以惊人的速度和规模迅速转向。在未来几年内，当 AI 技术再经历几次类似 2023 年那样的重大突破，相关讨论也随之深入后，一个关键时刻就会到来：我们将清晰地认识到，我们即将实现通用人工智能，紧接着就是超级智能的出现。尽管具体实施方式可能有所不同，但最终的走向已经确定：美国政府将接管局面；各大领先的 AI 实验室将「不得不」合并；国会将投入数万亿美元用于芯片研发和能源保障；一个由民主国家组成的联盟将应运而生。

诚然，初创公司在很多领域都有出色表现 —— 但说到管理美国最重要的国防项目，单靠一家初创公司是远远不够的。我们需要政府的参与才能应对即将面临的全方位情报安全威胁；否则，私人开发的 AI 项目可能会让超级智能直接落入中共手中。我们需要政府来建立一个可靠的指挥系统；毕竟，如此重大的决策权不能交给某个公司 CEO（或某个非营利组织的董事会）。我们需要政府来应对超级智能带来的重大安全挑战，管理智能快速发展过程中的种种不确定性。我们需要政府部署超级智能来防范各种极端威胁，帮助我们度过必将到来的国际动荡期。我们需要政府来团结民主国家联盟，在与专制政权的竞争中取得胜利，同时为全球其他国家制定并实施技术管控机制。我本不愿如此 —— 但政府的参与确实不可或缺。（是的，这与具体是哪届政府无关。）

总之，我的核心观点与其说是在规劝什么「应该」发生，不如说是在陈述什么「必将」发生。在几年之内，这个国家级项目必将启动。

### The path to The Project

A turn-of-events seared into my memory is late February to mid-March of 2020. In those last weeks of February and early days of March, I was in utter despair: it seemed clear that we were on the covid-exponential: a plague was about to sweep the country, the collapse of our hospitals was imminent—and yet almost nobody took it seriously. The Mayor of New York was still dismissing Covid-fears as racism and encouraging people to go to Broadway shows. All I could do was buy masks and short the market.

And yet within just a few weeks, the entire country shut down and Congress had appropriated trillions of dollars (literally >10% of GDP). Seeing where the exponential might go ahead of time was too hard, but when the threat got close enough, existential enough, extraordinary forces were unleashed. The response was late, crude, blunt—but it came, and it was dramatic.

The next few years in AI will feel similar. We're in the midgame now. 2023 was already a wild shift. AGI went from a fringe topic you'd be hesitant to associate with, to the subject of major Senate hearings and summits of world leaders. Given how early we are still, the level of USG engagement has been impressive to me. A couple more "2023"s, and the Overton window will be blown completely open.

As we race through the OOMs, the leaps will continue. By 2025/2026 or so I expect the next truly shocking step-changes; AI will drive $100B+ annual revenues for big tech companies and outcompete PhDs in raw problem-solving smarts. Much as the Covid stock-market collapse made many take covid seriously, we'll have $10T companies and the AI mania will be everywhere. If that's not enough, by 2027/28, we'll have models trained on the $100B+ cluster; full-fledged AI agents/drop-in remote workers will start to widely automate software engineering and other cognitive jobs. Each year, the acceleration will feel dizzying.

While many don't yet see the possibility of AGI, eventually a consensus will form. Some, like Szilard, saw the possibility of an atomic bomb much earlier than others. Their alarm was not well-received initially; the possibility of a bomb was dismissed as remote (or at least, it was felt that the conservative and proper thing was to play down the possibility). Szilard's fervent secrecy appeals were mocked and ignored. But many scientists, initially skeptical, started realizing a bomb was possible as more and more empirical results came in. Once a majority of scientists came to believe we were on the cusp of a bomb, the government, in turn, saw the national security exigency as too great—and the Manhattan Project got underway.

As the OOMs go from theoretical extrapolation to (extraordinary) empirical reality, gradually, a consensus will form, too, among the leading scientists and executives and government officials: we are on the cusp, on the cusp of AGI, on the cusp of an intelligence explosion, on the cusp of superintelligence. And somewhere along here, we'll get the first genuinely terrifying demonstrations of AI: perhaps the oft-discussed "helping novices make bioweapons," or autonomously hacking critical systems, or something else entirely. It will become clear: like it or not, this technology will be an utterly decisive military technology. Even if we're lucky enough to not be in a major war, it seems likely that the CCP will have taken notice and launched a formidable AGI effort. Perhaps the eventual (inevitable) discovery of the CCP's infiltration of America's leading AI labs will cause a big stir.

Somewhere around 26/27 or so, the mood in Washington will become somber. People will start to viscerally feel what is happening; they will be scared. From the halls of the Pentagon to the backroom Congressional briefings will ring the obvious question, the question on everybody's minds: do we need an AGI Manhattan Project? Slowly at first, then all at once, it will become clear: this is happening, things are going to get wild, this is the most important challenge for the national security of the United States since the invention of the atomic bomb. In one form or another, the national security state will get very heavily involved. The Project will be the necessary, indeed the only plausible, response.

Of course, this is an extremely abbreviated account—a lot depends on when and how consensus forms, key warning shots, and so on. DC is infamously dysfunctional. As with Covid, and even the Manhattan Project, the government will be incredibly late and hamfisted. After Einstein's letter to the President in 1939 (drafted by Szilard), an Advisory Committee on Uranium was formed. But officials were incompetent, and not much happened initially. For example, Fermi only got $6k (about $135k in today's dollars) to support his research, and even that was not given easily and only received after months of waiting. Szilard believed that the project was delayed for at least a year by the short-sightedness and sluggishness of the authorities. In March 1941, the British government finally concluded a bomb was inevitable. The US committee initially entirely ignored this British report for months—until finally in December 1941, a full-scale atomic bomb effort was launched.

There are many ways this could be operationalized in practice. To be clear, this doesn't need to look like literal nationalization, with AI lab researchers now employed by the military or whatever (though it might!). Rather, I expect a more suave orchestration. The relationship with the DoD might look like the relationship the DoD has with Boeing or Lockheed Martin. Perhaps via defense contracting or similar, a joint venture between the major cloud compute providers, AI labs, and the government is established, making it functionally a project of the national security state. Much like the AI labs "voluntarily" made commitments to the White House in 2023, Western labs might more-or-less "voluntarily" agree to merge in the national effort. And likely Congress will have to be involved, given the trillions of investment involved, and for checks-and-balances. How all these details shake out is a story for another day.

But by late 26/27/28 it will be underway. The core AGI research team (a few hundred researchers) will move to a secure location; the trillion-dollar cluster will be built in record-speed; The Project will be on.

通向未来的征程

2020 年 2 月底至 3 月中旬的那段经历，永远铭刻在我的记忆中。二月末三月初的那些日子里，我陷入了深深的绝望：显然我们正处于新冠病毒的爆发期，一场瘟疫即将席卷全国，医疗系统随时可能崩溃 —— 然而几乎没有人当回事。纽约市长那时还在说担心新冠是种族歧视，继续鼓励人们去看百老汇演出。我能做的，就只有买口罩和做空股市。

然而仅仅几周之后，整个国家就开始封锁，国会批准了数万亿美元的拨款（超过国民生产总值的 10%）。虽然我们很难提前预见指数增长的恐怖，但当威胁真正逼近，到了关乎生死存亡的时刻，社会就会爆发出非凡的力量。尽管响应来得晚了些，措施也比较粗暴 —— 但总算有所行动，而且规模惊人。

未来几年，AI 的发展轨迹可能会与之相似。我们现在正处于关键阶段。2023 年已经发生了翻天覆地的变化。通用人工智能（AGI）从一个你都不好意思提起的边缘话题，一跃成为美国参议院重要听证会和世界领导人峰会的核心议题。考虑到现在仍属早期阶段，美国政府表现出的参与度令我印象深刻。再经历几个「2023 年」这样的转折点，社会对 AI 的接受度和讨论范围将会彻底突破现有界限。

随着 AI 计算能力的指数级提升，技术突破会接踵而至。到 2025/2026 年左右，我预计会出现下一个真正令人震惊的突破；AI 将为科技巨头带来超过 1000 亿美元的年收入，在解决原始问题的智慧上超越博士生。就像当初新冠导致的股市崩盘让许多人开始重视疫情一样，到时候我们将会看到市值 10 万亿美元的公司出现，AI 热潮将席卷全球。如果这还不够震撼，到 2027/28 年，我们将会拥有在价值超过 1000 亿美元的超级计算集群上训练的模型；完全成熟的 AI 智能体将可以取代远程工作者，开始大规模自动化软件开发和其他脑力工作。每一年的发展速度都会让人目不暇接。

尽管现在很多人还看不到通用人工智能的可能性，但最终共识终将形成。就像物理学家希拉德（Szilard）比其他人更早预见到原子弹的可能性一样。最初，他的警告并未得到重视；人们认为制造原子弹的可能性很小（或者至少认为，谨慎明智的做法是淡化这种可能性）。希拉德极力呼吁对相关研究保密，却遭到嘲笑和漠视。但随着越来越多的实验数据涌现，许多最初持怀疑态度的科学家开始意识到制造原子弹确实可行。当大多数科学家相信他们即将造出原子弹时，政府也随之认识到这关乎重大国家安全 —— 于是曼哈顿计划正式启动。

随着 AI 能力的突破从理论预测变成现实，顶尖科学家、企业高管和政府官员中间也会逐渐达成共识：我们正站在历史的转折点，即将迎来通用人工智能的诞生、智能的爆发性增长，以及超人类智能的出现。在这个过程中，我们可能会看到第一个真正令人恐惧的 AI 演示：也许是常被讨论的「协助新手制造生物武器」，或者自主入侵关键系统，抑或是其他完全意想不到的情况。到那时，一个事实将变得明确：不管是否愿意接受，这项技术都将成为决定性的军事力量。即使我们有幸避免陷入重大战争，中国共产党也很可能已经注意到这点，并启动了规模宏大的 AGI 项目。也许最终不可避免地会发现中共渗透美国顶尖 AI 实验室的事实，这将引发轩然大波。

到 2026/2027 年左右，华盛顿的氛围将变得凝重。人们会真切地意识到这场变革的冲击；恐惧感将开始蔓延。从五角大楼的走廊到国会的闭门会议，一个显而易见的问题将萦绕在每个人心头：我们是否需要启动一个针对通用人工智能的曼哈顿计划？事态的发展将是循序渐进，而后骤然加速：这场革命势不可挡，局势将急剧变化，这将成为自原子弹发明以来美国国家安全面临的最严峻挑战。无论采取何种形式，国家安全部门都将全面参与其中。这个计划将成为必然的选择，也是唯一可行的应对方案。

当然，这只是一个粗略的概述 —— 很多细节都取决于共识如何形成、关键的警示信号何时出现等因素。华盛顿的决策效率低下是众所周知的。就像应对新冠疫情，甚至是曼哈顿计划一样，政府的反应总是异常迟缓且缺乏灵活性。在爱因斯坦 1939 年致总统的信（由希拉德起草）之后，政府成立了一个铀研究咨询委员会。但官员们能力不足，初期几乎毫无进展。举例来说，著名物理学家费米（Fermi）仅获得了 6000 美元（相当于今天的 13.5 万美元）的研究经费，而且即便是这笔微薄的资金，也是经过数月等待才最终到位。希拉德认为，由于当局的短视和拖沓，整个项目至少被推迟了一年。1941 年 3 月，英国政府最终得出结论，认为原子弹的研制势在必行。然而，美国委员会起初对这份英国报告置之不理达数月之久 —— 直到 1941 年 12 月，才正式启动了全面的原子弹研制计划。

这个现代版的「超级计划」在实践中可以采取多种形式。需要明确的是，这并非必须是传统意义上的国有化，不一定要让 AI 实验室的研究人员直接隶属于军方（当然这也是可能的！）。相反，我预计会采取一种更为巧妙的组织方式。与国防部的关系可能会类似于其与波音或洛克希德·马丁公司的合作模式。也许通过国防合同或类似机制，主要的云计算供应商、AI 实验室和政府将组建一个联合项目，实质上使其成为国家安全体系的重要组成部分。就像 2023 年各大 AI 实验室「主动」向白宫作出承诺一样，西方实验室可能也会或多或少「自愿」加入这个国家项目。考虑到所需的数万亿美元投资规模，以及权力制衡的需要，国会必然会参与其中。至于这些具体细节如何安排，那是未来需要解决的问题。

但到 2026-2028 年底，这个宏大的计划将正式启动。核心 AGI 研究团队（数百名顶尖研究人员）将迁入特定的安全区域；造价万亿美元的超级计算集群将以前所未有的速度建成；一个足以改变人类历史进程的计划就此展开。

### Why The Project is the only way

I am under no illusions about the government. Governments face all sorts of limitations and poor incentives. I am a big believer in the American private sector, and would almost never advocate for heavy government involvement in technology or industry.

I used to apply this same framework to AGI—until I joined an AI lab. AI labs are very good at some things: they've been able to take AI from an academic science project to the commercial big stage, in a way only a startup can. But ultimately, AI labs are still startups. We simply shouldn't expect startups to be equipped to handle superintelligence.

There are no good options here—but I don't see another way. When a technology becomes this important for national security, we will need the USG.

Superintelligence will be the United States' most important national defense project

I've discussed the power of superintelligence in previous pieces. Within years, superintelligence would completely shake up the military balance of power. By the early 2030s, the entirety of the US arsenal (like it or not, the bedrock of global peace and security) will probably be obsolete. It will not just be a matter of modernization, but a wholesale replacement.

Simply put, it will become clear that the development of AGI will fall in a category more like nukes than the internet. Yes, of course it'll be dual-use—but nuclear technology was dual-use too. The civilian applications will have their time. But in the fog of the AGI endgame, for better or for worse, national security will be the primary backdrop.

We will need to completely reshape US forces, within a matter of years, in the face of rapid technological change—or risk being completely outmatched by adversaries who do. Perhaps most of all, the initial priority will be to deploy superintelligence for defensive applications, to develop countermeasures to survive untold new threats: adversaries with superhuman hacking capabilities, new classes of stealthy drone swarms that could execute a preemptive strike on our nuclear deterrent, the proliferation of advances in synthetic biology that can be weaponized, turbulent international (and national) power struggles, and rogue superintelligence projects.

Whether nominally private or not, the AGI project will need to be, will be, integrally a defense project, and it will require extremely close cooperation with the national security state.

A sane chain of command for superintelligence

The power—and the challenges—of superintelligence will fall into a very different reference class than anything else we're used to seeing from tech companies. It seems pretty clear: this should not be under the unilateral command of a random CEO. Indeed, in the private-labs-developing-superintelligence world, it's quite plausible individual CEOs would have the power to literally coup the US government. Imagine if Elon Musk had final command of the nuclear arsenal. (Or if a random nonprofit board could decide to seize control of the nuclear arsenal.)

It is perhaps obvious, but: as a society, we've decided democratic governments should control the military; superintelligence will be, at least at first, the most powerful military weapon. The radical proposal is not The Project; the radical proposal is taking a bet on private AI CEOs wielding military power and becoming benevolent dictators.

(Indeed, in the private AI lab world, it would likely be even worse than random CEOs with the nuclear button—part of AI labs' abysmal security is their utter lack of internal controls. That is, random AI lab employees (with zero vetting) could go rogue unnoticed.)

We will need a sane chain of command—along with all the other processes and safeguards that necessarily come with responsibly wielding what will be comparable to a WMD—and it'll require the government to do so. In some sense, this is simply a Burkean argument: the institutions, constitutions, laws, courts, checks and balances, norms and common dedication to the liberal democratic order (e.g., generals refusing to follow illegal orders), and so on that check the power of the government have withstood the test of hundreds of years. Special AI lab governance structures, meanwhile, collapsed the first time they were tested. The US military could already kill basically every civilian in the United States, or seize power, if it wanted to—and the way we keep government power over nuclear weapons in check is not through lots of private companies with their own nuclear arsenals. There's only one chain of command and set of institutions that has proven itself up to this task.

Again, perhaps you are a true libertarian and disagree normatively (let Elon Musk and Sam Altman command their own nuclear arsenals!) But once it becomes clear that superintelligence is a principal matter of national security, I'm sure this is how the men and women in DC will look at it.

The civilian uses of superintelligence

Of course, that doesn't mean the civilian applications of superintelligence will be reserved for the government.

The nuclear chain reaction was first harnessed as a government project—and nuclear weapons permanently reserved for the government—but civilian nuclear energy flourished as private projects (in the 60s and 70s, before environmentalists shut it down).

Boeing made the B-29 (the most expensive defense R&D project during WWII, more expensive than the Manhattan Project) and the B-47 and B-52 long-range bombers in partnership with the military—before using that technology for the Boeing 707, the commercial plane that ushered in the jet era. And today, while Boeing can only sell stealth fighter jets to the government, it can freely develop and sell civilian jets privately.

And so it went for radar, satellites, rockets, gene technology, WWII factories, and so on.

The initial development of superintelligence will be dominated by the national security exigency to survive and stabilize an incredibly volatile period. And the military uses of superintelligence will remain reserved for the government, and safety norms will be enforced. But once the initial peril has passed, and the world has stabilized, the natural path is for the companies involved in the national consortium (and others) to privately pursue civilian applications.

Even in worlds with The Project, a private, pluralistic, market-based, flourishing ecosystem of civilian applications of superintelligence will have its day.

Security

I've gone on about this at length in a previous piece in the series. On the current course, we may as well give up on having any American AGI effort; China can promptly steal all the algorithmic breakthroughs and the model weights (literally a copy of superintelligence) directly. It's not even clear we'll get to "North Korea-proof" security for superintelligence on the current course. In the private-startups-developing-AGI-world, superintelligence would proliferate to dozens of rogue states. It's simply untenable.

If we're going to be at all serious about this, we obviously need to lock this stuff down. Most private companies have failed to take this seriously. But in any case, if we are to eventually face the full force of Chinese espionage (e.g., stealing the weights being the MSS's #1 priority), it's probably impossible for a private company to get good enough security. It will require extensive cooperation from the US intelligence community at that point to sufficiently secure AGI. This will involve invasive restrictions on AI labs and on the core team of AGI researchers, from extreme vetting to constant monitoring to working from a SCIF to reduced freedom to leave; and it will require infrastructure only the government can provide, ultimately including the physical security of the AGI datacenters themselves.

In some sense, security alone is sufficient to necessitate the government project—both the free world's preeminence and AI safety are doomed if we can't lock this stuff down. (In fact, I think it's fairly likely to be a major factor in the ultimate trigger: once the Chinese infiltration of the AGI labs becomes clear, every Senator and Congressperson and national security official will… have a strong opinion on the matter.)

Safety

Simply put: there are a lot of ways for us to mess this up—from ensuring we can reliably control and trust the billions of superintelligent agents that will soon be in charge of our economy and military (the superalignment problem) to and controlling the risks of misuse of new means of mass destruction.

Some AI labs claim to be committed to safety: acknowledging that what they are building, if gone awry, could cause catastrophe and promising that they will do what is necessary when the time comes. I do not know if we can trust their promise enough to stake the lives of every American on it. More importantly, so far, they have not demonstrated the competence, trustworthiness, or seriousness necessary for what they themselves acknowledge they are building.

At core, they are startups, with all the usual commercial incentives. Competition could push all of them to simply race through the intelligence explosion, and there will at least be some actors that will be willing to throw safety by the wayside. In particular, we may want to "spend some of our lead" to have time to solve safety challenges, but Western labs will need to coordinate to do so. (And of course, private labs will have already had their AGI weights stolen, so their safety precautions won't even matter; we'll be at the mercy of the CCP's and North Korea's safety precautions.)

One answer is regulation. That may be appropriate in worlds in which AI develops more slowly, but I fear that regulation simply won't be up to the nature of the challenge of the intelligence explosion. What's necessary will be less like spending a few years doing careful evaluations and pushing some safety standards through a bureaucracy. It'll be more like fighting a war.

We'll face an insane year in which the situation is shifting extremely rapidly every week, in which hard calls based on ambiguous data will be life-or-death, in which the solutions—even the problems themselves—won't be close to fully clear ahead of time but come down to competence in a "fog of war," which will involve insane tradeoffs like "some of our alignment measurements are looking ambiguous, we don't really understand what's going on anymore, it might be fine but there's some warning signs that the next generation of superintelligence might go awry, should we delay the next training run by 3 months to get more confidence on safety—but oh no, the latest intelligence reports indicate China stole our weights and is racing ahead on their own intelligence explosion, what should we do?".

I'm not confident that a government project would be competent in dealing with this—but the "superintelligence developed by startups" alternative seems much closer to "praying for the best" than commonly recognized. We'll need a chain of command that can bring to the table the seriousness that making these difficult tradeoffs will require.

Stabilizing the international situation

The intelligence explosion and its immediate aftermath will bring forth one of the most volatile and tense situations mankind has ever faced. Our generation is not used to this. But in this initial period, the task at hand will not be to build cool products. It will be to somehow, desperately, make it through this period.

We'll need the government project to win the race against the authoritarian powers—and to give us the clear lead and breathing room necessary to navigate the perils of this situation. We might as well give up if we can't prevent the instant theft of superintelligence model weights. We will want to bundle Western efforts: bring together our best scientists, use every GPU we can find, and ensure the trillions of dollars of cluster buildouts happen in the United States. We will need to protect the datacenters against adversary sabotage, or outright attack.

Perhaps, most of all, it will take American leadership to develop—and if necessary, enforce—a nonproliferation regime. We'll need to subvert Russia, North Korea, Iran, and terrorist groups from using their own superintelligence to develop technology and weaponry that would let them hold the world hostage. We'll need to use superintelligence to harden the security of our critical infrastructure, military, and government to defend against extreme new hacking capabilities. We'll need to use superintelligence to stabilize the offense/defense balance of advances in biology or similar. We'll need to develop tools to safely control superintelligence, and to shut down rogue superintelligences that come out of others' uncareful projects. AI systems and robots will be moving at 10-100x+ human speed; everything will start happening extremely quickly. We'll need to be ready to handle whatever other six-sigma upheavals—and concomitant threats—come out of compressing a century's worth of technological progress into a few years.

At least in this initial period, we will be faced with the most extraordinary national security exigency. Perhaps, nobody is up for this task. But of the options we have, The Project is the only sane one.

为何国家主导是必由之路

我对政府的局限性有清醒的认识。政府面临着各种制约因素和不当激励。我一直都是美国私营部门的坚定支持者，几乎从不赞同政府深度介入技术或产业发展。

我过去也是用这种思维方式来看待通用人工智能的 —— 直到我加入了一家 AI 实验室。AI 实验室确实在某些方面表现出色：他们成功地将 AI 从实验室项目转变为商业现实，这种转变只有创业公司才能做到。但本质上，AI 实验室终究还是创业公司。我们不能天真地期待创业公司有能力掌控超级智能（Superintelligence）。

虽然没有完美的解决方案 —— 但我们别无选择。当一项技术对国家安全变得如此重要时，美国政府的全面接管将势在必行。

超级智能：美国的关键国防战略

我在之前的文章中已经探讨过超级智能的强大潜力。在短短几年内，超级智能将彻底改变全球军事力量的格局。到 2030 年代初，美国现有的整个军事体系（不管是否愿意承认，这确实是全球和平与安全的基石）都可能被淘汰。这已经不仅仅是装备升级的问题，而是一次彻底的革命性更迭。

简而言之，通用人工智能的发展将更类似于核武器而非互联网。诚然，它具有双重属性 —— 就像核技术既可用于发电，也可用于武器。民用领域终将有其发展空间。但在通用人工智能发展的关键阶段，无论我们愿意与否，国家安全都将成为首要考虑因素。

我们需要在短短几年内彻底重构美国的军事力量，以应对快速的技术变革 —— 否则就会面临被积极发展这项技术的对手彻底超越的风险。最紧迫的任务或许是将超级智能应用于防御系统，以应对一系列前所未有的新威胁：具备超越人类能力的黑客对手、能够突破核威慑进行先发制人打击的新型隐形无人机集群、可能被武器化的合成生物学技术扩散、动荡的国际（和国内）地缘政治局势，以及可能失去控制的超级智能项目。

无论在形式上是否归属私营企业，通用人工智能项目都必须也必然会成为国防体系的重要组成部分，需要与国家安全机构保持最紧密的协作。

建立超级智能的科学管理体系

超级智能带来的力量 —— 以及随之而来的挑战 —— 将完全不同于我们已经见识过的任何科技公司的产品。有一点似乎很明确：这种力量不应该由某个商业公司的首席执行官单独掌控。事实上，在由私人实验室开发超级智能的情况下，个别公司的 CEO 很可能获得足以颠覆美国政府的实际能力。试想一下，如果埃隆·马斯克拥有核武器系统的最终决策权会是什么情况。（或者如果某个非营利组织的董事会可以决定接管核武器系统的控制权。）

这个道理也许很浅显：作为一个社会，我们已经确立了由民主政府控制军事力量的原则；而超级智能至少在初期将成为最强大的军事武器。真正激进的想法不是这个国家主导的项目，而是寄希望于让私营公司的 AI 领袖掌握军事力量，并期待他们成为开明的统治者。

事实上，在私营 AI 实验室的运营模式下，情况可能会比由企业 CEO 掌握核按钮更加危险 —— 这些实验室的安全隐患主要在于完全缺乏内部监管机制。这意味着，那些未经任何安全审查的 AI 实验室员工可能会在无人察觉的情况下采取危险行动。

我们需要建立一个科学的管理体系 —— 包括所有必要的流程和安全保障，这些都是安全掌控堪比大规模杀伤性武器的超级智能所必需的 —— 而这些只有通过政府才能实现。从本质上来说，这是一个关于制度传承的论点：那些用于约束政府权力的体系，包括各种制度、宪法、法律、司法机构、权力制衡机制，以及对民主自由秩序的共同维护（比如军队将领拒绝执行违法命令），这些都在数百年的历史中经受住了考验。相比之下，AI 实验室特有的管理架构在首次面临挑战时就已经失效。美国军方如果想要的话，现在就有能力消灭几乎所有美国平民，或者实施政变 —— 但我们控制政府使用核武器的方式并不是靠设立多个拥有自己核武库的私营公司。历史已经证明，只有这一套指挥系统和制度框架能够胜任这项重任。

当然，你可能是一个坚定的自由主义者（libertarian），从原则上反对这种做法（那就让埃隆·马斯克和山姆·奥特曼各自指挥自己的核武器吧！）。但一旦超级智能被确定为关乎国家安全的核心议题，我相信华盛顿的决策者们必然会采取这种立场。

超级智能的民间应用

当然，这并不意味着超级智能的民用领域将被政府完全掌控。

核裂变技术最初确实是作为政府项目被开发的 —— 核武器的控制权永久保留在政府手中 —— 但民用核能产业在私营领域蓬勃发展（在 20 世纪 60 年代和 70 年代，直到环保运动导致其发展受阻）。

波音公司与军方合作，先后研制了 B-29（二战期间最昂贵的国防研发项目，投入甚至超过曼哈顿计划）、B-47 和 B-52 等远程轰炸机。随后，他们将这些技术应用于研发波音 707 客机，开创了民用航空的喷气发动机时代。如今的波音公司，虽然只能向政府提供隐形战斗机，但在民用客机领域拥有完全的研发和销售自主权。

这种模式同样适用于雷达、卫星、火箭、基因技术，甚至二战时期的工业生产设施等诸多领域。它们都经历了从军事到民用的成功转化。

在发展初期，超级智能的研发将不得不优先考虑国家安全需求，以帮助人类社会度过一个极度动荡的时期。军事用途的超级智能技术将继续由政府掌控，相关安全规范也将得到严格执行。但当最初的危险阶段过去，全球局势逐渐稳定后，参与国家项目的企业（以及其他公司）自然会开始探索民用领域的应用。

即使在实施这个国家主导项目的情况下，一个由私营企业主导、多元共生、市场驱动的超级智能民用应用领域终将蓬勃发展。

关于安全问题

我在本系列的上一篇文章中已经详细探讨过这个问题。如果继续沿着目前的发展路径，我们可能要放弃在通用人工智能领域的所有努力，因为中国可能会迅速窃取所有的算法创新和模型核心数据（这实际上就等同于获得了超级智能的完整副本）。按照目前的发展态势，我们甚至无法确保能够建立起最基本的安全防护体系。在由私营创业公司主导开发通用人工智能的情况下，超级智能技术可能会扩散到数十个具有潜在威胁的国家。这种情况显然无法接受。

如果我们要认真对待这个问题，就必须建立严格的管控机制。目前大多数私营企业都没有充分重视这一点。但即便如此，当我们最终不得不面对中国全方位的间谍活动时（比如窃取 AI 模型核心数据可能成为中国国家安全部的首要任务），仅凭私营企业可能无法建立足够完善的安全体系。到那时，只有与美国情报部门展开深度合作，才能真正确保通用人工智能的安全。这意味着需要对 AI 实验室和核心研究团队实施严格的管理措施，包括严格的安全审查、全天候监控、在特殊安全设施中工作，以及限制人员流动；同时还需要依托只有政府才能提供的基础设施，特别是确保 AI 数据中心的物理安全。

从某种程度上说，仅安全因素就足以证明政府主导的必要性 —— 如果我们无法确保这些技术的绝对安全，自由世界的优势和 AI 安全都将面临严重威胁。（事实上，我认为这很可能成为最终促使行动的关键因素：一旦中国渗透美国顶级 AI 实验室的事实被揭露，每一位参议员、国会议员和国家安全高官都将不得不严肃应对这一局势。）

技术安全与管控

简而言之：我们面临着诸多潜在风险 —— 从如何可靠地控制和信任即将管理我们经济和军事系统的数以亿计的超级智能体（这涉及超级智能系统的对齐问题），到如何防范新型大规模杀伤性技术被滥用的风险。

一些 AI 实验室声称他们高度重视安全问题：他们承认自己正在开发的技术一旦失控可能会引发灾难性后果，并承诺会在关键时刻采取必要的防范措施。但是，我们能否完全信任这些承诺，将美国全体公民的生命安全押在上面？更重要的是，迄今为止，这些实验室还没有展现出与其所开发技术相匹配的能力、可信度和严谨态度。

归根结底，这些都是创业公司，不可避免地受到商业利益的驱使。在竞争压力下，它们可能会不顾一切地推进智能技术的快速发展，而且必然会有一些机构为了速度而牺牲安全。特别值得注意的是，我们可能需要适当放缓发展速度，给自己留出时间来解决安全挑战，但这需要西方各大实验室之间的密切配合。（况且，私人实验室的 AI 核心技术可能已经遭到窃取，这使得他们的安全措施形同虚设；我们的安全反而要依赖于中国和朝鲜的自我约束。）

有人提出监管可能是解决方案。这在 AI 发展相对缓慢的情况下或许可行，但我担心单纯的监管根本无法应对超级智能爆发式发展带来的挑战。我们需要的不是花费数年时间进行评估，然后通过繁琐的行政程序来制定一些安全标准。这更像是一场需要立即应对的战争。

我们将面临一个局势急剧变化的关键时期，形势每周都在剧烈变化。在这个过程中，我们不得不根据不完整的信息作出生死攸关的决策。解决方案 —— 甚至问题本身 —— 都无法提前预知，而是需要在混沌的局势中凭借实战经验来判断。我们将面临极其艰难的抉择，例如："我们的一些 AI 系统对齐性测试结果不够明确，很多现象我们已经难以理解。虽然表面看起来可能没有问题，但有迹象表明下一代超级智能系统可能存在风险。我们是否应该暂缓三个月的训练计划，以确保更高的安全性？但与此同时，最新情报显示中国已经获取了我们的核心技术数据，并在加速推进他们自己的超级智能开发，在这种情况下我们该如何应对？"

我并不确信政府主导的项目在应对这些挑战时就一定会做得很好 —— 但相比之下，让初创公司来开发超级智能这个选项，实际上比大多数人意识到的更像是在碰运气。我们需要一个能够以足够严谨的态度来权衡这些艰难决策的管理体系。

维护全球稳定

超级智能的爆发性发展及其直接影响将给人类带来史上最动荡的时期之一。我们这个时代的人还没有经历过如此严峻的挑战。在这个初始阶段，我们的首要任务不是开发新颖的商业产品，而是要竭尽全力确保人类安全地度过这个关键时期。

我们需要通过政府主导的项目在与专制国家的竞争中取得优势 —— 这将给我们足够的领先地位和必要的时间来应对潜在的危机。如果我们无法防止超级智能核心技术被立即窃取，那其他努力都将是徒劳。我们需要整合西方世界的力量：集中最优秀的科学家，调动所有可用的图形处理器（GPU）资源，确保投资数万亿美元建设的超级计算集群落户美国。同时，我们还需要保护这些关键计算设施免受对手的破坏或直接攻击。

更为关键的是，美国需要发挥领导作用，建立并在必要时执行一个超级智能技术管控机制。我们必须防止俄罗斯、朝鲜、伊朗和恐怖组织利用自己的超级智能系统开发足以威胁全球的技术和武器。我们需要运用超级智能来强化关键基础设施、军事设施和政府系统的安全防护，以抵御新一代的网络攻击。我们需要借助超级智能来维持生物科技等领域的攻防平衡。我们需要开发相关工具来安全地管控超级智能系统，并能够及时关停其他机构因疏忽而导致失控的超级智能项目。未来的 AI 系统和机器人将以超出人类 10-100 倍的速度运转，一切变化都会极其迅速。我们需要为应对各种极端情况做好准备 —— 包括应对那些由于将一个世纪的技术进步压缩到短短几年内而产生的重大威胁。

在这个关键阶段，我们正面临着前所未有的国家安全挑战。诚然，这个任务的难度可能超出任何人的能力范围。但在所有可行的选择中，建立这个国家主导的项目是我们唯一明智的选择。

### The Project is inevitable; whether it's good is not

Ultimately, my main claim here is descriptive: whether we like it or not, superintelligence won't look like an SF startup, and in some way will be primarily in the domain of national security. I've brought up The Project a lot to my San Francisco friends in the past year. Perhaps what's surprised me most is how surprised most people are about the idea. They simply haven't considered the possibility. But once they consider it, most agree that it seems obvious. If we are at all right about what we think we are building, of course, by the end this will be (in some form) a government project. If a lab developed literal superintelligence tomorrow, of course the Feds would step in.

One important free variable is not if but when. Does the government not realize what's happening until we're in the middle of an intelligence explosion—or will it realize a couple years beforehand? If the government project is inevitable, earlier seems better. We'll dearly need those couple years to do the security crash program, to get the key officials up to speed and prepared, to build a functioning merged lab, and so on. It'll be far more chaotic if the government only steps in at the very end (and the secrets and weights will have already been stolen).

Another important free variable is the international coalition we can rally: both a tighter alliance of democracies for developing superintelligence, and a broader benefit-sharing offer made to the rest of the world.

The former might look like the Quebec Agreement: a secret pact between Churchill and Roosevelt to pool their resources to develop nuclear weapons, while not using them against each other or against others without mutual consent. We'll want to bring in the UK (Deepmind), East Asian allies like Japan and South Korea (chip supply chain), and NATO/other core democratic allies (broader industrial base). A united effort will have more resources, talent, and control the whole supply chain; enable close coordination on safety, national security, and military challenges; and provide helpful checks and balances on wielding the power of superintelligence.

The latter might look like Atoms for Peace, the IAEA, and the NPT. We should offer to share the peaceful benefits of superintelligence with a broader group of countries (including non-democracies), and commit to not offensively using superintelligence against them. In exchange, they refrain from pursuing their own superintelligence projects, make safety commitments on the deployment of AI systems, and accept restrictions on dual-use applications. The hope is that this offer reduces the incentives for arms races and proliferation, and brings a broad coalition under a US-led umbrella for the post-superintelligence world order.

Perhaps the most important free variable is simply whether the inevitable government project will be competent. How will it be organized? How can we get this done? How will the checks and balances work, and what does a sane chain of command look like? Scarcely any attention has gone into figuring this out. Almost all other AI lab and AI governance politicking is a sideshow. This is the ballgame.

### The endgame

And so by 27/28, the endgame will be on. By 28/29 the intelligence explosion will be underway; by 2030, we will have summoned superintelligence, in all its power and might.

Oppenheimer and Groves.

Whoever they put in charge of The Project is going to have a hell of a task: to build AGI, and to build it fast; to put the American economy on wartime footing to make hundreds of millions of GPUs; to lock it all down, weed out the spies, and fend off all-out attacks by the CCP; to somehow manage a hundred million AGIs furiously automating AI research, making a decade's leaps in a year, and soon producing AI systems vastly smarter than the smartest humans; to somehow keep things together enough that this doesn't go off the rails and produce rogue superintelligence that tries to seize control from its human overseers; to use those superintelligences to develop whatever new technologies will be necessary to stabilize the situation and stay ahead of adversaries, rapidly remaking US forces to integrate those; all while navigating what will likely be the tensest international situation ever seen. They better be good, I'll say that.

For those of us who get the call to come along for the ride, it'll be . . . stressful. But it will be our duty to serve the free world—and all of humanity. If we make it through and get to look back on those years, it will be the most important thing we ever did. And while whatever secure facility they find probably won't have the pleasantries of today's ridiculously-overcomped-AI-researcher-lifestyle, it won't be so bad. SF already feels like a peculiar AI-researcher-college-town; probably this won't be so different. It'll be the same weirdly-small circle sweating the scaling curves during the day and hanging out over the weekend, kibitzing over AGI and the lab-politics-of-the-day.

Except, well—the stakes will be all too real.

See you in the desert, friends.

Reunion of atomic scientists on the fourth anniversary of the first controlled nuclear fission reaction at UChicago.

Next post in the series:

V. Parting Thoughts

### Reference

Note that while private companies help develop components for nuclear weapons, they are never allowed to possess a completed and assembled nuclear weapon. In comparison, the mainline version of the "AGI government project" I am putting forward here is unprecedentedly privatized, for the WMD reference class.↩

Congress—even the Vice President!—didn't know about the Manhattan Project. We probably shouldn't repeat that here; I'd even suggest that key officials for The Project require Senate confirmation.↩

It wouldn't even require cooperation from AI lab employees at this point, since they'll have been mostly automated by this point.↩

And as Sam Altman once said, every year we get closer to AGI everybody will gain +10 crazy points.↩

In fact, the government having the biggest guns was an enormous civilizational achievement! Rather than medieval-like fights of all against all, we would sort out disagreements via courts, pluralistic institutions, and so on.↩

Or perhaps you say, just open-source everything. The issue with simply open sourcing everything is that it's not a happy world of a thousand flowers blooming in the US, but a world in which the CCP has free access to US-developed superintelligence, and can outbuild (and apply less caution/regulation) and take over the world. And the other issue, of course, is the proliferation of super-WMDs to every rogue state and terrorist group in the world. I don't think it'll end well. It's a bit like how having no government at all is more likely to lead to tyranny (or destruction) than freedom.

In any case, people overrate the importance of open-source as we get closer to AGI. Given cluster costs escalating to hundreds of billions, and key algorithmic secrets now being proprietary rather than published as they were a couple years ago, it'll be 2-3 or so leading players, rather than some happy community of decentralized coders building AGI.

I do think a different variant of open source will continue to play an important role: models that lag a couple years behind being open sourced, helping the benefits of the technology diffuse broadly.

To my Progress Studies brethren: you should think about this, this will be the culmination of your intellectual project! You spend all this time studying American government research institutions, their decline over the last half-century, and what it would take to make them effective again. Tell me: how will we make The Project effective?