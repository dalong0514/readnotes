Leopold Aschenbrenner.(2024).2024109Situational-Awareness-The-Decade-Ahead.Dwarkesh podcast => I. From GPT-4 to AGI: Counting the OOMs

## V. Parting Thoughts

What if we're right?

In this piece:

AGI realism

What if we're right?

"I remember the spring of 1941 to this day. I realized then that a nuclear bomb was not only possible — it was inevitable. Sooner or later these ideas could not be peculiar to us. Everybody would think about them before long, and some country would put them into action. […]

And there was nobody to talk to about it, I had many sleepless nights. But I did realize how very very serious it could be. And I had then to start taking sleeping pills. It was the only remedy, I've never stopped since then. It's 28 years, and I don't think I've missed a single night in all those 28 years."

James Chadwick (Physics Nobel Laureate and author of the 1941 British government report on the inevitability of an atomic bomb, which finally spurred the Manhattan Project into action)

Before the decade is out, we will have built superintelligence. That is what most of this series has been about. For most people I talk to in SF, that's where the screen goes black. But the decade after—the 2030s—will be at least as eventful. By the end of it, the world will have been utterly, unrecognizably transformed. A new world order will have been forged. But alas—that's a story for another time.

We must come to a close, for now. Let me make a few final remarks.

AGI realism

This is all much to contemplate—and many cannot. "Deep learning is hitting a wall!" they proclaim, every year. It's just another tech boom, the pundits say confidently. But even among those at the SF-epicenter, the discourse has become polarized between two fundamentally unserious rallying cries.

On the one end there are the doomers. They have been obsessing over AGI for many years; I give them a lot of credit for their prescience. But their thinking has become ossified, untethered from the empirical realities of deep learning, their proposals naive and unworkable, and they fail to engage with the very real authoritarian threat. Rabid claims of 99% odds of doom, calls to indefinitely pause AI—they are clearly not the way.

On the other end are the e/accs. Narrowly, they have some good points: AI progress must continue. But beneath their shallow Twitter shitposting, they are a sham; dilettantes who just want to build their wrapper startups rather than stare AGI in the face. They claim to be ardent defenders of American freedom, but can't resist the siren song of unsavory dictators' cash. In truth, they are real stagnationists. In their attempt to deny the risks, they deny AGI; essentially, all we'll get is cool chatbots, which surely aren't dangerous. (That's some underwhelming accelerationism in my book.)

But as I see it, the smartest people in the space have converged on a different perspective, a third way, one I will dub AGI Realism. The core tenets are simple:

Superintelligence is a matter of national security. We are rapidly building machines smarter than the smartest humans. This is not another cool Silicon Valley boom; this isn't some random community of coders writing an innocent open source software package; this isn't fun and games. Superintelligence is going to be wild; it will be the most powerful weapon mankind has ever built. And for any of us involved, it'll be the most important thing we ever do.

America must lead. The torch of liberty will not survive Xi getting AGI first. (And, realistically, American leadership is the only path to safe AGI, too.) That means we can't simply "pause"; it means we need to rapidly scale up US power production to build the AGI clusters in the US. But it also means amateur startup security delivering the nuclear secrets to the CCP won't cut it anymore, and it means the core AGI infrastructure must be controlled by America, not some dictator in the Middle East. American AI labs must put the national interest first.

We need to not screw it up. Recognizing the power of superintelligence also means recognizing its peril. There are very real safety risks; very real risks this all goes awry—whether it be because mankind uses the destructive power brought forth for our mutual annihilation, or because, yes, the alien species we're summoning is one we cannot yet fully control. These are manageable—but improvising won't cut it. Navigating these perils will require good people bringing a level of seriousness to the table that has not yet been offered.

As the acceleration intensifies, I only expect the discourse to get more shrill. But my greatest hope is that there will be those who feel the weight of what is coming, and take it as a solemn call to duty.

What if we're right?

At this point, you may think that I and all the other SF-folk are totally crazy. But consider, just for a moment: what if they're right? These are the people who invented and built this technology; they think AGI will be developed this decade; and, though there's a fairly wide spectrum, many of them take very seriously the possibility that the road to superintelligence will play out as I've described in this series.

Almost certainly I've gotten important parts of the story wrong; if reality turns out to be anywhere near this crazy, the error bars will be very large. Moreover, as I said at the outset, I think there's a wide range of possibilities. But I think it is important to be concrete. And in this series I've laid out what I currently believe is the single most likely scenario for the rest of the decade—the rest of this decade.

Because—it's starting to feel real, very real. A few years ago, at least for me, I took these ideas seriously—but they were abstract, quarantined in models and probability estimates. Now it feels extremely visceral. I can see it. I can see how AGI will be built. It's no longer about estimates of human brain size and hypotheticals and theoretical extrapolations and all that—I can basically tell you the cluster AGI will be trained on and when it will be built, the rough combination of algorithms we'll use, the unsolved problems and the path to solving them, the list of people that will matter. I can see it. It is extremely visceral. Sure, going all-in leveraged long Nvidia in early 2023 has been great and all, but the burdens of history are heavy. I would not choose this.

But the scariest realization is that there is no crack team coming to handle this. As a kid you have this glorified view of the world, that when things get real there are the heroic scientists, the uber-competent military men, the calm leaders who are on it, who will save the day. It is not so. The world is incredibly small; when the facade comes off, it's usually just a few folks behind the scenes who are the live players, who are desperately trying to keep things from falling apart.

Right now, there's perhaps a few hundred people in the world who realize what's about to hit us, who understand just how crazy things are about to get, who have situational awareness. I probably either personally know or am one degree of separation from everyone who could plausibly run The Project. The few folks behind the scenes who are desperately trying to keep things from falling apart are you and your buddies and their buddies. That's it. That's all there is.

Someday it will be out of our hands. But right now, at least for the next few years of midgame, the fate of the world rests on these people.

Will the free world prevail?

Will we tame superintelligence, or will it tame us?

Will humanity skirt self-destruction once more?

The stakes are no less.

These are great and honorable people. But they are just people. Soon, the AIs will be running the world, but we're in for one last rodeo. May their final stewardship bring honor to mankind.

结语：深思后的忧虑

如果我们的预测是正确的？

本文将讨论：

通用人工智能的现实主义态度

如果我们的预测成真

」直到今天我仍然清晰地记得 1941 年的春天。那时我意识到核弹不仅是可能的 —— 而且是不可避免的。迟早这些想法不会只是我们独有的。很快每个人都会想到这些，某个国家就会付诸行动。[...]

而且没有人可以讨论这件事，我经历了无数个不眠之夜。但我确实意识到这可能会多么严重。从那时起我不得不开始服用安眠药。这是唯一的办法，从那以后我就再也没有停过。28 年过去了，在这 28 年里我想我没有错过一个晚上。"

—— 詹姆斯·查德威克（诺贝尔物理学奖得主，1941 年英国政府关于原子弹不可避免性报告的作者，该报告最终推动了曼哈顿计划的启动）

在这个十年结束之前，我们将建造出超级智能。这就是本系列文章的主要内容。对于我在旧金山认识的大多数人来说，他们的想象到此为止。但接下来的十年 ——2030 年代 —— 将至少同样精彩。到那时，世界将彻底改变，变得面目全非。一个新的世界秩序将会形成。但这是另一个故事了。

现在，我们必须暂告一段落。让我做几点最后的说明。

关于通用人工智能的现实主义态度

这些都需要深入思考 —— 但很多人做不到。"深度学习正在遇到瓶颈！」他们每年都这样宣称。专家们信誓旦旦地说这不过是另一次科技泡沫。但即使在旧金山这个核心地带，讨论已经在两种根本不严肃的口号之间变得两极分化。

一方是悲观主义者。他们多年来一直关注通用人工智能的发展，我非常钦佩他们的远见。然而，他们的思维方式已经固化，与深度学习（Deep Learning）的实际发展脱节。他们提出的解决方案过于理想化且难以实施，更重要的是，他们忽视了真实存在的威权主义威胁。他们声称人工智能有 99% 的概率导致人类毁灭，呼吁无限期暂停 AI 研究 —— 这显然不是明智之举。

另一方是有效加速主义者（Effective Accelerationists，e/acc）。从某些具体观点来看，他们说得有道理：AI 的发展确实不能停止。但在他们在 Twitter 上发表的浅薄言论背后，他们的本质令人失望；这些外行人只想借助 AI 热潮创建一些表面性的创业项目，而不愿意真正面对通用人工智能带来的深层挑战。他们声称自己是美国自由价值的捍卫者，却经不住某些独裁政权资金的诱惑。实际上，他们才是真正的发展阻碍者。在否认 AI 风险的过程中，他们实际上否定了通用人工智能的重要性；在他们看来，我们最终只会得到一些高级的聊天机器人，这当然构不成什么威胁。（就加速主义而言，这种观点未免过于保守。）

然而根据我的观察，该领域最具洞察力的专家们已经形成了一个不同的观点，一种中间路线，我称之为「通用人工智能现实主义」（AGI Realism）。其核心理念非常简明：

超级智能是关乎国家安全的战略性议题。我们正在快速开发出智能水平超越人类顶尖智慧的机器系统。这不是硅谷的又一次技术热潮，不是某个松散的开发者社区在开发普通的开源软件，更不是简单的技术游戏。超级智能将具有颠覆性的力量，它可能成为人类历史上最强大的技术武器。对于所有参与其中的人来说，这将是我们一生中最重要的事业。

美国必须保持领先地位。如果其他国家率先开发出通用人工智能，将严重威胁全球的民主自由价值。（实际上，美国的领导地位也是确保 AI 安全发展的唯一途径。）这意味着我们不能简单地「暂停」发展；相反，我们需要迅速提升国家的基础设施能力，在美国本土建立通用人工智能的研发中心。这也意味着，我们不能容忍任何可能危及国家安全的松懈行为，核心的通用人工智能基础设施必须置于合适的监管之下。美国的人工智能研究机构必须将国家战略利益放在首位。

我们必须确保万无一失。认识到超级智能的力量，同时也意味着要认识到它带来的潜在风险。这些安全隐患都是真实存在的；如果处理不当，后果将是灾难性的 —— 无论是人类滥用这种强大的力量导致自我毁灭，还是因为我们创造出的这种全新的智能形态超出了我们当前的控制能力。这些挑战虽然可以应对，但绝不能采取草率的态度。要成功驾驭这些风险，需要有识之士以前所未有的谨慎和严肃态度来处理。

随着技术发展的加速，相关讨论可能会变得更加激烈。但我最大的希望是，会有人真正意识到这项技术即将带来的深远影响，并将应对这一挑战视为一种历史使命。

如果我们的预测是正确的呢？

此时此刻，你可能会认为我和其他硅谷技术圈的人都疯了。但请思考一下：如果我们是对的呢？这些人正是发明和开发这项技术的专家；他们相信通用人工智能将在这十年内问世；虽然他们的具体观点各不相同，但很多人都认为，通往超级智能的发展道路很可能会按照我在本系列文章中描述的方式展开。

毫无疑问，我对未来发展的预测中一定存在一些重要的偏差；如果现实真的朝着如此剧烈变革的方向发展，其不确定性必然会非常大。而且，正如我在开篇所说，我认为未来存在着多种可能性。但我认为提出具体的预测场景很有必要。在这个系列文章中，我阐述了我认为最有可能在这个十年剩余时间内发生的情景 —— 就是我们正在经历的这个十年。

因为这一切正在变得真实，无比真实。就在几年前，虽然我就已经认真思考这些问题，但它们仍然显得抽象而遥远，仅存在于各种模型分析和概率计算中。而现在，这种感觉变得异常清晰。我能够预见到通用人工智能将如何实现。这不再是简单地估算人脑容量、作理论假设或推演可能性 —— 我几乎可以具体指出通用人工智能将在哪些计算集群上训练，何时能够建成，我们将采用什么样的算法组合，还存在哪些待解决的问题及其解决方案，以及哪些关键人物将参与其中。这一切都清晰可见。诚然，在 2023 年初果断投资 Nvidia 股票获得了丰厚回报，但历史赋予我们的使命让人感到沉重。如果有选择的话，我宁愿不必承担这样的重任。

但最令人不安的认识是：并没有一支精英团队在等待接管这项任务。在我们年少时，总是对世界抱有一种理想化的想象，认为当重大危机来临时，总会有杰出的科学家、经验丰富的军事专家和镇定自若的领导者挺身而出，拯救世界于水火。然而现实并非如此。这个领域的圈子其实很小；当你掀开表象，往往只有少数几个人在幕后默默工作，竭尽全力维持着局面的稳定。

当前，全球可能只有几百人真正意识到即将到来的剧变，理解事态将如何天翻地覆，具备这种战略层面的洞察力。我可能直接认识，或者最多通过一个中间人就能接触到所有可能主导这个项目的关键人物。那些在幕后努力维持局面稳定的少数人，不过就是你和你的同仁们，以及他们的合作伙伴。就这些人而已。这就是全部了。

终有一天，这种局面将超出人类的掌控范围。但就目前而言，至少在这场转折的关键几年里，世界的命运就掌握在这些人手中。

自由世界能否最终胜出？

我们能驾驭超级智能，还是会被超级智能所驾驭？

人类能否再一次避免自我毁灭的命运？

这些都是人类面临的终极考验。

这些都是杰出而崇高的人物。但他们终究也是普通人。很快，人工智能就将主导这个世界的运转，但在此之前，我们还要进行最后的历史性抉择。愿他们这最后的守护者身份为人类带来新的荣光。