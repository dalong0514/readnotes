Leopold Aschenbrenner.(2024).2024109Situational-Awareness-The-Decade-Ahead.Dwarkesh podcast => I. From GPT-4 to AGI: Counting the OOMs

## II. From AGI to Superintelligence: the Intelligence Explosion

AI progress won't stop at human-level. Hundreds of millions of AGIs could automate AI research, compressing a decade of algorithmic progress (5+ OOMs) into ≤1 year. We would rapidly go from human-level to vastly superhuman AI systems. The power—and the peril—of superintelligence would be dramatic.

In this piece:

Automating AI research

Possible bottlenecks

The power of superintelligence

Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,' and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make.

I. J. Good (1965)

The Bomb and The Super

In the common imagination, the Cold War's terrors principally trace back to Los Alamos, with the invention of the atomic bomb. But The Bomb, alone, is perhaps overrated. Going from The Bomb to The Super—hydrogen bombs—was arguably just as important.

In the Tokyo air raids, hundreds of bombers dropped thousands of tons of conventional bombs on the city. Later that year, Little Boy, dropped on Hiroshima, unleashed similar destructive power in a single device. But just 7 years later, Teller's hydrogen bomb multiplied yields a thousand-fold once again—a single bomb with more explosive power than all of the bombs dropped in the entirety of WWII combined.

The Bomb was a more efficient bombing campaign. The Super was a country-annihilating device.

So it will be with AGI and Superintelligence.

AI progress won't stop at human-level. After initially learning from the best human games, AlphaGo started playing against itself—and it quickly became superhuman, playing extremely creative and complex moves that a human would never have come up with.

We discussed the path to AGI in the previous piece. Once we get AGI, we'll turn the crank one more time—or two or three more times—and AI systems will become superhuman—vastly superhuman. They will become qualitatively smarter than you or I, much smarter, perhaps similar to how you or I are qualitatively smarter than an elementary schooler.

The jump to superintelligence would be wild enough at the current rapid but continuous rate of AI progress (if we could make the jump to AGI in 4 years from GPT-4, what might another 4 or 8 years after that bring?). But it could be much faster than that, if AGI automates AI research itself.

Once we get AGI, we won't just have one AGI. I'll walk through the numbers later, but: given inference GPU fleets by then, we'll likely be able to run many millions of them (perhaps 100 million human-equivalents, and soon after at 10x+ human speed). Even if they can't yet walk around the office or make coffee, they will be able to do ML research on a computer. Rather than a few hundred researchers and engineers at a leading AI lab, we'd have more than 100,000x that—furiously working on algorithmic breakthroughs, day and night. Yes, recursive self-improvement, but no sci-fi required; they would need only to accelerate the existing trendlines of algorithmic progress (currently at ~0.5 OOMs/year).

Automated AI research could accelerate algorithmic progress, leading to 5+ OOMs of effective compute gains in a year. The AI systems we'd have by the end of an intelligence explosion would be vastly smarter than humans.

Automated AI research could probably compress a human-decade of algorithmic progress into less than a year (and that seems conservative). That'd be 5+ OOMs, another GPT-2-to-GPT-4-sized jump, on top of AGI—a qualitative jump like that from a preschooler to a smart high schooler, on top of AI systems already as smart as expert AI researchers/engineers.

There are several plausible bottlenecks—including limited compute for experiments, complementarities with humans, and algorithmic progress becoming harder—which I'll address, but none seem sufficient to definitively slow things down.

Before we know it, we would have superintelligence on our hands—AI systems vastly smarter than humans, capable of novel, creative, complicated behavior we couldn't even begin to understand—perhaps even a small civilization of billions of them. Their power would be vast, too. Applying superintelligence to R&D in other fields, explosive progress would broaden from just ML research; soon they'd solve robotics, make dramatic leaps across other fields of science and technology within years, and an industrial explosion would follow. Superintelligence would likely provide a decisive military advantage, and unfold untold powers of destruction. We will be faced with one of the most intense and volatile moments of human history.

从通用人工智能到超级智能：智能爆炸

AI 的发展不会止步于人类水平。数以亿计的通用人工智能（AGI）可以实现 AI 研究的自动化，将需要十年才能实现的算法进步（提升超过 5 个数量级）压缩到不到一年时间内完成。我们将迅速从具备人类水平的 AI，跃升至远超人类的 AI 系统。超级智能带来的力量 —— 以及威胁 —— 将是前所未有的。

本文包含：

自动化 AI 研究

可能的瓶颈

超级智能的力量

超级智能机器的定义是：一台能在所有智力活动上远远超越任何人类的机器，不论这个人有多聪明。因为机器设计本身就是智力活动之一，所以超级智能机器必然能设计出更优秀的机器。这无疑会引发「智能爆炸」，使人类的智能远远落后。因此，第一台超级智能机器将是人类需要制造的最后一项发明。

—— I. J. Good（1965)

原子弹和氢弹

在普通人的想象中，冷战的恐怖主要源于洛斯阿拉莫斯实验室研制出的原子弹。但仅仅是原子弹，其影响或许被夸大了。可以说，从原子弹到氢弹的跨越同样具有重要意义。

在东京空袭中，数百架轰炸机向城市投下了数千吨常规炸弹。同年晚些时候，投放在广岛的原子弹「Little Boy」仅用一个装置就释放了相当的破坏力。但仅仅 7 年后，物理学家 Teller 研制的氢弹让破坏力再次提升了千倍 —— 仅一颗氢弹的爆炸威力就超过了整个二战期间投下的所有炸弹的总和。

原子弹代表了更高效的轰炸手段，而氢弹则是能够毁灭整个国家的武器。

通用人工智能和超级智能的发展轨迹也将遵循类似的模式。

AI 的发展不会止步于人类水平。以围棋 AI 系统 AlphaGo 为例，它最初通过学习顶尖棋手的对局提升实力，之后开始进行自我对弈 —— 很快就超越了人类，能够下出极具创造性和复杂性的棋步，这些招法是人类棋手从未设想过的。

我们在上一篇文章中探讨了实现通用人工智能的路径。一旦我们开发出通用人工智能，通过进一步的迭代升级，AI 系统将达到超越人类的水平 —— 远远超越人类。它们在本质上将比你我更加智慧，这种差距可能就像成年人与小学生之间的智力差距一样显著。

即使按照当前 AI 快速但平稳的发展速度，达到超级智能的飞跃也将是惊人的。(想想看：如果我们能在 GPT-4 之后的 4 年内实现通用人工智能，那么再过 4 年或 8 年会发生什么？）但如果通用人工智能能够实现 AI 研究的自动化，这个过程可能会更快。

一旦我们实现了通用人工智能，我们不会仅仅停留在一个系统上。我稍后会详细解释具体数据，但是：考虑到那时的 AI 推理专用 GPU 集群规模，我们很可能能够同时运行数百万个通用人工智能系统（相当于 1 亿个人类的智力水平，而且很快能达到人类思维速度的 10 倍以上）。即使这些系统还不能在办公室里走动或煮咖啡，它们也完全可以在计算机上进行机器学习（Machine Learning，ML）研究。相比当前顶尖 AI 实验室仅有的几百名研究人员和工程师，我们将拥有超过 10 万倍的研究力量 —— 它们将昼夜不停地致力于突破算法瓶颈。没错，这就是递归式自我提升（AI 系统不断改进自身）的过程，这并非科幻小说：它们只需要加速当前算法进步的趋势（目前每年大约提升 3 倍左右）。

通过自动化的 AI 研究，算法性能将得到显著提升，在短短一年内就能实现相当于提升千万倍的计算效率。经过这样的智能爆炸，最终诞生的 AI 系统将远远超越人类的智慧水平。

自动化的 AI 研究很可能将人类需要十年才能实现的算法进步压缩到不到一年的时间内（这个预测可能还是比较保守的）。这意味着在已经达到通用人工智能水平的基础上，还能实现相当于从 GPT-2 到 GPT-4 那样跨越性的进步，带来超过 10 万倍的性能提升。这种飞跃就像一个孩子从幼儿园直接跃升到优秀高中生的水平，而且是建立在已经具备专业 AI 研究人员和工程师智能水平的系统基础之上。

当然，这个发展过程中存在一些潜在的制约因素 —— 比如实验所需的计算资源限制、需要与人类协同配合、以及算法突破的难度不断提高等。我会在后文讨论这些问题，但目前看来这些因素都不足以显著减缓发展速度。

在我们意识到之前，超级智能就将出现在我们面前 —— 这些远超人类智慧的 AI 系统能够展现出超出我们理解范围的创新性和复杂性行为。它们可能会形成一个由数十亿智能体组成的庞大网络。它们所掌握的力量将是无比巨大的。当超级智能开始投入其他领域的研发时，爆炸性的进步将不再局限于机器学习研究领域：它们将很快攻克机器人技术的难题，在各个科技领域实现突破性进展，随之带来新一轮产业革命。超级智能系统可能会让拥有它的一方获得压倒性的军事优势，同时也可能释放出难以想象的破坏力。

这将是人类历史上最具挑战性和不确定性的时刻之一。

### Automating AI research

We don't need to automate everything—just AI research. A common objection to transformative impacts of AGI is that it will be hard for AI to do everything. Look at robotics, for instance, doubters say; that will be a gnarly problem, even if AI is cognitively at the levels of PhDs. Or take automating biology R&D, which might require lots of physical lab-work and human experiments.

But we don't need robotics—we don't need many things—for AI to automate AI research. The jobs of AI researchers and engineers at leading labs can be done fully virtually and don't run into real-world bottlenecks in the same way (though it will still be limited by compute, which I'll address later). And the job of an AI researcher is fairly straightforward, in the grand scheme of things: read ML literature and come up with new questions or ideas, implement experiments to test those ideas, interpret the results, and repeat. This all seems squarely in the domain where simple extrapolations of current AI capabilities could easily take us to or beyond the levels of the best humans by the end of 2027.

It's worth emphasizing just how straightforward and hacky some of the biggest machine learning breakthroughs of the last decade have been: "oh, just add some normalization" (LayerNorm/BatchNorm) or "do f(x)+x instead of f(x)" (residual connections)" or "fix an implementation bug" (Kaplan → Chinchilla scaling laws). AI research can be automated. And automating AI research is all it takes to kick off extraordinary feedback loops.

We'd be able to run millions of copies (and soon at 10x+ human speed) of the automated AI researchers. Even by 2027, we should expect GPU fleets in the 10s of millions. Training clusters alone should be approaching ~3 OOMs larger, already putting us at 10 million+ A100-equivalents. Inference fleets should be much larger still. (More on all this in a later piece.)

That would let us run many millions of copies of our automated AI researchers, perhaps 100 million human-researcher-equivalents, running day and night. There's some assumptions that flow into the exact numbers, including that humans "think" at 100 tokens/minute (just a rough order of magnitude estimate, e.g. consider your internal monologue) and extrapolating historical trends and Chinchilla scaling laws on per-token inference costs for frontier models remaining in the same ballpark. We'd also want to reserve some of the GPUs for running experiments and training new models. Full calculation in a footnote.

Another way of thinking about it is that given inference fleets in 2027, we should be able to generate an entire internet's worth of tokens, every single day. In any case, the exact numbers don't matter that much, beyond a simple plausibility demonstration.

Moreover, our automated AI researchers may soon be able to run at much faster than human-speed:

By taking some inference penalties, we can trade off running fewer copies in exchange for running them at faster serial speed. (For example, we could go from ~5x human speed to ~100x human speed by "only" running 1 million copies of the automated researchers.)

More importantly, the first algorithmic innovation the automated AI researchers work on is getting a 10x or 100x speedup. Gemini 1.5 Flash is ~10x faster than the originally-released GPT-4, merely a year later, while providing similar performance to the originally-released GPT-4 on reasoning benchmarks. If that's the algorithmic speedup a few hundred human researchers can find in a year, the automated AI researchers will be able to find similar wins very quickly.

That is: expect 100 million automated researchers each working at 100x human speed not long after we begin to be able to automate AI research. They'll each be able to do a year's worth of work in a few days. The increase in research effort—compared to a few hundred puny human researchers at a leading AI lab today, working at a puny 1x human speed—will be extraordinary.

This could easily dramatically accelerate existing trends of algorithmic progress, compressing a decade of advances into a year. We need not postulate anything totally novel for automated AI research to intensely speed up AI progress. Walking through the numbers in the previous piece, we saw that algorithmic progress has been a central driver of deep learning progress in the last decade; we noted a trendline of ~0.5 OOMs/year on algorithmic efficiencies alone, with additional large algorithmic gains from unhobbling on top. (I think the import of algorithmic progress has been underrated by many, and properly appreciating it is important for appreciating the possibility of an intelligence explosion.)

Could our millions of automated AI researchers (soon working at 10x or 100x human speed) compress the algorithmic progress human researchers would have found in a decade into a year instead? That would be 5+ OOMs in a year.

Don't just imagine 100 million junior software engineer interns here (we'll get those earlier, in the next couple years!). Real automated AI researchers will be very smart—and in addition to their raw quantitative advantage, automated AI researchers will have other enormous advantages over human researchers:

They'll be able to read every single ML paper ever written, have been able to deeply think about every single previous experiment ever run at the lab, learn in parallel from each of their copies, and rapidly accumulate the equivalent of millennia of experience. They'll be able to develop far deeper intuitions about ML than any human.

They'll be easily able to write millions of lines of complex code, keep the entire codebase in context, and spend human-decades (or more) checking and rechecking every line of code for bugs and optimizations. They'll be superbly competent at all parts of the job.

You won't have to individually train up each automated AI researcher (indeed, training and onboarding 100 million new human hires would be difficult). Instead, you can just teach and onboard one of them—and then make replicas. (And you won't have to worry about politicking, cultural acclimation, and so on, and they'll work with peak energy and focus day and night.)

Vast numbers of automated AI researchers will be able to share context (perhaps even accessing each others' latent space and so on), enabling much more efficient collaboration and coordination compared to human researchers.

And of course, however smart our initial automated AI researchers would be, we'd soon be able to make further OOM-jumps, producing even smarter models, even more capable at automated AI research.

Imagine an automated Alec Radford—imagine 100 million automated Alec Radfords. I think just about every researcher at OpenAI would agree that if they had 10 Alec Radfords, let alone 100 or 1,000 or 1 million running at 10x or 100x human speed, they could very quickly solve very many of their problems. Even with various other bottlenecks (more in a moment), compressing a decade of algorithmic progress into a year as a result seems very plausible. (A 10x acceleration from a million times more research effort, which seems conservative if anything.)

That would be 5+ OOMs right there. 5 OOMs of algorithmic wins would be a similar scaleup to what produced the GPT-2-to-GPT-4 jump, a capability jump from ~a preschooler to ~a smart high schooler. Imagine such a qualitative jump on top of AGI, on top of Alec Radford.

It's strikingly plausible we'd go from AGI to superintelligence very quickly, perhaps in less than one year.

Possible bottlenecks

While this basic story is surprisingly strong—and is supported by thorough economic modeling work—there are some real and plausible bottlenecks that will probably slow down an automated-AI-research intelligence explosion.

I'll give a summary here, and then discuss these in more detail in the optional sections below for those interested:

Limited compute: AI research doesn't just take good ideas, thinking, or math—but running experiments to get empirical signal on your ideas. A million times more research effort via automated research labor won't mean a million times faster progress, because compute will still be limited—and limited compute for experiments will be the bottleneck. Still, even if this won't be a 1,000,000x speedup, I find it hard to imagine that the automated AI researchers couldn't use the compute at least 10x more effectively: they'll be able to get incredible ML intuition (having internalized the whole ML literature and every previous experiment every run!) and centuries-equivalent of thinking-time to figure out exactly the right experiment to run, configure it optimally, and get the maximum value of information; they'll be able to spend centuries-equivalent of engineer-time before running even tiny experiments to avoid bugs and get them right on the first try; they can make tradeoffs to economize on compute by focusing on the biggest wins; and they'll be able to try tons of smaller-scale experiments (and given effective compute scaleups by then, "smaller-scale" means being able to train 100,000 GPT-4-level models in a year to try architecture breakthroughs). Some human researchers and engineers are able to produce 10x the progress as others, even with the same amount of compute—and this should apply even moreso to automated AI researchers. I do think this is the most important bottleneck, and I address it in more depth below.

Complementarities/long tail: A classic lesson from economics (cf Baumol's growth disease) is that if you can automate, say, 70% of something, you get some gains but quickly the remaining 30% become your bottleneck. For anything that falls short of full automation—say, really good copilots—human AI researchers would remain a major bottleneck, making the overall increase in the rate of algorithmic progress relatively small. Moreover, there's likely some long tail of capabilities required for automating AI research—the last 10% of the job of an AI researcher might be particularly hard to automate. This could soften takeoff some, though my best guess is that this only delays things by a couple years. Perhaps 2026/27-models speed are the proto-automated-researcher, it takes another year or two for some final unhobbling, a somewhat better model, inference speedups, and working out kinks to get to full automation, and finally by 2028 we get the 10x acceleration (and superintelligence by the end of the decade).

Inherent limits to algorithmic progress: Maybe another 5 OOMs of algorithmic efficiency will be fundamentally impossible? I doubt it. While there will definitely be upper limits, if we got 5 OOMs in the last decade, we should probably expect at least another decade's-worth of progress to be possible. More directly, current architectures and training algorithms are still very rudimentary, and it seems that much more efficient schemes should be possible. Biological reference classes also support dramatically more efficient algorithms being plausible.

Ideas get harder to find, so the automated AI researchers will merely sustain, rather than accelerate, the current rate of progress: One objection is that although automated research would increase effective research effort a lot, ideas also get harder to find. That is, while it takes only a few hundred top researchers at a lab to sustain 0.5 OOMs/year today, as we exhaust the low-hanging fruit, it will take more and more effort to sustain that progress—and so the 100 million automated researchers will be merely what's necessary to sustain progress. I think this basic model is correct, but the empirics don't add up: the magnitude of the increase in research effort—a million-fold—is way, way larger than the historical trends of the growth in research effort that's been necessary to sustain progress. In econ modeling terms, it's a bizarre "knife-edge assumption" to assume that the increase in research effort from automation will be just enough to keep progress constant.

Ideas get harder to find and there are diminishing returns, so the intelligence explosion will quickly fizzle: Related to the above objection, even if the automated AI researchers lead to an initial burst of progress, whether rapid progress can be sustained depends on the shape of the diminishing returns curve to algorithmic progress. Again, my best read of the empirical evidence is that the exponents shake out in favor of explosive/accelerating progress. In any case, the sheer size of the one-time boost—from 100s to 100s of millions of AI researchers—probably overcomes diminishing returns here for at least a good number of OOMs of algorithmic progress, even though it of course can't be indefinitely self-sustaining.

Limited compute for experiments

Complementarities and long tails to 100% automation

Fundamental limits to algorithmic progress

Ideas get harder to find and diminishing returns

Overall, these factors may slow things down somewhat: the most extreme versions of intelligence explosion (say, overnight) seem implausible. And they may result in a somewhat longer runup (perhaps we need to wait an extra year or two from more sluggish, proto-automated researchers to the true automated Alec Radfords, before things kick off in full force). But they certainly don't rule out a very rapid intelligence explosion. A year—or at most just a few years, but perhaps even just a few months—in which we go from fully-automated AI researchers to vastly superhuman AI systems should be our mainline expectation.

The power of superintelligence

Whether or not you agree with the strongest form of these arguments—whether we get a <1 year intelligence explosion, or it takes a few years—it is clear: we must confront the possibility of superintelligence.

The AI systems we'll likely have by the end of this decade will be unimaginably powerful.

Of course, they'll be quantitatively superhuman. On our fleets of 100s of millions of GPUs by the end of the decade, we'll be able to run a civilization of billions of them, and they will be able to "think" orders of magnitude faster than humans. They'll be able to quickly master any domain, write trillions of lines of code, read every research paper in every scientific field ever written (they'll be perfectly interdisciplinary!) and write new ones before you've gotten past the abstract of one, learn from the parallel experience of every one of its of copies, gain billions of human-equivalent years of experience with some new innovation in a matter of weeks, work 100% of the time with peak energy and focus and won't be slowed down by that one teammate who is lagging, and so on.

More importantly—but harder to imagine—they'll be qualitatively superhuman. As a narrow example of this, large-scale RL runs have been able to produce completely novel and creative behaviors beyond human understanding, such as the famous move 37 in AlphaGo vs. Lee Sedol. Superintelligence will be this across many domains. It'll find exploits in the human code too subtle for any human to notice, and it'll generate code too complicated for any human to understand even if the model spent decades trying to explain it. Extremely difficult scientific and technological problems that a human would be stuck on for decades will seem just so obvious to them. We'll be like high-schoolers stuck on Newtonian physics while it's off exploring quantum mechanics.

As a visualization of how wild this could be, look at some Youtube videos of video game speedruns, such as this one of beating Minecraft in 20 seconds.

Beating Minecraft in 20 seconds. (If you have no idea what's going on in this video, you're in good company; even most normal players of Minecraft have almost no clue what's going on.)

Now imagine this applied to all domains of science, technology, and the economy. The error bars here, of course, are extremely large. Still, it's important to consider just how consequential this would be.

What does it feel like to stand here? Illustration from Wait But Why/Tim Urban.

In the intelligence explosion, explosive progress was initially only in the narrow domain of automated AI research. As we get superintelligence, and apply our billions of (now superintelligent) agents to R&D across many fields, I expect explosive progress to broaden:

An AI capabilities explosion. Perhaps our initial AGIs had limitations that prevented them fully automating work in some other domains (rather than just in the AI research domain); automated AI research will quickly solve these, enabling automation of any and all cognitive work.

Solve robotics. Superintelligence won't stay purely cognitive for long. Getting robotics to work well is primarily an ML algorithms problem (rather than a hardware problem), and our automated AI researchers will likely be able to solve it (more below). Factories would go from human-run, to AI-directed using human physical labor, to soon being fully run by swarms of robots.

Dramatically accelerate scientific and technological progress. Yes, Einstein alone couldn't develop neuroscience and build a semiconductor industry, but a billion superintelligent automated scientists, engineers, technologists, and robot technicians (with the robots moving at 10x or more human speed!) would make extraordinary advances in many fields in the space of years. (Here's a nice short story visualizing what AI-driven R&D might look like.) The billion superintelligences would be able to compress the R&D effort humans researchers would have done in the next century into years. Imagine if the technological progress of the 20th century were compressed into less than a decade. We would have gone from flying being thought a mirage, to airplanes, to a man on the moon and ICBMs in a matter of years. This is what I expect the 2030s to look like across science and technology.

An industrial and economic explosion. Extremely accelerated technological progress, combined with the ability to automate all human labor, could dramatically accelerate economic growth (think: self-replicating robot factories quickly covering all of the Nevada desert). The increase in growth probably wouldn't just be from 2%/year to 2.5%/year; rather, this would be a fundamental shift in the growth regime, more comparable to the historical step-change from very slow growth to a couple percent a year with the industrial revolution. We could see economic growth rates of 30%/year and beyond, quite possibly multiple doublings a year. This follows fairly straightforwardly from economists' models of economic growth. To be sure, this may well be delayed by societal frictions; arcane regulation might ensure lawyers and doctors still need to be human, even if AI systems were much better at those jobs; surely sand will be thrown into the gears of rapidly expanding robo-factories as society resists the pace of change; and perhaps we'll want to retain human nannies; all of which would slow the growth of the overall GDP statistics. Still, in whatever domains we remove human-created barriers (e.g., competition might force us to do so for military production), we'd see an industrial explosion.

Growth mode	Date began

to dominate	Doubling time of

the global economy

(years)

Hunting	2,000,000 B.C.	230,000

Farming	4700 B.C.	860

Science and commerce	1730 A.D.	58

Industry	1903 A.D.	15

Superintelligence?	2030 A.D.?	???

A shift in the growth regime is not unprecedented: as civilization went from hunting, to farming, to the blossoming of science and commerce, to industry, the pace of global economic growth accelerated. Superintelligence could kick off another shift in growth mode. Based on Robin Hanson's "Long-run growth as a sequence of exponential modes".

Provide a decisive and overwhelming military advantage. Even early cognitive superintelligence might be enough here; perhaps some superhuman hacking scheme can deactivate adversary militaries. In any case, military power and technological progress has been tightly linked historically, and with extraordinarily rapid technological progress will come concomitant military revolutions. The drone swarms and roboarmies will be a big deal, but they are just the beginning; we should expect completely new kinds of weapons, from novel WMDs to invulnerable laser-based missile defense to things we can't yet fathom. Compared to pre-superintelligence arsenals, it'll be like 21st century militaries fighting a 19th century brigade of horses and bayonets. (I discuss how superintelligence could lead to a decisive military advantage in a later piece.)

Be able to overthrow the US government. Whoever controls superintelligence will quite possibly have enough power to seize control from pre-superintelligence forces. Even without robots, the small civilization of superintelligences would be able to hack any undefended military, election, television, etc. system, cunningly persuade generals and electorates, economically outcompete nation-states, design new synthetic bioweapons and then pay a human in bitcoin to synthesize it, and so on. In the early 1500s, Cortes and about 500 Spaniards conquered the Aztec empire of several million; Pizarro and ~300 Spaniards conquered the Inca empire of several million; Alfonso and ~1000 Portuguese conquered the Indian Ocean. They didn't have god-like power, but the Old World's technological edge and an advantage in strategic and diplomatic cunning led to an utterly decisive advantage. Superintelligence might look similar.

Robots

Explosive growth starts in the narrower domain of AI R&D; as we apply superintelligence to R&D in other fields, explosive growth will broaden.

How all of this plays out over the 2030s is hard to predict (and a story for another time). But one thing, at least, is clear: we will be rapidly plunged into the most extreme situation humanity has ever faced.

Human-level AI systems, AGI, would be highly consequential in their own right—but in some sense, they would simply be a more efficient version of what we already know. But, very plausibly, within just a year, we would transition to much more alien systems, systems whose understanding and abilities—whose raw power—would exceed those even of humanity combined. There is a real possibility that we will lose control, as we are forced to hand off trust to AI systems during this rapid transition.

More generally, everything will just start happening incredibly fast. And the world will start going insane. Suppose we had gone through the geopolitical fever-pitches and man-made perils of the 20th century in mere years; that is the sort of situation we should expect post-superintelligence. By the end of it, superintelligent AI systems will be running our military and economy. During all of this insanity, we'd have extremely scarce time to make the right decisions. The challenges will be immense. It will take everything we've got to make it through in one piece.

The intelligence explosion and the immediate post-superintelligence period will be one of the most volatile, tense, dangerous, and wildest periods ever in human history.

And by the end of the decade, we'll likely be in the midst of it.

Confronting the possibility of an intelligence explosion—the emergence of superintelligence—often echoes the early debates around the possibility of a nuclear chain reaction—and the atomic bomb it would enable. HG Wells predicted the atomic bomb in a 1914 novel. When Szilard first conceived of the idea of a chain reaction in 1933, he couldn't convince anyone of it; it was pure theory. Once fission was empirically discovered in 1938, Szilard freaked out again and argued strongly for secrecy, and a few people started to wake up to the possibility of a bomb. Einstein hadn't considered the possibility of a chain reaction, but when Szilard confronted him, he was quick to see the implications and willing to do anything that was needed to be done; he was willing to sound the alarm, and wasn't afraid of sounding foolish. But Fermi, Bohr, and most scientists thought the "conservative" thing was to play it down, rather than take seriously the extraordinary implications of the possibility of a bomb. Secrecy (to avoid sharing their advances with the Germans) and other all-out efforts seemed absurd to them. A chain reaction sounded too crazy. (Even when, as it turned out, a bomb was but half a decade from becoming reality.)

We must once again confront the possibility of a chain reaction. Perhaps it sounds speculative to you. But among senior scientists at AI labs, many see a rapid intelligence explosion as strikingly plausible. They can see it. Superintelligence is possible.

Next post in series:

III. The Challenges – IIIa. Racing to the Trillion-Dollar Cluster

And much of the Cold War's perversities (cf Daniel Ellsberg's book) stemmed from merely replacing A-bombs with H-bombs, without adjusting nuclear policy and war plans to the massive capability increase.↩

The job of an AI researcher is also a job that AI researchers at AI labs just, well, know really well—so it'll be particularly intuitive to them to optimize models to be good at that job. And there will be huge incentives to do so to help them accelerate their research and their labs' competitive edge.↩

This suggests an important point in terms of the sequencing of risks from AI, by the way. A common AI threat model people point to is AI systems developing novel bioweapons, and that posing catastrophic risk. But if AI research is more straightforward to automate than biology R&D, we might get an intelligence explosion before we get extreme AI biothreats. This matters, for example, with regard to whether we should expect "bio warning shots" in time before things get crazy on AI.↩

As noted earlier, the GPT-4 API costs less today than GPT-3 when it was released—this suggests that the trend of inference efficiency wins is fast enough to keep inference costs roughly constant even as models get much more powerful. Similarly, there have been huge inference cost wins in just the year since GPT-4 was released; for example, the current version of Gemini 1.5 Pro outperforms the original GPT-4, while being roughly 10x cheaper.

We can also ground this somewhat more by considering Chinchilla scaling laws. On Chinchilla scaling laws, model size—and thus inference costs—grow with the square root of training cost, i.e. half the OOMs of the OOM scaleup of effective compute. However, in the previous piece, I suggested that algorithmic efficiency was advancing at roughly the same pace as compute scaleup, i.e. it made up roughly half of the OOMs of effective compute scaleup. If these algorithmic wins also translate into inference efficiency, that means that the algorithmic efficiencies would compensate for the naive increase in inference cost.

In practice, training compute efficiencies often, but not always, translate into inference efficiency wins. However, there are also separately many inference efficiency wins that are not training efficiency wins. So, at least in terms of the rough ballpark, assuming the $/token of frontier models stays roughly similar doesn't seem crazy.

(Of course, they'll use more tokens, i.e. more test-time compute. But that's already part of the calculation here, by pricing human-equivalents as 100 tokens/minute.)↩

GPT-4 Turbo is about $0.03/1K tokens. We supposed we would have 10s of millions of A100 equivalents, which cost ~$1 hour per GPU if A100-equivalents. If we used the API costs to translate GPUs into tokens generated, that implies 10s of millions GPUs * $1/GPU-hour * 33K tokens/$ = ~ one trillion tokens/ hour. Suppose a human does 100 tokens/min of thinking, that means a human-equivalent is 6,000 tokens/hour. One trillion tokens/hour divided by 6,000 tokens/human-hour = ~200 million human-equivalents—i.e. as if running 200 million human researchers, day and night. (And even if we reserve half the GPUs for experiment compute, we get 100 million human-researcher-equivalents.)↩

The previous footnote estimated ~1T tokens/hour, i.e. 24T tokens a day. In the previous piece, I noted that a public deduplicated CommonCrawl had around 30T tokens.↩

Jacob Steinhardt estimates that k^3 parallel copies of a model can be replaced with a single model that is k^2 faster, given some math on inference tradeoffs with a tiling scheme (that theoretically works even for k of 100 or more). Suppose initial speeds were already ~5x human speed (based on, say, GPT-4 speed on release). Then, by taking this inference penalty (with k= ~5), we'd be able to run ~1 million automated AI researchers at ~100x human speed.↩

This source benchmarks throughput of Flash at ~6x GPT-4 Turbo, and GPT-4 Turbo was faster than original GPT-4. Latency is probably also roughly 10x faster.↩

Alec Radford is an incredibly gifted and prolific researcher/engineer at OpenAI, behind many of the most important advances, though he flies under the radar some.↩

25 OOMs of algorithmic progress on top of GPT-4, for example, are clearly impossible: that would imply it would be possible to train a GPT-4-level model with just a handful of FLOP.↩

The 10x speed robots doing physical R&D in the real world is the "slow version"; in reality the superintelligences will try to do as much R&D as possible in simulation, like AlphaFold or manufacturing "digital twins".↩

Why isn't "factorio-world"—build a factory, that produces more factories, producing even more factories, doubling factories until eventually your entire planet is quickly covered in factories—possible today? Well, labor is constrained—you can accumulate capital (factories, tools, etc.), but that runs into diminishing returns because it's constrained by a fixed labor force. With robots and AI systems being able to fully automate labor, that removes that constraint; robo-factories could produce more robo-factories in an ~unconstrained way, leading to an industrial explosion. See more economic growth models of this here.↩

