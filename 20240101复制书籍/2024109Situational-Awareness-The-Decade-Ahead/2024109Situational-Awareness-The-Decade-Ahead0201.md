Leopold Aschenbrenner.(2024).2024109Situational-Awareness-The-Decade-Ahead.Dwarkesh podcast => II. From AGI to Superintelligence: the Intelligence Explosion

## II. From AGI to Superintelligence: the Intelligence Explosion

AI progress won't stop at human-level. Hundreds of millions of AGIs could automate AI research, compressing a decade of algorithmic progress (5+ OOMs) into ≤1 year. We would rapidly go from human-level to vastly superhuman AI systems. The power—and the peril—of superintelligence would be dramatic.

In this piece:

Automating AI research

Possible bottlenecks

The power of superintelligence

Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,' and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make.

I. J. Good (1965)

The Bomb and The Super

In the common imagination, the Cold War's terrors principally trace back to Los Alamos, with the invention of the atomic bomb. But The Bomb, alone, is perhaps overrated. Going from The Bomb to The Super—hydrogen bombs—was arguably just as important.

In the Tokyo air raids, hundreds of bombers dropped thousands of tons of conventional bombs on the city. Later that year, Little Boy, dropped on Hiroshima, unleashed similar destructive power in a single device. But just 7 years later, Teller's hydrogen bomb multiplied yields a thousand-fold once again—a single bomb with more explosive power than all of the bombs dropped in the entirety of WWII combined.

The Bomb was a more efficient bombing campaign. The Super was a country-annihilating device.

So it will be with AGI and Superintelligence.

AI progress won't stop at human-level. After initially learning from the best human games, AlphaGo started playing against itself—and it quickly became superhuman, playing extremely creative and complex moves that a human would never have come up with.

We discussed the path to AGI in the previous piece. Once we get AGI, we'll turn the crank one more time—or two or three more times—and AI systems will become superhuman—vastly superhuman. They will become qualitatively smarter than you or I, much smarter, perhaps similar to how you or I are qualitatively smarter than an elementary schooler.

The jump to superintelligence would be wild enough at the current rapid but continuous rate of AI progress (if we could make the jump to AGI in 4 years from GPT-4, what might another 4 or 8 years after that bring?). But it could be much faster than that, if AGI automates AI research itself.

Once we get AGI, we won't just have one AGI. I'll walk through the numbers later, but: given inference GPU fleets by then, we'll likely be able to run many millions of them (perhaps 100 million human-equivalents, and soon after at 10x+ human speed). Even if they can't yet walk around the office or make coffee, they will be able to do ML research on a computer. Rather than a few hundred researchers and engineers at a leading AI lab, we'd have more than 100,000x that—furiously working on algorithmic breakthroughs, day and night. Yes, recursive self-improvement, but no sci-fi required; they would need only to accelerate the existing trendlines of algorithmic progress (currently at ~0.5 OOMs/year).

Automated AI research could accelerate algorithmic progress, leading to 5+ OOMs of effective compute gains in a year. The AI systems we'd have by the end of an intelligence explosion would be vastly smarter than humans.

Automated AI research could probably compress a human-decade of algorithmic progress into less than a year (and that seems conservative). That'd be 5+ OOMs, another GPT-2-to-GPT-4-sized jump, on top of AGI—a qualitative jump like that from a preschooler to a smart high schooler, on top of AI systems already as smart as expert AI researchers/engineers.

There are several plausible bottlenecks—including limited compute for experiments, complementarities with humans, and algorithmic progress becoming harder—which I'll address, but none seem sufficient to definitively slow things down.

Before we know it, we would have superintelligence on our hands—AI systems vastly smarter than humans, capable of novel, creative, complicated behavior we couldn't even begin to understand—perhaps even a small civilization of billions of them. Their power would be vast, too. Applying superintelligence to R&D in other fields, explosive progress would broaden from just ML research; soon they'd solve robotics, make dramatic leaps across other fields of science and technology within years, and an industrial explosion would follow. Superintelligence would likely provide a decisive military advantage, and unfold untold powers of destruction. We will be faced with one of the most intense and volatile moments of human history.

从通用人工智能到超级智能：智能爆炸

AI 的发展不会止步于人类水平。数以亿计的通用人工智能（AGI）可以实现 AI 研究的自动化，将需要十年才能实现的算法进步（提升超过 5 个数量级）压缩到不到一年时间内完成。我们将迅速从具备人类水平的 AI，跃升至远超人类的 AI 系统。超级智能带来的力量 —— 以及威胁 —— 将是前所未有的。

本文包含：

自动化 AI 研究

可能的瓶颈

超级智能的力量

超级智能机器的定义是：一台能在所有智力活动上远远超越任何人类的机器，不论这个人有多聪明。因为机器设计本身就是智力活动之一，所以超级智能机器必然能设计出更优秀的机器。这无疑会引发「智能爆炸」，使人类的智能远远落后。因此，第一台超级智能机器将是人类需要制造的最后一项发明。

—— I. J. Good（1965)

原子弹和氢弹

在普通人的想象中，冷战的恐怖主要源于洛斯阿拉莫斯实验室研制出的原子弹。但仅仅是原子弹，其影响或许被夸大了。可以说，从原子弹到氢弹的跨越同样具有重要意义。

在东京空袭中，数百架轰炸机向城市投下了数千吨常规炸弹。同年晚些时候，投放在广岛的原子弹「Little Boy」仅用一个装置就释放了相当的破坏力。但仅仅 7 年后，物理学家 Teller 研制的氢弹让破坏力再次提升了千倍 —— 仅一颗氢弹的爆炸威力就超过了整个二战期间投下的所有炸弹的总和。

原子弹代表了更高效的轰炸手段，而氢弹则是能够毁灭整个国家的武器。

通用人工智能和超级智能的发展轨迹也将遵循类似的模式。

AI 的发展不会止步于人类水平。以围棋 AI 系统 AlphaGo 为例，它最初通过学习顶尖棋手的对局提升实力，之后开始进行自我对弈 —— 很快就超越了人类，能够下出极具创造性和复杂性的棋步，这些招法是人类棋手从未设想过的。

我们在上一篇文章中探讨了实现通用人工智能的路径。一旦我们开发出通用人工智能，通过进一步的迭代升级，AI 系统将达到超越人类的水平 —— 远远超越人类。它们在本质上将比你我更加智慧，这种差距可能就像成年人与小学生之间的智力差距一样显著。

即使按照当前 AI 快速但平稳的发展速度，达到超级智能的飞跃也将是惊人的。(想想看：如果我们能在 GPT-4 之后的 4 年内实现通用人工智能，那么再过 4 年或 8 年会发生什么？）但如果通用人工智能能够实现 AI 研究的自动化，这个过程可能会更快。

一旦我们实现了通用人工智能，我们不会仅仅停留在一个系统上。我稍后会详细解释具体数据，但是：考虑到那时的 AI 推理专用 GPU 集群规模，我们很可能能够同时运行数百万个通用人工智能系统（相当于 1 亿个人类的智力水平，而且很快能达到人类思维速度的 10 倍以上）。即使这些系统还不能在办公室里走动或煮咖啡，它们也完全可以在计算机上进行机器学习（Machine Learning，ML）研究。相比当前顶尖 AI 实验室仅有的几百名研究人员和工程师，我们将拥有超过 10 万倍的研究力量 —— 它们将昼夜不停地致力于突破算法瓶颈。没错，这就是递归式自我提升（AI 系统不断改进自身）的过程，这并非科幻小说：它们只需要加速当前算法进步的趋势（目前每年大约提升 3 倍左右）。

通过自动化的 AI 研究，算法性能将得到显著提升，在短短一年内就能实现相当于提升千万倍的计算效率。经过这样的智能爆炸，最终诞生的 AI 系统将远远超越人类的智慧水平。

自动化的 AI 研究很可能将人类需要十年才能实现的算法进步压缩到不到一年的时间内（这个预测可能还是比较保守的）。这意味着在已经达到通用人工智能水平的基础上，还能实现相当于从 GPT-2 到 GPT-4 那样跨越性的进步，带来超过 10 万倍的性能提升。这种飞跃就像一个孩子从幼儿园直接跃升到优秀高中生的水平，而且是建立在已经具备专业 AI 研究人员和工程师智能水平的系统基础之上。

当然，这个发展过程中存在一些潜在的制约因素 —— 比如实验所需的计算资源限制、需要与人类协同配合、以及算法突破的难度不断提高等。我会在后文讨论这些问题，但目前看来这些因素都不足以显著减缓发展速度。

在我们意识到之前，超级智能就将出现在我们面前 —— 这些远超人类智慧的 AI 系统能够展现出超出我们理解范围的创新性和复杂性行为。它们可能会形成一个由数十亿智能体组成的庞大网络。它们所掌握的力量将是无比巨大的。当超级智能开始投入其他领域的研发时，爆炸性的进步将不再局限于机器学习研究领域：它们将很快攻克机器人技术的难题，在各个科技领域实现突破性进展，随之带来新一轮产业革命。超级智能系统可能会让拥有它的一方获得压倒性的军事优势，同时也可能释放出难以想象的破坏力。

这将是人类历史上最具挑战性和不确定性的时刻之一。

### Automating AI research

We don't need to automate everything—just AI research. A common objection to transformative impacts of AGI is that it will be hard for AI to do everything. Look at robotics, for instance, doubters say; that will be a gnarly problem, even if AI is cognitively at the levels of PhDs. Or take automating biology R&D, which might require lots of physical lab-work and human experiments.

But we don't need robotics—we don't need many things—for AI to automate AI research. The jobs of AI researchers and engineers at leading labs can be done fully virtually and don't run into real-world bottlenecks in the same way (though it will still be limited by compute, which I'll address later). And the job of an AI researcher is fairly straightforward, in the grand scheme of things: read ML literature and come up with new questions or ideas, implement experiments to test those ideas, interpret the results, and repeat. This all seems squarely in the domain where simple extrapolations of current AI capabilities could easily take us to or beyond the levels of the best humans by the end of 2027.

It's worth emphasizing just how straightforward and hacky some of the biggest machine learning breakthroughs of the last decade have been: "oh, just add some normalization" (LayerNorm/BatchNorm) or "do f(x)+x instead of f(x)" (residual connections)" or "fix an implementation bug" (Kaplan → Chinchilla scaling laws). AI research can be automated. And automating AI research is all it takes to kick off extraordinary feedback loops.

We'd be able to run millions of copies (and soon at 10x+ human speed) of the automated AI researchers. Even by 2027, we should expect GPU fleets in the 10s of millions. Training clusters alone should be approaching ~3 OOMs larger, already putting us at 10 million+ A100-equivalents. Inference fleets should be much larger still. (More on all this in a later piece.)

That would let us run many millions of copies of our automated AI researchers, perhaps 100 million human-researcher-equivalents, running day and night. There's some assumptions that flow into the exact numbers, including that humans "think" at 100 tokens/minute (just a rough order of magnitude estimate, e.g. consider your internal monologue) and extrapolating historical trends and Chinchilla scaling laws on per-token inference costs for frontier models remaining in the same ballpark. We'd also want to reserve some of the GPUs for running experiments and training new models. Full calculation in a footnote.

Another way of thinking about it is that given inference fleets in 2027, we should be able to generate an entire internet's worth of tokens, every single day. In any case, the exact numbers don't matter that much, beyond a simple plausibility demonstration.

Moreover, our automated AI researchers may soon be able to run at much faster than human-speed:

By taking some inference penalties, we can trade off running fewer copies in exchange for running them at faster serial speed. (For example, we could go from ~5x human speed to ~100x human speed by "only" running 1 million copies of the automated researchers.)

More importantly, the first algorithmic innovation the automated AI researchers work on is getting a 10x or 100x speedup. Gemini 1.5 Flash is ~10x faster than the originally-released GPT-4, merely a year later, while providing similar performance to the originally-released GPT-4 on reasoning benchmarks. If that's the algorithmic speedup a few hundred human researchers can find in a year, the automated AI researchers will be able to find similar wins very quickly.

That is: expect 100 million automated researchers each working at 100x human speed not long after we begin to be able to automate AI research. They'll each be able to do a year's worth of work in a few days. The increase in research effort—compared to a few hundred puny human researchers at a leading AI lab today, working at a puny 1x human speed—will be extraordinary.

This could easily dramatically accelerate existing trends of algorithmic progress, compressing a decade of advances into a year. We need not postulate anything totally novel for automated AI research to intensely speed up AI progress. Walking through the numbers in the previous piece, we saw that algorithmic progress has been a central driver of deep learning progress in the last decade; we noted a trendline of ~0.5 OOMs/year on algorithmic efficiencies alone, with additional large algorithmic gains from unhobbling on top. (I think the import of algorithmic progress has been underrated by many, and properly appreciating it is important for appreciating the possibility of an intelligence explosion.)

Could our millions of automated AI researchers (soon working at 10x or 100x human speed) compress the algorithmic progress human researchers would have found in a decade into a year instead? That would be 5+ OOMs in a year.

Don't just imagine 100 million junior software engineer interns here (we'll get those earlier, in the next couple years!). Real automated AI researchers will be very smart—and in addition to their raw quantitative advantage, automated AI researchers will have other enormous advantages over human researchers:

They'll be able to read every single ML paper ever written, have been able to deeply think about every single previous experiment ever run at the lab, learn in parallel from each of their copies, and rapidly accumulate the equivalent of millennia of experience. They'll be able to develop far deeper intuitions about ML than any human.

They'll be easily able to write millions of lines of complex code, keep the entire codebase in context, and spend human-decades (or more) checking and rechecking every line of code for bugs and optimizations. They'll be superbly competent at all parts of the job.

You won't have to individually train up each automated AI researcher (indeed, training and onboarding 100 million new human hires would be difficult). Instead, you can just teach and onboard one of them—and then make replicas. (And you won't have to worry about politicking, cultural acclimation, and so on, and they'll work with peak energy and focus day and night.)

Vast numbers of automated AI researchers will be able to share context (perhaps even accessing each others' latent space and so on), enabling much more efficient collaboration and coordination compared to human researchers.

And of course, however smart our initial automated AI researchers would be, we'd soon be able to make further OOM-jumps, producing even smarter models, even more capable at automated AI research.

Imagine an automated Alec Radford—imagine 100 million automated Alec Radfords. I think just about every researcher at OpenAI would agree that if they had 10 Alec Radfords, let alone 100 or 1,000 or 1 million running at 10x or 100x human speed, they could very quickly solve very many of their problems. Even with various other bottlenecks (more in a moment), compressing a decade of algorithmic progress into a year as a result seems very plausible. (A 10x acceleration from a million times more research effort, which seems conservative if anything.)

That would be 5+ OOMs right there. 5 OOMs of algorithmic wins would be a similar scaleup to what produced the GPT-2-to-GPT-4 jump, a capability jump from ~a preschooler to ~a smart high schooler. Imagine such a qualitative jump on top of AGI, on top of Alec Radford.

It's strikingly plausible we'd go from AGI to superintelligence very quickly, perhaps in less than one year.

自动化 AI 研究

我们不需要自动化所有领域 —— 只需要实现 AI 研究的自动化。一些人质疑通用人工智能（AGI）能否带来根本性的改变，他们认为 AI 要完成所有任务将面临重重困难。比如说机器人技术，持怀疑态度的人指出：即使 AI 的认知能力达到顶尖专家的水平，这仍然是个难以解决的问题。又如生物学研发的自动化，可能需要大量实际的实验室操作和人体实验。

但要实现 AI 研究的自动化，我们并不需要先解决机器人技术 —— 实际上很多技术都不是必需的。顶尖 AI 实验室的研究人员和工程师的工作可以完全在数字环境中完成，不会像其他领域那样受到现实世界的各种限制（虽然仍然会受到计算资源的制约，这个问题我稍后会详细讨论）。

从本质上来说，AI 研究人员的工作流程相对简单：阅读机器学习（Machine Learning，ML）相关文献并提出新的问题或想法，设计实验来验证这些想法，分析实验结果，然后循环往复。按照当前 AI 能力的发展趋势来看，在 2027 年底前，AI 就可能在这些方面达到或超越最优秀人类专家的水平。

值得注意的是，过去十年中一些最重大的机器学习突破其实出奇简单，甚至看起来有些随意：比如「添加一个数据标准化步骤」（LayerNorm/BatchNorm），或是「在原有函数上加上输入值」（残差连接），又或者仅仅是「修复了一个程序错误」（从 Kaplan 到 Chinchilla 的规模化法则）。这些例子说明 AI 研究是完全可以实现自动化的。而一旦 AI 研究实现自动化，就足以触发惊人的良性循环效应。

到 2027 年，我们将能够同时运行数百万个自动化 AI 研究系统（并且很快就能让它们以超过人类 10 倍的速度运行）。即便是保守估计，我们预计到那时 AI 专用图形处理器（GPU）集群的规模将达到数千万台。仅仅是用于训练的计算集群就将增长约 1000 倍，相当于超过 1000 万台目前最先进的 NVIDIA A100 GPU 的算力。而用于推理的计算集群规模可能会更大。（这些具体数据我们会在后续详细讨论。）

有了这样的计算能力，我们就能够部署数百万个自动化 AI 研究系统，大约相当于 1 亿个人类研究员的工作能力，并且能够持续不断地工作。这个估算基于一些具体假设，比如假定人类的思维速度大约是每分钟处理 100 个基本单元（token，这只是一个粗略估计，可以参考你的内心思考过程的速度），同时根据历史发展趋势和 Chinchilla 规模化法则推测，未来顶尖 AI 模型的单个处理单元成本将保持在相近水平。当然，我们还需要预留一部分 GPU 来进行实验和训练新的模型。（详细的计算方法请参见文末注解）

换个角度看，到 2027 年，我们的 AI 推理计算集群每天将能够处理相当于整个互联网内容量的信息。当然，具体的数字并不是最关键的，重要的是这种规模的可行性已经得到证实。

更值得注意的是，我们的自动化 AI 研究系统很快就能以远超人类的速度运行。这体现在两个方面：

首先，我们可以通过调整计算资源分配来实现速度与规模的平衡。比如说，我们可以选择运行较少的系统副本（降低到 100 万个），但让每个系统的运行速度提升到人类的 100 倍，而不是维持更多副本以 5 倍人类速度运行。

其次，也是更重要的是，这些自动化 AI 研究系统的首要任务就是实现速度的大幅提升，目标是达到目前速度的 10 倍到 100 倍。我们已经看到了这种可能性：Google 最新推出的 Gemini 1.5 Flash 模型比一年前发布的 GPT-4 快了约 10 倍，同时在处理复杂问题的能力上与当时的 GPT-4 相当。想想看，如果几百名人类研究员一年内就能实现这样的突破，那么数量庞大的自动化 AI 研究系统必然能更快地取得类似或更大的进展。

换句话说：在 AI 研究实现自动化后不久，我们预计将拥有 1 亿个 AI 研究系统，每个系统都能以人类 100 倍的速度开展研究工作。这意味着它们每个都能在短短几天内完成相当于人类一年的研究量。与当今顶尖 AI 实验室仅有几百名研究员的规模相比，研究效能的提升将是天文数字级的。

这种规模的研究力量很容易就能大大加快当前算法发展的步伐，将原本需要十年才能实现的进展压缩到一年内完成。要达到这种加速效果，我们甚至不需要任何革命性的创新，仅仅是将现有的 AI 研究自动化就足够了。回顾我们在前文分析的数据，算法的改进是过去十年深度学习发展的核心推动力。仅从算法效率提升来看，每年就能实现大约三倍的进步，再加上其他方面的技术突破，进展更是显著。（我认为很多人都低估了算法改进的重要性，而只有真正理解了这一点，才能明白为什么智能呈指数级爆发式增长是完全可能的。）

设想一下：当我们拥有数百万个自动化 AI 研究系统（很快就能以人类 10 倍或 100 倍的速度运行），它们能否将人类研究人员需要十年时间才能实现的算法突破压缩到一年内完成？这意味着在短短一年内就能实现十万倍的性能提升。

这些系统可不仅仅是 1 亿个初级程序员实习生的水平（这种水平的 AI 我们在未来几年内就能实现）。真正的自动化 AI 研究系统将具备极高的智能水平。除了数量上的巨大优势，它们相比人类研究人员还具有许多独特的优势：

首先是知识积累和学习能力：

- 它们能够阅读和理解所有已发表的机器学习论文
- 能够深入分析实验室进行过的每一个实验
- 所有系统之间可以即时共享学习成果
- 能在极短时间内积累相当于数千年的研究经验

这些优势让它们能够形成远超人类的专业洞察力。

其次是编程和调试能力：

- 能够编写和管理数百万行复杂的程序代码
- 能够完整理解和掌握整个项目的程序结构
- 可以投入相当于人类几十年的时间来反复检查和优化每一行代码

这使得它们在技术实现的各个环节都能保持极高的水准。

更重要的是效率优势：

- 不需要对每个系统进行单独培训（要知道培训和适应 1 亿名人类新员工几乎是不可能的任务）
- 只需要培训一个系统，然后复制其能力
- 不会受到人际关系、团队磨合等因素的影响
- 能够持续保持最佳状态和专注度工作

这些 AI 研究系统能够即时分享信息和见解（甚至能够直接共享思维模式），这种协作效率远超人类研究团队之间的合作。

更令人惊叹的是，无论我们最初开发的 AI 研究系统多么智能，它们很快就能实现进一步的突破，创造出更智能的新一代系统，在 AI 研究领域展现出更强大的能力。

设想一下：如果我们能够创造出一个具备 OpenAI（世界顶尖 AI 研究机构之一）首席科学家 Alec Radford 水平的 AI 系统，然后将其复制一亿份。OpenAI 的研究人员都会认同，如果他们能有 10 个 Radford 级别的研究员，更不用说 100 个、1000 个，或者 100 万个能以人类 10-100 倍速度工作的 Radford，他们就能迅速突破目前面临的诸多技术难题。即使考虑到各种现实限制（我们稍后会详细讨论），在一年内实现原本需要十年的算法进步是完全可能的。（研究力量增加了 100 万倍，带来 10 倍的研发速度提升，这个预估可能还是太保守了。）

这意味着我们能实现十万倍以上的性能提升。要知道，仅仅五万倍的算法进步就足以造就从 GPT-2 到 GPT-4 的跨越，使 AI 的能力从幼儿园水平提升到优秀高中生的水平。现在想象一下，如果这样的飞跃是建立在已经达到通用人工智能水平、已经具备 Radford 级别专业能力的系统之上，会是什么景象。

基于以上分析，我们有充分理由相信，从实现通用人工智能到达到超级智能的跨越可能会异常迅速，也许只需要不到一年的时间。

### Possible bottlenecks

While this basic story is surprisingly strong—and is supported by thorough economic modeling work—there are some real and plausible bottlenecks that will probably slow down an automated-AI-research intelligence explosion.

I'll give a summary here, and then discuss these in more detail in the optional sections below for those interested:

Limited compute: AI research doesn't just take good ideas, thinking, or math—but running experiments to get empirical signal on your ideas. A million times more research effort via automated research labor won't mean a million times faster progress, because compute will still be limited—and limited compute for experiments will be the bottleneck. Still, even if this won't be a 1,000,000x speedup, I find it hard to imagine that the automated AI researchers couldn't use the compute at least 10x more effectively: they'll be able to get incredible ML intuition (having internalized the whole ML literature and every previous experiment every run!) and centuries-equivalent of thinking-time to figure out exactly the right experiment to run, configure it optimally, and get the maximum value of information; they'll be able to spend centuries-equivalent of engineer-time before running even tiny experiments to avoid bugs and get them right on the first try; they can make tradeoffs to economize on compute by focusing on the biggest wins; and they'll be able to try tons of smaller-scale experiments (and given effective compute scaleups by then, "smaller-scale" means being able to train 100,000 GPT-4-level models in a year to try architecture breakthroughs). Some human researchers and engineers are able to produce 10x the progress as others, even with the same amount of compute—and this should apply even moreso to automated AI researchers. I do think this is the most important bottleneck, and I address it in more depth below.

Complementarities/long tail: A classic lesson from economics (cf Baumol's growth disease) is that if you can automate, say, 70% of something, you get some gains but quickly the remaining 30% become your bottleneck. For anything that falls short of full automation—say, really good copilots—human AI researchers would remain a major bottleneck, making the overall increase in the rate of algorithmic progress relatively small. Moreover, there's likely some long tail of capabilities required for automating AI research—the last 10% of the job of an AI researcher might be particularly hard to automate. This could soften takeoff some, though my best guess is that this only delays things by a couple years. Perhaps 2026/27-models speed are the proto-automated-researcher, it takes another year or two for some final unhobbling, a somewhat better model, inference speedups, and working out kinks to get to full automation, and finally by 2028 we get the 10x acceleration (and superintelligence by the end of the decade).

Inherent limits to algorithmic progress: Maybe another 5 OOMs of algorithmic efficiency will be fundamentally impossible? I doubt it. While there will definitely be upper limits, if we got 5 OOMs in the last decade, we should probably expect at least another decade's-worth of progress to be possible. More directly, current architectures and training algorithms are still very rudimentary, and it seems that much more efficient schemes should be possible. Biological reference classes also support dramatically more efficient algorithms being plausible.

Ideas get harder to find, so the automated AI researchers will merely sustain, rather than accelerate, the current rate of progress: One objection is that although automated research would increase effective research effort a lot, ideas also get harder to find. That is, while it takes only a few hundred top researchers at a lab to sustain 0.5 OOMs/year today, as we exhaust the low-hanging fruit, it will take more and more effort to sustain that progress—and so the 100 million automated researchers will be merely what's necessary to sustain progress. I think this basic model is correct, but the empirics don't add up: the magnitude of the increase in research effort—a million-fold—is way, way larger than the historical trends of the growth in research effort that's been necessary to sustain progress. In econ modeling terms, it's a bizarre "knife-edge assumption" to assume that the increase in research effort from automation will be just enough to keep progress constant.

Ideas get harder to find and there are diminishing returns, so the intelligence explosion will quickly fizzle: Related to the above objection, even if the automated AI researchers lead to an initial burst of progress, whether rapid progress can be sustained depends on the shape of the diminishing returns curve to algorithmic progress. Again, my best read of the empirical evidence is that the exponents shake out in favor of explosive/accelerating progress. In any case, the sheer size of the one-time boost—from 100s to 100s of millions of AI researchers—probably overcomes diminishing returns here for at least a good number of OOMs of algorithmic progress, even though it of course can't be indefinitely self-sustaining.

Limited compute for experiments

Complementarities and long tails to 100% automation

Fundamental limits to algorithmic progress

Ideas get harder to find and diminishing returns

Overall, these factors may slow things down somewhat: the most extreme versions of intelligence explosion (say, overnight) seem implausible. And they may result in a somewhat longer runup (perhaps we need to wait an extra year or two from more sluggish, proto-automated researchers to the true automated Alec Radfords, before things kick off in full force). But they certainly don't rule out a very rapid intelligence explosion. A year—or at most just a few years, but perhaps even just a few months—in which we go from fully-automated AI researchers to vastly superhuman AI systems should be our mainline expectation.

潜在的发展制约因素

尽管前文描述的发展前景令人信服 —— 并且得到了深入的经济模型分析支持 —— 但确实存在一些现实的限制因素，可能会减缓由自动化 AI 研究带来的智能爆发式增长。

让我先概述这些限制因素，然后在后续章节中为感兴趣的读者详细展开讨论：

计算资源的限制：

AI 研究不仅需要优秀的创意、缜密的思考或数学推导，还需要通过大量实验来验证这些想法是否可行。即使我们能让研究效率提升百万倍，也不意味着研究进展能加快百万倍，因为计算资源始终是有限的 —— 实验所需的计算资源将成为制约发展速度的关键因素。

虽然不会达到一百万倍的效率提升，但我相信自动化 AI 研究系统至少能让计算资源的使用效率提高 10 倍，这基于以下几个原因：

1、专业知识和洞察力：

- 它们能够完全掌握所有机器学习相关文献
- 能够充分理解和借鉴以往的每一个实验经验
- 拥有相当于数百年的时间来思考和规划最佳实验方案
- 能够优化实验配置，最大化每次实验的价值

2、精确的执行能力：

- 在启动实验前，能投入相当于数百年工程师工时的时间来细致检查
- 大幅降低试错成本，提高一次性成功的几率
- 能够精准平衡资源分配，确保投入产出比最大化

3、规模化实验能力：

- 能够同时开展大量小规模实验
- 到那时的技术水平，即使是「小规模」实验也相当可观（比如一年内能够训练 10 万个媲美 GPT-4 这样顶尖 AI 模型的系统来测试不同的技术方案）

我们知道，即使使用相同的计算资源，顶尖的人类研究员和工程师也能比普通研究员创造出 10 倍的研究成果。这种效率差异在自动化 AI 研究系统中可能会表现得更加明显。

这个计算资源的限制确实是最关键的发展瓶颈，让我们在接下来更深入地探讨这个问题。

人机协作的必要性和完全自动化的困难：

经济学有一个著名的规律（经济学家鲍莫尔提出的「成本病」理论）：即使你能够实现某项工作 70% 的自动化，获得一定的效率提升，但剩下的 30% 很快就会成为新的发展瓶颈。这个规律同样适用于 AI 研究：

1、部分自动化的局限性：

- 如果只能实现部分自动化（比如开发出非常优秀的 AI 辅助工具），人类研究员的工作效率仍将是限制因素
- 这种情况下，整体的算法进展速度提升可能相对有限

2、「最后一公里」问题：

- AI 研究工作中最后 10% 的内容可能特别难以实现自动化
- 这些难以自动化的部分可能会减缓发展速度

3、可能的时间线：

- 2026-2027 年：初步实现基础的自动化研究功能
- 之后 1-2 年：

* 进一步完善模型性能
* 提高运算速度
* 解决各种技术问题
* 最终实现完全自动化

- 2028 年：达到研究效率提升 10 倍的目标
- 2029-2030 年：可能实现超级智能

虽然这些因素会使发展过程更加渐进，但我估计它们最多只会让整个进程延后几年时间。

算法进步的自然极限：

有人可能会问：AI 性能还能再提升十万倍吗？我对这种质疑持怀疑态度。虽然技术发展确实存在极限，但考虑到我们在过去十年已经实现了十万倍的效率提升，那么至少再实现一个十年的同等进步应该是可能的。更直接地说：

- 目前的 AI 系统架构和训练方法仍处于相对初级阶段
- 还有很大的优化空间来开发更高效的方案
- 从生物进化的角度来看，开发出效率更高的 AI 算法是完全可能的

研究难度递增论：

有人认为，虽然自动化研究能大幅提升研究投入，但新发现会越来越难获得。他们的论据是：

1.、现状：目前一个顶级实验室只需要几百名研究员，就能保持每年约三倍的性能提升速度

2、未来趋势：

- 随着容易实现的突破被逐渐用尽
- 维持同样的进步速度会需要越来越多的投入
- 因此，即使有 1 亿个自动化研究系统，可能也只是刚好够维持当前的进步速度

这种观点的基本逻辑没错，但实际数据并不支持这一结论：自动化带来的研究力量提升（提高千倍万倍）远远超过了历史上维持技术进步所需要的人力增长趋势。用经济学的术语来说，认为自动化带来的巨大研究力量恰好只能维持当前进步速度，这种假设过于牵强。

研究难度递增与边际效益递减论：

即便自动化 AI 研究系统最初能带来突破性进展，但能否持续保持快速发展的势头，取决于算法进步的边际效益递减程度。对此我的看法是：

- 现有的实证数据表明，技术进步更可能呈现爆发式加速发展
- 即使存在边际效益递减，从几百人扩展到上亿个研究系统的巨大跨越，足以在相当长时间内克服这种效益递减的影响
- 这种规模的提升至少能支撑多轮重大算法突破，尽管不可能永远保持这种增长速度

主要制约因素总结：

1、实验所需的计算资源限制。

2、完全自动化过程中的配套问题和难点。

3、算法进步的自然限制。

4、研究难度提升和收益递减效应。

综合影响评估：

这些制约因素确实会在某种程度上放缓发展速度：

- 最极端的预测（例如一夜之间实现超级智能）显然不切实际

- 可能需要更长的过渡期：
* 先经过 1-2 年的初级自动化研究阶段
* 逐步提升到 Radford（顶级 AI 科学家）水平的完全自动化系统
* 最后才能全面加速发展

但这些因素并不会阻止快速的智能爆发式发展。我们应该预期：从实现完全自动化的 AI 研究系统到达到远超人类的 AI 水平，整个过程可能只需要一年时间，最多不超过几年，甚至可能只需要几个月。

### The power of superintelligence

Whether or not you agree with the strongest form of these arguments—whether we get a <1 year intelligence explosion, or it takes a few years—it is clear: we must confront the possibility of superintelligence.

The AI systems we'll likely have by the end of this decade will be unimaginably powerful.

Of course, they'll be quantitatively superhuman. On our fleets of 100s of millions of GPUs by the end of the decade, we'll be able to run a civilization of billions of them, and they will be able to "think" orders of magnitude faster than humans. They'll be able to quickly master any domain, write trillions of lines of code, read every research paper in every scientific field ever written (they'll be perfectly interdisciplinary!) and write new ones before you've gotten past the abstract of one, learn from the parallel experience of every one of its of copies, gain billions of human-equivalent years of experience with some new innovation in a matter of weeks, work 100% of the time with peak energy and focus and won't be slowed down by that one teammate who is lagging, and so on.

More importantly—but harder to imagine—they'll be qualitatively superhuman. As a narrow example of this, large-scale RL runs have been able to produce completely novel and creative behaviors beyond human understanding, such as the famous move 37 in AlphaGo vs. Lee Sedol. Superintelligence will be this across many domains. It'll find exploits in the human code too subtle for any human to notice, and it'll generate code too complicated for any human to understand even if the model spent decades trying to explain it. Extremely difficult scientific and technological problems that a human would be stuck on for decades will seem just so obvious to them. We'll be like high-schoolers stuck on Newtonian physics while it's off exploring quantum mechanics.

As a visualization of how wild this could be, look at some Youtube videos of video game speedruns, such as this one of beating Minecraft in 20 seconds.

Beating Minecraft in 20 seconds. (If you have no idea what's going on in this video, you're in good company; even most normal players of Minecraft have almost no clue what's going on.)

Now imagine this applied to all domains of science, technology, and the economy. The error bars here, of course, are extremely large. Still, it's important to consider just how consequential this would be.

What does it feel like to stand here? Illustration from Wait But Why/Tim Urban.

In the intelligence explosion, explosive progress was initially only in the narrow domain of automated AI research. As we get superintelligence, and apply our billions of (now superintelligent) agents to R&D across many fields, I expect explosive progress to broaden:

An AI capabilities explosion. Perhaps our initial AGIs had limitations that prevented them fully automating work in some other domains (rather than just in the AI research domain); automated AI research will quickly solve these, enabling automation of any and all cognitive work.

Solve robotics. Superintelligence won't stay purely cognitive for long. Getting robotics to work well is primarily an ML algorithms problem (rather than a hardware problem), and our automated AI researchers will likely be able to solve it (more below). Factories would go from human-run, to AI-directed using human physical labor, to soon being fully run by swarms of robots.

Dramatically accelerate scientific and technological progress. Yes, Einstein alone couldn't develop neuroscience and build a semiconductor industry, but a billion superintelligent automated scientists, engineers, technologists, and robot technicians (with the robots moving at 10x or more human speed!) would make extraordinary advances in many fields in the space of years. (Here's a nice short story visualizing what AI-driven R&D might look like.) The billion superintelligences would be able to compress the R&D effort humans researchers would have done in the next century into years. Imagine if the technological progress of the 20th century were compressed into less than a decade. We would have gone from flying being thought a mirage, to airplanes, to a man on the moon and ICBMs in a matter of years. This is what I expect the 2030s to look like across science and technology.

An industrial and economic explosion. Extremely accelerated technological progress, combined with the ability to automate all human labor, could dramatically accelerate economic growth (think: self-replicating robot factories quickly covering all of the Nevada desert). The increase in growth probably wouldn't just be from 2%/year to 2.5%/year; rather, this would be a fundamental shift in the growth regime, more comparable to the historical step-change from very slow growth to a couple percent a year with the industrial revolution. We could see economic growth rates of 30%/year and beyond, quite possibly multiple doublings a year. This follows fairly straightforwardly from economists' models of economic growth. To be sure, this may well be delayed by societal frictions; arcane regulation might ensure lawyers and doctors still need to be human, even if AI systems were much better at those jobs; surely sand will be thrown into the gears of rapidly expanding robo-factories as society resists the pace of change; and perhaps we'll want to retain human nannies; all of which would slow the growth of the overall GDP statistics. Still, in whatever domains we remove human-created barriers (e.g., competition might force us to do so for military production), we'd see an industrial explosion.

Growth mode	Date began

to dominate	Doubling time of

the global economy

(years)

Hunting	2,000,000 B.C.	230,000

Farming	4700 B.C.	860

Science and commerce	1730 A.D.	58

Industry	1903 A.D.	15

Superintelligence?	2030 A.D.?	???

A shift in the growth regime is not unprecedented: as civilization went from hunting, to farming, to the blossoming of science and commerce, to industry, the pace of global economic growth accelerated. Superintelligence could kick off another shift in growth mode. Based on Robin Hanson's "Long-run growth as a sequence of exponential modes".

Provide a decisive and overwhelming military advantage. Even early cognitive superintelligence might be enough here; perhaps some superhuman hacking scheme can deactivate adversary militaries. In any case, military power and technological progress has been tightly linked historically, and with extraordinarily rapid technological progress will come concomitant military revolutions. The drone swarms and roboarmies will be a big deal, but they are just the beginning; we should expect completely new kinds of weapons, from novel WMDs to invulnerable laser-based missile defense to things we can't yet fathom. Compared to pre-superintelligence arsenals, it'll be like 21st century militaries fighting a 19th century brigade of horses and bayonets. (I discuss how superintelligence could lead to a decisive military advantage in a later piece.)

Be able to overthrow the US government. Whoever controls superintelligence will quite possibly have enough power to seize control from pre-superintelligence forces. Even without robots, the small civilization of superintelligences would be able to hack any undefended military, election, television, etc. system, cunningly persuade generals and electorates, economically outcompete nation-states, design new synthetic bioweapons and then pay a human in bitcoin to synthesize it, and so on. In the early 1500s, Cortes and about 500 Spaniards conquered the Aztec empire of several million; Pizarro and ~300 Spaniards conquered the Inca empire of several million; Alfonso and ~1000 Portuguese conquered the Indian Ocean. They didn't have god-like power, but the Old World's technological edge and an advantage in strategic and diplomatic cunning led to an utterly decisive advantage. Superintelligence might look similar.

Explosive growth starts in the narrower domain of AI R&D; as we apply superintelligence to R&D in other fields, explosive growth will broaden.

How all of this plays out over the 2030s is hard to predict (and a story for another time). But one thing, at least, is clear: we will be rapidly plunged into the most extreme situation humanity has ever faced.

Human-level AI systems, AGI, would be highly consequential in their own right—but in some sense, they would simply be a more efficient version of what we already know. But, very plausibly, within just a year, we would transition to much more alien systems, systems whose understanding and abilities—whose raw power—would exceed those even of humanity combined. There is a real possibility that we will lose control, as we are forced to hand off trust to AI systems during this rapid transition.

More generally, everything will just start happening incredibly fast. And the world will start going insane. Suppose we had gone through the geopolitical fever-pitches and man-made perils of the 20th century in mere years; that is the sort of situation we should expect post-superintelligence. By the end of it, superintelligent AI systems will be running our military and economy. During all of this insanity, we'd have extremely scarce time to make the right decisions. The challenges will be immense. It will take everything we've got to make it through in one piece.

The intelligence explosion and the immediate post-superintelligence period will be one of the most volatile, tense, dangerous, and wildest periods ever in human history.

And by the end of the decade, we'll likely be in the midst of it.

Confronting the possibility of an intelligence explosion—the emergence of superintelligence—often echoes the early debates around the possibility of a nuclear chain reaction—and the atomic bomb it would enable. HG Wells predicted the atomic bomb in a 1914 novel. When Szilard first conceived of the idea of a chain reaction in 1933, he couldn't convince anyone of it; it was pure theory. Once fission was empirically discovered in 1938, Szilard freaked out again and argued strongly for secrecy, and a few people started to wake up to the possibility of a bomb. Einstein hadn't considered the possibility of a chain reaction, but when Szilard confronted him, he was quick to see the implications and willing to do anything that was needed to be done; he was willing to sound the alarm, and wasn't afraid of sounding foolish. But Fermi, Bohr, and most scientists thought the "conservative" thing was to play it down, rather than take seriously the extraordinary implications of the possibility of a bomb. Secrecy (to avoid sharing their advances with the Germans) and other all-out efforts seemed absurd to them. A chain reaction sounded too crazy. (Even when, as it turned out, a bomb was but half a decade from becoming reality.)

We must once again confront the possibility of a chain reaction. Perhaps it sounds speculative to you. But among senior scientists at AI labs, many see a rapid intelligence explosion as strikingly plausible. They can see it. Superintelligence is possible.

超级智能：一股无可匹敌的力量

不管你是否认同这些观点中最极端的预测 —— 无论是在一年内还是几年内出现智能爆发 —— 有一点是确定的：我们必须正视超级智能（Superintelligence）出现的可能性。

到本世纪 20 年代末，我们可能开发出的 AI 系统将具备难以想象的能力。

从数量上来说，这些系统必将超越人类。到那时，依托着数以亿计的 GPU 计算集群，我们将能够运行一个由数十亿 AI 智能体组成的智能网络，它们的计算和思考速度将比人类快出几个数量级。这些系统能够迅速掌握任何专业领域，能够编写数万亿行代码，能够阅读所有科学领域的所有研究论文并形成完美的跨学科认知。在你还在读一篇论文摘要的时候，它们已经能写出新的研究成果。它们可以从所有分布式副本的并行经验中学习，在短短几周内积累相当于数十亿人年的创新经验。它们能够保持全天候的巅峰状态和专注度工作，不会受到任何效率低下因素的影响。

更为关键的是 —— 尽管这一点更难以想象 —— 这些系统将在能力本质上完全超越人类。举个具体的例子，在大规模强化学习（Large-scale Reinforcement Learning）实验中，AI 已经能够展现出完全超出人类理解范畴的创新行为，比如在 AlphaGo 与李世乭的对战中那著名的第 37 手棋（这一手棋开创了围棋史上前所未有的下法）。未来的超级智能将在众多领域都展现出这种超越性。它们能够发现人类代码中极其细微的漏洞，这些漏洞可能永远都逃不过人类的眼睛；它们还会生成极其复杂的代码，即便 AI 花上几十年来解释，人类也无法完全理解。那些可能让人类科学家困惑数十年的极难科学技术问题，对它们而言可能就像简单的常识。这就好比我们还在苦苦研究高中物理的牛顿定律，而它们已经在探索量子力学的前沿。

要形象地理解这种能力有多么惊人，你可以看看一些游戏速通（Speed Run）的 Youtube 视频，比如有人在 20 秒内通关 Minecraft 的记录。

关于这个 20 秒通关 Minecraft 的视频（如果你看不懂视频中发生了什么，不用担心；就连大多数 Minecraft 的普通玩家也搞不清楚那些高端操作）。

现在，请想象这种突破极限的能力被应用到科学、技术和经济的所有领域。诚然，我们对未来的预测仍存在巨大的不确定性。但是，认真思考这种变革带来的深远影响依然十分重要。

图：站在这个时代的转折点是什么感觉？（来源：Wait But Why/Tim Urban）

在智能爆发的初期，突破性的发展主要集中在 AI 自动化研究这个特定领域。但随着超级智能的出现，当我们将数十亿具备超级智能的 AI 智能体投入到各个领域的研发工作中时，我预计这种突破性进展将向更广泛的领域扩展：

AI 能力的全面突破。我们最初的通用人工智能（Artificial General Intelligence，AGI）在某些领域的自动化方面可能还存在局限（不像在 AI 研究领域那样完善）；但自动化的 AI 研究将很快突破这些限制，最终实现所有认知工作的完全自动化。

机器人技术的重大突破。超级智能不会永远局限于认知层面的运算。让机器人技术实现突破主要取决于机器学习（Machine Learning）算法的进展，而不是硬件的限制，这正是我们的 AI 研究系统所擅长的领域（详细内容将在后文展开）。在这种发展趋势下，工业生产将经历三个阶段的演变：从现在的人工操作，到过渡期的 AI 指导下的人工劳动，最终发展到完全由智能机器人集群自主运营的新模式。

科技发展的空前加速。即便是爱因斯坦这样的天才，也不可能独自发展出神经科学并建立起整个半导体产业。但是，想象一下由数十亿具有超级智能的自动化科学家、工程师、技术专家和机器人技术人员组成的研发团队（而且这些机器人的运作速度是人类的 10 倍以上！），他们将在短短几年内在众多领域实现突破性进展。（有一个精彩的短篇故事生动展现了 AI 驱动的研发场景）这数十亿超级智能体能够在几年内完成人类研究人员需要一个世纪才能完成的研发工作。设想一下，如果把 20 世纪的所有技术进步压缩到不到十年的时间内会是什么景象：人类从质疑飞行的可能性，到发明飞机，再到实现载人登月和发展洲际导弹，这一切仅仅用了几年时间。这就是我对 2030 年代科技发展图景的预期。

产业和经济的革命性变革。技术进步的空前加速，结合全面的人类劳动自动化，可能会带来经济增长的质的飞跃（例如：具有自我复制能力的智能工厂可能很快就会遍布内华达沙漠）。这种增长不会仅仅是从年增长率 2% 提升到 2.5% 这样的渐进式变化；相反，这将是一次增长模式的根本转变，就像工业革命时期那样，从极其缓慢的增长跃升至每年数个百分点的历史性突破。我们可能会看到年增长率达到 30% 甚至更高，很可能会出现一年内多次经济规模翻倍的情况。这一预测可以从经济学家的经济增长模型中得到论证。当然，这种转变可能会受到社会调适过程的影响而有所延迟：复杂的法律法规可能会要求律师和医生必须由人类担任，即便 AI 系统在这些岗位上表现更优；社会对快速变革的抵触情绪必然会限制智能工厂的扩张速度；我们可能还会倾向于保留人类保姆等特定职业。这些因素都会减缓整体经济指标的增长速度。然而，在那些我们能够突破人为阻碍的领域（比如军事生产领域，竞争压力可能会迫使我们这样做），我们将见证产业的爆发式发展。

表格：人类文明发展的增长模式演变

| 增长模式 | 开始主导的日期 | 全球经济翻倍所需时间（年）|
| --- | --- | --- |
| 狩猎采集 | 公元前 2,000,000 年 | 230,000 |
| 农业社会 | 公元前 4700 年 | 860 |
| 科技商贸 | 公元 1730 年 | 58 |
| 工业文明 | 公元 1903 年 | 15 |
| 超级智能？ | 公元 2030 年？ | ??? |

经济增长模式的根本转变在人类历史上并非首次出现：随着文明从狩猎采集时代、农业时代、科技商贸繁荣时期，直到工业时代，全球经济增长的步伐不断加快。超级智能的出现可能会引发又一次增长模式的重大转变。（数据来源：根据 Robin Hanson 的研究《长期增长：一系列指数模式的演进》整理）

在军事领域带来决定性优势。即使是初期的认知超级智能（Cognitive Superintelligence）也可能在这方面发挥关键作用；比如，某些超越人类能力的网络攻击手段可能就足以使对手的军事系统瘫痪。纵观历史，军事力量始终与技术进步密不可分。随着技术的快速发展，军事领域必将经历深刻的革命性变革。智能无人机集群和机器人军队的出现只是个开始；我们应该预见到全新类型武器的出现，从新型大规模杀伤性武器，到完全防御的激光反导系统，再到现在我们无法想象的武器系统。与超级智能时代之前的军事力量相比，这种差距就像是现代化军队与百年前的骑兵部队之间的巨大鸿沟。（关于超级智能如何带来决定性军事优势的详细讨论，我会在后续文章中展开。）

足以改变现有政治秩序。掌握超级智能的实体很可能拥有足够的力量来重塑当前的权力结构。即使在没有机器人的情况下，由超级智能构建的系统也能够突破任何未经特殊防护的军事、选举、传媒等关键基础设施，能够通过深刻的洞察力影响决策者和公众，在经济竞争中超越现有的国家实体，甚至能够设计新型生物制剂并通过加密货币支付方式促使人类完成相关合成等。这让人想起历史上的一些类似案例：16 世纪早期，科尔特斯率领约 500 名西班牙人征服了拥有数百万人口的阿兹特克帝国；皮萨罗带领约 300 名西班牙人征服了同样人口规模的印加帝国；阿方索指挥约 1000 名葡萄牙人控制了整个印度洋地区。这些征服者并非拥有神力，而是凭借着旧世界的技术优势以及战略和外交上的巧妙运用，获得了压倒性的优势。超级智能可能会以类似的方式展现其影响力。

这种革命性的发展最初将始于 AI 研发这一特定领域；随着我们将超级智能应用到其他领域的研发中，这种变革的范围将不断扩大。

至于这些变化在 2030 年代具体会如何展开，现在还难以准确预测（这需要另文详述）。但有一点是确定的：人类社会将快速进入一个前所未有的重大转折期。

即便是达到人类水平的 AI 系统（也就是通用人工智能）本身就会产生深远影响，但从某种意义上说，它们仍然只是对现有技术的优化和提升。然而，很可能仅仅在一年之内，我们就会迎来一个质的飞跃，出现一种完全不同的系统。这些系统的认知能力和行动能力 —— 它们所具备的根本性力量 —— 将超越整个人类文明的总和。在这个快速转型的过程中，我们可能不得不将重要决策的信任转移给 AI 系统，由此带来失去控制的真实风险。

从更宏观的角度来看，一切变革都将以前所未有的速度展开。我们所熟知的世界秩序将发生剧烈动荡。设想一下，如果把 20 世纪所有的地缘政治危机和人为灾难压缩到短短几年内，这就是超级智能时代可能出现的情况。最终，我们的军事和经济体系都将由超级智能 AI 系统运营。在这场剧烈的变革中，我们用于做出正确决策的时间将极其有限。我们将面临前所未有的挑战，需要调动一切可能的资源才能安全度过这个转折期。

智能爆发时期及其之后的初期阶段，很可能将成为人类历史上最具变革性、最紧张、最具风险性和最具颠覆性的时期之一。

而到本世纪 20 年代结束时，我们很可能已经身处这个时代。

面对超级智能可能出现的智能爆发现象，人们不禁想起早期科学家们对核裂变链式反应（Nuclear Chain Reaction）的争论 —— 这种反应最终导致了原子弹的诞生。早在 1914 年，科幻作家 H·G·威尔斯就在他的小说中预言了原子弹的出现。1933 年，物理学家利奥·西拉德首次提出链式反应的概念时，却无人相信这个纯理论的设想。直到 1938 年核裂变被实验证实，西拉德再次强烈警告这一发现的潜在影响，极力主张对相关研究保密，这时才有少数人开始意识到原子弹的可能性。即便是爱因斯坦最初也没有想到链式反应的可能性，但当西拉德向他解释这一概念时，他很快理解了其深远影响，并愿意采取一切必要行动。他毫不犹豫地发出警告，不在乎是否会被认为危言耸听。然而，费米、玻尔等大多数科学家认为，采取「保守」态度、淡化处理更为妥当，而不是认真对待这一可能带来重大影响的发现。在他们看来，保密措施（以防止研究成果泄露给德国）和其他极端举措都显得过于荒谬。链式反应的概念对他们而言太过匪夷所思。（然而事实证明，仅仅半个十年后，原子弹就成为了现实。）

如今，我们必须再次直面这样一个具有链式反应特征的重大突破。虽然超级智能的概念可能听起来像是科幻小说，但在各大人工智能研究机构中，许多资深科学家都认为快速智能爆发是极有可能发生的现象。他们已经看到了这种可能性的端倪。超级智能的出现，正如当年的原子弹一样，很可能会从理论变为现实。

Next post in series:

III. The Challenges – IIIa. Racing to the Trillion-Dollar Cluster

### Reference

And much of the Cold War's perversities (cf Daniel Ellsberg's book) stemmed from merely replacing A-bombs with H-bombs, without adjusting nuclear policy and war plans to the massive capability increase.↩

The job of an AI researcher is also a job that AI researchers at AI labs just, well, know really well—so it'll be particularly intuitive to them to optimize models to be good at that job. And there will be huge incentives to do so to help them accelerate their research and their labs' competitive edge.↩

This suggests an important point in terms of the sequencing of risks from AI, by the way. A common AI threat model people point to is AI systems developing novel bioweapons, and that posing catastrophic risk. But if AI research is more straightforward to automate than biology R&D, we might get an intelligence explosion before we get extreme AI biothreats. This matters, for example, with regard to whether we should expect "bio warning shots" in time before things get crazy on AI.↩

As noted earlier, the GPT-4 API costs less today than GPT-3 when it was released—this suggests that the trend of inference efficiency wins is fast enough to keep inference costs roughly constant even as models get much more powerful. Similarly, there have been huge inference cost wins in just the year since GPT-4 was released; for example, the current version of Gemini 1.5 Pro outperforms the original GPT-4, while being roughly 10x cheaper.

We can also ground this somewhat more by considering Chinchilla scaling laws. On Chinchilla scaling laws, model size—and thus inference costs—grow with the square root of training cost, i.e. half the OOMs of the OOM scaleup of effective compute. However, in the previous piece, I suggested that algorithmic efficiency was advancing at roughly the same pace as compute scaleup, i.e. it made up roughly half of the OOMs of effective compute scaleup. If these algorithmic wins also translate into inference efficiency, that means that the algorithmic efficiencies would compensate for the naive increase in inference cost.

In practice, training compute efficiencies often, but not always, translate into inference efficiency wins. However, there are also separately many inference efficiency wins that are not training efficiency wins. So, at least in terms of the rough ballpark, assuming the $/token of frontier models stays roughly similar doesn't seem crazy.

(Of course, they'll use more tokens, i.e. more test-time compute. But that's already part of the calculation here, by pricing human-equivalents as 100 tokens/minute.)↩

GPT-4 Turbo is about $0.03/1K tokens. We supposed we would have 10s of millions of A100 equivalents, which cost ~$1 hour per GPU if A100-equivalents. If we used the API costs to translate GPUs into tokens generated, that implies 10s of millions GPUs * $1/GPU-hour * 33K tokens/$ = ~ one trillion tokens/ hour. Suppose a human does 100 tokens/min of thinking, that means a human-equivalent is 6,000 tokens/hour. One trillion tokens/hour divided by 6,000 tokens/human-hour = ~200 million human-equivalents—i.e. as if running 200 million human researchers, day and night. (And even if we reserve half the GPUs for experiment compute, we get 100 million human-researcher-equivalents.)↩

The previous footnote estimated ~1T tokens/hour, i.e. 24T tokens a day. In the previous piece, I noted that a public deduplicated CommonCrawl had around 30T tokens.↩

Jacob Steinhardt estimates that k^3 parallel copies of a model can be replaced with a single model that is k^2 faster, given some math on inference tradeoffs with a tiling scheme (that theoretically works even for k of 100 or more). Suppose initial speeds were already ~5x human speed (based on, say, GPT-4 speed on release). Then, by taking this inference penalty (with k= ~5), we'd be able to run ~1 million automated AI researchers at ~100x human speed.↩

This source benchmarks throughput of Flash at ~6x GPT-4 Turbo, and GPT-4 Turbo was faster than original GPT-4. Latency is probably also roughly 10x faster.↩

Alec Radford is an incredibly gifted and prolific researcher/engineer at OpenAI, behind many of the most important advances, though he flies under the radar some.↩

25 OOMs of algorithmic progress on top of GPT-4, for example, are clearly impossible: that would imply it would be possible to train a GPT-4-level model with just a handful of FLOP.↩

The 10x speed robots doing physical R&D in the real world is the "slow version"; in reality the superintelligences will try to do as much R&D as possible in simulation, like AlphaFold or manufacturing "digital twins".↩

Why isn't "factorio-world"—build a factory, that produces more factories, producing even more factories, doubling factories until eventually your entire planet is quickly covered in factories—possible today? Well, labor is constrained—you can accumulate capital (factories, tools, etc.), but that runs into diminishing returns because it's constrained by a fixed labor force. With robots and AI systems being able to fully automate labor, that removes that constraint; robo-factories could produce more robo-factories in an ~unconstrained way, leading to an industrial explosion. See more economic growth models of this here.↩