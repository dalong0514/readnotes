Vlatko Vedral.(2018).2024111Decoding-Reality.Oxford University Press => 1001Children of the Aimless Chance: Randomness versus Determinism

## 1001Children of the Aimless Chance: Randomness versus Determinism

In our search for the ultimate law, P, that allows us to encode the whole of reality we have come across a very fundamental obstacle. As Deutsch argued, P cannot be all-encompassing, simply because it cannot explain its own origins. We need a law more fundamental than P, from which P can be derived. But then this more fundamental law also needs to come from somewhere. This is like the metaphor of the painter in the lunatic asylum, who is trying to paint a picture of the garden he is sitting in. He can never find a way to completely include himself in the picture and gets caught in an infinite regression.

Does this mean we can never understand the whole of reality? Maybe so, given that any postulate that we start from needs its own explanation. Any law that underlies reality ultimately needs an a priori law. This puts us in a bit of a ‘Catch 22' situation. So, are we resigned to failure or is there a way out? Is there some fundamental level at which events have no a priori causes and we can break the infinite regression?

What does it mean for an event to have no a priori cause? This means that, even with all prior knowledge, we cannot infer that this event will take place. Furthermore, if there were genuinely acausal events in this Universe, this would imply a fundamentally random element of reality that cannot be reduced to anything deterministic.

This is a hugely controversial area, with various proponents of religion, science, and philosophy all having a quite contrasting set of views on this. Often people get very emotional over this question, as it has profound implications for us as human beings. Could it be that some events just don't have first causes? The British philosopher Bertrand Russell thought so. In Russell's famous debate with Reverend Copleston on the origin of the world, Copleston thought everything must have a cause, and therefore the world has a cause – and this cause is ultimately God himself. Copleston asks: ‘But your general point then, Lord Russell, is that it's illegitimate even to ask the question of the cause of the world?', to which Russell replies: ‘Yes, that's my position', and the whole debate reaches a dead-end as the two diametrically opposing views refused to compromise.

Interestingly, the quantum theory of information adds a novel twist to this millennia-old question of determinism versus randomness. This question is not just important for our understanding of reality, it is also important to us on a very personal level. The answer to this impacts on whether there is any room for genuinely free action in an ordered and structured Universe like ours. If the laws of reality govern everything, then even our actions would be subjugated and determined by them. This of course leaves us no room for the human element we call ‘free will' – a property that we strongly feel distinguishes us from non-living matter (and other animals). It is also seen as the basis of our consciousness.

Most of us in the West feel that determinism cannot completely govern reality because we are certain that we have free will, though what exactly this amounts to is far from uncontroversial. For the sake of discussion, let us define free will as the capacity for persons to control their actions in a manner not imposed by previous events, i.e. as containing some element of randomness as well as some element of determinism. So, if we accept the notion that we do indeed have free will, then we are already, in some sense, entertaining the idea that there may be a random element to reality (obviously not all elements of reality can be random, because this too would exclude any role for free will).

This still raises an intriguing question, simply because either of the two possible answers – ‘yes we do have free will', or ‘no we don't'–seem to lead to a contradiction. For example, suppose you answer with ‘yes, we have free will'. How would you demonstrate the validity of this statement? You would need to act in a way that would not be predetermined by anything. But how can this ever be, when whatever you do, can, in fact, be predetermined by something? To further qualify this argument, say you decide to act out of character, e.g. having an introverted personality you decide to start a conversation with a complete stranger on the street. But, the very fact that you decided to act contrary to your usual predisposition, seems itself to be fully predetermined. It is simply so by the fact that you determined that you would act out of character to prove free will. In this case perhaps in trying to prove free will, you are more likely to demonstrate that actually you have none. Your emotions could have been controlled by some outside factors which lead you to the conclusion that you must act out of character. If they were, all you are really trying to do is deterministically fight determinism, which is by definition a deterministic process!

The considerable difficulty in demonstrating free will conclusively lead us to postulate that perhaps we cannot have it. But this answer feels completely contrary to the whole of human psychology. Can nothing good that I do be attributed to me? Is it all predetermined by my genes or my history or my parents or social order or the rest of the Universe? Even worse, we tend to reward people for doing good deeds and punish them for bad ones. This would seem to be completely misconceived if humans did indeed not have any free will. How can you punish someone for doing something when they are not free to do otherwise? Is our whole moral and judicial system based on the illusion of free will? This just feels wrong, although there is no logical reason why free will is necessary.

That we seem to have no way of proving that there is free will was poetically stated by the famous biologist Thomas Henry Huxley: ‘What proof is there that brutes are other than a superior race of marionettes, which eat without pleasure, cry without pain, desire nothing, know nothing, and only simulate intelligence?'

Free will lies somewhere between randomness and determinism which seem to be at the opposite extremes in reality. It's clear that neither pure randomness or pure determinism would leave any room for free will. If the world is completely random, then by definition we have no control over what will happen, and if the world is completely deterministic we also similarly have no control over what will happen as it would all be pre-scripted. So you are stuck between a rock and a hard place.

But are randomness and determinism actually opposite extremes when it comes to defining reality? Are they mutually exclusive, meaning that they cannot both exist within the same framework? Our latest model of physics, quantum theory, suggests that there is a way of combining the two. Every quantum event is fundamentally random, yet we find that large objects behave deterministically. How can this be?

The answer is that sometimes when we combine many random things, a more predictable outcome can emerge. This may seem paradoxical at first (shouldn't lots of random things give you something even more random?) but this is not necessarily the case.

Imagine you toss a coin 100 times. Each of the outcomes is very uncertain and you'd have difficulty predicting it. Your success would not be better than 50%, which amounts to just random guessing. But imagine that instead of betting on individual outcomes, you in fact bet on the number of heads and tails when 100 tosses have finished. This is much easier, since 50 heads and 50 tails is what you would expect if the coin was fair. So, each toss is random, but overall a predictable pattern emerges.

In physics we encounter this all the time. Every atom in a magnet can be thought of as a mini-magnet but its behaviour is very erratic. A single atom's magnetic axis is impossible to predict, unless we apply a strong external influence to align it. However, even without this external influence, all these randomly aligned atoms can together still produce a magnet with a clearly defined north and south. Therefore, a deterministic event can emerge out of a random one.

Randomness at the microscopic level therefore does not always propagate to the macroscopic level. It is perfectly plausible that in spite of quantum mechanics being our most accurate description of Nature, the world of large objects – the one that matters to us humans most in our everyday lives – is fully deterministic. This would imply that even though the world is random at the microscopic level, there is still no free will at the macroscopic level.

But what exactly does it mean to be random? We think of tossing a coin and observing the outcome (heads or tails) as a random process. It is random because before we observe the outcome it has an equal chance of being heads or tails. And it is very difficult to predict the outcome. But what if we knew all about the coin? These would include its weight, exactly how it was tossed, and any relevant properties of the air around it. Then Newton's laws tell us that we should be able to predict the outcome of each toss. Therefore, based on classical physics, randomness is superficial. There is no fundamental randomness once we have all the information.

Quantum physics, with its peculiar blend of randomness and determinism, obviously makes our debate about these topics far richer. Uncertainty in classical physics, unlike in quantum physics, is not fundamental but simply arises out of our ignorance of certain facts. George Boole expresses this idea clearly: ‘Probability is expectation founded upon partial knowledge. A perfect acquaintance with all the circumstances affecting the occurrence of an event would change expectation into certainty, and leave neither room nor demand for a theory of probabilities.' In quantum physics however this statement does not hold. One of the most fundamental and defining features of quantum theory is that even when we have all information about a system, the outcome is still probabilistic. According to quantum theory, reality can be fundamentally random and not just apparently random (i.e. where we may be missing some information).

Here is a very simple experiment that is a quantum mechanical equivalent of tossing a coin. Imagine a photon, which as we said is a particle of light, encountering a beam-splitter. Recall that a beam-splitter is just a mirror that has a certain silver coating on it, so that we can tune the probability with which the photon is reflected or transmitted. Let us say that these probabilities are made equal, so that we have the exact equivalent of a fair coin.

Just as a coin, when tossed, can land heads or tails, so the photon impinging on a beam-splitter can go through, or be reflected. And, as far as all experiments indicate, the beam-splitter experiment is completely random. Each time a photon is sent to a beam-splitter, we can in no way predict its path subsequently. Reflection or transmission are equally likely and occur randomly.

But now I would like to make a distinction between a coin and a photon. The behaviour of a coin is random, not fundamentally, but because it is unpredictable. The behaviour of a photon is not just unpredictable, it is genuinely random. What can this possibly mean?

The coin is governed by the laws of classical physics. If we knew the exact initial conditions of the coin toss, namely the speed and angle, then we would in principle be able to fully predict the outcome. But it may take us a very long time to be able to compute the outcome. Perhaps it takes years or longer to do this. Nevertheless, the equations that govern the coin dynamics are fully deterministic and in principle can be solved to tell us the outcome. Therefore, the coin toss only appears random. It is really deterministic, but difficult to predict.

Now let's consider the situation with the photon and the beam-splitter. Here the equations are quantum, and they are also deterministic. The indeterminism of quantum mechanics manifests itself only through deliberate or non-deliberate (environmental) measurements. When the system is undisturbed by either, we have a clear and well-defined deterministic view of it as described by the Schrödinger equation. And what this equation tells us is that after the beam-splitter, the photon has both gone through and has been reflected. Yes. Both possibilities occur in reality. The photon is now in two places at once. It is behind the beam-splitter, having gone through, and in front of it, having been reflected. Unlike the coin toss, it is very simple to solve quantum equations to reach this conclusion. But the conclusion seems paradoxical!

And what happened to randomness? It comes in through the backdoor. Imagine that we now want to check where the photon is, i.e. we want to make a deliberate measurement. If we put detectors behind and in front of the beam-splitter they will then record the presence or absence of the photon. Let us say that the detection, when it happens, is amplified to a loud click. What happens in the actual experiments? The detectors click randomly.

There is absolutely no way to tell which detector will click in each particular run. This randomness is genuine; it is not just unpredictable as a coin toss is. Why? How do we know that the equation telling us that the photon is on both sides before it is measured cannot be supplemented by another deterministic law to tell us what happens when the photon is detected?

The reason why we know that the ‘coin toss type' randomness is not involved is as follows. Suppose that instead of measuring the photon after the first beam-splitter we put another beam-splitter after the first one instead. What happens then? If the behaviour of the photon was classical, then it would be deterministic at both beam-splitters, but the outcome would be even more difficult to predict. We would therefore expect the photon to have random outcomes at the second beam-splitter too. Half of the time it should be detected in front of it and half of the time behind it.

But this is not what happens in the lab. In an actual experiment, the photon now deterministically always ends up behind the second beam-splitter. So, two quantum random processes have, in a way, cancelled out to give something deterministic. And this is something that cannot happen classically. Imagine that you toss a fair coin twice and you find out that it always comes out heads both times. But when you toss it once sometimes it is heads and sometimes it is tails. This could, of course, never happen. But in the case of a photon it does. This is why we believe that there is something very different in the behaviour of the photon to what any classical system would do.

So what have we learnt? We've learnt that all elementary quantum events are in fact fundamentally random. But this of course doesn't mean that a reality built on these random events must necessarily be random. We have seen how randomness and determinism can co-exist and moreover determinism can emerge from random origins. This gives us some insight into the origins and nature of reality.

There is a beautiful quantum protocol that illustrates how randomness and determinism can work hand in hand to produce a stunning outcome. Think Star Trek and the teleportation chamber. Does anyone actually believe that teleportation is possible? No? Well you'd better believe it! Teleportation ‘dematerializes' an object positioned at a location A only to make it reappear at a distant location B at some later time. OK, so quantum teleportation differs slightly, because we are not teleporting the whole object but just its quantum information from particle A to particle B, but the principle is the same (after all, the whole thesis of this book is that we are all just bits of information).

As all quantum particles are indistinguishable anyway, this amounts to ‘real' teleportation. What I mean by this is that all electrons, for example, have identical properties (mass, charge) and the only distinguishing feature is how they spin. Given that all other properties of two electrons are the same, if we also manage to encode one electron spin into another, then we can consider this a successful transfer of quantum information between the two. In other words, the second electron becomes an identical copy of the first, and the two are now indistinguishable. This is true for all particles such as protons, atoms, and so on.

One way of performing teleportation (and certainly the way portrayed in various science fiction movies, e.g. The Fly) is first to learn all the properties of that object (i.e. get all the information about what makes that object) and then send this information as a classical string of data to location B where the object is then re-created.

One problem with this proposal is that, if we have a single electron and we don't know its spin, we cannot determine it because this would require us to make a measurement thereby invariably damaging the original quantum information in the spin. So it would seem that the laws of quantum mechanics prohibit teleportation of a single quantum system (unless we know its state in advance).

However, it will turn out that there is no need to learn the state of the system in order to teleport it. All we need to do is use mutual quantum information, of the same sort that exists in a quantum computer. This provides super-correlation between locations A and B (technically known as quantum entanglement), so that quantum information can be transferred. Even though the quantum measurement driving the teleportation is still random, this can be overcome by sending some auxiliary classical information from A to B. The actual measurement outcome at A would be communicated to B and this could even be done over a standard telephone line. After the teleportation is completed, the original state of the particle at A is destroyed. This then illustrates how teleportation at the quantum level can actually be performed – and this in spite of the intrinsic randomness.

Currently we can teleport only individual atoms and photons over only a couple of metres. The basic principle has therefore been experimentally verified, first by Anton Zeilinger's research group at the University of Vienna, and independently by Francesco de Martini's group at the University of Rome, but the question still remains if we can ever use this to teleport larger objects and ultimately humans. With humans things could get even more complicated. If we faithfully teleported every atom in your body, would that necessarily be the same as teleporting you? In other words, is your body all there is to you? The answer to this is that we really have no idea!

Now, in a more general sense, in order to distinguish superficial (classical) from real (quantum) randomness we really need a clearer measure in order to quantify the difference. Interestingly enough, one such measure does exist, and although it is closely related to the Shannon entropy, it has quite a different motivation. To understand why the Shannon entropy may itself not be so useful as a measure of randomness, consider the following example.

A typical random process as indicated is a coin toss. It generates a sequence of heads (H) and tails (T). Typically, after 10 tosses, say, we expect to have something looking like HHTHTTTHTH. We much less expect to see something like HHHHHHHHHH. The first sequence just looks like one that should come out of a random source, like a coin. The second looks too ordered to be genuinely random. We might also think of the second one as being less likely. But this is a mistake.

Here we therefore have a problem. Since heads and tails have an equal probability of one-half, any sequence of heads and tails is as likely to come up as any other sequence (people playing the lottery rarely appreciate this fact: 1, 2, 3, 4, 5, 6 is as likely as 2, 3, 17, 30, 41, 45). It is true that a sequence with half heads and half tails is more likely than the one with all heads. But this is because there are more such sequences: there is only one sequence involving all heads, while there are about a thousand sequences with 50–50 heads/tails (HHHHHTTTTT, or HTHTHTHTHT, or TTTHHTTHHH, and so on).

Any particular random sequence is as likely as any other. But, still, we have a strong feeling that while the sequence HHTHTTTHTH looks random, the sequence HHHHHHHHHH, appears to be very orderly. Their probability seems not to be able to capture this basic difference since both are equally likely (or unlikely; the probability is about one in a thousand for either). And this is why Shannon's entropy fails to quantify randomness, as it is based only on probabilities.

The measure used to solve the problem of quantifying randomness was ultimately introduced by a Russian mathematician, Andrey Kolmogorov, in the late 1950s. His solution is in some sense very operational – or, better still, physical. His solution to randomness was as follows. How random a sequence is depends on how difficult it is to produce this sequence. Produce it with what? Answer: a computer.

Imagine writing a program for your computer to generate a sequence of heads and tails. The sequence of all heads (HHHHHHHHHH) only needs one instruction: ‘print 10 heads'. But a sequence HTHHHTTHTH cannot be generated as easily. In fact the program probably just needs to say ‘print the sequence HTHHHTTHTH'. Therefore, when we have a random looking sequence, the program to generate it is at least as long as the sequence itself (in our case it is longer since it requires additional instructions, e.g. ‘initialize computer, print the sequence, end program', etc. but this will be a marginal difference when the string is very long anyway). However, when something is orderly, the program can be much shorter. The quantity that tells us by how much orderly things can be compressed to shorter programs is known as Kolmogorov's complexity.

Are there any problems with this definition? If we are not careful it seems that different computers could give us different estimates of how random something is. Fortunately we remind ourselves of Turing's notion of the universal computer, a computer that can simulate any other. So, to avoid such issues, Kolmogorov suggested calculating the Kolmogorov complexity using this universal computer.

Using Kolmogorov's ideas, we can re-examine the difference between classical superficial randomness and quantum fundamental randomness. We said that the sequence of tosses of a coin of the type HHHTTTHTTHH is random because it cannot be given a shorter program to generate it. Imagine having all the information that fully captures this coin toss (e.g. the relative weight difference between the head and tail face of the coin, speed of rotation, height thrown to, etc.). With all this information fixed, to the correct level of precision, we could write a program that could generate the sequence for any number of coin tosses. This is because classical physics is fully deterministic. The length of this program in classical physics would necessarily be shorter than doing the coin tosses themselves (for a large number of coin tosses). In the quantum case, this would not be so. Given the problem of predicting the sequence of photon clicks in a detector (our quantum coin toss) the program used to describe the sequence could only describe it by actually running each experiment individually. Therefore, the program would have to be at least the same length as the sequence of detector clicks it is trying to describe. There is no short-cut!

An intriguing possibility now emerges. Can we apply Kolmogorov's logic to understand the origins of reality? Can we say that our laws that describe reality are exactly this – an attempt to understand the dynamics of the Universe and reduce its complexity? In this way, the laws themselves (in physics, biology, economics, sociology) could all be seen as short programs describing the makeup of reality. For example, in physics, rather than running each and every experiment (i.e. writing the whole program) we can write a shortened program which only uses the current laws of physics to predict the outcome of each experiment. This latter program would hence be significantly more efficient than the program that actually runs all the experiments individually. In this way we can consider our duty as scientists to find the shortest program that represents reality. So let us look into how this reduction in complexity is achieved in science.

To understand what exactly is at stake, we first need to understand the logic of science, physics in particular. This was something that the philosopher Karl Popper devoted his life to. His way of understanding science will be the key to understanding reality in my final chapter. So let us summarize it here.

When Popper was growing up in the 1920s, physics was reaching its peak, but some other disciplines started to emerge, which are now known as social sciences. Sigmund Freud was pioneering psychology (through psychoanalysis) and sociology and political sciences were also emerging. While some people would be happy to call social sciences by the name of ‘soft' sciences (as opposed to hard, fact-based science, such as physics), Popper was altogether very concerned with attaching the name science to anything like psychoanalysis.

His principal aim was to devise a criterion for calling something a science in the first place. The crucial idea that occurred to him is the following. While it is easy to falsify a physical theory (just do an experiment whose results clearly contradict the theory, and quantum experiments were a shining example of falsifying classical physics in Popper's time), it is not so easy to falsify a psychological theory.

How many times have you heard the remark that someone is not confident because their mother did not love them? Then again, you also hear that others are confident precisely because their mothers did not love them and they had to rely on themselves more. And so here is the problem. The theory ‘his mother did not love him' seems to be able to explain too much. So much so that it can be used to justify two diametrically opposing facts, someone being confident and another person not being confident. This means that such a theory can never be falsified, or shown to be wrong, in practice.

The famous eighteenth-century Scottish philosopher David Hume was particularly bothered by non-falsifiability of some claims (psychoanalysis didn't exist in his time, he was much more bothered by philosophy and religion). He phrased it as follows. Any number of white swans we see in the world cannot prove to us the conjecture that ‘all swans are white'. However, a sight of one single black swan is enough to destroy it. And so it is with science. Newton's physics had been tested for 200 years and always found to be correct. But one test in the late nineteenth century – in the shape of blackbody radiation – was enough to destroy it. The black body was the black swan of physics, destroying the hypothesis that all physics obeys classical physics. Of course this does not mean that classical physics is of no use to us whatsoever, it just means that there must now be a new theory (i.e. quantum mechanics) that takes into account classical physics plus its black swan. In contrast to a concrete statement that can be falsified, e.g. ‘all swans in this river are white', stands a statement like ‘God works in mysterious ways' – I mean, how the bleep are you going to disprove that?

How can we therefore be sure about anything that comes from science? We cannot. But, rather than this being a problem, Popper thought that this was the whole point of science! Namely, a theory is only genuine if there is a way of falsifying it! If under no circumstances can you disprove your theory (i.e. create an experiment to rule it out), then this theory is worthless as far as knowledge is concerned because you can never test it. Popper therefore turned a seemingly negative feature of science (the fact that any theory could be proven wrong) into its most fundamental and necessary feature. It is through centuries of refutations, falsifying (i.e. refuting), and improving on theories (i.e. conjecturing), that science has progressed to where it is today.

Let us see if we can interpret Popper's logic within the context of information theory. Once a theory is established by a few experiments, we then start to gain more confidence in its validity (though we may be, and usually are, ultimately proven wrong). As a result we attribute a higher chance to the theory passing the next test. If the theory passes it, the principles of information would say that this is a low-information event. The reason is that the higher the probability of an event, the smaller the surprise when the event happens.

As our confidence grows, the probability we attribute to the theory being falsified becomes smaller and smaller. In the current experiments testing quantum mechanics, not many in the physics community expect quantum mechanics to fail. But this is precisely why it would be a great shock if it did. So, falsifying something usually carries far more information – both emotional and physical – than confirming it.

Any information in physics comes from the scientific method Popper called ‘conjectures and refutations'. But even this method can be seen as a form of information processing. This will result in a rigorous and well-defined statement of Occam's razor. We can think of scientific theories as programs run on a universal computer, with the output being the result of whatever experiment we are trying to model. We say that our theory is powerful, if we can compress all sorts of observations into very few equations. The more we are able to compress, the better we believe we can understand something. Because then from very few laws we can generate the whole of reality.

Occam's razor just says that if there are many theories that explain something, then we should choose the shortest one as the correct one. The shortest description of Nature that generates all possible observations is to be preferred over a very long description. To quote Leibniz, whom we met in relation to one of the proofs of the existence of God, ‘God has chosen that which is the most simple in hypotheses and the most rich in phenomena'. This statement would imply that the information in the Universe is highly compressible into a few simple laws.

But now we face an interesting question. Any theory we come up with will be finite, namely it will contain a fixed (hopefully small) set of rules. And this means – as was first fully realized within information theory by Gregory Chaitin, an American mathematician – that it can only produce a finite set of results. In other words, there will be many experimental outcomes that could not be compressed within the theory. And this effectively implies that they are random. This was also realized by Leibniz who stated: ‘But when a rule is extremely complex, that which conforms to it passes as random'. This perfectly encapsulates the Kolmogorov view of randomness: when the rule is as complicated as the outcome it needs to produce, then the outcome must be seen as complex or, in other words, random.

Following this logic quantum randomness can be encapsulated in two principles, as first argued by the Italian physicist Carlo Rovelli. One of them is borrowed from classical information and simply states that the most elementary quantum system cannot hold more than one bit of information. This is almost self-evident as by definition a bit is the smallest unit of information. The second principle is that we can always obtain new information. This principle, when combined with the first, captures the fundamental randomness we see in quantum events. The only way we can obtain new information when we seemingly have all information, is if this new information is random. Can it be that this is just a restatement of the fact that a finite number of axioms can only lead to a finite number of outcomes? If this was so, the implications would be amazing.

There is still a school of thought that views randomness in quantum theory as due to its incompleteness, i.e. due to our lack of knowledge of a more detailed deterministic underlying theory. However, if we view the growth of our knowledge in physics through compression, it may suggest to us that randomness is inherent in the Universe, and therefore it must be part of any physical description of reality. Randomness could simply be there because our description of reality is always (by construction) finite and anything requiring more information than that would appear to be random (since our description could not predict it).

This would mean that randomness in quantum physics is far from unexpected – in fact according to this logic it is actually essential. Furthermore, it would mean that whatever theory – if any – superseded quantum physics, it would still have to contain some random features. This is a very profound conclusion. Given that physics is always evolving, having fundamental randomness sets a serious constraint on any new theory.

Having the potential to be wrong is what Popper identified as the key aspect of scientific knowledge. However, this should also be true of any other form of knowledge (philosophical, psychological, religious, historical, artistic, you name it). In this respect useful scientific knowledge is the same as the financial gain in betting or stock markets. If there is no risk then there is nothing to gain – i.e. no free lunch! It's not just scientific knowledge or economic profit that grows in this way, any useful information within whatever context you can describe, always grows in this way.

Science is therefore just a form of betting on future outcomes. The idea of representing our uncertainty about the Universe through gambling was, in fact, already suggested by the famous German philosopher Immanuel Kant in his Critique of Pure Reason, as long ago as 1781. Kant equated betting with the pragmatic belief in the validity of our theories. The logarithmic scoring rule (so central to Shannon) is, in a sense, a practical implementation of this philosophical suggestion.

The interplay between randomness and determinism in fact propagates throughout the book. For Popper, randomness lies in making conjectures in science, and determinism in refuting these conjectures by deliberate experimentation. This was, according to him, the only way to secure information about the world, i.e. knowledge. But in other elements of reality, useful information also emerges in exactly the same way.

For example, consider the process of evolution of biological information. Biologists mainly think of information processing in living systems as created through evolution. Evolution has two components, one is the random mutation in the genetic code and the second is deterministic natural selection by the environment of this new feature. In this way, evolution of biological information is seen to emerge analogously to creating any useful scientific knowledge.

The same is true in economics, where the central goal is to understand and predict market behaviour. Whether it is economic policy, or a simple financial investment decision, any conjectured strategy will be played out in the market, and therefore falsified or confirmed.

We have also argued that social dynamics is another form of information processing. And furthermore the level of development of a society may be viewed as synonymous with its capacity to process information. Randomness appears between elements of a society, whereas through interaction with one another, at the collective level we see deterministic characteristics through all sorts of phase transitions that societies undergo. It is probably not an accident that societies that have applied the method of conjectures and refutations more vigorously have had the opportunity to develop faster.

In physics, randomness was seen to be crucial in the concept of heat, and in fact the whole Universe was seen as evolving towards the state of maximal entropy (maximal randomness or disorder). The deterministic part of this process was to use available information to design schemes to extract useful work efficiently. The whole of thermodynamics can be seen as a battle between deliberate Maxwell's demons, trying to extract order from disorder, and natural randomizing processes.

Everywhere we look we see underlying bits of information. Furthermore this information always obeys the same evolution through randomness and determinism, independently of context. So can a combination of randomness and determinism produce all information and everything else we see around us?

无目的机遇的孩子：随机性与确定性在追寻能够描述整个现实的终极法则 P 的过程中，我们遇到了一个根本性的难题。正如物理学家 Deutsch 指出的，这个法则不可能是完美无缺的，因为它无法解释自身的起源。我们需要一个比 P 更基础的法则，从中可以推导出 P。但这个更基础的法则同样需要来源。这就像是精神病院里的画家，试图绘制自己所在花园的画面，却永远无法将自己完全纳入画中，最终陷入无限循环。

这是否意味着我们永远无法全面理解现实？很可能如此。因为我们提出的任何理论假设都需要进一步的解释。构成现实的每一个法则，最终都需要一个先验基础（即在经验之前就已存在的基本原理）。这使我们陷入了一个进退两难的困境。那么，我们是只能接受这种认知的局限性，还是存在某种可能的突破？是否存在某个基本层面，事件本身并无先前的确定原因，从而打破这种无限推演？

什么是没有预先确定原因的事件？简单来说，就是即便我们掌握了所有已知信息，依然无法预测这个事件是否会发生。更有意思的是，如果宇宙中真的存在这种没有明确因果链的事件，那将意味着现实世界中存在一种无法被完全解释的随机性 — 一种不受任何确定性规律约束的基本特性。

这是一个极具哲学和认知争议的领域，宗教、科学与哲学的不同阵营对此持有截然不同的本体论观点。这一问题往往会引发强烈的情感共鸣，因为它触及了人类存在的根本性形而上学追问。是否存在某些事件本质上不具有首要因果？英国著名哲学家伯特兰·罗素对此持肯定态度。

在罗素与科普斯顿神父关于宇宙本源的经典辩论中，科普斯顿坚持「万物必有因」的形而上学信念，认为世界必定有其根本原因，而这个终极原因即为神圣存在。科普斯顿追问："罗素勋爵，您的根本立场是认为，探询世界起源本身就是一个不合逻辑的问题吗？」罗素毫不犹豫地回应："完全正确」。

在这场思想的巅峰对决中，两种根本对立的世界观最终未能找到共同语言：一种坚持因果的必然性和神圣秩序，另一种则质疑因果追溯的终极可能性，展现出人类认知边界的根本张力。

信息的量子理论为人类探讨了千年的决定论与随机性之争带来了一个新的视角。这个问题不仅关乎我们对现实的理解，更触及我们个人内心深处的根本思考。它关乎在我们这样一个有序 structured 宇宙中是否存在真正的自由意志。如果现实的法则主宰一切，那么我们的每一个行为都将被这些法则所决定，这似乎剥夺了人类独特的自由选择空间。

这引发了一个深刻的哲学命题：我们引以为豪的「自由意志」，究竟是真实存在，还是仅仅是一种幻觉？这种被我们视为区别于机械和动物的特质，是否真的能在严丝合缝的物理法则中找到栖身之所？事实上，它也被视为人类意识的重要基础。

在西方文化中，我们大多数人都认为单纯的决定论无法完全解释现实，因为我们坚信自己拥有自由意志。尽管对于自由意志的具体内涵仍存在诸多争议。为了深入讨论，我们可以将自由意志定义为：个人能够超越既有事件的约束，自主控制自身行为的能力。这种能力既包含一定的随机性，也保留着确定性的元素。

换言之，如果我们承认自由意志的存在，实际上就已经默认现实中可能存在某种随机性。但这并不意味着现实可以完全由随机性主导 —— 若是如此，自由意志本身也将丧失意义。现实更像是确定性和随机性的微妙平衡。

这个问题依然引人深思：无论我们选择「是的，人类确实拥有自由意志」还是「不，我们没有」，似乎都会陷入某种逻辑矛盾。

假设你认为人类有自由意志，那么你该如何证明？你需要证明存在一种不受任何既定因素影响的行为。然而，在现实中，我们的每一个选择似乎都受制于某种预设的逻辑。

举个例子，一个内向的人突然决定在街头与陌生人攀谈，表面上看起来是一个自主的选择。但仔细分析，这种「反常」的行为本身可能恰恰是被预先决定的。你「选择」违背常态，其实已经暗示了这个行为背后可能有更深层的决定性因素。

更有趣的是，当你试图证明自由意志存在时，你可能恰恰证明了相反的结论。你的情绪和决定可能受到外部因素的微妙操控，使你产生了「突破常规」的错觉。本质上，你可能只是在用看似「自由」的方式，落入了更为隐蔽的决定论陷阱。

这就像是一个有趣的悖论：越是试图证明自由，越可能暴露出我们行为背后的必然性。

在证明自由意志的困难中，我们不得不怀疑人类是否真的拥有自由意志。但这种想法与我们对人类心理的直觉认知完全相悖。难道我做的好事不能归功于我自己吗？难道一切都被我的基因、个人历史、家庭教养、社会环境或宇宙规律所决定？

更令人困惑的是，我们社会习惯于奖励好人、惩罚作恶者。如果人类完全没有自由意志，这种做法岂不是毫无意义？当一个人没有选择的余地时，我们凭什么惩罚他？我们的道德和法律体系难道不是建立在自由意志这一虚幻概念之上吗？

尽管理性分析无法证明自由意志的必要性，但这种想法直觉上让人感到不安。正如著名生物学家托马斯·亨利·赫胥黎诗意地质疑的那样：野生动物是否只是一种高级的行动木偶，它们麻木地进食、哭泣，没有真正的感受和意识，仅仅是在模仿智慧？

自由意志巧妙地栖息在随机性与确定性之间，这两种看似水火不容的极端状态交织着现实的本质。显而易见的是，纯粹的随机或完全的确定都无法为自由意志留下生存的空间。若世界完全由随机主导，我们便注定无法掌控未来；若世界完全被预定轨迹支配，我们同样будет失去主导权，一切皆已被命运提前编织。这就像是被夹在两块巨石之间，进退维谷。

但是，在描绘现实的图景时，随机性和确定性真的是绝对对立的吗？它们是否真如我们想象的那样水火不容？最前沿的量子物理理论为我们提供了一个令人惊叹的视角：在微观世界中，每一个量子事件本质上都充满随机性，然而当我们放眼宏观世界时，又呈现出令人惊讶的确定性。这看似是一个不可调和的悖论，但又如何解释？

答案蕴含在复杂系统的奇妙之中：当众多随机元素聚合时，反而可能孕育出令人意外的规律性。这一发现乍看上去似乎违背直觉 —— 难道叠加随机不应该带来更大的混沌吗？然而，自然的逻辑往往超出我们的想象。

想象你抛掷一枚硬币 100 次。每次抛掷的结果都是不可预测的，就像掷骰子一样完全随机 —— 你猜对的概率就是 50%，无异于盲目猜测。但如果我们换个角度，不再关注单次抛掷的结果，而是看整体的正反面分布，情况就大不相同了。在 100 次抛掷后，我们很自然地期望看到大约 50 个正面和 50 个反面，这就是概率的神奇之处。

物理世界常常上演着类似的「混沌中的秩序」戏码。以磁铁为例，它由无数微小的原子组成，每个原子就像一个迷你磁铁，其磁轴方向完全随机且不可预测。单独看每个原子，你根本无法判断它的朝向 —— 除非用强大的外部磁场将其强制对齐。然而，奇妙的是，这些看似杂乱无章的原子在集体作用下，竟然能精准地形成一个具有清晰北极和南极的宏观磁铁。这就是科学中常说的：微观的随机性可以孕育宏观的确定性。

微观世界的不确定性并不意味着宏观世界也同样不确定。尽管量子力学是我们描述自然界最精确的理论，但我们日常生活中的大尺度世界仍可能是完全可预测的。这表明，即便微观世界充满偶然性，在宏观层面，我们的行为可能仍然遵循严格的因果规律，不存在真正的自由意志。

什么是随机？我们可以以掷硬币为例。掷硬币前，正反面出现的概率相等，结果难以预测，这就是随机。但如果我们掌握了硬币的方方面面信息 —— 比如重量、掷硬币的精确角度、周围空气的状态等，那么根据牛顿力学，我们理论上可以准确预测每次掷硬币的结果。这说明，在经典物理学视角下，随机性其实只是我们认知的局限，本质上世界是确定的。

量子物理学以其令人着迷的随机性与确定性交织，为我们的讨论增添了深刻的哲学意味。与量子物理学不同，经典物理学中的不确定性并非本质特征，而仅仅是源于认知的局限。正如数学家乔治·布尔精辟地指出："概率是建立在部分知识基础上的期望。如果我们能够完全了解影响事件发生的所有细节，期望就会转变为确定性，概率理论也将失去存在的意义。"

然而，在量子物理学的世界里，这一论断却不再成立。量子理论最根本的特征之一是，即便我们掌握了系统的全部信息，其结果仍将呈现概率性。更为根本的是，根据量子理论，现实本身可能内在地具有随机性，而非仅仅是由于我们认知上的局限而显得随机。

这是一个极其巧妙的实验，堪称抛硬币的量子力学版本。想象一个光子（光的基本粒子）遇到了分束器。分束器其实是一种特殊的镜子，通过银色涂层可以精确调控光子被反射或透射的概率。在这个实验中，我们将反射和透射的概率设置为相等，就像是一个完全公平的硬币。

就像抛硬币会出现正面或反面一样，光子撞击分束器时也可能穿过或被反射。所有实验都表明，这个过程是完全随机的。每次将光子送入分束器，科学家都无法预测它最终会走哪条路径。反射和透射的可能性完全相同，纯属偶然。

但这里有一个非常有趣的区别：硬币的「随机」只是因为我们无法精确测量其受到的物理影响，本质上仍遵循确定性规律。而光子的随机性则更为根本 ── 它真的是完全不可预测的。这种根本的随机性究竟意味着什么？这正是量子世界令人着迷的地方。

硬币的运动完全遵循经典物理学定律。如果我们能精确掌握硬币抛掷的初始条件，如初始速度和角度，理论上就可以完全预测其落地结果。然而，计算这个结果可能需要耗费极长的时间，甚至可能是数年。尽管如此，描述硬币运动的物理方程是完全确定的，原则上可以通过求解这些方程来预测结果。换言之，硬币抛掷看似随机，实际上是完全可以确定的，只是在实践中极其难以精确预测。

现在让我们探讨光子与分光镜（beam-splitter）的有趣案例。在量子世界中，方程看似确定，但实际充满了概率性。量子力学的诡异之处在于：只有当我们进行测量时，系统的不确定性才会真正显现。在没有外部干扰的情况下，系统可以用薛定谔方程（Schrödinger equation）精确地描述。

这个方程揭示了一个令人惊叹的现象：光子可以同时存在于两个位置。换言之，它既穿过了分光镜，又被分光镜反射。这看似违背常理，但在量子世界中却是真实存在的。与传统的随机事件（如掷硬币）不同，量子系统中的这种「同时存在」是可以通过严谨的数学方程直接推导出来的。尽管如此，这个结论依然令人难以置信 —— 一个粒子竟然可以同时处于两个不同的空间位置！

量子世界中的随机性究竟从何而来？它神秘地通过「后门」悄然进入。设想我们想精确定位光子，进行一次精心设计的测量。假如在光学分束器（一种将光束分成两部分的光学元件）的前后安装探测器，它们将记录光子的存在与否。当探测发生时，探测器会产生一个放大的响亮信号。

在实际实验中，探测器的响应呈现出完全随机的特征。在每次实验运行中，我们都无法预测哪个探测器会被触发。这种量子随机性与传统随机事件（如掷硬币）有本质不同：它不仅仅是不可预测，而是具有根本的不确定性。

我们如何确定描述光子在测量前同时存在于两侧的量子方程不能被某种确定性定律补充，从而精确预测光子探测的结果呢？这个问题直指量子力学最深层的哲学之谜。

我们可以通过以下论证，排除「简单随机」模型的影响。设想在第一个光束分离器后，我们不测量光子，而是再放置一个分束器。在这种情况下会发生什么？如果光子遵循传统的经典物理行为，它在两个分束器上的路径理应是确定的，但实际结果将变得更加不可预测。按照经典物理的预期，我们会认为光子在第二个分束器上仍会呈现随机分布。也就是说，光子被检测到分束器前方和后方的概率各占一半。

然而，现实实验中的情况并非如此。在实际实验中，光子会以确定的方式出现在第二个光学分束器后。这意味着两个量子随机事件相互抵消，最终呈现出确定性结果 — 这在经典物理系统中是不可想象的。

让我们用一个有趣的类比来理解：假设你掷一枚公平的硬币两次，诡异的是，它每次都是正面朝上。但如果单独掷一次，有时是正面，有时是反面。这在传统物理世界是绝对不可能发生的。然而，在量子世界中，这种看似荒谬的情况却真实存在。这正是量子系统如此独特和令人着迷的原因。

通过这个实验，我们揭示了一个重要的量子力学原理：所有基本量子事件本质上都是随机的。但这并不意味着由这些随机事件构建的现实本身就是混沌无序的。事实上，我们观察到随机性和确定性可以共存，并且确定性甚至可以从看似混乱的随机起源中涌现。这为我们理解宇宙的深层运作机制提供了独特的视角。

在量子世界中，有一种令人惊叹的协议展示了随机性和确定性如何惊人地协同工作。这就像是科幻电影《星际迷航》中的传送舱，听起来几乎不可能 —— 但科学家们已经证明，这种看似不可能的传输确实存在！

量子传送（Quantum Teleportation）并不像科幻作品中那样直接传送整个物体，而是传递物体的量子信息。想象一下，我们可以将一个粒子在 A 地的所有量子特性「复制」到 B 地的另一个粒子上。这个过程虽然不会像传送舱那样瞬间移动物理实体，但在信息层面上实现了惊人的「瞬移」。

在更深层次上，这种现象揭示了一个 profound 的观点：信息或许是构成我们世界最基本的元素。就像这本书所暗示的，我们可能只是由复杂信息编织而成的存在。

在量子世界里，所有粒子都有一个有趣的特点：它们本质上是「相同的」。就拿电子来说，除了自旋方向不同，它们的质量、电荷都完全一致。想象一下，如果我们能够精确地将一个电子的自旋「复制」到另一个电子上，那么这两个电子就变得无法区分了 — 这正是量子传送的神奇之处！这个原理不仅适用于电子，对于质子、原子等其他微观粒子同样适用。

量子传送在科幻电影中常常被描绘得很有戏剧性，比如经典影片《苍蝇》。其基本思路是：先彻底「扫描」一个物体，获取构成它的所有信息，然后将这些信息作为数据传输到另一个地点，最后在目标地点重新「重建」这个物体。

这个方案存在一个关键问题：对于一个单个电子，如果我们不知道其自旋状态，就无法确定其状态。这是因为测量过程本身会立即破坏原始的量子信息 — 这正是量子力学中著名的测量坍缩效应。根据海森堡不确定性原理，一旦我们试图测量电子的自旋，就会不可逆地改变其量子状态。因此，量子力学似乎禁止对未知量子系统进行隐形传态（除非我们预先精确知道其量子态）。

在量子世界中，传送一个粒子的状态并不像我们想象的那么复杂。关键在于利用量子系统中的特殊信息关联，类似于量子计算机内部的信息传递机制。这种关联会在 A 和 B 两个位置之间建立一种神奇的「即时通信」通道，科学家称之为量子纠缠 — 一种超越传统物理定律的奇特现象。

虽然量子传送过程中的测量看起来充满随机性，但科学家已经找到了破解这一难题的方法。通过在 A 和 B 之间传递一些辅助的经典信息，比如通过普通电话线，就可以有效地协调这种随机性。传送完成后，原始位置的粒子状态会被「删除」，就像信息在不同载体间完成了一次神奇的转移。

这种看似不可能的量子传送，实际上揭示了微观世界中信息传递的非凡潜力，突破了我们对物理世界的传统认知。

目前，我们仅能在几米范围内实现单个原子和光子的量子隐形传态。这一基本原理已通过实验验证，首先由维也纳大学的 Anton Zeilinger 研究团队证实，随后罗马大学的 Francesco de Martini 团队独立验证。然而，是否能够利用这种技术传送更大的物体，乃至最终传送人类，这个问题仍悬而未决。涉及人类时，复杂性会进一步增加。如果我们精确地传送构成你身体的每一个原子，这是否真正等同于传送「你"？换言之，你的身体就是你的全部吗？对此，科学界尚未给出明确的答案！

从更广泛的科学视角看，为了准确区分表面的（经典）随机性和本质的（量子）随机性，我们需要一种更精确的量化方法。值得注意的是，这样的量化指标确实存在。尽管该指标与香农熵密切相关，但其理论基础却截然不同。要理解为什么香农熵可能不足以衡量随机性，我们可以通过一个具体的例证来说明。

在随机过程中，硬币投掷是一个典型的例子。它会产生一串正面（H）和反面（T）的组合。通常在投掷 10 次后，我们期望得到类似 HHTHTTTHTH 这样的结果。而像 HHHHHHHHHH 这种全是正面的序列则不太可能出现。第一个序列看起来很「随机」，就像是用硬币随机掷出的；第二个序列看起来太过有序，不像是真正的随机结果。我们可能会直觉地认为全正面的序列不太可能发生。但这 actually 是一个误解。

这就引出了一个有趣的问题。由于硬币每次正反面朝上的概率都是 50%，实际上任何一种正反面组合的序列出现的概率都是相同的（这是很多买彩票的人难以理解的：比如 1，2，3，4，5，6 和 2，3，17，30，41，45 这样的号码组合概率完全相同）。诚然，有一半正面、一半反面的序列看起来比全正面的序列「更可能」发生。但这仅仅是因为这种序列的组合方式更多：全正面的序列只有一种，而 50-50 正反面的序列则有成千上万种（比如 HHHHHTTTTT、HTHTHTHTHT、TTTHHTTHHH 等）。

在随机序列中，任何特定序列出现的可能性都是相同的。然而，我们直觉上会感受到一个有趣的现象：序列 HHTHTTTHTH 看起来是随机的，而序列 HHHHHHHHHH 则显得非常有序。尽管从概率角度看，这两个序列同样罕见（约千分之一的概率），但它们给人的感受截然不同。这恰恰揭示了传统概率理论的局限性 ——Shannon 信息熵（Shannon Entropy）仅仅依赖概率，无法真正量化随机性的本质。

为了解决这一问题，俄罗斯数学家 Andrey Kolmogorov 在 1950 年代末提出了一种极具洞察力的方法。他从计算的角度重新定义了随机性：一个序列的随机程度，取决于用计算机生成该序列的复杂程度。换言之，生成序列越困难，该序列就越「随机」。

想象一下为你的电脑编写一个程序，用于生成硬币的正反面序列。全是正面朝上的序列（HHHHHHHHHH）只需要一个简单指令：' 打印 10 个正面 '。但对于看似随机的序列 HTHHHTTHTH，生成就没那么容易了。事实上，程序可能只能直接 ' 打印这个序列 HTHHHTTHTH'。

当我们面对看似杂乱无章的序列时，生成它的程序长度至少要与序列本身一样长（考虑到还需要一些额外指令，如 ' 初始化计算机、打印序列、结束程序 ' 等）。相比之下，当序列具有某种规律时，描述和生成它的程序可以变得非常简短。

衡量这种「可压缩性」的关键概念，就是柯尔莫戈洛夫复杂度（Kolmogorov complexity）。这个概念可以量化描述一个序列有多大程度上可以被简化或压缩，本质上反映了序列中隐藏的规律和结构。

这个定义是否存在根本性缺陷？如果我们在计算随机性时不够严谨，不同的计算机可能会对同一对象的随机性给出截然不同的评估。幸运的是，我们可以回溯到图灵提出的通用计算机概念 — 一种能够模拟任何其他计算机行为的理想计算机。为了消除这种潜在的计算不确定性，Kolmogorov 建议使用这个通用计算机来计算 Kolmogorov 复杂度，从而确保计算的一致性和普适性。

借助柯尔莫哥洛夫的思想，我们可以重新审视古典物理中的表面随机性与量子物理中的本质随机性之间的根本差异。在经典物理中，看似随机的事件（如抛硬币序列 HHHTTTHTTHH）实际上是可以通过完整的初始条件精确预测的。如果我们掌握了影响硬币抛掷结果的所有细节 —— 如硬币正反面的重量差异、旋转速度和抛掷高度等，理论上就可以构建一个精确的计算机程序来模拟和预测整个抛硬币过程。

然而，在量子世界中，情况则完全不同。以光子探测实验为例，即使我们拥有看似全面的实验参数，也无法预先确定光子检测事件的具体序列。在这种情况下，描述这一随机过程的计算机程序本身必须逐一模拟每一个独立的实验过程，其程序长度将与实际观测序列的长度相当。换言之，量子随机性是不可压缩的，没有任何捷径可以绕过这一本质的不确定性。

科学探索中，一个极具启发性的思路正在浮现：我们能否借助柯尔莫戈洛夫的逻辑，窥探现实的深层本质？换句话说，我们用来描述世界的科学定律，本质上就是人类试图理解宇宙运行机制，并将其繁杂现象简化的努力。

从这个角度看，无论是物理学、生物学、经济学还是社会学，其科学定律都可以被视为描述现实本质的精简「算法」。就拿物理学来说，科学家们不再需要逐一进行每一个具体实验（这相当于编写一个庞大而复杂的程序），而是可以构建一个更加简洁的理论模型，仅依靠现有的物理定律就能预测各种实验的结果。这种方法不仅大大提高了研究效率，还揭示了科学追求的终极目标：找到能用最简洁的方式解释复杂现实的基本原理。

科学家们的使命，正是不断缩减描述现实的「算法长度」，用最少的概念和公式，阐释最多的自然现象。

要真正把握问题的核心，我们首先需要理解科学的逻辑，尤其是物理学的逻辑。这正是哲学家卡尔·波普尔毕生致力研究的课题。他对科学的理解方法，将成为我最后一章阐释现实的关键。让我们在此对此进行简要总结。

在 1920 年代成长的波普尔，见证了物理学的巅峰时期，同时也看到了社会科学的兴起。西格蒙德·弗洛伊德正在用精神分析开辟心理学的新疆界，社会学和政治科学也悄然兴起。虽然一些人习惯将社会科学称为「软科学」，以区别于物理学等「硬科学」，但波普尔对于将「科学」这一称谓轻易地应用于精神分析等领域持严重保留态度。

波普尔（Popper）最初的研究目标是建立一个科学性的判断标准。他提出了一个关键的思想：在物理学领域，证伪（falsification）一个理论相对容易 - 只需设计一个实验，其结果明确地与现有理论相矛盾（量子实验正是当时证伪经典物理学的典型例子）。然而，在心理学理论中，证伪就变得极其困难。

想想我们经常听到的那些关于心理成因的解释：有人说一个人缺乏自信是因为母亲没有爱他，但同时又有人认为正是因为母亲的冷漠，他们才变得更加独立和自信。这个看似矛盾的现象正是一个理论是否科学的关键所在。

这类心理学解释存在一个根本问题：它们过于万能，可以同时解释相反的结果。"母亲没有爱」这样的理论可以被用来解释两种截然不同的心理状态 - 缺乏自信和充满自信。这种理论的特点是它们几乎可以解释任何结果，因此实际上是无法被证伪的，也就是说，无法通过具体的证据来证明它们是错误的。

18 世纪著名的苏格兰哲学家大卫·休谟，对于一些难以被反驳的说法深感困扰（彼时精神分析学尚未兴起，他更多地关注哲学和宗教）。他形象地举了一个例子：无论我们在世界上看到多少只白天鹅，都无法证明「所有天鹅都是白色」这一推测。但是，只要出现一只黑天鹅，这个推测就会立即被推翻。科学研究也是如此。

牛顿物理学被验证了 200 年，一直被认为是正确的。然而，在 19 世纪末，黑体辐射实验成为了一匹「黑天鹅」，彻底颠覆了「所有物理现象都遵循经典物理定律」的信念。这并不意味着经典物理学毫无价值，而是表明我们需要一种新的理论（量子力学），这个理论既要包含经典物理学的成果，又能解释那些看似矛盾的现象。

与可以被清晰验证的陈述（如「这条河里的所有天鹅都是白色"）不同，有些说法如「上帝以神秘莫测的方式运作」就显得模糊不清 —— 这种说法根本无从反驳，对吧？

我们能对科学得出的结论绝对确定吗？答案是不能。但正如哲学家卡尔·波普尔所洞察的，这恰恰构成了科学的魅力和本质！他认为，一个真正的科学理论必须是可被证伪的。如果一个理论在任何条件下都无法被否定或设计出有效的实验来检验，那么它对于知识探索而言就毫无价值。

波普尔巧妙地将科学看似的「缺陷"—— 即理论随时可能被推翻，转变为科学进步的核心机制。科学正是通过不断地质疑、证伪和改进理论，在反复推敲和检验中不断前进。这种开放和自我纠正的特性，使科学成为一个持续进化的知识体系，而非僵化的教条。

让我们尝试用信息论的视角解读波普尔的科学哲学逻辑。波普尔认为，科学理论的发展是一个不断检验和可能被推翻的过程。当一个理论经过最初的几次实验后，科学家们会逐渐对其可靠性增强信心（尽管历史表明，今天被认为正确的理论，未来很可能会被推翻）。

在信息论的框架下，当一个理论的验证概率增加时，其携带的信息量反而会减少。就像天气预报预测晴天，如果连续多天都是晴天，每一次的「晴天」所带来的信息价值就会越来越低。相比之下，如果天气预报突然出现意料之外的结果，那么这个「意外」本身就极具信息价值。

在科学研究中，对于当前看似牢不可破的理论（如量子力学），物理学家们普遍不认为它会失败。然而，正是这种高度自信，使得一旦理论真的被证伪，其冲击力将是巨大的。换句话说，颠覆性的发现往往比渐进式的确认更能推动科学的进步，因为它们能够根本性地改变我们的认知。

在物理学中，所有的知识都源于科学家波普尔提出的「猜想与反驳」的科学研究方法。这一方法本身也可以被看作是一种信息处理过程。通过这种方法，我们可以对著名的「奥卡姆剃刀原理」做出更严谨的阐释。

科学理论就像是运行在一台「通用计算机」上的程序，其输出结果就是我们试图通过实验模拟和解释的现象。当科学家能够用极少数的数学方程概括和解释大量复杂的观测结果时，我们就认为这个理论是强大且有价值的。理论能够压缩的信息越多，我们就越相信自己理解了事物的本质。因为通过极少数基本定律，科学家就可以推导和重建整个现实世界。

奥卡姆剃刀原理是一种科学思维方法，它主张：面对同一个问题的多种解释时，最简单直接的理论往往是最可能正确的。就像在解释自然现象时，我们更倾向于选择用最少的概念和最简单的规律来描述复杂的观测结果。正如哲学家莱布尼茨曾精辟地说过："上帝青睐那些假设最简单、yet 能解释最丰富现象的理论」。这一原理实际上揭示了一个深刻的科学真理：宇宙的运行可能遵循着极其简洁而优雅的基本法则。

在探索理论本质时，我们遇到了一个 provocative 的问题：任何理论都是有限的，即它只包含一组有限的规则（理想情况下规则数量很少）。美国数学家 Gregory Chaitin 在信息论研究中首次深入阐明了这一观点：有限规则意味着只能生成有限的结果。

这就产生了一个有趣的推论：总会存在一些实验结果无法被现有理论完全解释或「压缩」（在这里，"压缩」指用已知规则精确描述）。这些无法被解释的结果，在本质上看起来就是随机的。

早在很久以前，著名哲学家莱布尼茨就敏锐地观察到："当一个规则极其复杂时，符合这个规则的事物看起来就像是随机的。」这正是柯尔莫戈洛夫（Kolmogorov）对随机性的经典理解：当描述某个现象所需的规则与现象本身一样复杂时，我们就只能将其视为随机或复杂的。

遵循这种逻辑，量子随机性（量子系统中固有的不确定性和随机行为）可以被概括为两个基本原则，这最初是由意大利物理学家卡洛·罗维利提出的。

第一个原则借鉴自经典信息论，简单地指出最基本的量子系统不能存储超过一个比特的信息。这一点几乎是显而易见的，因为根据定义，比特是信息的最小计量单位。

第二个原则更为有趣：我们总是可以获得新信息。当这两个原则结合时，它们揭示了量子事件中的基本随机性本质。关键在于：当我们貌似已经掌握了所有信息时，获得新信息的唯一途径就是这些信息必须是随机的。

这是否只是重述了一个朴素的数学逻辑 —— 有限的公理只能导致有限的结果？如果真是如此，其理论意义将极其深远。

至今仍有一种观点认为，量子理论中的随机性源于理论的不完备性，即源于我们对更深层确定性理论的认知局限。然而，如果我们以知识压缩的角度审视物理学的认知进程，这可能揭示随机性是宇宙的内在属性，因此必然是描述现实的任何物理理论中不可或缺的部分。随机性可能 simply 存在，因为我们对现实的描述本质上总是有限的，任何超出这一范畴、需要更多信息的事物都会显得随机（因为我们的描述无法预测它）。

这意味着量子物理中的随机性远非偶然 - 事实上，依照这种逻辑，它实际上是必然的。更进一步，这还意味着无论未来可能取代量子物理的理论是什么 - 如果真的存在 - 它都必然要包含某些随机特征。这是一个极其深刻的结论。考虑到物理学永远在不断演进，这种基础性的随机性将对任何新兴理论构成重大约束。

波普尔发现，科学知识最关键的特点在于它具有被证伪的可能性。这一特征不仅仅适用于科学，还延伸到哲学、心理学、宗教、历史、艺术等各种知识形态。就这一点而言，科学知识与金融投资有着惊人的相似之处 - 犹如投资股市或下注赌博，没有风险就意味着没有收益。打个通俗的比喻，就是「天下没有白吃的午餐」。这种增长模式并非只限于科学知识或经济利润，任何有价值的信息都遵循这一规律。

科学本质上可以看作是一种对未来的「押注」。早在 1781 年，德国著名哲学家伊曼纽尔·康德就在他的《纯粹理性批判》中，提出了用赌博来刻画人类认知宇宙不确定性的颇具洞见的观点。康德将赌博等同于对理论有效性的实用主义信念。而后来由信息论奠基人克劳德·香农（Claude Shannon）发展的对数评分规则，可以被视为康德这一哲学洞见的实践性诠释。

随机性与确定性的辩证关系贯穿整个论述。在波普尔的科学认知模型中，科学进步依赖于两个关键环节：首先是科学家大胆地提出随机性的猜想，随后通过严谨的实验 deliberately 地检验和反驳这些猜想。这是获取关于世界真实知识的唯一路径。更有趣的是，这种认知模式同样适用于现实世界的其他领域。

以生物进化为例，生物信息的产生过程堪称这一认知模型的绝佳注脚。生物进化本质上是一个信息生成的过程，由两个看似矛盾 yet 相互依存的机制推动：基因中偶然发生的随机性变异（random mutation），以及环境对这些变异进行的确定性筛选。这与科学家提出假说并通过实验检验的过程惊人地相似。

经济领域同样如此。市场被视为一个巨大的试验场，任何经济策略或投资假说都将在现实中接受「实验」，要么被市场无情地证伪，要么得到验证和强化。

我们认为，社会动态实际上是一种独特的信息处理方式。一个社会的发展程度，可以通过其处理和组织信息的能力来衡量。在社会的微观层面，元素之间存在随机性，但在宏观层面，这些元素通过相互作用，呈现出令人惊讶的确定性特征，就像社会经历的各种结构性转变一样。那些勇于提出大胆猜想并勇于接受批评和修正的社会，往往能更快地实现社会进步。

在物理学领域，随机性是理解热现象的关键。科学家们甚至将整个宇宙视为朝着熵增（即趋向更加混乱和无序）的方向不断演化。在这个过程中，人类的智慧体现在如何利用有限的信息，设计高效的方案来提取有用的能量。整个热力学，本质上可以看作是人类智慧（象征性地称为「麦克斯韦魔」，这是以物理学家 James Clerk Maxwell 的名字命名的思想实验中的概念）与自然随机化过程之间的持续较量。

无论我们望向何方，都能看到潜藏的信息本质。更重要的是，这些信息无论在什么情境下，都以相同的方式通过随机性和确定性演变。那么，随机性和确定性的组合是否真能解释我们周围的一切现象？





### Key points

Randomness and determinism together can be seen to underlie every aspect of reality.

This is linked to the age-old question of free will; a question that has entertained us since the days of Ancient Greece.

Through a series of conjectures and refutations we can now see how knowledge evolves.

Quantum mechanics opens the door to genuine randomness (i.e. events which, at their most fundamental level, have no underlying cause).

Kolmogorov captured the essence of randomness in that a collection of outcomes from a random process cannot be generated in any simpler way than by actually running that process (i.e. you just have to suck it and see).

关键要点：

随机性和确定性联合作用，可以被视为构成现实世界的根本机制。

这一观点勾连着自由意志的古老哲学难题，这个问题自古希腊时代起就深深地困扰着人类思考者。

通过不断提出假设并加以质疑，我们逐渐揭示了知识演进的本质。

量子力学为「真正的随机性」开辟了新的认知边界，即在最微观层面存在着没有明确因果关系的事件。

Kolmogorov 对随机性的洞见令人深思：某个随机过程的结果集，唯一可靠的生成方式就是实际运行该过程本身 —— 没有捷径，只能亲身体验和观察。