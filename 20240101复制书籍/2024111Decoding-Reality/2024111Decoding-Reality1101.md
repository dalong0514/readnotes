Vlatko Vedral.(2018).2024111Decoding-Reality.Oxford University Press => 1101Sand Reckoning: Whose Information is It, Anyway

## PART THREE

The first part of the book was all about how information underpins a number of everyday processes. Information is as critical to the biological propagation of life as it is to structuring a stock portfolio or getting useful work out of random energy.

The second part of the book showed us that there is more to information than meets the eye. When our underlying model of physics is adjusted from a classical view to a quantum one, we find that our theory of information is also adjusted and becomes even more insightful than before. The key advantage of quantum information is that a quantum bit can exist in several states at the same time, the flip-side of this being its inherent degree of randomness. This can sometimes deter us when we want to use quantum information to perform useful work, but it also presents some opportunities that we can exploit to our advantage (recall that in quantum cryptography, randomness was used to eliminate eavesdropping).

More importantly, for any information to become more complex or higher quality (e.g. when we gain more knowledge about the Universe) the genuinely random element is key. We need to keep coming up with new information through this random element and then have a process that deterministically eliminates that which is incorrect or not required. It is this deterministic element that allows us to form useful knowledge.

The first two parts of the book unify a number of seemingly unrelated ideas through the common language of information processing, but the question still remains: where does even this information come from? This takes us right back to what we asked right at the beginning of this book, namely the question of creation ex nihilo.

All the theories and experiments presented in the first two parts are beyond reasonable doubt; these are all peer reviewed and well accepted facts. In Part Three of the book we move into more speculative and uncharted territory.

本书第一部分主要阐述了信息如何在我们的日常生活中发挥着基础性作用。无论是生物生命的延续，还是构建股票投资组合，甚至是将混乱的能量转化为有用的工作，信息都扮演着至关重要的角色。

第二部分则向我们揭示了信息更深层次的内涵。当我们将物理学的认知从经典理论转向量子理论时，信息理论也随之演进，展现出了更深刻的洞见。量子信息的独特优势在于量子比特能够同时处于多个状态，但这也带来了固有的随机性。这种随机性有时会给我们利用量子信息做有用功带来挑战，但同时也创造了新的机遇（就像在量子密码学中，随机性被用来防止信息被窃取）。

更值得注意的是，任何信息要实现进化或提升（比如我们对宇宙的认知不断加深），随机性都起着不可或缺的作用。我们需要通过随机性不断产生新的信息，再通过确定性的筛选过程去伪存真。正是这种确定性的筛选机制让我们能够积累有用的知识。

本书前两部分通过信息处理这一共同语言，将许多表面上毫不相关的概念串联在一起，但仍有一个问题悬而未决：这些信息本身又是从何而来的？这让我们回到了本书开篇就提出的问题：如何从虚无中创造万物？

前两部分所介绍的理论和实验都是经过严格验证的，这些都是经过同行评审并被学术界广泛认可的科学事实。而在第三部分，我们将踏上一段更具探索性的未知旅程。

## 1101Sand Reckoning: Whose Information is It, Anyway?

In Chapter 9 we discussed the idea of a universal Turing machine. This machine is capable of simulating any other machine given sufficient time and energy. For example, we discussed how your fridge microprocessor could be programmed to run Microsoft Windows, then we described Moore's logic, that computers are becoming faster and smaller. Therefore, one day, a single atom may be able to simulate fully what a present day PC can do.

This leads us to the fascinating possibility that every little constituent of our Universe may be able to simulate any other, given enough time and energy. The Universe therefore consists of a great number of little universal quantum computers. But this surely makes the Universe itself the largest quantum computer. So how powerful is our largest quantum computer? How many bits, how many computational steps? What is the total amount of information that the computer can hold?

Since our view is that everything in reality is composed of information, it would be useful to know how much information there is in total and whether this total amount is growing or shrinking. The Second Law already tells us that the physical entropy in the Universe is always increasing. Since physical entropy has the same form as Shannon's information, the Second Law also tells us that the information content of the Universe can only ever increase too. But what does this mean for us? If we consider our objective to be a full understanding of the Universe then we have to accept that the finish line is always moving further and further away from us.

We define our reality through the laws and principles that we establish from the information that we gather. Quantum mechanics, for example, gives us a very different reality to what classical mechanics told us. In the Stone Age, the caveman's perception of reality and what was possible was also markedly different from what Newton would have understood. In this way we process information from the Universe to create our reality. We can think of the Universe as a large balloon, within which there is a smaller balloon, our reality. Our reality is based on our knowledge of the Universe (via the laws through which we define it) and as we improve our understanding of the Universe, through conjectures and refutations and evolutions of our laws and principles, the smaller balloon expands to fill the larger balloon. So is the rate at which the Universe keeps surprising us greater than the rate at which we can evolve our reality? In other words, will we ever understand our whole Universe?

Here Popperian logic comes to our aid. The very logic of conjectures and refutations, which is at the root of how we construct our understanding of reality, tells us that we cannot answer this question. Only if we know that the Universe can no longer surprise us, that there is no new physical theory superseding that which we currently have, can we be sure that one day we might be able to understand our whole Universe. But how do we know that no new event in the Universe will ever surprise us and cause us to change our view of reality? The answer is that we don't. We just don't know whether one day we will be surprised. This is the ultimate unknown unknown. Even though we can never know whether we will know everything, this does not prevent us from knowing how much there is to know with the current understanding. So how much information is there in the Universe? Where do we even start with such an absurd calculation? Interestingly we are not the first ones to tackle this issue and far greater minds before mine have grappled with this question.

One of the greatest human minds in history is Archimedes of Syracuse (circa. 287–212 BC). Archimedes made enormous contributions to astronomy, mathematics, engineering, and philosophy, and had a reputation at the time as being not only theoretically very astute but also very practical. In fact, he was regularly employed by the state for his ingenious ideas especially when they were in a pickle and didn't know who else to turn to. In one story, which saw Syracuse under attack from warring ships, his idea was to focus the Sun's energy using large curved mirrors, to burn enemy sails before the ships docked. This according to folklore was one of several times he had saved the city.

Uncompromising in his pursuit of science, Archimedes died as he lived. The last words attributed to Archimedes are ‘do not disturb my circles' and it is said he made this remark to a soldier just before he was slain. Whilst the soldier originally came to summon him, Archimedes' complete apathy to the soldier's concerns, and total focus on his work, resulted in a tragic end to an otherwise spectacular life.

One piece of his research was commissioned by the Syracusian monarch, King Gelos II, and resulted in what is considered the first research paper ever. The task was to calculate the number of grains of sand that could fill the Universe. Not the kind of task you would set to any ordinary man. It is certainly not clear what use King Gelos II ever made of this, or whether he ever used it for anything other than fanciful conversation.

Sand was the smallest thing known at that time so it was natural to phrase questions in terms of volumes of sand. With reference to the heliocentric model of Aristarchus of Samos (circa. 310–230 BC), Archimedes reasoned that the Universe was spherical and that the ratio of the diameter of the Universe to the diameter of the orbit of the Earth around the Sun equalled the ratio of the diameter of the orbit of the Earth around the Sun to the diameter of the Earth. You could see how this calculation was not everybody's cup of tea. In order to obtain an upper bound, Archimedes used overestimates of his data.

Firstly, to even begin to describe numbers of such epic proportions he needed to extend the current Greek numbering system. In practice, up until then they never had a language for such large numbers, but he needed it now. For example, Archimedes introduced the word ‘myriad' to represent the number we would now call 10,000. Following from that a myriad myriads would be 100 million (i.e. 10,000 times 10,000), and so on. However, the real difficulty came as he had to make assumptions about the size of the Universe, using a very limited knowledge of astronomy (by our standards). Here is what he reasoned:

1That the perimeter of the Earth was no bigger than 300 myriad stadia (roughly 50,000 kilometres – this is very close to the actual perimeter).

2That the Moon was no larger than the Earth (it's actually much smaller) and that the Sun was no more than about 30 times larger than the Moon (this is a huge underestimate).

3That the angular diameter of the Sun, as seen from the Earth, was greater than approximately half a degree (this is correct and not a bad estimate either).

With these assumptions, Archimedes then calculated that the diameter of the Universe was no more than 10 to the power of 14 stadia (i.e. in modern terms two light-years), and that it would require no more than 1063 (one followed by 63 zeros) grains of sand to fill it. In Archimedes' estimate, if you think of every point in space as a bit, in the sense that it either contains a grain of sand or not, then the number of bits according to him is 2 to the power of 10 to the power of 63, i.e. 2 to the power of 1063. This is a phenomenally large number, and later we'll compare it with the equivalent number generated more recently, some two millennia after Archimedes took the first stab.

Archimedes, of course, never had the level of understanding of the world that we do now. So the question is how much more accurate can we be with a two thousand year head-start.

Over the last two thousand years we have seen Popper's method working tirelessly to produce ever better approximations of reality. Scientists would come up with conjectures on how to describe elements of reality in the shortest and simplest way and then they would perform observations and experiments to test their models (conjectures and refutations). As the models are refuted, as new information or understanding comes to light, they are discarded and new models arise from their ashes to take their place. So far we have been looking at biological, computational, social, and economic aspects of reality. What we have seen is that information is a natural way in which to unify these seemingly different disciplines. As discussed throughout this book, at its heart, the nature of information processing depends entirely on the laws of physics. So to calculate the amount of information in the Universe it is natural that we resort to our best understanding of reality to date.

The two theories which encapsulate our current best understanding of reality are quantum physics and gravity. There are, of course, other theories (biological, social, economic, and so on) that have a legitimate claim to this and we have given them an equal seat at Calvino's table. However, it is generally regarded that quantum theory and gravity are the most fundamental descriptions available to us. In the same way that Kolmogorov viewed the information content or complexity of a message in terms of the shortest program used to describe it, we currently view the information content of reality in terms of quantum physics and gravity – which are our shortest programs used to describe reality.

We have already seen in Chapter 10 how quantum theory can be understood in terms of information, so now can we also do the same for gravity?

Gravity is quite distinct from quantum theory. Whilst the effects of quantum theory can be felt at the microscopic and macroscopic level, with effects becoming less influential for large bodies, gravity works the other way around: gravity dominates large bodies (e.g. planets) and becomes less influential for microscopic bodies. No current experiment can detect gravity between two atoms, no matter how close they are.

These two theories may seem to have their place at opposite ends of the spectrum, but they are, in fact, intricately related. Finding a single unified theory connecting quantum physics and gravity has been seen as the holy grail of physics for some time now and is one of the ‘bleeding edge' areas of physics. With tongue in cheek I will argue how gravity can be described as a consequence of quantum information (expounding on a view that is extremely controversial). This will, I believe, be the strongest indictment to date that quantum information does indeed provide the underlying description of reality.

The modern view of gravity, through Einstein's general relativity, is to see it as a curvature of space and time. In everyday language you may be more aware of it as a universal force of attraction, like when you throw a ball into the air and it comes back down. Einstein's view is, however, the most general and accurate description of gravity. In this view, time and space are inseparable and both curved interdependently. To visualize this, imagine a simple example where your bed represents space-time. As things are placed on the bed they create impressions by indenting the surface. If you put a football onto your bed it might make a slight impression; if you put yourself on to the bed this would make a much bigger impression. If you are sitting on the bed and you put the ball sufficiently near to you, it will be pulled in by your impression. In exactly the same way, all bodies attract one another in space-time because they create indentations in the space-time fabric which then propagate and affect each other. Understanding the curvature of space-time (or your bed as you sit on it) is therefore the key to describing the effects of gravity.

Any curvature in space-time necessarily means that distances and time intervals both become dependent on the mass of the object curving the space-time. For example, the time for a person closer to Earth runs faster than for someone further from Earth (assuming this person is not close to some other massive object) simply because the person closer to the Earth is more affected by the Earth's gravitational pull; in other words, the curvature of space-time is greater closer to the Earth. Given that we want to explain gravity in terms of quantum information, the question then is what is the connection between the geometry of the curvature and the concept of information (or entropy)? Amazingly enough, the answer lies in that quirky property we met in the last chapter, quantum mutual information.

We have met the concept of entropy in several chapters already. This concept is synonymous with the information content of a message or system. The higher the entropy of a system the more information it carries. In the first part of the book we used entropy for many different purposes; it quantified the capacity of a channel, the disorder in any physical system, the profit from a bet in terms of the risk taken, and social interconnectedness. For the purposes of the following discussion we should think of entropy as physical entropy, quantifying disorder in physical systems.

There is a very interesting relationship between the uncertainty within a certain system – as measured by its entropy – and its size. Suppose that we look at an object enclosed within a certain boundary. How complex would we expect it to be? A natural answer would be that the entropy of the object depends on its size, in particular its volume. Say that a molecule contains one million atoms arranged in a ball. If each atom has a certain entropy associated with it then it seems natural to expect that the total entropy is just the number of atoms, times the entropy of each atom. Therefore, the total entropy would scale as molecular volume.

Interestingly, it turns out that at low temperatures (such as the current state of the Universe), the entropy usually scales with the area and not the volume of an object (and as we know from elementary maths, the area of any object is necessarily always smaller than its volume). If you think of a ball-shaped molecule, the volume is made up of all the atoms on its surface plus the atoms inside. Therefore, if we are now asking what the maximum entropy of the ball-shaped molecule is, we might say it is proportional to the total number atoms in the ball (i.e. the volume), given that each atom should independently be able to contribute to the overall uncertainty. However, and rather surprisingly, what quantum theory is telling us is that, no – entropy is actually proportional to the total number of atoms on the surface (i.e. a significantly smaller ratio).

So why is there this difference between what seems logical and what quantum theory tells us? To understand this we have to look into quantum theory again, and specifically towards the nature of quantum mutual information. Recall that quantum mutual information is a form of super-correlation between different objects and that this super-correlation is fundamental to the difference between quantum and classical information processing (e.g. as we see in quantum computation).

Suppose that we divide the total Universe into two, the system, such as the molecule above, and the rest – which is everything outside of the molecule. Now, the quantum mutual information between the molecule and the rest is simply equal to the entropy of the molecule. But, quantum mutual information is not at all a property of the molecule, it can only be referenced as a joint property, i.e. a quantum correlation between objects. In this case it is a joint property between the molecule and the rest of the Universe. Therefore, it logically follows that the degree of quantum mutual information between these two must be proportional to something that is common to both, in this case the boundary – i.e. the surface area of the molecule!

This is a very profound conclusion. We think of entropy as the information content of an object. The fact that this information content is not within the object, but lies on its surface area, seems surprising to say the least! What this means is that the information content of anything does not reside in the object itself, but is a relational property of the object in connection with the rest of the Universe.

This also implies another important result. It is in fact a possibility which will be explored later. Within this formalism it is entirely possible for the Universe to have zero information content, whereas subsets of the Universe may have some information. The Universe is not correlated to anything outside of the Universe (by definition). However there are parts of the Universe that are correlated to each other. As soon as we partition the Universe into two or more distinct regions we begin to generate information, and this information is equal to the area of the partition, not to the size of the regions. The very act of partitioning, dividing, and pigeonholing necessarily increases information, as you cut through any parts that may be correlated.

We can present this conclusion pictorially as follows. Imagine that atoms inside the molecule are connected to atoms in the Universe via a series of ribbons. A limit on the number of ribbons that we can connect to the Universe is constrained by the surface area of the molecule (i.e. how many ribbons can we get through the surface of the molecule – as it is only of finite size). The information shared between the molecule and the Universe can be seen as proportional to the number of ribbons connecting the two. And this is logically why information scales as the area of the surface of the molecule.

The physicist Leonard Susskind proposed to call the relationship between entropy and area, the holographic principle. Holography has traditionally been part of optics and is the study of how to faithfully encode three-dimensional images onto photographic two-dimensional films. This is typically done by illuminating the object with laser light, which gets reflected off the object and the reflection is then recorded onto photographic film. When the plate is subsequently illuminated, a three-dimensional image of the object appears where once the real object stood. This phenomenon is probably familiar to the reader from its countless usage in magazines, stickers, toys, and science fiction films.

Optical holography was invented by Denis Gabor in the 1960s working in the labs at Imperial College in London (one of which became my office some 40 years later). He received the Nobel Prize for his ideas in 1971 (interestingly, the year I was born). He used holography to construct an optical version of Maxwell's demon (an idea which I also researched during my PhD). The whole surprise about his discovery is that he showed that two dimensions were sufficient to store all information about three dimensions, for which he duly received the Nobel Prize (sorry – though I thought hard but there's unfortunately no connection between us on this one!).

It's easy to see how two dimensions are recorded but where does the third dimension come from? It is this third dimension that allows us to see a hologram in three dimensions. The answer to this lies in the relational properties of light, known as interference. Going back to the experimental setup, light carries an internal clock, and when the light reflected from the object interferes with the light directly hitting the two-dimensional photographic film then an interference pattern is produced where the timing from the clock acts as the third dimension. This means that when you look at a hologram, you see the standard two-dimensional image, but you are also seeing light reflected back to you at slightly different times and this is what gives you the perception of a three-dimensional image.

Susskind suggested that we should not be surprised that information (entropy) scales with surface area, but rather we should elevate this to a point of principle. By this he means that this principle should be correct for anything in the Universe (anything that carries energy – e.g. matter, light). Furthermore, the key property behind this was quantum mutual information which we now see as being between anything on one side of the object and whatever is on the other side. Now we have all the pieces to derive gravity from this logic.

Einstein's equation in general relativity describes the effect of energy-mass on the geometrical structure of four-dimensional space-time. His equation says, to paraphrase John Wheeler, that matter tells space-time how to curve, while space-time (when curved) instructs matter how to move (e.g. the Earth moves around the Sun because the Sun curves space-time significantly). Can the energy-curvature relationship that encapsulates gravity be derived from quantum information theory?

An ingenious argument was given in the mid-1990s by Ted Jacobson to support the answer ‘yes' and we now have all the ingredients to recreate it. So far we have already discussed how the thermodynamical entropy is proportional to the geometry of the system. It is well known in thermodynamics that the entropy of a system multiplied by its temperature is the same as the energy of that system. Therefore a larger mass, which represents larger energy (based on the mass–energy equivalence), will imply a larger curvature in space-time.

A simple energy conservation statement between entropy and energy becomes Einstein's gravitational equation, relating mass to curvature. In this case entropy encapsulates geometry. A more massive object therefore, according to thermodynamics, produces larger entropy. However, we saw that the entropy is also related to the surface area surrounding the mass, according to the holographic principle that we have just discussed. Therefore, the more massive the object, the larger the indentation of the surrounding area.

It is very helpful to illustrate this with an example. Take space-time without any mass or energy (i.e. an empty Universe). Now divide it into two by taking a sheet of light, shining it directly through the middle (whatever that means in an empty Universe). This light is unaffected by anything else since the Universe is empty. Now imagine introducing a massive object on one side (massive here simply meaning an object with a large mass). From thermodynamics, this changes the entropy, which from holographic principles affects the area that the light travels, which will have to be bent now to take into account the change in geometry. This, in fact, was how general relativity was first tested by Arthur Eddington in 1919. He confirmed that the apparent change in the position of a star followed exactly the prediction by Einstein in his work on general relativity. Here the light from the star was being curved en route to us via the gravitational pull of the Sun – hence the star appears as if it had changed position.

Interestingly the same idea can be applied to the detection of massive dark objects such as black holes. How can you see a black hole when by definition it is black and it does not emit any light? What the black hole does have, however, is a huge gravitational force, and we can use this fact to ‘see' it. Whilst we cannot observe the black hole directly we can observe the impact of its gravitational force on matter and especially light around it. In particular the light of any star directly behind a black hole, instead of being dispersed in the usual manner, is instead highly focused as it gets caught up in the gravitational pull of the black hole. From Earth we observe the light from a star becoming significantly more intense than normal, before it settles back down to its normal level of light intensity. This change of intensity can be explained by a black hole passing in front of the star. In technical terms the effect is known as gravitational lensing.

Information, as measured by entropy, is now seen to underpin both quantum mechanics and gravity. In quantum mechanics, the entropy of a system is finite but we can always generate more entropy (which implies randomness). The fact that this quantum entropy is proportional to the area can then be coupled to the First Law of thermodynamics, which states that energy is conserved, to infer the equations of gravity. It is very interesting to note that quantum physics and gravity are frequently viewed as incompatible. However, this argument would suggest that, far from it, they are actually intimately related (which is why Jacobson's paper caused a lot of excitement).

We have already said that some aspects of this argument are speculative. However, what we can conclude from the whole discussion is that gravity does not add anything novel to the nature of information processing. All the theory required already exists through application of quantum principles. Even if the details of the argument are not correct, still the properties of quantum information are the same with or without gravity.

So with this in mind let's return to the question of how much information can maximally be squeezed into the total Universe as we know it. We have already said that information is proportional to area, and how exactly it is proportional has been estimated by the Israeli physicist Jacob Bekenstein. His relationship, known as the Bekenstein bound, is simply stated as follows: the number of bits that can be packed into any system is at most 1044 bits of information times the system's mass in kilograms and its maximum length in metres (the square of this length is the system's area) . As an aside we note that Bekenstein's work on black hole entropy prompted the British physicist Stephen Hawking to conclude that (after all) black hole are not as black as they seem. They emit the so-called Hawking radiation, whose ultimate origin is quantum.

It is amazing that to calculate something as profound as the information carrying capacity of any object, out of its infinitely many possible properties, we only need two: area and mass. As a practical application, this easily allows us to calculate the information carrying capacity of our heads. Say that a typical head is 20 centimetres in diameter and weighs 5 kilograms. That means that a typical human head can store 10 to the power of 44 bits of information. Compare this to the best current computers which are still only of the order of 10 to the power of 14 bits of information. We therefore need 1030 to get the equivalent information carrying ability of a human head!

For us, the task now is to apply the Bekenstein bound to calculate the total number of bits in the Universe, which, if you recall, was our original motivation. Astronomers have already given us a rough estimate of the Universe's size and weight, say 15 billion light-years in diameter and a mass of about 10 to the power of 42 kilograms (ironically, this coincides with the ‘forty two' from Hitchhiker's Guide to the Galaxy). When you plug this information into the Bekenstein formula, the capacity of the Universe ends up being on the order of 10 120 bits of information. This is a stupendously large number, but ultimately it is not infinite. (In fact mathematicians will argue that it is still closer to 0 than it is to infinity!) Also it's worth noting that Archimedes estimated 10 to the power 63 grains of sand in the Universe. If, as before, we take a grain of sand as analogous to a bit of information, then this is a pretty good guess of the Universe's information carrying capacity from someone who lived more than two millennia ago.

Since we have been equating the Universe to a quantum computer, it would also be applicable to talk about the processing speed of our Universe. This can be estimated immediately from Bekenstein's bound. If you take the age of the Universe as 10 to the power of 17 seconds and the fact that the Universe has generated 10120 bits (these are our current estimates), then we can say that the total capacity for information processing is about 10103 per second. Comparing this to a modern computer (your everyday Pentium 4 – whose processing capacity is not more than 1010 bits per second) we can see that we would need 1093 such computers to simulate the Universe. This is 10 followed by 93 zeros. Therefore, if we had to rely only on our computers to understand the Universe we would not get very far! This is an amazing indication of the power of the human mind!

For comparison, at the other end of the spectrum, lie small objects such as atoms and atomic nuclei. A hydrogen atom, according to Bekenstein, can encode about four million bits, while, a proton can encode only about 40 bits (simply because it is so much smaller than the atom itself). If we were skilful enough (and we are currently far away from this) we could run a quantum computation with just one hydrogen atom that would factorize a 1000-digit number, something we said was extremely difficult for any current computer.

How confident are we that this number of bits that we have calculated is the total number of bits of information in the Universe? Popper has already told us that you can view science as a machine that compresses bits in the Universe into laws, and these laws in turn are then used to generate reality. So how do we know whether tomorrow we will observe an experiment that will change the compression and give us a new law? The answer is that we don't! For a theory to have stood for 200 years and to then be refuted by a single experiment is standard fare for scientific progress. In the same way that quantum information has superseded classical information, it is likely that in the future we may progress to a new order of information processing, based on additional elements of reality unknown or not fully understood by us at this time.

So can we push this method of conjectures and refutations to the extreme? Can we present a consistent view of reality without even worrying about what the final theory would be?

沙粒之数：信息究竟属于谁？

在第 9 章中，我们探讨了通用图灵机（Universal Turing Machine）的概念。这种机器只要有足够的时间和能量，就能模拟任何其他机器。举个例子，我们讨论过你冰箱里的微处理器（Microprocessor）理论上可以被编程运行 Microsoft Windows，然后我们描述了摩尔定律（Moore's Law），即计算机正在变得越来越快、越来越小。因此，未来某一天，一个单独的原子可能就能完全模拟出现今个人电脑的所有功能。

这个设想引发了一个令人着迷的推测：只要有足够的时间和能量，宇宙中的每一个基本粒子都可能模拟任何其他粒子。这意味着，宇宙是由无数个微小的通用量子计算机构成的。从这个角度来看，宇宙本身就是最大的量子计算机。那么，这台终极量子计算机究竟有多强大？它包含多少比特，能执行多少计算步骤？它能存储的信息总量又是多少？

既然我们认为现实中的一切都是由信息构成的，那么了解宇宙中信息的总量，以及这个总量是在增加还是减少就显得尤为重要。热力学第二定律已经告诉我们，宇宙中的物理熵（Physical Entropy）始终在增加。由于物理熵与香农（Shannon）信息论中定义的信息具有相同的数学形式，这也意味着宇宙中的信息含量只能不断增加。但这个结论对我们的认知有什么影响呢？如果我们的终极目标是完全理解宇宙，那么我们必须承认：这个目标正在不断远离我们。

我们是通过从观测中获取信息，并在此基础上建立各种定律和原理来认知现实的。比如，量子力学向我们展示了一个与经典力学完全不同的世界。回顾历史，石器时代的人类对现实和可能性的理解，与后来牛顿时代的认知有着天壤之别。这说明我们是通过不断处理来自宇宙的信息来构建我们的认知世界。这个过程可以比作两个同心域：外层是浩瀚的宇宙，内层则是我们对宇宙的认知。我们的认知范围基于我们对宇宙的理解（通过各种科学定律），随着我们通过科学假设的提出、验证和修正，以及各种定律原理的演进，我们的认知范围在不断扩大。那么问题来了：宇宙向我们展示新现象的速度是否快于我们拓展认知的速度？换句话说，我们真的有可能完全理解整个宇宙吗？

在这个问题上，哲学家波普尔（Karl Popper）的科学方法论给了我们启示。科学发展的基本逻辑 —— 提出假设并通过实验检验 —— 告诉我们这个问题可能永远无法被完全回答。只有当我们确信宇宙不会再出现任何超出现有理论预测的现象，也就是说不存在超越我们当前物理理论的新理论时，我们才能说有朝一日可能完全理解整个宇宙。但问题在于：我们怎么能确定宇宙不会出现让我们改变认知的新现象呢？答案很简单：我们无法确定。我们永远不知道明天会发现什么。这就是科学探索中最根本的不确定性。虽然我们无法预知自己是否能最终理解一切，但这并不妨碍我们在现有知识框架下评估需要了解的内容。那么，宇宙中究竟蕴含着多少信息？这个看似不可能完成的计算应该从何处着手？有趣的是，在这个问题上我们并非开创者，在人类历史上已经有伟大的科学家思考过这个问题。

最早思考这个问题的是古希腊最伟大的科学家之一，叙拉古的阿基米德（约公元前 287 年至公元前 212 年）。他在天文学、数学、工程学和哲学等领域都做出了开创性的贡献。他不仅以理论造诣深厚著称，在实际应用方面也极具天赋。事实上，每当城邦陷入困境、无人能解决问题时，他常常会被征召提供解决方案。有一个著名的故事：当叙拉古遭受敌军战船围攻时，他想出了使用巨型抛物面镜聚焦太阳光，在敌船靠岸前烧毁其船帆的方法。根据历史记载，这只是他多次运用科学智慧拯救城市的例子之一。

阿基米德对科学的执着贯穿了他的一生，直到生命的最后一刻。据记载，在他生命的最后时刻，面对前来传令的罗马士兵，他只说了一句话："请勿妨碍我的几何作图」。虽然这名士兵本是来传达命令的，但阿基米德对外界的打扰毫不理会，全神贯注于他的数学研究，这种执着最终导致了这位伟大科学家的悲剧性离世。

他最著名的研究之一是受叙拉古国王格洛斯二世的委托，完成了被历史学家认为是人类历史上第一篇系统性科学论文的作品。研究的主题是计算填满整个宇宙需要多少沙粒，这显然不是一个普通人能够完成的任务。至于格洛斯二世最终是如何使用这个研究成果，或者仅仅将其作为一个有趣的话题来讨论，历史并没有给出明确的记载。

在那个时代，沙粒被认为是人类可观测到的最小物质，因此用沙粒的数量来度量宇宙的规模是很自然的想法。阿基米德参考了他的同时代人，来自萨摩斯的阿里斯塔克（约公元前 310-230 年）提出的日心说模型，推导出了一个巧妙的假设：宇宙是球形的，且地球绕太阳轨道的直径与整个宇宙直径的比值，等于这个轨道直径与地球直径的比值。这个计算方法的精妙之处显然超越了当时大多数人的理解能力。为了确保计算的可靠性，阿基米德在计算过程中特意选择了各项参数的最大可能值。

首先，为了表达如此巨大的数量，阿基米德需要扩展当时希腊的数字系统。在那个时代，人们从未需要表达如此巨大的数字，因此也就没有相应的数学语言。为此，阿基米德创造性地引入了「myriad」（万）这个概念来表示 10,000。在此基础上，他定义「myriad myriads」（万万）为一亿（即 10,000 乘以 10,000），并以此类推构建更大的数字。然而，真正的挑战在于他必须基于当时有限的天文学知识（相比现代标准）对宇宙的尺度做出假设。他的推理过程如下：

1、地球的周长不超过 300 万个斯塔迪亚（一种古希腊长度单位，按此计算约为 50,000 公里，这与地球实际周长非常接近）。

2、月球的体积不会超过地球（实际上月球远小于地球），而太阳的体积不会超过月球的 30 倍（这大大低估了太阳的实际大小）。

3、从地球观测，太阳的视角直径大约为半度（这个观测相当准确）。

基于这些假设，阿基米德计算出宇宙的直径不会超过 10^14 斯塔迪亚（用现代单位换算约为两光年，光年是光在一年内传播的距离），并推算出填充这个宇宙需要不超过 10^63（即 1 后面跟着 63 个零）颗沙粒。

在阿基米德的估算体系中，如果我们把空间中的每一个点都看作一个比特（bit），即这个点要么包含一粒沙子，要么不包含，那么根据他的计算，总比特数将是 2^(10^63）。这是一个难以想象的巨大数字。在后文中，我们会将这个结果与两千年后科学家们得出的数值进行比较。

当然，阿基米德不可能拥有我们今天这样的科学认知。那么，在经过两千年的科学发展之后，我们现代的计算能达到怎样的精确度呢？

在过去的两千年中，科学界一直遵循着波普尔提出的科学方法论，不断提升我们对现实的认知精度。科学家们提出假说，试图用最简洁的方式描述自然现象，然后通过观察和实验来检验这些模型（这就是假说 - 检验的科学方法）。当某个理论模型被证明有误，或者出现了新的发现和认知时，旧理论就会被新的理论所取代。到目前为止，我们已经从生物学、计算科学、社会学和经济学等多个维度研究了现实世界。我们发现，信息这个概念为统一这些不同学科提供了一个自然的桥梁。正如本书一直强调的，从根本上说，信息处理的本质完全建立在物理学定律之上。因此，要计算宇宙中的信息总量，我们必然要借助当前最先进的科学理论体系。

当前对现实世界最深入的理解来自两大理论体系：量子物理学（Quantum Physics）和引力理论（Gravity）。当然，其他学科如生物学、社会学、经济学等也都对认知现实有着重要贡献，它们在科学体系中都占有同等重要的地位。不过，科学界普遍认为量子理论和引力理论提供了最基础的自然规律描述。就像数学家柯尔莫戈洛夫（Kolmogorov）用最简程序来衡量信息的复杂度一样，我们现在用量子物理学和引力理论这两个最基础的理论来理解现实世界中的信息本质。

在第 10 章中，我们已经了解了如何从信息的视角来理解量子理论，那么我们能否用同样的方式来理解引力呢？

引力与量子理论有着本质的区别。量子理论的效应在微观（Microscopic）和宏观（Macroscopic）尺度上都能观察到，只是随着物体质量的增大而影响减弱。而引力则恰恰相反：它在大尺度物体（如行星）之间表现得最为显著，而在微观世界中的作用极其微弱。即使在实验室中将两个原子放置于最接近的距离，我们现有的技术仍无法探测到它们之间的引力相互作用。

这两个理论表面上似乎属于物理学研究的两个极端，但实际上它们有着深刻的内在联系。寻找统一量子物理学和引力理论的终极理论，长期以来被视为物理学最重要的研究目标之一，这也是当前物理学最具挑战性的前沿研究领域。接下来，我将尝试说明引力如何可以被解释为量子信息的必然结果（尽管这个观点在学术界仍存在较大争议）。我认为，这个论证将为量子信息是现实世界基本描述这一观点提供最有力的支持。

引力的现代理论来自爱因斯坦的广义相对论，它将引力解释为时空（Space-time）的弯曲。在日常生活中，我们通常把引力理解为一种普遍的吸引力，比如当我们把球抛向空中时它会落回地面。但爱因斯坦的理论为我们提供了对引力最完整和最准确的描述。在这个理论中，时间和空间是不可分割的整体，它们会相互影响并共同发生弯曲。我们可以用一个简单的模型来理解这个概念：假设有一张弹性薄膜代表时空。当我们在薄膜上放置物体时，它会在表面产生凹陷。质量较小的物体（如乒乓球）会产生微小的凹陷，而质量较大的物体则会产生更明显的变形。如果我们在薄膜上近距离放置两个物体，较小的物体会在较大物体产生的时空弯曲影响下向其运动。

这个原理同样适用于宇宙中的所有物体：它们通过在时空结构中产生变形而相互作用，这些变形会影响周围其他物体的运动。因此，要理解引力效应，关键就在于理解时空的弯曲规律。

时空的弯曲会直接影响到距离的测量和时间的流逝，这种影响与造成弯曲的物体质量密切相关。举个例子：靠近地球表面的人经历的时间流逝会比远离地球的人（假设此人也没有受到其他大质量天体的影响）稍快一些，这是因为靠近地球表面的人受到更强的引力作用；换句话说，在距离地球更近的位置，时空弯曲更为显著。现在的问题是：如果我们想用量子信息理论来解释引力，那么时空的几何弯曲与信息（或物理熵）这两个概念之间存在什么联系呢？令人惊叹的是，答案就藏在我们上一章讨论过的一个重要概念中 —— 量子互信息（Quantum Mutual Information）。

熵是我们在前几章反复提到的一个核心概念。它可以用来度量系统中包含的信息量：系统的熵越高，意味着它包含的信息就越多。在本书第一部分，我们从多个角度讨论了熵的应用：它可以用来衡量通信信道的容量、物理系统的无序程度、投资决策中的风险收益比，甚至可以用来分析社会网络的连接复杂度。在接下来的讨论中，我们将主要关注物理熵的概念，即用它来量化物理系统中的无序程度。

系统的不确定性（用熵来度量）与其物理尺寸之间存在着一个非常有趣的关系。考虑一个被特定边界包围的物体，我们如何预测它的复杂程度？从直觉上看，物体的熵应该与其大小有关，特别是与其体积相关。举个例子，假设有一个含有一百万个原子的分子，这些原子构成了一个球形结构。如果每个原子都具有一定的熵，那么从逻辑上推理，整个分子的总熵应该等于原子数量与单个原子熵的乘积。这意味着总熵应该与分子的体积成正比。

一个令人深思的发现是：在低温条件下（比如宇宙现在的平均温度状态），物体的熵通常与其表面积而非体积成正比（从数学角度来看，物体的表面积总是小于其体积）。让我们以一个球形分子为例：它的体积包含了表面和内部的所有原子。从直觉上看，我们可能会认为这个球形分子的最大熵应该与其包含的原子总数（即体积）成正比，因为每个原子似乎都应该能独立地对系统的不确定性做出贡献。然而，量子理论给出了一个出人意料的结论：系统的熵实际上与表面原子的数量（这显然是一个小得多的数值）成正比。

为什么我们的直觉推理与量子理论的预言会有如此大的差异呢？要理解这一点，我们需要深入研究量子理论，特别是量子互信息（Quantum Mutual Information）的本质。之前我们提到过，量子互信息描述了物体之间一种独特的关联方式，这种关联是量子信息处理区别于经典信息处理的根本特征（正如我们在量子计算研究中所看到的）。

让我们把整个宇宙划分为两个部分：一个是我们关注的系统（比如前面提到的分子），另一个是系统外的所有其他部分。在这种情况下，分子与宇宙其余部分之间的量子互信息在数值上等于分子的熵。然而，需要注意的是，量子互信息并非分子的独立属性，它本质上是一个描述相互作用的量，体现了物体之间的量子关联性。在这个例子中，它描述的是分子与宇宙其余部分之间的关联程度。由此我们可以得出一个重要推论：这种量子互信息的大小必然与两个部分的共同边界 —— 也就是分子的表面积 —— 成正比！

这个结论具有深远的物理意义。传统上我们认为熵反映了物体所包含的信息量，但现在我们发现这些信息并不是储存在物体内部，而是体现在其表面积上，这完全颠覆了我们的直觉认知！这表明，任何物体的信息内容本质上不是物体的内在属性，而是描述该物体与宇宙其余部分相互关联的物理量。

这一发现还引出了另一个重要推论，这个推论我们稍后会详细讨论。根据这个理论框架，整个宇宙的总信息含量可能为零，但宇宙中的局部区域却可以包含一定量的信息。这是因为宇宙作为一个整体，按照定义不可能与任何外部系统产生关联。然而，宇宙内部的不同区域之间却可以存在关联。当我们将宇宙划分为两个或更多的区域时，信息就开始产生了，而且这些信息的量与划分边界的面积相关，而不是与区域的体积相关。划分区域的行为本身就会产生新的信息，因为这个过程会影响原本存在的量子关联。

我们可以用一个形象的模型来理解这个结论。想象分子中的原子通过某种量子关联与宇宙其他部分相连。这种关联的数量受到分子表面积的限制（就像通过有限大小的表面能传递的关联数量是有限的）。分子与宇宙之间共享的信息量可以理解为与这些量子关联的数量成正比。这就从数学上解释了为什么信息量与表面积而不是体积成正比。

物理学家伦纳德·萨斯坎德（Leonard Susskind）将熵与表面积之间的这种关系命名为全息原理（Holographic Principle）。在此之前，全息技术主要是光学领域的研究内容，它研究如何将三维物体的信息精确地记录在二维平面上。这种技术通常通过激光照射物体实现：激光照射到物体上产生的反射光被记录在感光材料上。当我们再次用光照射这个记录介质时，原物体的三维图像就会重现。这种技术已被广泛应用于杂志、贴纸、玩具和科幻电影中，许多人可能都见过全息影像。

光学全息术的发明者是丹尼斯·加博（Denis Gabor），他于 20 世纪 60 年代在伦敦帝国理工学院的实验室完成了这项开创性工作。由于这一重要发现，他在 1971 年获得了诺贝尔物理学奖。加博利用全息技术构建了一个光学版本的麦克斯韦妖（Maxwell's Demon，一个著名的热力学思想实验）。他的发现最引人注目的地方在于证明了二维平面足以完整保存三维空间的所有信息，这一突破性发现为他赢得了诺贝尔奖。

二维平面记录信息的原理比较容易理解，但第三维空间信息是如何重现的呢？正是这第三维信息使我们能够看到立体的全息图像。其中的关键在于光的一个基本性质 —— 干涉（Interference）。在实验装置中，光波携带着特定的相位信息，当从物体反射的光波与直接照射到二维光敏材料上的参考光发生干涉时，会形成特殊的干涉图案，其中光波的相位差信息实际上编码了空间的深度信息。这意味着当我们观察全息图时，我们不仅看到了常规的二维图像，还通过不同时刻返回的光波重建了物体的深度信息，从而产生了三维立体的视觉效果。

萨斯坎德提出，我们不应该仅仅把信息（熵）与表面积的比例关系视为一个巧合，而应该将其提升为一个基本物理原理。这意味着这个原理应该适用于宇宙中的所有物理系统（任何具有能量的实体，包括物质和辐射）。更重要的是，这背后的本质是量子互信息，它描述了物体任意一侧与另一侧之间的量子关联。有了这些理论基础，我们就可以从信息论的角度来推导引力理论了。

爱因斯坦的广义相对论方程描述了物质的能量 - 质量如何影响四维时空的几何结构。正如物理学家约翰·惠勒（John Wheeler）所说，这个方程表明：物质决定了时空如何弯曲，而弯曲的时空又指导物质如何运动。例如，地球之所以围绕太阳运行，是因为太阳的巨大质量显著地扭曲了周围的时空。那么，这种描述引力的能量与时空曲率关系是否可以从量子信息理论中推导出来呢？

在 20 世纪 90 年代中期，物理学家泰德·雅各布森（Ted Jacobson）提出了一个富有洞见的理论论证，支持这种可能性。现在我们已经掌握了理解这个论证所需的全部理论工具。前文我们已经讨论过，热力学熵与系统的几何特性成正比。根据热力学基本定律，系统的熵与其温度的乘积等于该系统的能量。因此，根据爱因斯坦的质量 - 能量等价原理（E=mc²），更大的质量意味着更大的能量，这必然导致时空产生更大的曲率。

熵与能量之间的能量守恒关系可以转化为爱因斯坦的引力场方程，从而建立起质量与时空曲率之间的关联。在这个理论框架中，熵实际上反映了时空的几何性质。根据热力学定律，更大质量的物体会产生更大的熵值。而根据前面讨论的全息原理，这个熵值又与物体周围的表面积相关。因此，物体的质量越大，对周围时空的弯曲程度就越大。

让我们用一个思想实验来理解这个概念。想象一个不含任何物质或能量的时空（即真空宇宙）。现在我们用一束光将这个空间分成两半（尽管在真空宇宙中这种描述本身就是一个理论简化）。在这个空旷的宇宙中，光束会沿直线传播，因为没有任何物质干扰它的路径。如果我们在空间的一侧放置一个大质量天体（这里的「大质量」指具有显著引力效应的物体），根据热力学原理，这会改变系统的熵，而根据全息原理，这种改变会影响光的传播路径，迫使光线发生弯曲以适应新的时空几何。这正是天文学家爱丁顿（Eddington）在 1919 年首次验证广义相对论时所利用的原理。

爱丁顿通过观测证实：恒星的视位置变化与爱因斯坦广义相对论的预测完全吻合。这种现象的原理是：来自恒星的光在到达地球的路径上受到太阳引力场的影响而发生弯曲，使得我们观测到的恒星位置发生了偏移。

这个原理同样可以用来探测宇宙中的大质量黑暗天体，比如黑洞。黑洞由于其物理特性不会发出任何光，那么我们如何探测到它们呢？虽然黑洞本身不可见，但它们具有极强的引力场，我们可以利用这一特性来「间接观测」它们。尽管我们无法直接看到黑洞，但我们可以观察到它的引力场对周围物质和光线的影响。特别是，当黑洞位于某个恒星前方时，来自恒星的光不会像通常那样散射，而是会被黑洞的强大引力场聚焦。从地球上观测，我们会发现这颗恒星的亮度暂时显著增加，随后又恢复到正常水平。这种亮度变化现象可以用黑洞从恒星前方经过来解释。在天体物理学中，这种现象被称为引力透镜效应（Gravitational Lensing）。

现在我们发现，以熵衡量的信息是量子力学和引力理论的共同基础。在量子力学框架下，虽然系统的熵在任一时刻都是有限的，但系统可以持续演化产生新的熵（这体现了量子系统的随机性特征）。将量子熵与表面积成正比的性质，结合热力学第一定律（能量守恒定律），我们就能推导出引力场方程。这个发现特别引人注目，因为在传统观点中，量子物理和引力理论常被认为是难以调和的。然而，这个理论框架表明它们之间存在着本质的联系（这正是雅各布森的研究引起学术界广泛关注的原因）。

当然，我们必须承认这个理论推导中还包含一些有待验证的假设。但从整个讨论中我们可以得出一个重要结论：引力并未给信息处理理论带来根本性的改变。量子理论的基本原理已经包含了所有必要的理论要素。即使这个推导过程中的某些具体步骤可能需要修正，量子信息的基本特性在考虑或不考虑引力效应的情况下都保持不变。

基于这些理解，让我们回到一个根本问题：宇宙能容纳的最大信息量是多少？我们已经知道信息量与表面积成正比，以色列物理学家雅各布·贝肯斯坦（Jacob Bekenstein）对这个比例关系进行了精确的量化。他提出的关系式被称为贝肯斯坦界限（Bekenstein Bound），其表述非常优雅：任何物理系统所能包含的信息量上限等于 10^44 乘以该系统的质量（单位：千克）和其最大尺度（单位：米）的乘积（其中尺度的平方即为系统的表面积）。值得一提的是，贝肯斯坦关于黑洞熵的研究启发了英国物理学家斯蒂芬·霍金，使他发现黑洞并非完全「黑暗」。黑洞会发出被称为霍金辐射（Hawking Radiation）的量子辐射。

这个发现具有深远意义：在物体的众多物理特性中，仅仅通过面积和质量这两个参数，我们就能计算出如此基础的性质 —— 信息容量。让我们用这个理论进行一个有趣的计算：假设一个普通人的大脑尺寸约为 20 厘米，质量约为 5 千克，根据贝肯斯坦界限，它的理论信息存储容量可达 10^44 比特。相比之下，当今最先进的计算机的信息存储容量仅为 10^14 比特。这意味着要达到人类大脑的信息处理能力，我们需要 10^30 倍于当前的计算能力！

现在让我们运用贝肯斯坦界限来计算整个宇宙所能容纳的信息总量，这正是我们探讨这个问题的初衷。根据天文学家的观测估计，宇宙的直径约为 150 亿光年（约 1.4×10^26 米），总质量约为 10^42 千克（巧合的是，这个数字与科幻小说《银河系漫游指南》中的神秘数字「42」有着有趣的呼应）。将这些数据代入贝肯斯坦公式，我们得出宇宙的信息容量上限约为 10^120 比特。这个数字虽然大得难以想象，但它仍然是有限的（数学家们会指出，相对于无穷大来说，这个数字其实更接近于 0）。有趣的是，回顾阿基米德对宇宙中沙粒数量的估算 ——10^63，如果我们将每个沙粒对应于一个信息比特，这个在两千多年前做出的估计，从数量级的角度来看，展现出了令人称奇的洞见。

既然我们一直将宇宙比作一台量子计算机，那么分析其信息处理能力就显得很有意义。我们可以基于贝肯斯坦界限来进行这个估算。根据现有观测，宇宙的年龄约为 10^17 秒，在这段时间里产生了约 10^120 比特的信息。由此我们可以推算出宇宙的信息处理速率约为 10^103 比特 / 秒。相比之下，现代个人计算机的处理能力仅为 10^10 比特 / 秒。这意味着要在计算机上完整模拟宇宙的运行，我们需要 10^93 台（即 1 后面跟着 93 个零）这样的计算机同时运算。这个数字清楚地表明，仅凭计算机的运算能力，我们离完整模拟宇宙的目标还有很大距离。这也从侧面反映了人类认知能力的独特性：我们能够通过理论分析来理解如此复杂的系统，而无需完整的计算机模拟。

让我们来看看微观尺度下的情况。根据贝肯斯坦理论，一个氢原子的理论信息存储容量约为 400 万比特，而质子由于其体积远小于原子，其信息容量仅约为 40 比特。这意味着，如果我们能够完全掌握量子计算技术（虽然目前的技术水平还远未达到这个程度），理论上我们可以利用单个氢原子进行量子计算，完成对 1000 位数进行质因数分解这样的复杂运算 —— 这种运算即使对当今最先进的经典计算机而言也是极其困难的任务。

那么，我们对于上述计算得出的宇宙信息容量有多大的把握呢？按照波普尔的科学哲学，科学可以被视为一个将宇宙中的信息压缩为自然法则的过程，而这些法则又反过来帮助我们理解现实。但问题在于：我们无法预知明天的实验会不会推翻现有的认知，从而导致我们建立新的理论体系。这种情况在科学史上并不罕见 —— 一个存在了 200 年的理论可能会被一个关键实验所推翻。正如量子信息理论取代了经典信息理论一样，未来我们可能会发展出更高层次的信息处理理论，这种理论可能基于目前我们尚未发现或完全理解的物理规律。

那么，我们能否将这种科学探索方法推向更深层次？我们是否可能在不依赖于最终统一理论的情况下，建立起一个自洽的现实世界模型？

### Key points

The entropy of any system is proportional to the surface area of that system. This is known as the holographic principle and it is a consequence of quantum mutual information.

Using the holographic principle, we can estimate the number of bits in the Universe as well as the number of elementary units of information processing that it can hold.

Interestingly a similar calculation was performed by Archimedes some 2500 years ago, when he tried to estimate the number of grains of sand in the Universe (and his answer was not so far off).

The power of the Universe as a quantum computer is finite but way beyond anything we can currently imagine or have any idea how to use.

关键要点：

1、物理系统的熵与其表面积成正比，这就是著名的全息原理，它源自量子互信息的基本性质。

2、通过全息原理，我们能够估算宇宙的信息容量以及其基本信息处理单元的数量。

3、值得注意的是，大约 2500 年前，阿基米德在尝试计算宇宙中沙粒数量时，得出的结果从数量级的角度来看与现代计算惊人地接近。

4、从量子计算的角度来看，宇宙具有有限但巨大的计算能力，这种能力远远超出了我们当前的理论认知和技术水平所能达到的范围。