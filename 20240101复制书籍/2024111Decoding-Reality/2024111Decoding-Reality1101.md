Vlatko Vedral.(2018).2024111Decoding-Reality.Oxford University Press => 0101


PART THREE

The first part of the book was all about how information underpins a number of everyday processes. Information is as critical to the biological propagation of life as it is to structuring a stock portfolio or getting useful work out of random energy.

The second part of the book showed us that there is more to information than meets the eye. When our underlying model of physics is adjusted from a classical view to a quantum one, we find that our theory of information is also adjusted and becomes even more insightful than before. The key advantage of quantum information is that a quantum bit can exist in several states at the same time, the flip-side of this being its inherent degree of randomness. This can sometimes deter us when we want to use quantum information to perform useful work, but it also presents some opportunities that we can exploit to our advantage (recall that in quantum cryptography, randomness was used to eliminate eavesdropping).

More importantly, for any information to become more complex or higher quality (e.g. when we gain more knowledge about the Universe) the genuinely random element is key. We need to keep coming up with new information through this random element and then have a process that deterministically eliminates that which is incorrect or not required. It is this deterministic element that allows us to form useful knowledge.

The first two parts of the book unify a number of seemingly unrelated ideas through the common language of information processing, but the question still remains: where does even this information come from? This takes us right back to what we asked right at the beginning of this book, namely the question of creation ex nihilo.

All the theories and experiments presented in the first two parts are beyond reasonable doubt; these are all peer reviewed and well accepted facts. In Part Three of the book we move into more speculative and uncharted territory.

1101Sand Reckoning: Whose Information is It, Anyway?

In Chapter 9 we discussed the idea of a universal Turing machine. This machine is capable of simulating any other machine given sufficient time and energy. For example, we discussed how your fridge microprocessor could be programmed to run Microsoft Windows, then we described Moore's logic, that computers are becoming faster and smaller. Therefore, one day, a single atom may be able to simulate fully what a present day PC can do.

This leads us to the fascinating possibility that every little constituent of our Universe may be able to simulate any other, given enough time and energy. The Universe therefore consists of a great number of little universal quantum computers. But this surely makes the Universe itself the largest quantum computer. So how powerful is our largest quantum computer? How many bits, how many computational steps? What is the total amount of information that the computer can hold?

Since our view is that everything in reality is composed of information, it would be useful to know how much information there is in total and whether this total amount is growing or shrinking. The Second Law already tells us that the physical entropy in the Universe is always increasing. Since physical entropy has the same form as Shannon's information, the Second Law also tells us that the information content of the Universe can only ever increase too. But what does this mean for us? If we consider our objective to be a full understanding of the Universe then we have to accept that the finish line is always moving further and further away from us.

We define our reality through the laws and principles that we establish from the information that we gather. Quantum mechanics, for example, gives us a very different reality to what classical mechanics told us. In the Stone Age, the caveman's perception of reality and what was possible was also markedly different from what Newton would have understood. In this way we process information from the Universe to create our reality. We can think of the Universe as a large balloon, within which there is a smaller balloon, our reality. Our reality is based on our knowledge of the Universe (via the laws through which we define it) and as we improve our understanding of the Universe, through conjectures and refutations and evolutions of our laws and principles, the smaller balloon expands to fill the larger balloon. So is the rate at which the Universe keeps surprising us greater than the rate at which we can evolve our reality? In other words, will we ever understand our whole Universe?

Here Popperian logic comes to our aid. The very logic of conjectures and refutations, which is at the root of how we construct our understanding of reality, tells us that we cannot answer this question. Only if we know that the Universe can no longer surprise us, that there is no new physical theory superseding that which we currently have, can we be sure that one day we might be able to understand our whole Universe. But how do we know that no new event in the Universe will ever surprise us and cause us to change our view of reality? The answer is that we don't. We just don't know whether one day we will be surprised. This is the ultimate unknown unknown. Even though we can never know whether we will know everything, this does not prevent us from knowing how much there is to know with the current understanding. So how much information is there in the Universe? Where do we even start with such an absurd calculation? Interestingly we are not the first ones to tackle this issue and far greater minds before mine have grappled with this question.

One of the greatest human minds in history is Archimedes of Syracuse (circa. 287–212 BC). Archimedes made enormous contributions to astronomy, mathematics, engineering, and philosophy, and had a reputation at the time as being not only theoretically very astute but also very practical. In fact, he was regularly employed by the state for his ingenious ideas especially when they were in a pickle and didn't know who else to turn to. In one story, which saw Syracuse under attack from warring ships, his idea was to focus the Sun's energy using large curved mirrors, to burn enemy sails before the ships docked. This according to folklore was one of several times he had saved the city.

Uncompromising in his pursuit of science, Archimedes died as he lived. The last words attributed to Archimedes are ‘do not disturb my circles' and it is said he made this remark to a soldier just before he was slain. Whilst the soldier originally came to summon him, Archimedes' complete apathy to the soldier's concerns, and total focus on his work, resulted in a tragic end to an otherwise spectacular life.

One piece of his research was commissioned by the Syracusian monarch, King Gelos II, and resulted in what is considered the first research paper ever. The task was to calculate the number of grains of sand that could fill the Universe. Not the kind of task you would set to any ordinary man. It is certainly not clear what use King Gelos II ever made of this, or whether he ever used it for anything other than fanciful conversation.

Sand was the smallest thing known at that time so it was natural to phrase questions in terms of volumes of sand. With reference to the heliocentric model of Aristarchus of Samos (circa. 310–230 BC), Archimedes reasoned that the Universe was spherical and that the ratio of the diameter of the Universe to the diameter of the orbit of the Earth around the Sun equalled the ratio of the diameter of the orbit of the Earth around the Sun to the diameter of the Earth. You could see how this calculation was not everybody's cup of tea. In order to obtain an upper bound, Archimedes used overestimates of his data.

Firstly, to even begin to describe numbers of such epic proportions he needed to extend the current Greek numbering system. In practice, up until then they never had a language for such large numbers, but he needed it now. For example, Archimedes introduced the word ‘myriad' to represent the number we would now call 10,000. Following from that a myriad myriads would be 100 million (i.e. 10,000 times 10,000), and so on. However, the real difficulty came as he had to make assumptions about the size of the Universe, using a very limited knowledge of astronomy (by our standards). Here is what he reasoned:

1That the perimeter of the Earth was no bigger than 300 myriad stadia (roughly 50,000 kilometres – this is very close to the actual perimeter).

2That the Moon was no larger than the Earth (it's actually much smaller) and that the Sun was no more than about 30 times larger than the Moon (this is a huge underestimate).

3That the angular diameter of the Sun, as seen from the Earth, was greater than approximately half a degree (this is correct and not a bad estimate either).

With these assumptions, Archimedes then calculated that the diameter of the Universe was no more than 10 to the power of 14 stadia (i.e. in modern terms two light-years), and that it would require no more than 1063 (one followed by 63 zeros) grains of sand to fill it. In Archimedes' estimate, if you think of every point in space as a bit, in the sense that it either contains a grain of sand or not, then the number of bits according to him is 2 to the power of 10 to the power of 63, i.e. 2 to the power of 1063. This is a phenomenally large number, and later we'll compare it with the equivalent number generated more recently, some two millennia after Archimedes took the first stab.

Archimedes, of course, never had the level of understanding of the world that we do now. So the question is how much more accurate can we be with a two thousand year head-start.

Over the last two thousand years we have seen Popper's method working tirelessly to produce ever better approximations of reality. Scientists would come up with conjectures on how to describe elements of reality in the shortest and simplest way and then they would perform observations and experiments to test their models (conjectures and refutations). As the models are refuted, as new information or understanding comes to light, they are discarded and new models arise from their ashes to take their place. So far we have been looking at biological, computational, social, and economic aspects of reality. What we have seen is that information is a natural way in which to unify these seemingly different disciplines. As discussed throughout this book, at its heart, the nature of information processing depends entirely on the laws of physics. So to calculate the amount of information in the Universe it is natural that we resort to our best understanding of reality to date.

The two theories which encapsulate our current best understanding of reality are quantum physics and gravity. There are, of course, other theories (biological, social, economic, and so on) that have a legitimate claim to this and we have given them an equal seat at Calvino's table. However, it is generally regarded that quantum theory and gravity are the most fundamental descriptions available to us. In the same way that Kolmogorov viewed the information content or complexity of a message in terms of the shortest program used to describe it, we currently view the information content of reality in terms of quantum physics and gravity – which are our shortest programs used to describe reality.

We have already seen in Chapter 10 how quantum theory can be understood in terms of information, so now can we also do the same for gravity?

Gravity is quite distinct from quantum theory. Whilst the effects of quantum theory can be felt at the microscopic and macroscopic level, with effects becoming less influential for large bodies, gravity works the other way around: gravity dominates large bodies (e.g. planets) and becomes less influential for microscopic bodies. No current experiment can detect gravity between two atoms, no matter how close they are.

These two theories may seem to have their place at opposite ends of the spectrum, but they are, in fact, intricately related. Finding a single unified theory connecting quantum physics and gravity has been seen as the holy grail of physics for some time now and is one of the ‘bleeding edge' areas of physics. With tongue in cheek I will argue how gravity can be described as a consequence of quantum information (expounding on a view that is extremely controversial). This will, I believe, be the strongest indictment to date that quantum information does indeed provide the underlying description of reality.

The modern view of gravity, through Einstein's general relativity, is to see it as a curvature of space and time. In everyday language you may be more aware of it as a universal force of attraction, like when you throw a ball into the air and it comes back down. Einstein's view is, however, the most general and accurate description of gravity. In this view, time and space are inseparable and both curved interdependently. To visualize this, imagine a simple example where your bed represents space-time. As things are placed on the bed they create impressions by indenting the surface. If you put a football onto your bed it might make a slight impression; if you put yourself on to the bed this would make a much bigger impression. If you are sitting on the bed and you put the ball sufficiently near to you, it will be pulled in by your impression. In exactly the same way, all bodies attract one another in space-time because they create indentations in the space-time fabric which then propagate and affect each other. Understanding the curvature of space-time (or your bed as you sit on it) is therefore the key to describing the effects of gravity.

Any curvature in space-time necessarily means that distances and time intervals both become dependent on the mass of the object curving the space-time. For example, the time for a person closer to Earth runs faster than for someone further from Earth (assuming this person is not close to some other massive object) simply because the person closer to the Earth is more affected by the Earth's gravitational pull; in other words, the curvature of space-time is greater closer to the Earth. Given that we want to explain gravity in terms of quantum information, the question then is what is the connection between the geometry of the curvature and the concept of information (or entropy)? Amazingly enough, the answer lies in that quirky property we met in the last chapter, quantum mutual information.

We have met the concept of entropy in several chapters already. This concept is synonymous with the information content of a message or system. The higher the entropy of a system the more information it carries. In the first part of the book we used entropy for many different purposes; it quantified the capacity of a channel, the disorder in any physical system, the profit from a bet in terms of the risk taken, and social interconnectedness. For the purposes of the following discussion we should think of entropy as physical entropy, quantifying disorder in physical systems.

There is a very interesting relationship between the uncertainty within a certain system – as measured by its entropy – and its size. Suppose that we look at an object enclosed within a certain boundary. How complex would we expect it to be? A natural answer would be that the entropy of the object depends on its size, in particular its volume. Say that a molecule contains one million atoms arranged in a ball. If each atom has a certain entropy associated with it then it seems natural to expect that the total entropy is just the number of atoms, times the entropy of each atom. Therefore, the total entropy would scale as molecular volume.

Interestingly, it turns out that at low temperatures (such as the current state of the Universe), the entropy usually scales with the area and not the volume of an object (and as we know from elementary maths, the area of any object is necessarily always smaller than its volume). If you think of a ball-shaped molecule, the volume is made up of all the atoms on its surface plus the atoms inside. Therefore, if we are now asking what the maximum entropy of the ball-shaped molecule is, we might say it is proportional to the total number atoms in the ball (i.e. the volume), given that each atom should independently be able to contribute to the overall uncertainty. However, and rather surprisingly, what quantum theory is telling us is that, no – entropy is actually proportional to the total number of atoms on the surface (i.e. a significantly smaller ratio).

So why is there this difference between what seems logical and what quantum theory tells us? To understand this we have to look into quantum theory again, and specifically towards the nature of quantum mutual information. Recall that quantum mutual information is a form of super-correlation between different objects and that this super-correlation is fundamental to the difference between quantum and classical information processing (e.g. as we see in quantum computation).

Suppose that we divide the total Universe into two, the system, such as the molecule above, and the rest – which is everything outside of the molecule. Now, the quantum mutual information between the molecule and the rest is simply equal to the entropy of the molecule. But, quantum mutual information is not at all a property of the molecule, it can only be referenced as a joint property, i.e. a quantum correlation between objects. In this case it is a joint property between the molecule and the rest of the Universe. Therefore, it logically follows that the degree of quantum mutual information between these two must be proportional to something that is common to both, in this case the boundary – i.e. the surface area of the molecule!

This is a very profound conclusion. We think of entropy as the information content of an object. The fact that this information content is not within the object, but lies on its surface area, seems surprising to say the least! What this means is that the information content of anything does not reside in the object itself, but is a relational property of the object in connection with the rest of the Universe.

This also implies another important result. It is in fact a possibility which will be explored later. Within this formalism it is entirely possible for the Universe to have zero information content, whereas subsets of the Universe may have some information. The Universe is not correlated to anything outside of the Universe (by definition). However there are parts of the Universe that are correlated to each other. As soon as we partition the Universe into two or more distinct regions we begin to generate information, and this information is equal to the area of the partition, not to the size of the regions. The very act of partitioning, dividing, and pigeonholing necessarily increases information, as you cut through any parts that may be correlated.

We can present this conclusion pictorially as follows. Imagine that atoms inside the molecule are connected to atoms in the Universe via a series of ribbons. A limit on the number of ribbons that we can connect to the Universe is constrained by the surface area of the molecule (i.e. how many ribbons can we get through the surface of the molecule – as it is only of finite size). The information shared between the molecule and the Universe can be seen as proportional to the number of ribbons connecting the two. And this is logically why information scales as the area of the surface of the molecule.

The physicist Leonard Susskind proposed to call the relationship between entropy and area, the holographic principle. Holography has traditionally been part of optics and is the study of how to faithfully encode three-dimensional images onto photographic two-dimensional films. This is typically done by illuminating the object with laser light, which gets reflected off the object and the reflection is then recorded onto photographic film. When the plate is subsequently illuminated, a three-dimensional image of the object appears where once the real object stood. This phenomenon is probably familiar to the reader from its countless usage in magazines, stickers, toys, and science fiction films.

Optical holography was invented by Denis Gabor in the 1960s working in the labs at Imperial College in London (one of which became my office some 40 years later). He received the Nobel Prize for his ideas in 1971 (interestingly, the year I was born). He used holography to construct an optical version of Maxwell's demon (an idea which I also researched during my PhD). The whole surprise about his discovery is that he showed that two dimensions were sufficient to store all information about three dimensions, for which he duly received the Nobel Prize (sorry – though I thought hard but there's unfortunately no connection between us on this one!).

It's easy to see how two dimensions are recorded but where does the third dimension come from? It is this third dimension that allows us to see a hologram in three dimensions. The answer to this lies in the relational properties of light, known as interference. Going back to the experimental setup, light carries an internal clock, and when the light reflected from the object interferes with the light directly hitting the two-dimensional photographic film then an interference pattern is produced where the timing from the clock acts as the third dimension. This means that when you look at a hologram, you see the standard two-dimensional image, but you are also seeing light reflected back to you at slightly different times and this is what gives you the perception of a three-dimensional image.

Susskind suggested that we should not be surprised that information (entropy) scales with surface area, but rather we should elevate this to a point of principle. By this he means that this principle should be correct for anything in the Universe (anything that carries energy – e.g. matter, light). Furthermore, the key property behind this was quantum mutual information which we now see as being between anything on one side of the object and whatever is on the other side. Now we have all the pieces to derive gravity from this logic.

Einstein's equation in general relativity describes the effect of energy-mass on the geometrical structure of four-dimensional space-time. His equation says, to paraphrase John Wheeler, that matter tells space-time how to curve, while space-time (when curved) instructs matter how to move (e.g. the Earth moves around the Sun because the Sun curves space-time significantly). Can the energy-curvature relationship that encapsulates gravity be derived from quantum information theory?

An ingenious argument was given in the mid-1990s by Ted Jacobson to support the answer ‘yes' and we now have all the ingredients to recreate it. So far we have already discussed how the thermodynamical entropy is proportional to the geometry of the system. It is well known in thermodynamics that the entropy of a system multiplied by its temperature is the same as the energy of that system. Therefore a larger mass, which represents larger energy (based on the mass–energy equivalence), will imply a larger curvature in space-time.

A simple energy conservation statement between entropy and energy becomes Einstein's gravitational equation, relating mass to curvature. In this case entropy encapsulates geometry. A more massive object therefore, according to thermodynamics, produces larger entropy. However, we saw that the entropy is also related to the surface area surrounding the mass, according to the holographic principle that we have just discussed. Therefore, the more massive the object, the larger the indentation of the surrounding area.

It is very helpful to illustrate this with an example. Take space-time without any mass or energy (i.e. an empty Universe). Now divide it into two by taking a sheet of light, shining it directly through the middle (whatever that means in an empty Universe). This light is unaffected by anything else since the Universe is empty. Now imagine introducing a massive object on one side (massive here simply meaning an object with a large mass). From thermodynamics, this changes the entropy, which from holographic principles affects the area that the light travels, which will have to be bent now to take into account the change in geometry. This, in fact, was how general relativity was first tested by Arthur Eddington in 1919. He confirmed that the apparent change in the position of a star followed exactly the prediction by Einstein in his work on general relativity. Here the light from the star was being curved en route to us via the gravitational pull of the Sun – hence the star appears as if it had changed position.

Interestingly the same idea can be applied to the detection of massive dark objects such as black holes. How can you see a black hole when by definition it is black and it does not emit any light? What the black hole does have, however, is a huge gravitational force, and we can use this fact to ‘see' it. Whilst we cannot observe the black hole directly we can observe the impact of its gravitational force on matter and especially light around it. In particular the light of any star directly behind a black hole, instead of being dispersed in the usual manner, is instead highly focused as it gets caught up in the gravitational pull of the black hole. From Earth we observe the light from a star becoming significantly more intense than normal, before it settles back down to its normal level of light intensity. This change of intensity can be explained by a black hole passing in front of the star. In technical terms the effect is known as gravitational lensing.

Information, as measured by entropy, is now seen to underpin both quantum mechanics and gravity. In quantum mechanics, the entropy of a system is finite but we can always generate more entropy (which implies randomness). The fact that this quantum entropy is proportional to the area can then be coupled to the First Law of thermodynamics, which states that energy is conserved, to infer the equations of gravity. It is very interesting to note that quantum physics and gravity are frequently viewed as incompatible. However, this argument would suggest that, far from it, they are actually intimately related (which is why Jacobson's paper caused a lot of excitement).

We have already said that some aspects of this argument are speculative. However, what we can conclude from the whole discussion is that gravity does not add anything novel to the nature of information processing. All the theory required already exists through application of quantum principles. Even if the details of the argument are not correct, still the properties of quantum information are the same with or without gravity.

So with this in mind let's return to the question of how much information can maximally be squeezed into the total Universe as we know it. We have already said that information is proportional to area, and how exactly it is proportional has been estimated by the Israeli physicist Jacob Bekenstein. His relationship, known as the Bekenstein bound, is simply stated as follows: the number of bits that can be packed into any system is at most 1044 bits of information times the system's mass in kilograms and its maximum length in metres (the square of this length is the system's area) . As an aside we note that Bekenstein's work on black hole entropy prompted the British physicist Stephen Hawking to conclude that (after all) black hole are not as black as they seem. They emit the so-called Hawking radiation, whose ultimate origin is quantum.

It is amazing that to calculate something as profound as the information carrying capacity of any object, out of its infinitely many possible properties, we only need two: area and mass. As a practical application, this easily allows us to calculate the information carrying capacity of our heads. Say that a typical head is 20 centimetres in diameter and weighs 5 kilograms. That means that a typical human head can store 10 to the power of 44 bits of information. Compare this to the best current computers which are still only of the order of 10 to the power of 14 bits of information. We therefore need 1030 to get the equivalent information carrying ability of a human head!

For us, the task now is to apply the Bekenstein bound to calculate the total number of bits in the Universe, which, if you recall, was our original motivation. Astronomers have already given us a rough estimate of the Universe's size and weight, say 15 billion light-years in diameter and a mass of about 10 to the power of 42 kilograms (ironically, this coincides with the ‘forty two' from Hitchhiker's Guide to the Galaxy). When you plug this information into the Bekenstein formula, the capacity of the Universe ends up being on the order of 10 120 bits of information. This is a stupendously large number, but ultimately it is not infinite. (In fact mathematicians will argue that it is still closer to 0 than it is to infinity!) Also it's worth noting that Archimedes estimated 10 to the power 63 grains of sand in the Universe. If, as before, we take a grain of sand as analogous to a bit of information, then this is a pretty good guess of the Universe's information carrying capacity from someone who lived more than two millennia ago.

Since we have been equating the Universe to a quantum computer, it would also be applicable to talk about the processing speed of our Universe. This can be estimated immediately from Bekenstein's bound. If you take the age of the Universe as 10 to the power of 17 seconds and the fact that the Universe has generated 10120 bits (these are our current estimates), then we can say that the total capacity for information processing is about 10103 per second. Comparing this to a modern computer (your everyday Pentium 4 – whose processing capacity is not more than 1010 bits per second) we can see that we would need 1093 such computers to simulate the Universe. This is 10 followed by 93 zeros. Therefore, if we had to rely only on our computers to understand the Universe we would not get very far! This is an amazing indication of the power of the human mind!

For comparison, at the other end of the spectrum, lie small objects such as atoms and atomic nuclei. A hydrogen atom, according to Bekenstein, can encode about four million bits, while, a proton can encode only about 40 bits (simply because it is so much smaller than the atom itself). If we were skilful enough (and we are currently far away from this) we could run a quantum computation with just one hydrogen atom that would factorize a 1000-digit number, something we said was extremely difficult for any current computer.

How confident are we that this number of bits that we have calculated is the total number of bits of information in the Universe? Popper has already told us that you can view science as a machine that compresses bits in the Universe into laws, and these laws in turn are then used to generate reality. So how do we know whether tomorrow we will observe an experiment that will change the compression and give us a new law? The answer is that we don't! For a theory to have stood for 200 years and to then be refuted by a single experiment is standard fare for scientific progress. In the same way that quantum information has superseded classical information, it is likely that in the future we may progress to a new order of information processing, based on additional elements of reality unknown or not fully understood by us at this time.

So can we push this method of conjectures and refutations to the extreme? Can we present a consistent view of reality without even worrying about what the final theory would be?

Key points

The entropy of any system is proportional to the surface area of that system. This is known as the holographic principle and it is a consequence of quantum mutual information.

Using the holographic principle, we can estimate the number of bits in the Universe as well as the number of elementary units of information processing that it can hold.

Interestingly a similar calculation was performed by Archimedes some 2500 years ago, when he tried to estimate the number of grains of sand in the Universe (and his answer was not so far off).

The power of the Universe as a quantum computer is finite but way beyond anything we can currently imagine or have any idea how to use.