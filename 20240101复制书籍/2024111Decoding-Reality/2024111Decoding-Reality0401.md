Vlatko Vedral.(2018).2024111Decoding-Reality.Oxford University Press => 0401Digital Romance: Life is a Four-Letter Word

## 0401Digital Romance: Life is a Four-Letter Word

Lennon and McCartney were ‘spot on' with their rendition of the age-old adage that ‘Life goes on'. As well as being one of the catchiest tunes from their 1968 White Album, this is also one of the simplest and most profound statements known to us. Underneath its innocuous exterior lies a message that is of fundamental importance. The fact that there was life on Earth almost since Earth was formed clearly demonstrates its robustness, and yet this is all the more impressive given that every day we see how fragile individual living beings appear to be. So the question that comes up again and again in scientific circles is ‘how can something that is so imperfect survive for so long?' This was one of the great mysteries of biology.

Interestingly it was a mathematician and not a biologist who made the first major step in answering this question. We met this mathematician earlier, when he advised Shannon to use the word ‘entropy' to define his information function – yes, enter once more, John von Neumann. Von Neumann showed how something that was near perfect could be constructed out of imperfect components. This seems a little weird, doesn't it? One would intuitively think that to construct something perfect, each piece would also necessarily have to be perfect. This is one of the central problems for living systems. But how could a mathematician without any access to experimental evidence, and to be fair not much knowledge of biology either, be able to understand life so well? Here is how.

Von Neumann actually asked the question: ‘How can we make something of long duration from parts that are very short lived?' Imagine we want to make sure that we write a message down that should exist for the next ten millennia, so that all the future generations can benefit from it. Imagine, for example, that I have discovered the secret to eternal happiness. (I haven't, of course, I am as clueless as you are, though sometimes I think the answer lies somewhere between a cigar and a single malt whiskey.) And imagine that I would like my great-great-great-grandchildren to benefit from my acquired wisdom. How can I ensure that my message lasts for such a long time, when I don't even know anything about the time to come?

The astute reader may begin to see how this is related to a question that Shannon addressed in Chapter 1, that of communication between Alice and Bob. Imagine that, instead of Alice, I am now the encoder of this very important message and the channel, which was the telephone wire in the old picture, is now time. Furthermore, the receiver of the message, previously Bob, is now a generation of people in the distant future to whom we wish to communicate the message. This just illustrates how widely we can interpret the meaning of the communication channel and its users. Just to reinforce the point, you can also think of me – the writer of this book – as Alice; the book as the channel (through which I am communicating my thoughts); and yourself, the reader, as Bob, the receiver of my thoughts.

You may say that the longevity of a message is easy to accomplish. Perhaps we can just make a very solid safe, lock the message inside, and then wait. However your message will only survive for as long as this solid structure does. Natural and man-made disasters, pandemics, disease, or other factors all have a bearing on how long our message survives. The Egyptians thought that pyramids looked pretty solid, but even they have eroded quite a bit over the last six millennia and may perish altogether in the next few millennia. In fact, even our very planet has a chance of being destroyed in the not-so-distant future from a variety of threats (maybe not all external, as you are no doubt aware). Taking all these considerations into account, how could I secure my message to my descendents with a high probability?

This story of transmitting a message through generations is what von Neumann had in mind when he formulated his question. This question also presents a beautiful metaphor for life, whose primary goal is to endure. From the previous discussion it would be quite short-sighted to use a solid immobile structure to contain the message. Such a structure is not necessarily resistant to environmental changes.

What we need is something that is able to deal with its environment and is equipped to respond to whatever is thrown at it. It needs to be able to adapt, move, avoid obstacles and danger whenever it is threatened. But it also needs to be able to deal with its own frailty. Whatever this information carrier is made of, it will always have a finite duration. No battery lasts forever, no heart beats forever.

To illustrate this point, let's consider that conceptually we are able to build a robot that is capable of surviving forever (infinite battery life, non-rusting components, etc.) to carry the information. Furthermore we can assume that this robot could at the same time deal with its complex environment as well as maintain itself when damaged. This could do the trick, but the worries are: how likely are we to make such a device (as we said – no battery lasts forever, no heart beats forever), and, even if we could make it to the best of our abilities, surely its luck would run out at some stage. It is near impossible for the robot to account for every possible damage and environmental influence.

So why produce one robot, why not produce a hundred or a thousand? This seems like a good idea as it will prolong the message, but, ultimately, we still only have a fixed number of robots and the population can only decrease through time. Sooner or later each one of our copies will run out of luck.

But here is where your ingenuity really comes in; you think to yourself: ‘Why don't I build a robust robot that can also reproduce itself several times?' It would then give the message to each copy that it creates of itself and the copies would similarly pass the message on to their copies, ad infinitum. So with every generation being capable of carrying the message as well as reproducing itself, we have a fighting chance of preserving the message indefinitely.

And this is in essence what von Neumann explored in his paper on self-reproducing automata. His main contribution was to show how reproduction can be accomplished with robots created out of imperfect parts. This approach was by no means uncontroversial with the scientific establishment during von Neumann's time.

There were two main objections to self-reproduction. First of all, and in the words of von Neumann, ‘If an automaton has the ability to construct another one, there must be a decrease in complication as we go from the parent to the construct. That is, if A can produce B, then A in some way must have contained a complete description of B. In this sense, it would therefore seem that a certain degenerating tendency must be expected, some decrease in complexity as one automaton makes another automaton.' This is a pretty damaging objection as it seems to completely contradict everyday experience. Life appears to be getting more and more complex, rather than simplifying into less complex organisms.

Note that what von Neumann calls ‘complication' of an automaton is closely related to its information content, i.e. the more complex an automaton, the more bits of information are required to describe it accurately. As a side-note, many different ways of talking about biological complexity have indeed been proposed and we will encounter some of them later in the book.

The second main objection to self-reproduction is related to the previous one, only that now it also seems to contradict logic and not just experience. If A has to make another machine B, it then seems that B needs somehow to be contained within A initially. But imagine that B wants to then reproduce into C. This means that C must have been contained in B, but since B is contained in A, C must also be contained in A. Is your head spinning yet? So, in essence, what we are trying to say is, if we want to make sure that something lasts through hundreds of generations, it would appear that we would have to store all the subsequent copies in the initial copy. If we generalize this to an infinite number of copies, then there is clearly a resource impossibility, given that A would then have to store an infinite amount of information.

This second objection reminds me of the metaphor about a resident of a lunatic asylum, who sets himself the task of painting a complete image of the world in all its minute detail. He needs to start somewhere, so he starts with painting the garden of the lunatic asylum he is in. After some time, just as he becomes very pleased with his depiction of the garden, he realizes that something is missing in his painting. He himself does not appear in the painting! While he painted the whole garden in all its minute detail and complexity, he forgot to include himself – the painter. To correct this he then includes an image of himself, only to discover that the painting is still incomplete. He is still outside of it! He painted himself in the painting, but he – the actual painter who painted the painter – still needs to be incorporated. So he amends this error, and now his painting contains a painter with a canvas containing the painter painting the garden. As he thinks about it more and more, and to his great horror, he realizes that the painting is still incomplete and, worse still, that he can actually never finish the job (being insane doesn't exclude being intelligent, as half of my department can testify to). Unwittingly, the painter has got himself caught up in what mathematicians call an infinite regression.

Referring back to Chapter 1, we saw that Wheeler and Deutsch had the same problem when imagining an ultimate law of Nature that then paints all the other laws. They wanted to have a law that was complete and did not require any laws outside of it to explain it. Similarly, when we compare to the painter, the paradox is that no matter how skilfully this law paints reality, it can never produce a picture that contains everything, because it always fails to take itself into account.

Nature appears to be facing the same challenge when trying to solve the problem of re-creating a living being. One living organism appears to need to store a copy of its successor, who appears to need to store a copy of its successor, and so on and so forth. How can we ever jump out of this infinite sequence? Is life as we know it really a logical impossibility?

Von Neumann was well aware of these objections, which is precisely why he wrote his paper, in order to refute them and show how reproduction is possible both logically and practically, and without reducing the complexity of the new generations. Given that von Neumann didn't have a formal training in biology and was only using the power of abstract thought, it is sometimes simply mind-blowing that he achieved this landmark result.

Von Neumann's key idea is based on the fact that there is a clear separation between different components of the process. If we imagine a message that contains all the instructions for producing copies of an object (say, a house, car, refrigerator, etc.), then a copier, which copies the instructions, along with a constructor, which constructs replicas using the instructions, is essentially all we need to propagate the object indefinitely through time. This is the essence of von Neumann's approach, however for completeness let us spell out all the components needed for the full self-reproduction (known in more modern language as self-replication). The reader may find the next couple of pages quite challenging, but it's well worth the effort to familiarize yourself with details of this landmark result.

Let M be a universal constructor machine, in the sense that it can construct any other object given the appropriate instruction, I. Then let X be a specialized copying machine, which can copy the instruction, I, and insert it into the relevant object M has constructed. For example, let's say that M is a constructor machine within a factory that can construct other complex machines. If we feed M with the instruction to construct a car, then it will construct a car. If we feed M with the instruction to construct a chair it will construct a chair. If we feed M with an instruction to construct itself (i.e. using the instruction which was originally used to construct it) then it will also do likewise and we will end up with two identical machines. Good business you might think. Imagine you need to use this machine in a different part of the factory. Unless you also provide a copy of the instructions, to copy a chair/table, etc. or the machine itself, it's not going to be of much use. So you use a Xerox machine to make a copy of the instructions manual and send it along with the machine.

The process of feeding an instruction, I, and copying the instruction has to be controlled by a mechanism, C. In the factory example the control mechanism would be an administrator feeding instructions to the machine and Xeroxing and including the instruction manual whenever a new copy of the machine is made.

Management will be over the moon, as potentially we can increase production markedly just by replicating the constructor across the factory and then across every factory the company has. If we have 1000 constructor machines then we can make 1000 chairs (cars, widgets) at a time and whenever we need to increase production further we always have the option of asking the constructor to make another copy of itself. To operate the new machine we would require another controller (unless you can convince the previous controller to run both machines simultaneously).

Wait a minute, though: in order to self-replicate, does the new controller know how to use the machine and make copies of the instructions? Probably not, he will have to be trained on both. Perhaps then, looking at this machine in its most general sense as a universal constructor, we should also send instructions on how to make a controller and on how to make the Xerox machine that will be used by the controller. If we could do this it would mean that with sufficient power and materials, this machine would be able to replicate itself indefinitely if required. So let's summarize how this indefinite construction system would work.

What we have to start with are: (i) the universal constructer, M; (ii) the Xerox, X; and (iii) the controller, C. To make this into a perfect self-replicating process we need the full set of instructions not only on how to construct M, but also to construct the controller, C, and the Xerox that the controller uses, X. So the combination of M, C, X, and the instructions on how to construct M, C, and X is then a perfect self-replicating entity, which we will denote E.

The controller takes the instructions on how to construct M, C, and X, and feeds them into M. M then constructs a replica of itself, M´, the controller, C ´, and the Xerox machine, X´. The controller also makes a copy of the instructions, I´, using the Xerox machine X. So now we have M´, C´, X´ and I´ and this set is ready to be sent out as the fully operational self-replicating entity, E ´.

Note that we have avoided the vicious circle we presented before – namely that the first entity needs to contain instructions on every subsequent entity in order to propagate the message indefinitely. The decisive step occurs in constructing an entity containing the universal constructor, the Xerox machine, and the controller, as well as instructions on making all three. This process is legitimate and proper according to the rules of (Boolean) logic. And this is all there is to von Neumann's argument.

Although von Neumann's logic is phrased within the narrow context of self-replication, it clearly avoids the infinite regression that we discussed earlier. Can it therefore also be applied to tackle other similar problems, such as Deutsch and Wheeler's ‘law without law‘? Could everything in the Universe really arise out of nothing in this same way? We will entertain this possibility in Part Three of the book.

In von Neumann's argument, although there is no logical obstacle to indefinite self-replication, we still have a big practical challenge, namely that we assumed that every part of this process is perfect. However what if there are errors at any stage? What happens if the process of self-replication somehow becomes compromised (for example, the controller forgets to photocopy a page of instructions, the photocopier runs out of toner, or the machine simply breaks down)? The next question therefore is what will happen to subsequent replicas of M that are based on these damaged instructions. It seems like the obvious answer is that the process would have to halt – it simply cannot continue. However, here comes von Neumann's second key insight. He showed that this is not the case and that even imperfect parts can lead to a sustained process of replication. This is achieved through the addition of redundancy, where we create a large number of copies of E. Whilst some of these replicas may be sufficiently affected that they do not pass the controller's quality test, the others will be accepted and will propagate to other parts of the company.

It is also worth highlighting that in reality it is not always the controller that does this quality check; this can also be done by external factors (for example, the environment). We can view proliferation of business franchises in this von Neumann self-replicating framework. Take Starbucks as an example of a successful franchise. The first Starbucks was opened in Seattle in the 1970s. It was obviously sufficiently successful in terms of selling coffee that expansion was a natural option. The challenge was copying the model that had worked for the original Starbucks so well. This has been achieved to an uncanny level of detail and there are now 16,000 near-identical copies of Starbucks in over 30 countries. When you see a Starbucks in Beijing or Athens, you've a fair idea that it's going to look and taste the same as your Starbucks down in the road in New Jersey.

On the other hand, a few Starbucks have closed because they didn't replicate the instruction set accurately – they produced coffee that wasn't faithful to the original brew, or the look and feel was not sufficiently reminiscent to encourage ‘punters' to part with their money. Many more have closed despite being perfect replicas of the original. With the latter group, the environment may have forced its closure (e.g. local views being ‘anti-Starbucks', local coffee houses being preferred, or even a general trend locally to shift away from coffee houses). For example, 600 closed in 2008 due to environmental factors such as the economic slowdown. Even a perfect replica is not a guarantee of success.

The most successful businesses are those that understand this. They are able to process information continuously from their environment, either through good management or skilful external consultants. This information feeds back into their own set of instructions, along with information on their internal capability to continuously evolve new instruction sets. Unlike living systems, businesses can be geared to modify their instruction sets in very short spaces of time. The speed at which this happens is known as agility. In this sense, agility is the key to business longevity.

Hewlett Packard was founded in a garage in Palo Alto in 1939 by two electrical engineers, William Hewlett and David Packard. They initially concentrated on making electronic test equipment, such as oscilloscopes and thermometers. Later, with the rise of electronics, they moved into semiconductor devices and calculators. In the late 1960s, they saw a niche in the market for minicomputers and they got involved. Today they are known as one of the leaders in personal computing, imaging and printing, and enterprise storage and software. As the environment changed (market demand, information age), HP were able to adapt their instruction set to ride the next wave of technological innovation.

Of course, von Neumann's basic intention was not to explain businesses and their success, or explain how factories could be more productive. He wanted to show that it's possible to build self-replicating robots that could be used to colonize and explore life on other planets. Little did he know that living organisms had already figured this out some three billion years before him!

The holy grail of biology in the 1930s and 1940s was the quest for the structure within a human cell that carries the replicating information so well articulated by von Neumann. This structure was thought to be responsible for the colour of our children's hair, their eyes, their height – it tells us the instruction set of our own operation and every replica that we will produce. There were many people in the race to find this structure including James Watson, Francis Crick, Rosalind Franklin, Maurice Wilkins, Erwin Schrödinger, and Linus Pauling, to name a few. This was an extremely exciting and important time; we were on the verge of something great for humanity – a view into a better understanding of who we are and where we come from.

Ultimately it was a former ornithology student and an ex-physicist who won the race. James Watson and Francis Crick (with help from some of the other noted names) discovered the main carrier of this biological instruction set to be a complex acidic molecule called DNA (deoxyribonucleic acid). DNA contains the instructions on how to produce a similar copy to the organism that carries the DNA, analogous to the instruction set, I, fed to the universal constructor machine, M. Nature is extremely careful with preserving this molecule. It is not just that each of us contains one DNA molecule; in fact almost every cell in every living organism contains DNA. Furthermore each and every DNA molecule is capable enough on its own to reproduce the whole organism (albeit within the right environment). This is an instance of the redundancy that von Neumann discussed. Watson and Crick (and Wilkins) were duly awarded the Nobel Prize in 1962 for physiology and medicine.

From von Neumann's universal constructor we saw that we needed four different components, the universal constructor, M, the Xerox, X, the controller, C, and the set of instructions, I. Together they make up the self-replicating entity, E. Comparing this to life, we can see the cell itself as the self-replicating entity E. Inside the cell there are four different components that enable it to do so:

i) the protein synthesizer machine, M,

ii) the biological nano-engine (akin to the Xerox copier), X,

iii) enzymes which act as controllers switching the nano-engine on and off, C, and

iv) the DNA information set, I.

To be fair, although the big picture view is now well accepted, there are still many details, e.g. how the nano-engine works, which are still being investigated.

So we see DNA as key to this process, as it contains the blueprint of how each cell operates and replicates. Based on it, the constructor machine within our cells synthesizes amino acids, which in turn make up various proteins and new cells for our bodies. Cell replication is of course an extremely complex process, however in essence it boils down to von Neumann's picture. The crucial step when creating new proteins is how DNA information is faithfully copied (or ‘Xeroxed') from one cell to another. We are here only looking at a cell's information carrying capacity because this is the most fundamental signature of durability of life. So how exactly does this Xeroxing process work? What if we run out of toner or paper or make a mistake?

Making a new strand of DNA is like making a new zipper by using the old zipper as a model. A zipper is a little simpler than a strand of DNA because a zipper only has one kind of tooth. Unlike the zipper, DNA has four teeth; A, G, C, and T (these letters represent the names of four different molecules known as bases – adenine, guanine, cytosine, and thymine).

The first thing that the DNA copier does (in most living cells) is to unwind (unzip) a section of the old strand of DNA. It then re-creates a full zip from each of the unwound sections by finding the exact complement for each tooth in the surrounding pool. The rule is that an A tooth can only be matched with a T tooth, and a C tooth can only be matched to a G tooth (and vice versa). This means that whenever the copier spots an A tooth on the strand of DNA, it immediately knows that this should be paired with a T tooth.

The fact that C and G fit together, and A and T fit together, is just like a lock and key mechanism. Some keys are either too big or too small for some locks, but some keys fit perfectly. Because these four combine only in specific pairs, once the unzipped strand of DNA is exposed to the surrounding pool (containing the free teeth) these free teeth float in and line up in the proper order. For example, a free A tooth from the surrounding pool will not generally combine with either C or G. This is the process by which copies of the DNA strand are made.

It is also interesting how Nature uses the idea of redundancy to increase the chances of producing a faithful copy. A group of three bases, such as ATC, is each associated with one amino acid. So given that there are four bases, A,C,T, and G, we have four times four times four, namely 64, possible three-base-long combinations – and hence the possibility of encoding 64 distinct amino acids. However, rather surprisingly, there are only 20 amino acids in total (these 20 make up all living matter, including our bodies) meaning that, rather surprisingly, there is more than one triplet associated with the same amino acid. So, for example, in Nature, ATT, ATC, ATG all encode the amino acid isoleucine, while AGA and AGG both encode arginine.

So what's the point of all this over-encoding? The main advantage is, as before, to help minimize errors when DNA is replicated. So, if instead of ATT the replication process makes a mistake and copies this as ATC (so the last letter has been copied wrongly), this will not even be noticed in the new replica organism since the triplets ATT and ATC simply encode the same protein isoleucine. Nature is leaving little to chance, what an ingenious idea!

Redundancy of this kind – several different sequences of bases to encode one and the same amino acid – is the standard way of error correcting. This is certainly true for modern computers and communications, but, amazingly, it is also true in the case of all human cognition.

As an example, here is an extract from an e-mail that hit my inbox a couple of years back: ‘Aoccdrnig to rscheearch at Cmabrigde Uinervtisy, it deosn't mttaer in waht oredr the ltteers in a wrod are, the olny iprmoetnt tihng is taht the frist and lsat ltteer be at the rghit pclae. The rset can be a toatl mses and you can sitll raed it wouthit porbelm. Tihs is bcuseae the huamn mnid deos not raed ervey lteter by istlef, but the wrod as a wlohe.' This example just goes to show that there is a great deal of redundancy in the English language as well, and not just in the genetic code. If it wasn't for that, my students would never be able to read most of the comments I scribble over their work.

Given that my book, like the genetic code, is intended to convey some information, you may wonder what its redundancy is. The answer I calculated, using Shannon's entropy from Chapter 1, is 7.4. This means that instead of writing my message in 200 pages, I probably could have summarized everything I wanted to say in 25 pages. However, I expect the reader would not have thanked me for this (or maybe they would?).

You may remember that one of the key points from Chapter 3 was the importance of using a universal language of discrete binary digits. You will notice that Nature here also seems to have come up with a discrete encoding for information. But instead of using two bases to encode things, as in the Boolean logic we saw earlier, Nature uses four discrete bases. So why does Nature bother with four bases when Shannon showed that two is enough to do everything? This is one of the key questions in biology and we will offer some fascinating speculations on this topic in Chapter 9.

Another very important question to address is why did Nature choose digital rather than any analogue (non-digital) encoding? In other words, why did Nature choose just four bases rather than an infinite continuous set of bases? We don't have a 100% watertight mathematical proof, but we have a pretty good argument to offer as to why the digital encoding is better than any analogue form. There are two reasons in favour of digital encoding: one is the reduced energy overhead to process information, and the other is the increased stability of information processing. Let's look at each of these.

First, let's consider the energy expenditure of information processing. Starting with 10 bits (10 systems with two possible states, zero and one) imagine that it costs a certain unit amount of energy to flip one bit. This is the most elementary information processing we can imagine. Flipping all 10 bits then costs 10 times this unit energy. In order to perform a similar operation in any analogue environment we would have to invest much more energy. In an analogue environment we would need 1024 different states (2 to the power of 10, 210) to represent 10 bits. This is a necessary characteristic of all analogue systems. As an intuitive insight as to why this is so, think that between the lowest and highest energy state of the analogue encoding we have 1024 units of energy instead of the 10 we had in digital encoding. This huge energy overhead is simply a consequence of the fact that, by definition, the analogue encoding must be treated as a whole (as opposed to bit-by-bit).

The second advantage to Nature's discretization of information in DNA is that of stability. In analogue encoding, again by definition, it is significantly harder to notice errors because analogue encoding is continuous and different states are therefore much harder to distinguish than with digital encoding. So even if Nature started three billion years ago encoding its message as a mix of analogue and digital encodings, you can see why today it is now purely digital. The huge energy expense and much larger sensitivity to errors means that digital encoding was always going to be the natural choice.

But it is not just Nature which chooses this digital route. All modern technology is based on digital principles and, as a result, is far more resilient to errors. Most of the errors now are due to humans or the interface between man and machine. What is reliable is the information processing behind the scenes, deep down inside the operating units of every microchip. The message of all this is therefore pretty clear: when it comes to information processing you should ‘be wise and discretize'.

In spite of the care and ingenuity Nature has shown in producing perfect copies of the instruction set I, some errors still get through uncorrected. These are called mutations. On average the error rate for DNA replication is around one in a million. This may not seem very high, but consider this: an error rate of 10 in a million could potentially mean that your next child is born a chimpanzee. Most of the mutations tend to be detrimental in that they impair the survival of the constructed living organism, however some mutations may lead to a new and improved form, better suited to its environment. This is the basis of the process of evolution known as ‘natural selection'.

Recall that one of the principal objections against any sustained process of reproduction is that complexity of subsequent copies has to decrease. We now know that this is simply not true and we know why. Complexity actually increases on average. The key component in this increase of complexity is the process of natural selection, first clearly identified by Charles Darwin. Natural selection is the process by which you are correlated to your environment because the traits that survive are those that are best suited to that environment. For example, if the whole world is submerged under water, naturally only those organisms which can breathe under water will survive. Moreover, whatever happens subsequent to the flooding, the genetic pool that will propagate will be built from the DNA of these surviving organisms.

Turning this on its head, your DNA can therefore be thought of as a historical record of the environmental changes that your ancestors have lived through (Dawkins describes it as the ‘genetic book of the Dead'). Since history is littered with environmental changes of some degree or another, it is clear that any DNA that propagates will only ever get more complex (as it contains more and more information about environmental changes). Note that without randomness this process would not work, given that it is random changes in our DNA that provide the variety on which natural selection operates, selecting mutations that lead to organisms better suited to their environment. As a general theme, we will see that meaningful information necessarily emerges only as an interplay between random events and deterministic selection. Each on its own is insufficient.

Now that we believe we understand the essence of biological information in terms of its durability, it would be useful to use this knowledge to correct genetic errors ourselves. This could potentially eliminate key terminal disabilities and illnesses and possibly improve our overall quality of life. This is the hotly debated and somewhat controversial area of research known as genetic engineering.

The trouble with altering genes artificially is that there is hardly ever a ‘one-to-one' correspondence between genes and the traits of an individual. There are no genes that just determine the colour of our eyes. The same set of genes will also determine some of our other features, like height, posture, etc. So if you want to genetically modify the colour of your baby's eyes, then by altering the responsible genes it is likely that you will also affect some other features of your child.

Until we have understood all these genetic inter-relationships it will be very difficult to utilize genetic engineering consistently for any positive purposes. It is not clear to what degree we will be able to understand these inter-relationships to make sure that our changes are always benign. Many hope that this will be the case one day, although Nature may not care one iota for our optimism. Personally, I don't see this as a serious moral dilemma yet, simply because we don't have enough information to consistently affect precise genetic characteristics of humans with a high enough degree of success. However, genetic engineering polarizes opinion between scientists and non-scientists (and even between scientists themselves) like no other issue.

As a postscript to the whole DNA story, I would like to say that Watson and Crick were almost scooped by another Austrian physicist, Erwin Schrödinger. Schrödinger was one of the pioneers of quantum mechanics (we will meet him again in Chapter 9) and after revolutionizing physics, he turned his attention to biology. Schrödinger was no slouch here either, almost deducing the exact mechanism for information reproduction some 10 years before Watson and Crick. His description of reproduction is correct in all its details, apart from Schrödinger thinking that the encoder in the replication must be a crystal (since crystals have a stable and periodic structure seemingly ideal for information carrying and processing). Watson and Crick later proved it not to be a crystal, but in fact an acid (DNA).

It turns out that this story is not really concluded, as some part of the encoding process may be done by some crystal-like structure, which itself carries some extra information. In other words, maybe information is also carried by something other than just DNA – i.e. DNA is not the carrier of all the biological information necessary to reproduce life. We know this because the relevant DNA content of bacteria, frogs, humans, and all living beings is roughly the same. Roughly (only!) 20,000 genes suffice to make any living being. But humans are clearly more complex than bacteria, and so it cannot all be in the DNA. Maybe Schrödinger could still be partly right, but there are also many other competing theories as to where this difference comes from.

On top of this there is the question of where the DNA comes from. Is there a simpler structure from which it has evolved? Going back to Schrödinger's work on the encoding of biological life in crystals, crystals are much simpler in structure than DNA and they grow much more spontaneously in Nature. So maybe, in contest with a number of other theories, they offer some insight into the evolution of DNA itself. Granted that crystals can be created and replicate spontaneously, we still have the question of how the information necessary to reproduce life moved from the crystals to DNA. This idea is not new. It was proposed by Alexander Graham Cairns-Smith some 40 years ago and continues to be a hotly debated area of research in biology.

All these questions are very interesting, but their exact solution is not important here. What is important, and will surely survive any future development in biology, is that at the root of life is the notion of information and the durability of life will depend on how we account for it.

As we saw in the initial chapter on creation ex nihilo, the fundamental question is why there is any information in the first place. For the replication of life we saw that we needed four main components, the protein synthesizer machine, M, the DNA Xerox copier X, the enzymes which act as controllers, C, and the DNA information set, I. It already looks very complex, but how did this complexity start from nothing?

Can we turn the question of ‘biological information from no information' on its head to perhaps make a bit more sense of it? There is an anthropic principle in science which answers the question of ‘Why is the Universe like it is?' with the response that ‘If it wasn't, then we wouldn't be here to observe it'. But this sounds like no answer at all. We will be back to wrestle with this issue in the final chapters.

数字浪漫：生命是一个四字词

列侬和麦卡特尼在诠释那句古老谚语「生命在继续」时，可谓洞见深刻。这首歌不仅是他们 1968 年白色专辑中最动听的旋律之一，更是人类认知中最简单却又最意味深长的真理之一。在这看似平常的表述背后，蕴含着一个极其重要的讯息。地球上的生命几乎与地球同龄这一事实，有力地证明了生命的顽强本质。更令人惊叹的是，尽管我们每天都目睹着个体生命是多么脆弱，生命整体却始终绵延不绝。因此，科学界经常思考这样一个问题：「为什么看似如此不完美的存在，却能持续如此漫长的时间？」这个问题成为了生物学最引人入胜的谜题之一。

有趣的是，在回答这个问题上取得重大突破的并非生物学家，而是一位数学家。这位数学家就是我们之前提到过的 John von Neumann（冯·诺依曼），他曾建议 Shannon 使用「熵」这个词来定义信息函数。冯·诺依曼证明了一个令人惊讶的现象：可以用不完美的组件构建出近乎完美的系统。这个发现令人难以置信，因为我们通常会认为，要构建一个完美的系统，其组成部分也必须完美无缺。这恰恰是生命系统面临的核心问题之一。然而，令人不解的是，一位既没有实验数据支持，对生物学也知之甚少的数学家，是如何能够如此深刻地理解生命的本质的呢？让我们一起来揭开这个谜题。

Von Neumann 曾经提出了一个发人深省的问题：「如何用短暂的部件创造出经久不衰的事物？」设想一下，我们希望留下一条信息，让它能够存续一万年之久，使后世 generations 都能从中获益。比方说，假设我找到了获得永远幸福的秘诀。(当然，这只是假设，实际上我跟你一样对此毫无头绪，虽然有时候我觉得答案可能就藏在雪茄和单一麦芽威士忌之间。）再假设，我想要让我的远房后代也能从这份智慧中受益。问题是，当我对未来一无所知的情况下，要如何确保这条信息能够经受住漫长时光的考验呢？

有心的读者可能已经注意到，这与 Shannon 在第一章中探讨的 Alice 和 Bob 之间的通信问题有着密切联系。让我们换个角度思考：现在不是 Alice，而是我成为了这个重要信息的编码者；原本在旧图中作为传输渠道的电话线，现在则变成了时间；而信息的接收者，不再是 Bob，变成了我们期望传达信息的未来世代。这个例子很好地展示了我们可以如何从更广阔的视角来理解通信渠道和使用者的概念。为了让这个观点更加清晰，你也可以这样想：我这个作者就像 Alice，这本书就是传输渠道（通过它我在传达自己的想法），而你们读者就像 Bob，是我思想的接收者。

你可能会觉得让一条信息长久流传下去是件容易的事。也许我们只需要造一个非常牢固的保险箱，把信息锁在里面就行了。但实际上，你的信息能保存多久，完全取决于这个保险箱能存在多久。不管是自然灾害还是人为灾难，疫情、疾病或其他因素，都会影响信息的保存时间。古埃及人曾经认为金字塔是最坚固的建筑，但六千年过去了，就连金字塔都已经风化得很厉害了，再过几千年可能就彻底消失了。更不用说，就连我们脚下的这颗星球，在不远的将来都可能因为各种威胁而毁灭（其中一些威胁可能就来自我们自己，这点想必你也心知肚明）。那么，考虑到这些因素，要怎样才能让我的信息有较大概率成功传递给后代呢？

这个代代相传的信息传递故事，正是冯·诺依曼（von Neumann）在提出他的问题时所思考的。这个问题也完美地比喻了生命的本质，因为生命的首要目标就是延续。从前面的讨论我们可以看出，用一个固定不变的结构来储存信息是不明智的选择，因为这种结构未必能经受住环境的变迁。

我们需要的是一个能够应对环境变化，并且能够对各种外部挑战做出及时反应的系统。它必须具备适应能力，能够移动，在面临威胁时懂得避开障碍和规避危险。同时，它还需要能够应对自身的局限性。无论这个信息载体是由什么物质构成，它都无法永恒存在。就像没有永不耗尽的电池，也没有永远跳动的心脏一样。

让我们从概念层面来思考这个问题：假设我们能够制造出一个可以永久运转的机器人（配备永不耗尽的电池、永不生锈的零件等）来保存信息。不仅如此，我们还可以假定这个机器人既能应对复杂的环境，又能在受损时自我修复。这种方案看似可行，但却面临两个关键问题：首先，我们能造出这样的设备的可能性有多大呢？要知道，没有任何电池可以永远使用，也没有任何心脏可以永远跳动。其次，即便我们竭尽全力打造出这样的机器人，它总有失效的一天。因为机器人不可能预见和应对所有可能发生的损坏和环境因素。

那么，与其制造一个机器人，为什么不制造成百上千个呢？这个想法乍看很有道理，因为可以延长信息的保存时间。但问题在于，我们最终仍然只能制造有限数量的机器人，而且这个数量只会随着时间推移逐渐减少。终有一天，所有的机器人都将遭遇不测。

这时候你的创造力就真正派上用场了；你会这样思考：「为什么不建造一个既能携带信息又能多次复制自己的可靠机器人呢？」这个机器人会将信息传递给它创造的每一个复制品，而这些复制品又会将信息传递给它们的复制品，这个过程会不断持续下去。这样一来，由于每一代都具备传递信息和自我复制的能力，我们就有希望永久地保存这个信息。

这实际上就是冯·诺依曼在他关于自复制自动机（self-reproducing automata）的论文中所研究的内容。他的主要贡献在于证明了即使使用有缺陷的部件，也能构建出具备自我复制能力的机器人。这种观点在冯·诺依曼的时代在科学界引发了相当大的争议。

关于自我复制现象，主要存在两个反对观点。首先，正如 von Neumann 所说，「如果一个自动机能够构建另一个自动机，那么从父代到子代必然会出现复杂度的降低。换句话说，如果 A 能够产生 B，那么 A 必然以某种方式包含了 B 的完整描述。因此，从这个角度来看，我们似乎必须预期会出现某种退化趋势，即当一个自动机制造另一个自动机时，复杂度会降低。」这是一个非常有力的质疑，因为它似乎完全违背了我们的日常经验。我们观察到的生命形式似乎是在不断变得更加复杂，而不是简化为更简单的生物。

值得注意的是，von Neumann 所说的自动机的「复杂性」与其信息含量密切相关，也就是说，自动机越复杂，就需要越多的比特来准确描述它。值得一提的是，关于如何讨论生物复杂性，学界已经提出了许多不同的方法，我们将在本书后续章节中详细介绍其中的一些观点。

关于自我复制的第二个主要反对观点与前一个观点有关，但这一观点不仅与经验不符，更似乎违背了逻辑本身。假设机器 A 需要制造一个复制品 B，那么 B 的信息就必须以某种形式预先存在于 A 中。进一步设想，当 B 要复制生成 C 时，C 的信息就必须存在于 B 中，而由于 B 的信息本身存在于 A 中，这就意味着 C 的信息实际上也必须预先存在于 A 中。这种递归关系引发了一个根本性问题：如果我们希望通过自我复制来维持数百代的传承，那么初始个体就必须储存所有后代的信息。将这个逻辑推广到无限代际，就会遇到物理资源的根本限制，因为这要求 A 具备存储无限信息的能力。

这第二个反对意见让我想起了一个关于精神病院里一位病友的比喻。这位病友想要完成一项任务：画一幅包含世界上所有微小细节的完整画作。他必须从某处着手，于是选择先画下他所在的精神病院的花园。过了一段时间，正当他对自己画的花园感到十分满意时，突然发现画中缺少了什么 —— 他自己！虽然他把花园的每一个细节都画得极其精细复杂，却忘记把自己这个画家也画进去。于是他把自己加入画中，但很快又发现画作依然不完整。因为他本人还是在画外！虽然画中已经有了他的形象，但那个真正在作画的他 —— 画这幅画的画家本人 —— 却还没被包含进去。他试图修正这个问题，现在画中出现了一个画家和一幅画布，画布上有个画家正在画这个花园。然而当他仔细思考后，慢慢意识到了一个令人困扰的问题：这幅画永远也不会完整。更要命的是，这项工作根本就没法完成（这证明了一个有趣的现象：精神失常和聪明才智并不冲突，我们系里一半的人就是明证）。就这样，这位画家不经意间陷入了数学家所说的「无限回归」（infinite regression），也就是一个永远无法终止的循环过程。

回顾第一章，我们发现 Wheeler 和 Deutsch 在构想一个能够阐释所有其他自然规律的终极法则时，都遇到了同样的难题。他们期望找到一个完整的法则，这个法则本身不需要借助任何外部法则来解释。这就像画家的比喻一样，问题在于不管这个法则如何精妙地刻画现实，它都无法创造出一幅包罗万象的图画，因为它永远无法把自己也画进去。

自然界在尝试解决生命繁衍这个问题时，似乎也遇到了类似的挑战。一个生命个体需要包含它后代的信息，而它的后代又需要包含下一代的信息，这样无休无止地延续下去。那么，我们究竟要如何打破这个无限循环呢？难道说，我们所认知的生命本身在逻辑上就是一个悖论吗？

Von Neumann 深知这些质疑的存在，这正是促使他撰写这篇论文的原因。他想要驳斥这些观点，并证明自我复制（reproduction）在逻辑和实践层面都是可行的，同时不会导致后代复杂度的降低。考虑到 Von Neumann 并未接受过正式的生物学训练，仅凭借抽象思维的力量就能够达成如此具有开创性的成就，这实在令人叹为观止。

Von Neumann 的核心思想在于他发现整个过程中的各个组件可以被清晰地区分开来。假设我们有一条包含了制造某个物体（比如房子、汽车、冰箱等）所需的完整指令信息，那么我们只需要两个关键部件就能让这个物体在时间长河中永续传承下去：一个用于复制指令的复制器，和一个根据指令建造复制品的构造器。这就是 Von Neumann 方法的精髓所在。不过为了完整性，让我们详细介绍实现完全自我复制（self-reproduction，在现代也称为自我复制 self-replication）所需的所有组件。虽然接下来几页的内容可能会让读者觉得不太容易理解，但为了掌握这项开创性研究成果的细节，这些努力都是值得的。

假设我们有一台通用构造机器 M，它能够根据给定的指令 I 构造出任何物体。同时我们还有一台专门的复制机器 X，它可以复制这些指令 I，并将其配套到 M 构造出来的相应物体上。举个例子，假设 M 是工厂里的一台可以制造各种复杂机器的构造设备。当我们输入造车指令时，它就能制造出一辆汽车；输入造椅指令时，它就能制造出一把椅子。如果我们输入它自身的构造指令（也就是最初用来制造这台机器的指令），它同样可以照做，最终我们就能得到两台完全相同的机器。这看起来似乎是个很有前景的设想。不过，设想你需要在工厂的另一个区域使用这台机器，如果不能同时提供制造椅子、桌子或机器本身所需的指令副本，这台机器就失去了大部分用处。这就像你需要用复印机复制一份说明手册，然后和机器一起发送到目的地一样。

指令 I 的输入和复制过程必须由控制机制 C 来管理。以工厂为例，这个控制机制可以是一名管理员，负责向机器输入指令，并在生产新机器时制作说明手册副本并将其随机器一起提供。

这个方案会让管理层非常振奋，因为我们可以通过在工厂内部以及公司旗下所有工厂推广构造器（constructor）来大幅提升产能。假设我们拥有 1000 台构造器，就能同时生产 1000 个产品（可以是椅子、汽车或其他部件）。而且当需要进一步扩大产能时，我们随时可以让构造器复制自己。当然，要操作新增的机器，我们还需要配备新的控制器（除非能说服原控制器同时操作两台机器）。

让我们思考一个问题：为了实现自我复制，新的控制器是否已经掌握了如何使用机器并复制指令的能力？答案很可能是否定的，因为它还需要在这两个方面进行训练。因此，如果我们从最广义的角度将这台机器视作通用构造器，那么我们还需要提供如何制造控制器以及制造控制器所需复印机的指令。这样一来，只要有充足的能源和原材料，这台机器就能够按需无限地进行自我复制。现在让我们来总结这个无限构造系统的运作原理。

这个系统最初包含三个基本组件：(i）通用构造器 M、(ii）复印机 X、(iii）控制器 C。要实现完美的自我复制过程，我们不仅需要构造 M 的完整指令，还需要构造控制器 C 和复印机 X 的全部指令。这样，通用构造器 M、控制器 C、复印机 X，以及构造这三者的完整指令集就构成了一个完美的自我复制实体，我们将其记为 E。

控制器（Controller）接收有关如何构建 M、C 和 X 的指令，并将其输入到 M 中。随后，M 开始构建出三个复制品：自身的复制品 M´、控制器的复制品 C´ 以及 Xerox 机器的复制品 X´。同时，控制器利用 Xerox 机器 X 制作了指令的副本 I´。这样，我们就得到了一套完整的复制品：M´、C´、X´ 和 I´，它们共同组成了一个功能完备的自复制实体 E´，可以随时投入使用。

这个设计巧妙地避免了我们之前提到的逻辑困境：如果要无限传递信息，第一个实体就需要包含所有后续实体的指令。关键突破在于构建了一个包含通用构造器（Universal Constructor)、Xerox 机器和控制器的实体，同时还包括了制造这三个组件的完整指令。根据布尔逻辑（Boolean logic）的规则，这个复制过程是完全合理且有效的。这就是冯·诺依曼在自复制系统研究中提出的核心论证。

虽然数学家冯·诺依曼提出的逻辑主要针对自我复制系统这一具体领域，但它成功避开了我们之前讨论的无限递归问题。那么，这种思路是否也能用来解决其他类似的难题呢？比如 Deutsch 和 Wheeler 提出的「无需基础法则的法则形成」问题？我们是否可以用类似的方式来解释宇宙万物是如何从最初的空无发展而来的？这些引人深思的问题，我们会在本书第三部分详细探讨。

在冯·诺依曼的论述中，尽管从逻辑上来说无限自我复制是可行的，但我们仍然面临着一个重大的现实挑战：我们假设了这个过程的每个环节都是完美无缺的。那么，如果在某个环节出现错误会怎样呢？如果复制过程出现故障（比如说，监控系统漏掉了一页说明书没有复印，复印机的墨粉耗尽了，或者机器本身发生故障），结果会是什么？接下来我们就要思考，按照这些受损的说明书制造出来的复制品 M 将会如何。表面上看，答案似乎很明显 —— 整个过程必须终止，无法继续进行。然而，正是在这一点上，冯·诺依曼提出了他的第二个关键性见解。他证明了事实并非如此，即使是有缺陷的部件也能维持复制过程的持续进行。这是通过引入冗余机制来实现的，也就是创建大量的 E 副本。虽然其中一些副本可能会因为质量问题而未能通过监控系统的检验，但其他合格的副本仍然可以在公司的各个部门之间传播使用。

需要特别指出的是，在现实中，质量检验工作并不一定由管理者来执行，有时也可能是由外部因素（比如市场环境）来完成。我们可以用冯·诺依曼自我复制系统（von Neumann self-replicating framework）的概念来理解连锁经营的扩张过程。以星巴克（Starbucks）这个成功的连锁品牌为例。第一家星巴克于 1970 年代在西雅图开业。由于其在咖啡销售上取得了显著成功，业务扩张就成为了顺理成章的选择。其中最大的挑战是如何复制原始星巴克的成功经营模式。而星巴克在这方面取得了惊人的成就，如今已在全球 30 多个国家开设了 16,000 家几乎一模一样的连锁店。无论你是在北京还是雅典的星巴克，都能确信这里的体验会和你在新泽西常去的那家星巴克完全一样。

另一方面，一些星巴克门店因为未能严格执行标准化经营模式而被迫关闭 —— 这些店铺制作的咖啡与品牌标准相去甚远，或者店铺的整体氛围与装修风格无法吸引消费者掏腰包。然而，更多的店铺即便完全符合品牌标准，最终也难逃关门的命运。这些完全达标的店铺之所以关闭，主要是受到外部环境的影响，比如当地民众对星巴克持排斥态度，更青睐本土咖啡店，或者该地区整体上对咖啡文化的热情在减退。举个例子，仅在 2008 年，就有 600 家店铺因经济衰退等环境因素而关闭。由此可见，即便完全复制品牌标准，也不能保证经营必定成功。

真正成功的企业都深谙此道。它们能够通过优秀的管理团队或专业的外部顾问，持续不断地收集和处理来自外部环境的信息。这些信息连同企业内部能力的评估，会不断地用于更新企业的运营策略和行动方案。与自然界的生物不同，企业能够在极短的时间内调整其运营方式。这种快速调整和适应变化的能力，我们称之为敏捷性（Agility）。因此，敏捷性成为了决定企业能否基业长青的关键因素。

惠普（Hewlett Packard）公司由两位电气工程师 William Hewlett 和 David Packard 于 1939 年在帕洛阿尔托的一个车库中创立。他们最初专注于制造电子测试设备，如示波器和温度计。随后，随着电子技术的发展，他们进入了半导体设备和计算器领域。在 1960 年代末，他们发现了小型计算机市场中的特殊机遇，并开始进军这一领域。如今，他们已成为个人计算、成像和打印、企业存储和软件领域的领军企业之一。随着环境的变化（包括市场需求和信息时代的到来），惠普能够及时调整其发展战略，搭上技术创新的每一波浪潮。

当然，约翰·冯·诺依曼（John von Neumann）的最初构想并非解释企业如何取得成功，也不是要说明工厂如何提高生产效率。他的目标是证明可以制造能够自我复制的机器人，用于在其他星球上进行殖民和生命探索。然而有趣的是，在他提出这个想法之前的 30 亿年，地球上的生命体就已经掌握了自我复制的奥秘。

在 20 世纪 30 和 40 年代，生物学家们最为追寻的终极目标，就是要找到人类细胞中那个负责携带遗传信息的结构，这种信息传递机制此前已被冯·诺依曼（von Neumann）清晰地阐述过。这个神秘的结构被认为控制着我们后代的各种特征，比如头发颜色、眼睛颜色和身高，它就像是一本详细的说明书，记载着我们自身以及未来子孙的所有生物特征。在探索这个结构的科学竞赛中，涌现出了许多杰出的科学家，其中包括 James Watson、Francis Crick、Rosalind Franklin、Maurice Wilkins、Erwin Schrödinger 和 Linus Pauling 等。那是一个令人振奋的重要历史时期，人类即将揭开重大发现的序幕，这个发现将帮助我们更深入地理解自己的本质和生命的起源。

在破解生命遗传密码的竞赛中，最终胜出的是一位曾经学习鸟类学的学生和一位物理学出身的科学家。James Watson 和 Francis Crick（在其他一些著名科学家的协助下）发现了承载生物遗传信息的关键分子 —— DNA（脱氧核糖核酸）。DNA 就像是一套详细的说明书，包含了如何制造一个与携带该 DNA 的生物体相似的生命体的全部信息，这就像是输入到机器中的程序指令一样。大自然为了确保生命的延续，采取了极其周密的保护措施。不仅我们每个人体内都有 DNA，实际上，几乎所有生物的每个细胞中都存在 DNA。更神奇的是，只要环境合适，每一个 DNA 分子都具备指导产生一个完整生物体的全部信息。这种信息的多重备份机制正是 von Neumann 在研究中提到的重要特性。正是由于这一重大发现，Watson 和 Crick（以及 Wilkins）在 1962 年共同获得了生理学或医学领域的诺贝尔奖。

从冯·诺依曼提出的通用构造器理论中，我们知道需要四个不同的组件：通用构造器 M、复制器 X、控制器 C 和指令集 I。这些组件共同构成了自我复制实体 E。将这个模型与生命进行对比，我们可以将细胞本身视为自我复制实体 E。在细胞内部存在四个使其能够进行自我复制的不同组件：

1、蛋白质合成机器 M。

2、生物纳米引擎（Biological Nano-engine，类似于复制器）X。

3、作为控制器的酶类 C，负责调控纳米引擎的开关。

4、DNA 信息集 I。

需要说明的是，尽管这种整体理论模型现已得到广泛认可，但仍有许多具体细节有待研究，比如纳米引擎的具体工作机制等。

DNA 在这个过程中扮演着关键角色，因为它储存着每个细胞运作和复制的蓝图。在这个蓝图的指导下，我们细胞内的蛋白质合成系统会制造氨基酸，这些氨基酸随后会形成各种蛋白质，并最终构建出我们身体的新细胞。尽管细胞复制是一个极其复杂的过程，但从本质上来说，它符合冯·诺依曼提出的自我复制模型。在合成新蛋白质的过程中，最关键的步骤是如何将 DNA 信息准确地从一个细胞传递到另一个细胞。我们之所以特别关注细胞的信息承载能力，是因为这是生命持久性最基本的特征。那么，这个 DNA 复制过程究竟是如何进行的呢？如果这个过程出现了差错，会发生什么呢？

如果要形象地理解 DNA 的复制过程，可以把它比作用旧拉链制造新拉链。不过，DNA 的结构要比拉链复杂得多：拉链只有一种类型的齿，而 DNA 却有四种不同的碱基，分别是 A（腺嘌呤，Adenine)、G（鸟嘌呤，Guanine)、C（胞嘧啶，Cytosine）和 T（胸腺嘧啶，Thymine）。

在大多数生物细胞中，DNA 复制首先需要将旧 DNA 双链的一个片段解旋开来（即解开双螺旋结构）。接着，在周围的核苷酸溶液中，每条解开的单链都会寻找到与其碱基（base）完全互补的配对物，从而重新形成完整的双链结构。这里遵循着严格的配对规则：腺嘌呤（A）只能与胸腺嘌呤（T）配对，胞嘧啶（C）只能与鸟嘌呤（G）配对（反之亦然）。因此，当细胞发现 DNA 链上有一个 A 碱基时，就会自动知道需要在这里配对一个 T 碱基。

这种 C 与 G、A 与 T 之间的专一性配对，就像锁和钥匙的配合机制一样精确。就像有些钥匙可能过大或过小而无法打开某些锁，而有些钥匙却能完美匹配一样。正是因为这四种碱基只能按特定方式配对，当解开的 DNA 单链暴露在含有游离碱基的核苷酸溶液中时，这些游离碱基就会准确地找到各自的互补位置并按正确顺序排列。比如，溶液中的游离 A 碱基通常不会与 C 或 G 结合。通过这种精确的配对机制，DNA 就完成了自身的复制过程。

自然界如何运用冗余（redundancy）的概念来提高产生准确复制的机会是一件非常有趣的事情。三个碱基（base）的组合，比如 ATC，每个组合都对应一个氨基酸。由于存在 A、C、T 和 G 四种碱基，我们可以得到四乘以四乘以四，即 64 种可能的三碱基组合 —— 这意味着理论上可以编码 64 种不同的氨基酸。然而，令人惊讶的是，实际上总共只有 20 种氨基酸（这 20 种构成了所有生命物质，包括我们的身体），这就导致了多个碱基三联体（triplet）会对应同一种氨基酸。例如，在自然界中，ATT、ATC、ATG 都能编码氨基酸异亮氨酸，而 AGA 和 AGG 则都编码精氨酸。

那么为什么会有这样的重复编码呢？主要好处和之前一样，是为了减少 DNA 复制过程中的错误。举个例子，如果在复制过程中，ATT 被错误地复制成了 ATC（最后一个字母出错了），这个错误在新的生物体中实际上并不会造成任何影响，因为 ATT 和 ATC 这两个三联体都编码相同的蛋白质异亮氨酸。这展现了自然界独特的智慧，通过这种冗余机制来确保生命信息的准确传递！

这种冗余现象 —— 用多个不同的碱基序列（base sequences）来编码同一种氨基酸（amino acid）—— 是一种标准的纠错机制。这种机制不仅在现代计算机和通信系统中普遍存在，更令人惊叹的是，它在人类所有的认知过程中也同样发挥着作用。

举个有趣的例子，几年前我收到过这样一封电邮：「据剑桥大学的研究显示，阅读一个单词时字母的顺序并不重要，只要首字母和尾字母的位置正确就行。中间的字母可以完全打乱，你依然能毫无障碍地阅读。这是因为人类大脑并不是在一个个识别字母，而是把整个单词当作一个整体来理解。」这个例子很好地说明了，不只是遗传密码（genetic code）中存在冗余，英语语言中也充满了冗余信息。正是有了这种冗余特性，我的学生才能看懂我在他们作业上潦草书写的各种评语。

既然我的这本书和遗传密码一样都是用来传递信息的，你可能会想知道它的信息冗余率是多少。通过第一章介绍的香农熵（Shannon's entropy）计算，我得到的答案是 7.4。这意味着我原本用 200 页写的内容，理论上用 25 页就能概括我想表达的所有内容。当然，我觉得读者们可能并不会喜欢这样精简的版本（或者他们会？）。

你可能还记得第三章的一个重要观点：使用通用的离散二进制数字语言的重要性。有趣的是，自然界似乎也为信息存储发展出了一种离散编码方式。但与我们之前讨论的布尔逻辑（Boolean logic）使用两种状态来编码不同，自然界选择了四种碱基来编码遗传信息。那么问题来了：既然香农已经证明两种状态就足以完成所有信息编码工作，为什么自然界要选择使用四种碱基呢？这是生物学中的一个关键问题，我们将在第九章中对这个迷人的话题进行深入探讨。

另一个值得探讨的重要问题是，为什么在自然界中形成了数字编码（digital encoding）而不是模拟编码（analogue encoding）的方式？换句话说，为什么进化的结果是选择了仅有四种碱基，而不是无限连续的碱基组合？虽然我们目前还没有一个完全严密的数学证明，但我们有充分的理由解释为什么数字编码比模拟形式更具优势。这主要体现在两个方面：首先是降低了信息处理所需的能量开销，其次是提高了信息处理的稳定性。下面我们就来详细分析这两点。

首先，让我们来探讨信息处理所需的能量消耗。假设我们有 10 个比特（bit）的信息，每个比特都有 0 和 1 两种可能的状态。如果翻转一个比特需要消耗一个单位的能量，这就是最基本的信息处理方式。那么，翻转所有 10 个比特就需要消耗 10 个单位的能量。

但在模拟（analog）环境中执行类似操作时，所需的能量会大得多。为了在模拟环境中表示这 10 个比特的信息，我们需要 1024 个不同的状态（即 2^10）。这是所有模拟系统的基本特征。要理解这一点，我们可以这样想：在模拟编码（analog encoding）中，从最低到最高能量状态之间需要 1024 个能量单位，而不是数字编码（digital encoding）中的仅仅 10 个单位。这种巨大的能量开销，本质上是因为模拟编码必须进行整体处理，而不能像数字信号那样逐位处理。

DNA 中信息离散化的第二个优势是稳定性。在模拟编码（analogue encoding）中，根据其定义特性，因为它是连续的，所以不同状态比数字编码（digital encoding）更难区分，因此也更难发现错误。这就解释了为什么即使自然界在三十亿年前最初采用的是模拟和数字混合的编码方式，而现在却完全演变成了数字化的形式。这是因为模拟编码需要消耗大量能量，且对错误特别敏感，所以数字化编码成为了必然的选择。

有趣的是，不仅是自然界选择了数字化这条路。现代所有的技术都建立在数字化原理之上，这使得它们有更强的抗错能力。如今，大多数错误都来自于人为因素或者人与机器的交互过程。而在每个微芯片内部进行的信息处理才是真正可靠的部分。这一切传达给我们一个明确的信息：在处理信息的时候，「离散化才是明智之选」。

尽管大自然在复制生命遗传指令时表现出了惊人的精密和智慧，但仍然会出现一些无法纠正的错误，我们称之为突变（mutations）。平均来说，DNA 复制的错误率大约是百万分之一。这个比例乍看似乎很低，但是想想看：如果错误率达到百万分之十，理论上可能会导致你的下一个孩子天生就是黑猩猩。大多数突变对生物体来说都是有害的，因为它们会降低生物的存活能力，不过有些突变却可能产生更优秀的性状，使生物更好地适应环境。这正是我们所说的「自然选择」（natural selection）进化过程的基础。

在讨论生命演化时，一个长期以来的普遍观点认为，在不断的繁衍过程中，后代的复杂程度必然会逐渐降低。但是现在我们知道，这个说法其实并不正确，而且我们也明白了其中的原因。事实上，生物的复杂性在整体上是呈上升趋势的。推动这种复杂性增加的关键因素就是自然选择（natural selection），这个概念最早由 Charles Darwin 明确提出。自然选择的本质是一个筛选过程：那些最适应环境的特征会得以保存，这使得生物与其所处环境之间形成了紧密的联系。举个例子，如果整个世界被水淹没，显然只有那些能够在水下生存的生物才能存活下来。而且，不管洪水过后发生什么，未来生命的遗传信息都将来自这些成功存活下来的生物的 DNA。

从另一个角度来看，我们的 DNA（脱氧核糖核酸）可以被视作一本记录着祖先们所经历的环境变化的历史书。正如 Dawkins 所说，这就像是一本「记载生命演化历程的基因书籍」。纵观历史，环境始终在发生着大大小小的改变，这就导致了一个显而易见的结果：随着时间推移，成功遗传下来的 DNA 会变得越来越复杂，因为它不断积累着应对环境变化所需的信息。

值得注意的是，如果没有随机性，这个进化过程就无法进行。正是 DNA 中发生的随机变化，为自然选择提供了原材料：那些能让生物更好适应环境的突变会被保留下来。这揭示了一个普遍规律：有意义的信息只能在随机事件和自然选择的共同作用下产生，缺少任何一方都无法实现这一过程。

既然我们认为已经理解了生物信息在持久性方面的本质，那么利用这些知识来自行纠正基因错误将会很有用。这可能消除致命性疾病和残疾，并有望提升我们整体的生活质量。这就是一个充满争议的研究领域，即基因工程（genetic engineering）。

在进行基因人工改造时面临的一个主要挑战是，基因与个体特征之间几乎不存在「一对一」的对应关系。没有任何基因是单独决定我们眼睛颜色的。同一组基因往往还会决定我们的其他特征，例如身高、体态等。因此，如果想要通过基因技术改变婴儿的眼睛颜色，在修改相关基因时很可能会同时影响孩子的其他特征。

在我们完全理解所有这些基因之间的相互关系之前，要可靠地运用基因工程（genetic engineering）来实现任何积极的目标都是非常困难的。我们还不确定自己能在多大程度上理解这些相互关系，从而确保我们对基因的改变都是安全无害的。虽然很多人都希望这一天终将到来，但大自然可能根本不会因为我们的乐观就改变其运行规律。就我个人而言，我还不认为这构成了一个严重的道德困境，原因很简单：我们目前还没有足够的知识来可靠地、成功地对人类特定基因特征进行精确改造。不过，基因工程却成为了一个极具争议的话题，它在科学家和非科学家之间（甚至在科学家群体内部）引发的分歧，是任何其他科技议题都无法相比的。

在结束这个 DNA 的故事之前，我想补充一点：Watson 和 Crick 的发现差点被另一位奥地利物理学家 Erwin Schrödinger 领先。Schrödinger 是量子力学的先驱之一（我们将在第 9 章再次提到他），在他为物理学带来革命性突破后，将研究重心转向了生物学。在生物学领域，Schrödinger 同样展现出了非凡的洞察力，他在 Watson 和 Crick 之前约 10 年就几乎推导出了遗传信息复制的具体机制。他对生物复制过程的描述几乎完全正确，只是他认为负责复制的遗传物质必定是晶体（因为晶体具有稳定且周期性的结构，似乎非常适合携带和处理信息）。而 Watson 和 Crick 后来证明，这个物质不是晶体，而是一种核酸（脱氧核糖核酸，DNA）。

有意思的是，关于生命信息编码的研究还远未到终点。科学家们发现，生物体内某些类似晶体的结构可能也参与了编码过程，并携带着额外的生物学信息。这意味着生命的遗传信息可能不仅仅存储在 DNA 中 —— 换句话说，DNA 可能并非是生命繁衍所需全部生物信息的唯一载体。这一推测的依据来自一个有趣的现象：无论是简单的细菌、青蛙，还是复杂的人类，所有生物体内的基因数量都惊人地相似。仅仅约 20,000 个基因就能构建出各种生命形式！但显然，人类比细菌要复杂得多，这说明生命的复杂性不可能仅仅由 DNA 决定。物理学家 Erwin Schrödinger 关于生命信息存储的观点也许部分正确，但对于这种复杂性差异的来源，科学界还存在着多种不同的解释。

DNA 的起源是另一个值得探讨的问题。它是否由更简单的结构演化而来？追溯到 Schrödinger 关于生物生命在晶体中编码的研究，我们发现晶体的结构比 DNA 简单得多，而且在自然界中更容易自发形成。因此，相比其他理论，晶体或许能为我们理解 DNA 的演化过程提供一些线索。尽管我们知道晶体能够自发形成和复制，但生命复制所需的信息是如何从晶体转移到 DNA 的，这个问题仍然存在。这并非新观点，早在约 40 年前就由 Alexander Graham Cairns-Smith 提出，至今仍是生物学界热议的研究课题。

这些问题都很引人入胜，但在这里我们不需要追究它们的具体答案。真正重要的是一个核心观点，这个观点在生物学未来的发展中也必将长存：信息是生命的本质，而生命能否持续存在，取决于我们如何理解这一信息的本质。

正如我们在第一章探讨生命起源的问题时所看到的，最根本的疑问是：为什么会有信息的存在？在研究生命如何复制时，我们发现需要四个关键组成部分：负责合成蛋白质的「分子机器」M、能够复制 DNA 的「复制工厂」X、扮演调控角色的酶类 C，以及携带遗传信息的 DNA 信息库 I。这个体系看起来已经十分复杂了，那么这种复杂性究竟是如何从零开始的呢？

让我们换个角度来思考「生物信息如何从零产生」这个问题，也许能帮助我们更好地理解它。在科学界有一个称为人择原理（Anthropic Principle）的概念，用来回答「为什么宇宙会是现在这个样子？」这个问题。它的回答是：「正是因为宇宙是现在这个样子，我们才得以存在并观察它。」乍听之下，这似乎是个循环论证。我们会在最后几章深入探讨这个引人深思的话题。

### Key points

Any self-replicating entity needs to have the following components: a universal constructing machine, M, a controller, C, a copier, X, and the set of instructions required to construct these three, I. With these it is possible to then create an entity that self-replicates indefinitely.

A macromolecule responsible for storing the instructions, I, in living systems is called DNA. DNA has four bases: A, C, T, and G. When DNA replicates inside our cells, each base has a specific pairing partner.

There is huge redundancy in how bases are combined to form amino acid chains. This is a form of error correction.

The digital encoding mechanism of DNA ensures that the message gets propagated with high fidelity.

Random mutations aided by natural selection necessarily lead to an increase in complexity of life.

The process of creating biological information from no prior biological information is another example of the question of creation ex nihilo. Natural selection does not tell us where biological information comes from – it just gives us a framework of how it propagates.

关键要点：

任何能够自我复制的系统都必须具备这四个基本组成部分：一个通用的构建机器 M、一个控制系统 C、一个复制装置 X，以及包含了构建这三个部分所需全部信息的指令集 I。只有具备了这些要素，才能构建出一个能够持续自我复制的系统。

在生命系统中，DNA 是一种负责存储遗传信息的大分子。DNA 由四种碱基（base）组成：A、C、T 和 G。当 DNA 在我们的细胞内进行复制时，每种碱基都有其特定的互补配对对象。

碱基组合形成氨基酸链的方式具有高度的冗余性，这实际上是一种生物体的错误校正机制。

DNA 的数字化编码机制确保了遗传信息能够高度准确地代代相传。

在自然选择的作用下，随机突变必然会导致生命形式变得越来越复杂。

从零开始创造生物信息的过程，是另一个关于「从无到有的创造」的典型例子。自然选择理论只能解释生物信息如何传播，却无法告诉我们生物信息最初是从何而来。

## 网页版翻译

数字浪漫：生命是一个四字单词

列侬和麦卡特尼在演绎这句古老谚语「生命在继续」时可谓切中要害。这不仅是他们 1968 年《白色专辑》中最朗朗上口的曲调之一，也是我们所知最简单却最深邃的哲理之一。在其看似平淡的表象下，蕴含着一个具有根本重要性的信息。地球上的生命几乎与地球同时诞生，这个事实清楚地展示了生命的顽强性。考虑到我们每天都能看到 individual 生命体是多么脆弱，这一点就显得更加令人惊叹。因此，在科学界经常出现这样一个问题:"如此不完美的事物怎么能存活如此之久？」这是生物学最大的谜团之一。

有趣的是，在解答这个问题上取得重大突破的不是生物学家，而是一位数学家。我们之前提到过这位数学家，当时他建议 Shannon 使用「熵（entropy)」这个词来定义其信息函数 - 没错，John von Neumann（冯·诺依曼）再次登场。冯·诺依曼证明了如何用不完美的组件构建近乎完美的系统。这听起来有点违反直觉，不是吗？人们本能地会认为，要构建完美的事物，每个组成部分也必须完美无缺。这正是生物系统面临的核心问题之一。但是一个既没有任何实验数据支持、又对生物学知之甚少的数学家，是如何能够如此透彻地理解生命的呢？让我来告诉你答案。

事实上，冯·诺依曼提出了一个更本质的问题："我们如何能用短暂的组件构建持久的系统？」设想我们想要留下一条信息，让它能存续一万年，使所有未来的世代都能从中获益。比如说，假设我发现了通往永恒幸福的密钥。(当然，这只是假设，我和你一样对此一无所知，尽管有时我觉得答案或许藏在雪茄和单一麦芽威士忌之间。）假如我希望我的子孙后代能从我获得的智慧中受益，那么在我对未来一无所知的情况下，要如何确保这条信息能够流传这么长时间呢？

读到这里，你可能已经察觉到这个问题与第一章中 Shannon 讨论的 Alice 和 Bob 之间的通信问题有着异曲同工之妙。现在，让我们把角色稍作调换：不是 Alice，而是我在编码这个至关重要的信息；不是电话线，而是时间本身成为了传输的通道；而接收信息的也不再是 Bob，而是我们希望与之对话的未来世代。这个例子很好地展示了我们可以如何更广泛地理解通信渠道及其使用者的概念。再举一个例子：你可以把我（这本书的作者）视为 Alice，把这本书视为通道（通过它我传达着我的思想），而你这位读者，就是接收我思想的 Bob。

你可能会说，让信息长期保存并不是什么难事。也许我们可以制作一个坚不可摧的保险箱，把信息锁在里面，然后静待时光流转。然而，这样的信息只能在保险箱完好无损的情况下得以保存。自然灾害、人为灾难、疫病大流行或其他因素都可能影响信息的存续时间。埃及人曾经认为金字塔坚不可摧，但事实是它们在过去六千年中已经严重风化，在未来的几千年里甚至可能完全消失。实际上，就连我们赖以生存的星球，在不远的将来也可能遭受各种威胁而毁灭（这些威胁或许不仅来自外部，相信你也意识到了这一点）。考虑到这些不确定因素，我们该如何才能确保信息能以较高的概率传递给后代呢？

这个跨越世代传递信息的难题，正是冯·诺依曼提出其理论时所思考的核心问题。这个问题同时也为生命提供了一个绝妙的隐喻，因为生命的根本目标就在于延续。从前面的讨论可以看出，仅仅依靠一个坚固但静止不变的结构来储存信息是一种过于简单的思维。这样的静态结构往往难以适应不断变化的环境。

我们真正需要的是一个能够应对环境变化，并且具备处理各种挑战能力的系统。它需要具备适应性，能够移动，在面临威胁时懂得规避障碍和危险。同时，它还必须能够克服自身的脆弱性。但问题在于，无论这个承载信息的载体是由什么材料制成，它都逃不过寿命有限的命运。就像没有永不耗尽的电池，也没有永不停歇的心跳一样。

为了更好地理解这一点，让我们做一个思想实验：假设从理论上来说，我们能够制造出一个可以永远运行的机器人（配备永不耗尽的电池，永不生锈的零件等）来承载信息。更进一步，我们假设这个机器人不仅能够应对复杂的环境变化，还能在受损时进行自我修复。这看起来似乎是个完美的解决方案，但现实中存在两个问题：首先，我们要制造出这样一个完美设备的可能性有多大（正如前面提到的，现实中没有永恒运转的动力系统）；其次，就算我们竭尽全力制造出了这样的机器人，它迟早也会遇到无法预料和处理的情况。毕竟，再完美的机器人也不可能预见和应对所有可能的损害和环境变化。

那么，与其只制造一个机器人，为什么不制造一百个或一千个呢？这看起来是个不错的主意，因为它能延长信息的保存时间。但问题是，我们最终仍然只有有限数量的机器人，而且这个数量只会随着时间推移而递减。终有一天，每一个复制品都将面临失效的命运。

然而，在这里一个更富创造性的想法浮现出来。你可能会想："为什么不构建一个既强大又能自我复制的机器人呢？」这样，它就能将信息传递给它创造的每个复制品，而这些复制品又能将信息传递给它们的复制品，形成一个生生不息的传递链。通过这种方式，每一代都具备携带信息和自我复制的能力，我们就有可能实现信息的永久保存。

这正是冯·诺依曼在他关于自我复制自动机（self-reproducing automata）的论文中深入探讨的内容。他最重要的贡献在于证明了即使使用不完美的组件，也能实现机器人的自我复制。这一突破性的观点在冯·诺依曼同时代的科学界引起了很大的争议。

关于自我复制存在两个主要的质疑。首先，用冯·诺依曼的话说："如果一个自动机（automaton）具有构建另一个自动机的能力，那么从制造者到被制造物之间必然会出现复杂度（complexity）的降低。换句话说，如果 A 能够制造 B，那么 A 就必须以某种方式包含了 B 的完整描述。从这个角度来看，这个过程似乎必然会出现某种简化的趋势，即当一个自动机制造另一个时，复杂度会降低。」这是一个极具挑战性的质疑，因为它似乎与我们的日常观察完全相悖。在现实中，生命似乎正在变得越来越复杂，而不是简化为更简单的生物形式。

值得注意的是，冯·诺依曼所说的自动机的「复杂度」与其信息含量有着密切的关系：一个自动机越复杂，就需要越多的信息位（比特）来准确描述它。在这里顺便提一下，关于如何讨论生物复杂性，学界已经提出了许多不同的方法，我们将在本书后面的章节中详细介绍其中的一些理论。

第二个主要的质疑与前一个相关，但它不仅仅违背经验，更是似乎违背了逻辑本身。设想一下：如果 A 要制造出机器 B，那么 B 的信息必须以某种方式预先存在于 A 中。而如果 B 要继续复制出 C，那就意味着 C 的信息必须存在于 B 中。但是因为 B 的信息本身就存在于 A 中，那么 C 的信息也必然要存在于 A 中。这个逻辑让你感到困惑了吗？本质上，这意味着如果我们想确保某个事物能够存续数百代，似乎就必须在最初的版本中就储存所有后续版本的信息。如果我们将这个推论扩展到无限代，就会发现这在资源上完全不可能实现，因为 A 将需要储存无限量的信息。

这第二个质疑让我想起了一个关于康复院病人的生动比喻：这位病人给自己设定了一个任务，要画一幅包含所有细节的完整世界图画。他必须从某个地方开始，于是决定先画下他所在康复院的花园。一段时间后，正当他对自己描绘的花园感到十分满意时，他突然意识到画作中少了什么至关重要的东西 —— 他自己！尽管他已经极其细致地描绘了整个花园的每一个细节，却忘记了把自己这个画家画进去。

为了修正这个问题，他将自己也画进了画中，但很快又发现画作依然不完整。因为真正的他还是在画作之外！虽然他画出了一个正在作画的自己，但那个正在画「画中画家」的真实画家，仍然不在画中。随着他越深入思考这个问题，一个可怕的认知让他惊恐不已：这幅画永远也画不完，更糟糕的是，这项任务本质上就无法完成（精神状态的困扰并不影响一个人的聪明才智，这一点从我们系里那些极具天赋的同事身上就可见一斑）。这位画家不经意间陷入了数学家所说的「无限回归」（infinite regression）困境。

回到第一章，我们看到 John Wheeler 和 David Deutsch 在探讨终极自然法则时也遇到了类似的问题。当他们试图构想一个能够解释所有其他法则的终极法则时，他们希望这个法则是完备的，不需要任何外部法则来解释它自身。这与画家的困境如出一辙：无论这个法则多么巧妙地描述现实，它都无法创造出一个包含万物的完整描述，因为它永远无法完整地包含自身。

在尝试解决生命体再造这个问题时，大自然似乎也面临着同样的挑战。每个生物体都需要存储其后代的信息，而这些后代又需要存储它们各自后代的信息，如此循环往复。那么，我们要如何打破这个看似无限的循环呢？难道我们所认知的生命形式从逻辑上来说真的是不可能存在的吗？

冯·诺依曼深刻理解这些质疑，这正是他撰写这篇论文的初衷 —— 他要驳斥这些质疑，并证明生命的复制在逻辑上和实践中都是可行的，而且这个过程不会导致新一代的复杂性降低。考虑到冯·诺依曼并未接受过正式的生物学训练，仅凭借抽象思维的力量就能获得这样开创性的成果，这着实令人叹服。

冯·诺依曼的关键洞见基于一个重要发现：在这个过程中，不同组件之间存在着清晰的功能分离。假设我们有一条包含了制造某个物体（比如房子、汽车或冰箱等）所有复制指令的信息，那么要实现这个物体的永久传承，我们本质上只需要两个部分：一个用于复制指令的复制器，以及一个根据这些指令构建复制品的构造器。这就是冯·诺依曼方法的核心。为了更全面地理解这一理论，让我们详细说明完整的自我复制（self-replication）所需的所有组件。尽管接下来的内容可能较为深奥，但掌握这个开创性成果的细节绝对值得我们投入时间和精力。

让我们设 M 为一个通用构造机器（universal constructor machine），它具有这样的特性：只要给予适当的指令 I，它就能构造出任何其他对象。与此同时，我们设 X 为一个专门的复制机器，它能够复制指令 I，并将这些指令植入到 M 所构造的相关对象中。

让我们用一个具体的例子来说明：假设 M 是工厂里的一台构造机器，能够制造各种复杂的机器。如果我们输入制造汽车的指令，它就会生产一辆汽车；如果输入制造椅子的指令，它就会制造一把椅子。更有趣的是，如果我们输入制造它自身的指令（也就是最初用来制造它的那套指令），它也能照做，最终我们就会得到两台完全相同的机器。

从商业角度来看，这是一个极具潜力的设计。想象一下，如果你需要在工厂的其他区域使用这台机器。但要注意的是，仅仅把机器搬过去是不够的 —— 除非你同时提供相关指令的副本（用于制造椅子、桌子或机器本身的指令），否则这台机器就无法发挥作用。这就好比你需要用数字复印系统来复制操作手册，并确保它随机器一起送达目的地。

整个过程中，输入指令 I 和复制这些指令的过程必须由一个控制机制 C（control mechanism）来管理。回到工厂的例子，这个控制机制就相当于一位管理员，他负责向机器输入指令，并在生产新机器时确保操作手册得到妥善复制和配套。

从企业管理的角度来看，这是一个激动人心的突破，因为我们可以通过在工厂内部甚至在公司的所有工厂中复制这种构造器，从而大幅提升生产效率。想象一下：如果我们拥有 1000 台构造机器，我们就能同时生产 1000 个产品，无论是椅子、汽车还是其他零部件。而且当需要进一步扩大产能时，我们随时可以指示构造器复制自己。当然，要操作新增的机器，我们还需要配备新的控制器（除非能说服原有的控制器同时管理两台机器）。

不过这里出现了一个新问题：为了实现真正的自我复制，新的控制器是否已经掌握了如何使用机器并制作指令副本的知识？答案很可能是否定的，他们需要接受相关培训。因此，从通用构造器的角度来看，我们还需要包含两个额外的指令：如何制造控制器的指令，以及如何制造控制器将使用的复印设备的指令。如果我们能够实现这一点，那么只要有充足的能源和原材料，这台机器就能够根据需求无限期地进行自我复制。让我们来系统地总结这个无限构造系统（infinite construction system）的运作机制。

让我们回顾一下起始条件：系统最初包含（i）通用构造器 M；(ii）复印机 X；以及（iii）控制器 C。要实现一个完整的自我复制系统，我们不仅需要有构造 M 的完整指令集，还需要包含如何构造控制器 C 和复印机 X 的指令。因此，当我们把 M、C、X 这三个组件，以及构造它们的完整指令集组合在一起时，就形成了一个完整的自我复制实体，我们用 E 来表示这个整体。

在这个系统中，复制过程是这样进行的：控制器首先读取关于如何构造 M、C 和 X 的指令，并将这些指令输入给 M。随后，M 开始工作，构造出三个复制品：它自己的副本 M'、新的控制器 C' 以及新的复印机 X'。同时，控制器利用现有的复印机 X 制作出指令集的副本 I'。这样，我们就得到了 M'、C'、X' 和 I' 的组合，这个新的组合构成了一个完整的、可独立运作的自我复制实体 E'。

需要注意的是，这个设计巧妙地避开了我们之前提到的逻辑困境 —— 即第一个实体需要包含所有后续实体的信息才能实现信息的永久传递。这个突破的关键在于：构造一个实体，它包含了通用构造器、复印机和控制器，以及构造这三个组件的完整指令。这个过程完全符合布尔逻辑（Boolean logic）的规则，既合理又可行。这就是冯·诺依曼论证的精髓所在。

虽然冯·诺依曼的这个理论最初是针对自我复制这个具体问题提出的，但它实际上为我们提供了一个避免无限回归的普遍解决方案。那么，这个方案是否也可以用来解决其他类似的问题呢？比如 Deutsch 和 Wheeler 提出的「无法则的法则」（law without law）问题？宇宙中的万物是否真的能够以同样的方式从虚无中产生？这些引人入胜的问题，我们将在本书第三部分中深入探讨。

在冯·诺依曼的论证中，虽然无限期自我复制在逻辑上是可行的，但我们仍然面临着一个重大的实践挑战：我们之前假设了这个过程的每个环节都是完美无缺的。但如果在某个阶段出现了错误会怎样呢？如果自我复制过程受到干扰（比如，控制器漏掉了一页指令，复印机耗材耗尽，或者机器发生故障）会发生什么？这就引出了下一个问题：基于这些受损指令生产出来的后续 M 复制品会有什么结果？表面上看，答案似乎很明显 —— 整个过程必须停止，无法继续进行。

然而，这正是冯·诺依曼提出的第二个关键洞见。他证明了这种停滞并非必然：即使使用有缺陷的组件，也可以维持一个持续的复制过程。这是通过引入冗余机制（redundancy）实现的 —— 我们创建大量的实体 E 的副本。虽然其中一些副本可能因为质量问题而无法通过控制器的验证测试，但其他合格的副本仍然可以继续在系统中传播和运作。

值得注意的是，在现实世界中，这种质量检验并不总是由控制器来执行，有时也会由外部因素（比如环境）来完成。我们可以用冯·诺依曼的自我复制框架来理解商业特许经营（franchise）的扩张过程。让我们以星巴克这个成功的特许经营案例为例。第一家星巴克于 20 世纪 70 年代在西雅图开业。由于其在咖啡销售方面取得了显著的成功，业务扩张就成为了水到渠成的选择。当时面临的主要挑战是如何复制让原始星巴克获得成功的商业模式。如今，这种复制已经精确到了令人惊叹的程度：在全球 30 多个国家中存在着 16,000 家几乎完全相同的星巴克分店。无论你是在北京还是在雅典遇到一家星巴克，都能确信它的装修风格和咖啡口味与你在纽约街角看到的那家几乎毫无二致。

然而，也有一些星巴克门店不得不关闭，原因是它们未能准确复制核心经营模式 —— 或许是因为他们的咖啡口味偏离了原始配方，又或者是店铺的整体氛围无法吸引顾客消费。更有趣的是，即使是那些完美复制原始模式的店铺也有倒闭的。这类情况往往是由环境因素造成的（比如当地消费者对连锁品牌持谨慎态度，更青睐本土咖啡馆，或者该地区整体咖啡消费习惯正在改变）。举例来说，仅在 2008 年，就有 600 家门店因经济衰退等环境因素而关闭。这表明，即使完美的复制也不能确保经营的成功。

最成功的企业都深谙此道。它们能够通过优秀的管理团队或专业的外部顾问持续不断地收集和处理来自市场环境的信息。这些信息不仅会反馈到企业的经营策略中，还会用来提升企业持续创新的内部能力。与生物系统不同，企业可以在极短的时间内调整其经营策略。这种调整和适应的速度在商业领域被称为敏捷性（agility）。从这个角度来看，敏捷性是企业基业长青的关键所在。

惠普公司（HP）由两位电气工程师 William Hewlett 和 David Packard 于 1939 年在帕洛阿尔托（Palo Alto）的一个车库中创立。公司最初专注于生产电子测试设备，如示波器和温度计。随后，伴随着电子技术的蓬勃发展，他们将业务扩展到半导体设备和计算器领域。到了 20 世纪 60 年代末，他们敏锐地发现了小型计算机市场的细分机会（niche market），并及时把握住了这个机遇。如今，惠普已成为个人计算、图像处理与打印、企业存储和软件等领域的领军企业之一。随着经营环境的变化（比如市场需求的转变和信息时代的到来），惠普总能及时调整其发展战略，成功把握每一波技术创新浪潮。

当然，冯·诺依曼提出这个理论的初衷并非解释企业的成功之道，也不是为了指导工厂提高生产效率。他的真正目标是证明我们可以建造自我复制的机器人，用于开拓和探索其他星球上的生命。有趣的是，他可能做梦也想不到，在他提出这个理论之前的 30 亿年，大自然就已经完美地解决了这个问题！

在 20 世纪 30 年代和 40 年代，生物学领域最重要的研究目标（可以说是「圣杯"）就是寻找人类细胞中那个携带复制信息的神秘结构。正如冯·诺依曼的理论所预言的那样，这个结构应该负责决定我们后代的所有特征：发色、眼睛颜色、身高等 —— 它不仅包含了生物体自身运作的完整指令，还包含了如何制造后代的全部信息。在这场探索竞赛中，汇聚了众多杰出科学家，包括 James Watson、Francis Crick、Rosalind Franklin、Maurice Wilkins、Erwin Schrödinger 和 Linus Pauling 等。这是一个激动人心的重要时期，人类正站在重大突破的门槛上 —— 我们即将更深入地理解「我们是谁」以及「我们从何而来」这样的根本问题。

最终，这场竞赛被一位曾经的鸟类学学生和一位前物理学家赢得。James Watson 和 Francis Crick（在其他上述科学家的帮助下）发现了这个生物学遗传密码的主要载体 —— 一种被称为 DNA（脱氧核糖核酸，deoxyribonucleic acid）的复杂酸性分子。DNA 包含了如何产生与携带该 DNA 的生物体相似后代的完整指令，这与冯·诺依曼理论中输入到通用构造机器 M 的指令集 I 极其相似。大自然对这个分子的保护机制令人惊叹：不仅每个生物个体都携带 DNA 分子，而且几乎每个生物体的每个细胞中都包含着完整的 DNA。

更令人震惊的是，在适当的环境条件下，每一个 DNA 分子都具备复制整个生物体的完整信息。这正是冯·诺依曼所说的冗余机制的绝佳例证。正是由于这些开创性的发现，Watson、Crick 和 Wilkins 在 1962 年共同获得了生理学或医学诺贝尔奖。

回顾冯·诺依曼的通用构造器理论，我们知道一个完整的系统需要四个关键组件：通用构造器 M、复印机 X、控制器 C 和指令集 I。这些组件共同构成了自我复制实体 E。当我们将这个模型与生命系统对照时，会发现细胞本身就是一个完美的自我复制实体 E。在细胞内部，同样存在着四个使其能够自我复制的关键组件：

1. 蛋白质合成机器（作为构造器 M）

2. 生物纳米引擎（相当于复印机 X，负责复制遗传信息）

3. 酶类系统（作为控制器 C，负责调控纳米引擎的开关）

4. DNA 信息集（作为指令集 I）

需要说明的是，虽然这个整体框架已经获得了广泛认可，但很多具体细节仍然存在争议，比如生物纳米引擎的具体工作机制等问题仍在深入研究中。

我们可以说，DNA 在生命复制过程中扮演着核心角色，因为它储存了每个细胞运作和复制的完整指令。在这些指令的指导下，我们细胞中的构造机器合成氨基酸，这些氨基酸又进一步组成了构建我们身体所需的各种蛋白质和新细胞。虽然细胞复制是一个极其复杂的过程，但其基本原理与冯·诺依曼描述的模型惊人地相似。在创造新蛋白质的过程中，最关键的步骤是如何准确地将 DNA 信息从一个细胞复制到另一个细胞。我们特别关注细胞的信息携带能力，因为这是生命得以持续的最基本特征。那么，这个精密的复制过程是如何运作的呢？如果出现类似于复印机耗材耗尽或发生错误的情况，会发生什么？

要理解 DNA 的复制过程，我们可以把它比作制作新拉链的过程，即用一个现有的拉链作为模板。不过，DNA 的结构比拉链要复杂得多：普通拉链只有一种类型的齿，而 DNA 有四种不同的「齿」，用 A、G、C 和 T 来表示。这四个字母代表四种不同的碱基（bases)：腺嘌呤（adenine)、鸟嘌呤（guanine)、胞嘧啶（cytosine）和胸腺嘧啶（thymine）。

在大多数生物细胞中，DNA 复制的第一步是解旋（unwind）原有 DNA 双链的一部分。这个过程就像解开拉链一样。接着，复制机制会为每个解开的部分重建完整的双链结构，方法是在周围的核苷酸池中找到每个碱基的精确互补配对。这里的配对规则非常严格：碱基 A 只能与 T 配对，C 只能与 G 配对（反之亦然）。这意味着当复制机制在 DNA 链上识别到一个 A 碱基时，它会立即知道需要配对一个 T 碱基。

这种精确的配对机制就像是一个完美的锁和钥匙系统。就像某些钥匙可能对特定的锁来说或大或小而不合适，但正确的钥匙却能完美匹配一样，这四种碱基只能按照特定的方式配对。当解旋的 DNA 单链暴露在含有游离碱基的核苷酸池中时，这些游离碱基就会自然地找到它们的配对位置并按正确的顺序排列。例如，池中的游离 A 碱基通常不会与 C 或 G 结合，而是会精确地找到它的配对伙伴 T。这就是 DNA 分子复制的基本机制，一个精确而优雅的自然过程。

自然界在使用冗余机制来提高复制准确性方面展现出了惊人的智慧。在 DNA 中，每三个碱基的组合（称为密码子或三联体 triplet）对应着一个氨基酸。由于存在 A、C、T、G 四种碱基，那么三个位置的组合总数就是 4 × 4 × 4 = 64 种不同的密码子，这意味着理论上可以编码 64 种不同的氨基酸。然而，令人惊讶的是，自然界中实际只有 20 种氨基酸（这 20 种氨基酸构成了所有生命物质，包括我们的身体）。这就产生了一个有趣的现象：多个不同的密码子会对应同一种氨基酸。比如在自然界中，ATT、ATC 和 ATG 这三个不同的密码子都编码同一种氨基酸 —— 异亮氨酸（isoleucine），而 AGA 和 AGG 都编码另一种氨基酸 —— 精氨酸（arginine）。

这种看似冗余的编码机制有什么意义呢？它的主要优势在于可以最大限度地降低 DNA 复制过程中的错误风险。举个例子：如果在复制过程中，ATT 被错误地复制成了 ATC（即最后一个碱基发生了错误），这个错误甚至不会影响新生成的生物体，因为 ATT 和 ATC 这两个密码子都编码同一种蛋白质 —— 异亮氨酸。这体现了大自然在进化过程中形成的精妙设计。

这种冗余机制 —— 即多个不同的碱基序列编码同一种氨基酸 —— 是一种标准的错误纠正（error correction）方法。这种机制不仅在现代计算机和通信系统中得到广泛应用，更令人惊叹的是，它在人类的所有认知过程中也普遍存在。

举个有趣的例子，这是几年前我收到的一封电子邮件中的内容："据剑桥大学研究表明，单词中的字母顺序并不重要，只要首尾字母在正确位置即可。即使其余字母完全混乱，你仍然可以毫无障碍地阅读。这是因为人类大脑并不是逐个字母地阅读，而是将单词作为整体来识别。」原文是这样的："Aoccdrnig to rscheearch at Cmabrigde Uinervtisy...」[保留原文乱序英文]。这个例子生动地说明了英语语言中也存在着大量的信息冗余，就像遗传密码一样。正是由于这种冗余性，我的学生们才能够辨认出我在他们作业上潦草书写的评语。

既然我的这本书，就像遗传密码一样，其目的是传递信息，你可能会好奇它包含了多少冗余信息。使用第一章介绍的香农熵（Shannon entropy）计算，得出的结果是 7.4。这意味着理论上我本可以用 25 页而不是 200 页来概括所有要表达的内容。不过，我想读者们可能并不会喜欢这种过于精简的版本（或者，谁知道呢，也许你们会更喜欢？）。

你可能还记得第三章中提到的一个重要观点：使用离散二进制数字（discrete binary digits）作为通用语言的重要性。有趣的是，在这里我们发现自然界似乎也发展出了自己的离散信息编码方式。但与我们在布尔逻辑（Boolean logic）中看到的使用两种状态不同，自然界选择使用四种离散碱基。这就引出了一个有趣的问题：既然 Shannon 已经证明两个状态就足以完成所有的信息编码，为什么自然界要「多此一举」使用四个碱基呢？这是生物学中的一个核心问题，我们将在第 9 章中对此进行深入探讨并提出一些富有启发性的见解。

另一个同样重要的问题是：为什么自然界选择了数字编码而不是模拟（analogue）编码？换句话说，为什么自然界选择了仅使用四个离散碱基，而不是采用连续的、无限可能的编码方式？虽然我们还没有完全严密的数学证明，但我们有充分的理由证明为什么数字编码优于模拟形式。支持数字编码的主要原因有两个：首先是显著降低了信息处理所需的能量消耗，其次是大大提高了信息处理的稳定性。让我们详细分析这两点。

首先，让我们来分析信息处理的能量消耗问题。假设我们有 10 个比特（bit）的系统（即 10 个可以有 0 和 1 两种状态的基本单元），且翻转一个比特所需的能量为一个单位。这是最基本的信息处理操作。要翻转全部 10 个比特，自然需要 10 个单位的能量。

然而，如果要在模拟系统中完成同样的操作，所需的能量将会大得多。在模拟环境中，要表示等同于 10 个比特的信息，我们需要能够区分 1024 个不同的状态（即 2 的 10 次方，2¹⁰）。这是所有模拟系统的基本特征。为了帮助理解这一点，我们可以这样思考：在模拟编码中，系统的最低能量状态和最高能量状态之间需要有 1024 个能量级别，而不是数字编码中简单的 10 个状态。之所以会出现这种巨大的能量差异，是因为模拟编码在本质上必须对整个状态空间进行处理，而不能像数字编码那样进行逐位处理。

自然界选择在 DNA 中使用离散化（discretization）信息的第二个优势在于其稳定性。在模拟编码系统中，由于其连续性的本质特征，很难准确识别和纠正错误，因为各种状态之间的界限往往模糊不清。这就解释了为什么即使自然界在 30 亿年前可能起初采用了模拟和数字混合的编码方式，但最终还是演化出了纯数字的编码系统。考虑到模拟系统所需的巨大能量消耗，以及其对错误的高度敏感性，数字编码成为自然选择几乎是必然的结果。

这种趋势不仅仅体现在自然界中。现代技术的发展也完全基于数字原理，这使得系统具有更强的错误恢复能力。在当今的技术系统中，大多数错误都来源于人为因素或人机交互界面（human-machine interface），而不是系统本身的信息处理过程。在每个微处理器的核心运算单元中进行的底层信息处理都具有极高的可靠性。这传递了一个清晰的信息：在信息处理领域，"拥抱离散化，远离模糊化」是明智之选。

尽管自然界在复制遗传指令时表现出了惊人的谨慎和创造性，但仍有一些错误无法被纠正，这些错误就是我们所说的突变（mutation）。DNA 复制的平均错误率约为百万分之一。这个比例乍看似乎很小，但我们不能掉以轻心：即使是百万分之十的错误率也可能导致生物体发生显著的变异。大多数突变对生物体的生存是有害的，但有些突变可能会产生更适应环境的新特征。这种机制正是「自然选择」（natural selection）这一进化过程的基础。

让我们回顾之前提到的一个主要质疑：任何持续的复制过程都会导致后代复杂性的降低。现在我们知道这个说法是错误的，而且我们也理解了其中的原因。事实上，生物的复杂性在平均意义上是在增加的。这种复杂性增加的核心机制就是查尔斯·达尔文首次清晰阐述的自然选择过程。自然选择使生物与其环境建立关联，因为只有最适应环境的特征才能存活下来。举个例子：如果整个世界被水淹没，那么显然只有能在水下生存的生物才能存活下来。而在这之后，不管环境如何变化，能够传递下去的基因库（gene pool）也只能来自这些存活下来的生物的 DNA。

从这个角度来看，你的 DNA 可以被视为一部记录了祖先所经历的所有环境变化的历史档案（理查德·道金斯将其形象地称为「死亡之书」（Book of the Dead)）。历史上环境变化无处不在，这意味着任何得以传承的 DNA 只会变得越来越复杂，因为它不断积累着关于环境变化的信息。需要注意的是，如果没有随机性，这个进化过程就无法实现。正是 DNA 中的随机变异提供了生物多样性，而自然选择则从中筛选出那些更适应环境的变异。这反映了一个普遍的规律：有意义的信息必然产生于随机事件和确定性选择的相互作用之中，缺少任何一个要素都无法实现这个过程。

既然我们已经理解了生物信息持久性的本质，那么运用这些知识来主动纠正基因缺陷就成为了可能。这可能帮助我们消除一些致命的遗传疾病和残疾，从而提升人类的整体生活质量。这就是我们现在所说的基因工程（genetic engineering），一个充满希望但也饱受争议的研究领域。

人工基因改造面临的一个主要挑战是：基因与个体特征之间很少存在简单的一一对应关系（one-to-one correspondence）。例如，并不存在单独决定眼睛颜色的基因。同一组基因组（genome）通常会同时影响多个表型特征，如身高、体态等。这意味着，如果你试图通过基因改造来改变孩子的眼睛颜色，修改相关基因很可能会同时影响孩子的其他生理特征。

在我们完全理解基因之间复杂的相互作用网络之前，要可靠地将基因工程应用于特定目的仍然面临巨大挑战。我们目前还无法确定在多大程度上能够理解这些复杂的基因互作关系，也无法保证我们的干预总能产生预期的效果。大自然的复杂性可能远超出我们现在的认知。从个人角度来看，我认为现阶段这还不构成紧迫的伦理困境，主要原因是我们尚未掌握足够的知识来精确且可靠地改变人类的遗传特征。然而不可否认，基因工程在科学界内外都引发了前所未有的广泛讨论和争议。

关于 DNA 发现的故事还有一个有趣的后记：另一位奥地利物理学家 Erwin Schrödinger 差点就抢在 Watson 和 Crick 之前作出这一重大发现。Schrödinger 是量子力学（quantum mechanics）的开创者之一（我们将在第 9 章再次提到他）。在为物理学领域做出革命性贡献后，他将研究重心转向了生物学。在这个新领域，Schrödinger 同样展现出了非凡的洞察力：他在 Watson 和 Crick 之前约 10 年就几乎完整地推导出了生物信息复制的机制。他对复制过程的描述几乎在所有细节上都是正确的，唯一的偏差在于他认为复制过程中的信息载体必定是晶体（因为晶体具有稳定的周期性结构，看似非常适合储存和处理信息）。后来 Watson 和 Crick 证明这个载体不是晶体，而是一种酸性分子 ——DNA。

不过，这个故事似乎还没有完全结束。有证据表明，生物信息的编码过程可能部分依赖于某种类似晶体的结构，这种结构本身可能携带着额外的信息。换句话说，DNA 可能不是唯一的生物信息载体 —— 它可能并不包含复制生命所需的全部信息。这一推测基于一个有趣的观察：细菌、青蛙、人类等所有生物体内的基因数量竟然惊人地相似。仅仅约 20,000 个基因就足以构建任何一种生命形式。但显然，人类比细菌要复杂得多，这表明生物的复杂性不可能仅仅由 DNA 决定。也许 Schrödinger 的晶体理论部分是正确的，当然，还有许多其他理论也在试图解释这种复杂性差异的来源。

这还引发了另一个根本性问题：DNA 本身是从何而来的？是否存在某种更简单的结构，使得 DNA 能够从中演化而来？回顾 Schrödinger 关于晶体编码生物信息的研究，我们发现晶体不仅结构比 DNA 简单得多，而且在自然界中能够自发形成和生长。因此，在众多解释生命起源的理论中，晶体理论可能为理解 DNA 的演化过程提供了独特的视角。即便我们假设晶体确实能够自发形成和自我复制，仍然存在一个关键问题：生命复制所需的复杂信息是如何从晶体转移到 DNA 的？这个观点最初由 Alexander Graham Cairns-Smith 在约 40 年前提出，至今仍是生物学研究中一个备受关注的前沿课题。

虽然这些问题都十分引人入胜，但在此我们不必深究其具体答案。真正重要的是一个更本质的认识，这个认识在生物学未来的发展中必将经受住考验：信息是生命的本质，而生命的持久性取决于我们如何理解和解释这种信息的本质。

正如我们在探讨「从无到有的创造」（creation ex nihilo）的章节中所看到的，最根本的问题是：为什么会有信息的存在？在研究生命的复制过程中，我们发现需要四个核心组件：蛋白质合成机器 M、DNA 复制机制 X、起调控作用的酶类 C，以及 DNA 信息集 I。这个系统已经表现出惊人的复杂性，那么这种复杂性最初是如何从虚无中产生的呢？

也许我们可以换一个角度来思考「从无到有的生物信息」这个问题？在科学中有一个称为人类原理（anthropic principle）的概念，它用「如果宇宙不是现在这个样子，我们就不可能存在于此来观察它」来回答「为什么宇宙是现在这个样子」的问题。但这个回答似乎并不能让人满意。我们将在本书的最后几章继续深入探讨这个深刻的问题。

关键要点

要实现自我复制，一个系统必须具备以下核心组件：

- 通用构造机器 M

- 控制器 C

- 复制机制 X

- 构造以上三个组件所需的完整指令集 I

只有具备了这些要素，才能创建出一个能够持续自我复制的实体。

