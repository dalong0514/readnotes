Vlatko Vedral.(2018).2024111Decoding-Reality.Oxford University Press => 0301Back to Basics: Bits and Pieces

PART ONE

## 0301Back to Basics: Bits and Pieces

The concept of information is so ubiquitous nowadays that it is simply unavoidable. It has revolutionized the way we perceive the world, and for someone not to know that we live in the information age would make you wonder where they've been for the last 30 years. In this information age we are no longer grappling with steam engines or locomotives; we are now grappling with understanding and improving our information processing abilities – to develop faster computers, more efficient ways to communicate across ever vaster distances, more balanced financial markets, and more efficient societies. A common misconception is that the information age is just technological. Well let me tell you once and for all that it is not! The information age at its heart is about affecting and better understanding just about any process Nature throws at us: physical, biological, sociological, whatever you name it – nothing escapes.

Even though many would accept that we live in the age of information, surprisingly the concept of information itself is still often not well understood. In order to see why this is so, it's perhaps worth reflecting a little on the age that preceded it, the industrial age. Central concepts within the industrial age, which can be said to have begun in the early eighteenth century in the north of England, were work and heat. People have, to date, found these concepts and their applicability much more intuitive and easier to grasp than the equivalent role information plays in the information age. In the industrial age, the useful application of work and heat was largely evident through the resulting machinery, the type of engineering, buildings, ships, trains, etc. It was easy to point your finger and say ‘look, this is a sign of the industrial age'.

In Leeds, for example, as I used to take my usual walk down Foundry Street in the area called Holbeck, traces of the industrial revolution were still quite evident. John Marshall's Temple Mills and Matthew Murray's Round Foundry are particularly striking examples; grand imposing buildings demanding respect and appreciation for the hundreds of people who worked in squalid conditions and around the clock to ensure that the country remained well fed, clothed, or transported. Murray is a typical example of an eighteenth-century entrepreneurial industrialist, making his fortune in Leeds by producing locomotives, steam engines, textile machines, and several other machines that all did work by exploiting heat energy.

The process of using energy in the form of heat to produce as much useful work as possible is very simple and intuitive. Feeding hot coal into an engine which then produces steam (heat) to drive the wheels of a train (work) is a process that seems quite easy to grasp from beginning to end. But why can't we say something similar about the information age? To me the concept of information is more widely applicable and even easier to understand than work or heat, so why does it still cause confusion? The answer is that I don't know, but, trust me, by the end of this book, if I have done my job properly, you will find it as easy to identify the role of information in its many guises as Murray found work or heat. As an added bonus, you will find information far more fundamental and widely applicable.

So what do we actually mean when we talk about information? While information is not a difficult concept to understand, it can sometimes lead to confusion given the number of contexts that the word is used in. To make matters worse there is simply not enough accessible material on information around. There has recently been a flurry of books, but many of these are overly technical and do not cater for the non-scientific reader. Being involved with several ‘science-communication' initiatives, when asked for accessible introductions to information, much to my frustration my recommendations have been somewhat limited. Fortunately, after 15 years of trying to explain information to myself (and unwittingly to most people I meet) I thought, what better way to burn some calories and midnight oil than to rise to the challenge?

There are two main reasons why the concept of information has not been made more accessible. One is simply the fact that there are many ways in which we could define it. For example, do we define information as a quantity which we can use to do something useful or could we still call it information even if it wasn't of any use to us? Is information objective or is it subjective? For example, would the same message or piece of news carry the same information for two different people? Is information inherently human or can animals also process information? Going even beyond this, is it a good thing to have a lot of information and to be able to process it quickly or can too much information drown you? These questions all add some colour and vigour to the challenge of achieving an agreed and acceptable definition of information.

The second trouble with information is that, once defined in a rigorous manner, it is measured in a way that is not easy to convey without mathematics. You may be very surprised to hear that even scientists balk at the thought of yet another equation. As a result, experts and non-experts alike have so far been avoiding popularizing this concept in a detailed and precise way. Even Stephen Hawking, when writing his bestselling A Brief History of Time, was famously advised by his editor that every equation he used would halve the number of copies sold.

In spite of all these challenges, there is an accepted and clear definition of information which is also objective, consistent, and widely applicable. By stripping away all irrelevant details we can distil the essence of what information means within a couple of pages.

Unsurprisingly, we find the basis of our modern concept of information in Ancient Greece. The Ancient Greeks laid the groundwork for its definition when they suggested that the information content of an event somehow depends only on how probable this event really is. Philosophers like Aristotle reasoned that the more surprised we are by an event the more information the event carries. By this logic, having a clear sunny autumn day in England would be a very surprising event, whilst experiencing drizzle randomly throughout this period would not shock anyone. This is because it is very likely, that is, the probability is high, that it will rain in England at any given instant of time. From this we can conclude that less likely events, the ones for which the probability of happening is very small, are those that surprise us more and therefore are the ones that carry more information.

Following this logic, we conclude that information has to be inversely proportional to probability, i.e. events with smaller probability carry more information. In this way, information is reduced to only probabilities and in turn probabilities can be given objective meaning independent of human interpretation or anything else (meaning that whilst you may not like the fact that it rains a lot in England, there is simply nothing you can do to change its probability of occurrence).

There is one more important property of information and together with objectivity it leads to the modern measure of information. Suppose that we are looking at the information in two subsequent but independent events. For example, there is a certain probability that I will go out tonight, say 70%, and also there is a certain probability, say 60%, that I will receive a call on my mobile (this can happen independently of whether I am in or out of my house). So, what is the probability that I will go out and receive a call while I am out? Since both events have to happen for this to materialize, the overall chance of this happening is the product of the two probabilities. This comes out to 42% (‘70 divided by 100' multiplied by ‘60 divided by 100').

How about the amount of information in these two independent events? If you are already surprised a little by an event and then another event occurs independently, your total surprise will increase, depending only on the probability of the new event. So the total information in two events should be the sum of the two individual amounts of information, given that they are independent events. Therefore, the formula for information must be a function such that the information of the product of two probabilities is the sum of the information contained in the individual events. Are you still with me? You'll get it, I promise. Amazingly enough it can be shown that there is only one such function that does this job and this function is the logarithm (log for short).

Logarithms were invented by the Scottish mathematician John Napier and have been extremely useful in simplifying long multiplications. The famous French eighteenth-century mathematician, Pierre Simon de Laplace, said about them ‘ . . . by shortening the labours, [they] doubled the life of the astronomer.' In those days astronomers needed to calculate trajectories of planets and other objects by hand, and this often resulted in whole reams of paper full of calculations. Of course, multiplication is much easier now since we all use calculators and computers, which paradoxically makes logarithms appear outdated and intimidating.

So in summary the modern definition of information is exactly this: the information content of an event is proportional to the log of its inverse probability of occurrence:

This definition is very powerful because we only need the presence of two conditions to be able to talk about information. One is the existence of events (something needs to be happening), and two is being able to calculate the probabilities of events happening. This is a very minimal requirement which can be recognized in just about anything that we see around us. In biology, for example, an event could be a genetic modification stimulated by the environment. In economics, on the other hand, an event could be a fall in a share price. In quantum physics, an event could be the emission of light by a laser when switched on. No matter what the event is, you can apply information theory to it. This is why I will be able to argue that information underlies every process we see in Nature.

Now that we have our definition of information, which let's face it is not that complicated, we can look at one of the earliest applications of information to solve real-world problems. The story starts with an American engineer, Claude Shannon, back in 1940s New Jersey, at the world-renowned Bell Laboratories.

Even before Shannon, the Bell Laboratories already had a fear-some reputation as a centre of excellence. The Bell Labs were a facility at the height of its powers in the mid-1900s, which went on to win a phenomenal number of awards (including six Nobel Prizes) for contributions to science and engineering and development of a wide range of revolutionary technologies (think: radio astronomy, the transistor, the laser, the UNIX operating system, and the C programming language).

It is no surprise that a major source of pride for the Bell Labs, named after the distinguished inventor Alexander Graham Bell, has always been telephone communications. This is the area Shannon worked in and his role was to investigate how to make communication more secure. For example, when ‘Alice' phones ‘Bob', she relies on people like Shannon to make sure no unauthorized person can intercept or listen in on her phone-call. At the time this was a hugely pertinent issue, given that America was entering World War II and secrecy had become of paramount importance. After several months of research, Shannon managed to come up with conditions which guaranteed that any communication can be made completely secure against unauthorized eavesdropping. (Interestingly his theory of cryptography is what forms the foundation of modern information security – every time you draw money from an ATM, or make a purchase over the Internet, you have Shannon to thank.)

Through this challenge Shannon became interested in how much people could communicate with one another through a physical system (e.g. a telephone network) in the first place. Shannon thought that perhaps rather than sending only one telephone call down a wire maybe we could send two or three or even maybe more. Of course this wasn't entirely an academic pursuit. When you work for one of the world's largest corporations anything that allows you to squeeze more profit out of your existing infrastructure is ultimately going to be good for your career. Anyway by analysing this question in more detail, Shannon came up with the rigorous definition of information that we discussed earlier (that information is proportional to the log of the probability of an event).

He summarized his findings in a ground-breaking paper in 1948. This paper gave birth to the field of modern information theory and changed the telecommunications landscape forever. The theory that he developed is eponymously referred to as Shannon's information theory.

Shannon imagined two users of a communication channel, Alice and Bob, using a phone line to talk to each other. One thing that Shannon realized was that, in order to analyse the information exchanged between Alice and Bob, he had to be as objective as possible. Shannon didn't care if Alice told Bob ‘I love you' or ‘I hate you', because from his perspective these two messages have exactly the same length and, ultimately, will earn Bell Labs the same amount of money. Human emotion, as we discussed, is not an objective property of the message, so Shannon discarded it; neither is the specific human language, so this went too. Bell Labs should be making profit no matter whether Alice and Bob are communicating in English, Spanish, or Swahili. The amount of information, in other words, should not depend on the way we choose to express it, but must have a more fundamental representation. Shannon found that the fundamental representation he was looking for had already been developed a century earlier by an English primary school teacher, George Boole.

Boole, whilst working on his grand theory of the Laws of Thought, published in 1854, reduced all human thought to just manipulations of zeros and ones. Boole's book began as follows: ‘The design of the following treatise is to investigate the fundamental laws of those operations of the mind by which reasoning is performed; to give expression to them in the symbolic language of a Calculus, and upon this foundation to establish the science of Logic and construct its method'. He showed that all such algebraic manipulations that you could want to do can be done just using two numbers, zero and one. A digit that is either a zero or a one is called a binary digit, or a ‘bit' for short, and Shannon used the concept of bits to develop his information theory.

As a side-note, interestingly the lack of zero was one of the limitations which prevented the Ancient Greeks from developing a full information theory – zeros just did not exist in Ancient Greece, as it never occurred to them that ‘nothing' deserves to be labelled by a number. Zero was, in fact, invented by the Indians, sometime before the birth of Christ, and the Indians communicated this knowledge to the Persians and Arabs in the middle ages, who in turn passed it onto the Europeans. The Europeans, armed with the zero, and some tricks learnt from the Ancient Greeks, now had a more flexible numbering system than the cumbersome Roman numerals. This numbering system was paramount to the progress made in science and mathematics, leading us eventually to the Renaissance, which in turn takes us to the present day. The story of the number zero is entirely fascinating within its own right and is really a topic befitting of an entire book.

Let us return to Shannon's task of optimizing communication between Alice and Bob. Armed with the Boolean universal alphabet, Alice could encode the message ‘I love you' into the symbol ‘1', while the message ‘I hate you' could be encoded into the symbol ‘0'. All we need now is to know the probability with which Alice will send ‘0' and the probability with which she will send ‘1'. In other words what is the probability that she loves Bob, and what is the probability that she does not?

Let us say that Bob is quite certain, say with 90% probability, that Alice will send him a ‘0', indicating that she hates him. Imagine now that he picks up the phone and hears a ‘1' sent to him via the communication channel. He would be very surprised, given that he attributed a low probability to it (only 10%) and thus this message carries more information. (Of course Alice and Bob do not actually talk in zeros and ones. Alice says ‘I love you' or ‘I hate you' and it is a device on either side of the phone line that encodes this information into bits and then decodes the bits into the original message of either ‘I love you' or ‘I hate you'.)

This framework can easily be extended to incorporate more complex messages such as ‘Let us meet in front of Nelson's Column in Trafalgar Square in London'. This could be encoded into a string of bits, like the following one: 00110010101000. Naturally we would like to use as few zeros and ones as possible per message as this would make more efficient use of the phone line (i.e. we could pack and send more messages down the channel). The general principle that Shannon deduced is that the less likely messages need to be encoded into longer strings and more likely messages into shorter strings of bits. The rationale behind this is that the messages that we communicate very frequently should be short; otherwise we needlessly waste the phone line capacity. It seems pretty obvious now, doesn't it?

If we consider language as a communication channel, this channel has evolved naturally into a more optimal state. Words we use most, such as ‘the', ‘of', ‘and', ‘to', are very short and this is because they have a high probability of occurring. Words that we are least likely to use, in contrast, remain very long as they have a low probability of occurring. In this way we can work out how efficient the English language is in comparison to German, French, or Swahili by seeing how many letters it takes to communicate the most commonly used words and phrases. Interestingly, George Zipf, in 1949, whilst independently analysing languages, came up with a similar argument and found that the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, which occurs twice as often as the fourth most frequent word, and so on.

Shannon similarly reasoned that the optimal encoding that maximizes the channel capacity (and the profit for Bell Labs) is to make the length of the message proportional to the ‘log of the inverse of the probability of the event occurring'. So the same information measure that we argued for turns out to quantify optimal channel capacity of any communication channel.

There has been a massive amount of work on extending Shannon's information theory directly or indirectly in a variety of disciplines. I jumped on the bandwagon in the late 1990s with my doctoral thesis, which translated how Shannon's information theory could be applied to quantum mechanics. I showed that the basic tenets of Shannon's information theory survive and have much to tell us about our latest model of physics. There is more on this in the second part of this book.

When I finished my thesis, my friends and colleagues gave me as a memento a framed picture of Shannon with all their signatures on the back of it. They knew how much Shannon's work had influenced me and thought that his picture would be an appropriate gift – given that I had spent more time with him than with any of them. In it, Shannon looked like a very thoughtful and distinguished scientist, doing science to better the world around him and to satisfy his own thirst for knowledge. It was a very nice thought.

I want to close this chapter with an amusing story. Shannon did not call his quantity information; he called it entropy. What we have so far been introducing as Shannon's information is, in fact, known as Shannon's entropy in the community of engineers, mathematicians, computer scientists, and physicists.

The word entropy appeared once Shannon had derived his ‘log of the inverse probability' formula and he approached John von Neumann, a great contemporary Hungarian-born American mathematician, asking for advice on how to name his newly invented quantity. Von Neumann suggested the word ‘entropy' to Shannon and an urban myth is that von Neumann did this to simply give Shannon an edge in all scientific debates, since no one really knows what entropy is! Von Neumann is well known for his witty remarks, which makes the urban myth plausible. However the real reason is that Shannon's measure already existed in physics under the name of entropy. The concept of entropy in physics had been developed by the German physicist Rudolf Clausius, some hundred years before Shannon.

Physical entropy has at first sight nothing to do with communications and channel capacity, but it is by no means an accident that the two have the same form. This will be the key to our discussion of the Second Law of thermodynamics and will also offer us insights into economic and social phenomena.

In summary, in order to solve his problem of optimizing channel capacity and to derive his information theory, Shannon stood on the shoulders of many other giants (to borrow Isaac Newton's famous phrase). The giants include the Ancient Greeks, George Boole, John Napier, and John von Neumann. There are no true loners in the world of knowledge – no scientific Clint Eastwoods. But this does not detract at all from Shannon's monumental achievement. Shannon demonstrated intellect, awareness, and drive to piece various ideas together and produce of one the greatest discoveries of the twentieth century.

回归本源：信息的基石

如今，信息这个概念已经无处不在，变得不可或缺。它彻底改变了我们认知世界的方式。如果有人不知道我们生活在信息时代，你会不禁想问他们在过去 30 年里都去了哪里。在这个信息时代，我们不再与蒸汽机或机车打交道；我们现在正在致力于理解和提升我们的信息处理能力 —— 开发更快速的计算机，实现更远距离的高效通信，构建更平衡的金融市场，以及打造更高效的社会。有人误认为信息时代仅仅关乎技术，这种想法是错误的。事实上，信息时代的核心是关于如何影响和更好地理解自然界中的各种过程：无论是物理的、生物的还是社会学的 —— 没有任何领域能够独善其身。

虽然很多人都承认我们生活在信息时代，但令人惊讶的是，信息本身的概念却常常没有被很好地理解。要理解这一点，也许我们应该回顾一下前一个时代 —— 工业时代。工业时代始于 18 世纪初的英格兰北部，其核心概念是功（work）和热能（heat）。迄今为止，人们发现这些概念及其应用要比信息时代中信息所扮演的角色更直观，更容易理解。在工业时代，功和热能的实际应用主要体现在机械、工程、建筑、轮船、火车等方面。人们可以轻易地指着这些并说：「看，这就是工业时代的标志。」

让我们以英国利兹（Leeds）为例。当我沿着霍尔贝克（Holbeck）区的铸造街（Foundry Street）散步时，工业革命的印记依然清晰可见。John Marshall 的圣殿工厂和 Matthew Murray 的圆形铸造厂就是最具代表性的例子。这些雄伟的建筑不仅令人肃然起敬，更让人想起那些在恶劣环境中昼夜不停工作的数百名工人，正是他们确保了这个国家的人民能够得到充足的食物、衣物和交通保障。Murray 是 18 世纪典型的企业家工业家，他通过生产机车、蒸汽机、纺织机以及其他利用热能转化为动力的机器在利兹积累了财富。

将热能转化为最大化有用功的过程非常简单直观。例如，将煤炭投入引擎产生蒸汽（热能），继而驱动火车车轮（机械功），这个过程从始至终都很容易理解。然而，为什么我们不能用同样的方式来理解信息时代呢？在我看来，信息的概念比工作或热能有着更广泛的适用性，甚至更容易理解，那么为什么它仍然让人感到困惑呢？坦白说，我也不知道确切的答案。不过我可以保证，当你读完本书时，如果我完成了我的任务，你将会发现理解信息在各种形式中的作用就像 Murray 理解功和热能一样简单。更重要的是，你会发现信息是一个更加基础且应用更为广泛的概念。

那么，当我们谈论信息时，我们究竟在说什么？虽然信息并不是一个难以理解的概念，但由于这个词在不同场合下有着不同的含义，有时会让人感到困惑。更令人遗憾的是，市面上缺乏通俗易懂的信息科学入门材料。最近虽然出版了一些相关书籍，但大多过于专业，对非科研背景的读者来说并不友好。作为科学传播项目的参与者，当有人向我询问适合入门的信息科学读物时，我常常感到无奈，因为能够推荐的内容实在太少。经过 15 年来不断地思考信息的本质（期间也不经意间向遇到的许多人解释这个概念），我觉得是时候接受这个挑战，写一本通俗易懂的信息科学入门书了。

信息概念难以普及主要有两个原因。首先是信息可以从多个角度来定义。比如，我们是应该将信息定义为一个具有实用价值的量，还是说即使没有实际用途也可以称之为信息？信息是客观存在的还是主观的？例如，同样的消息或新闻对不同的人而言是否包含相同的信息量？信息是否是人类独有的特征，还是说动物也能处理信息？更深层次地说，拥有并能够快速处理大量信息是一件好事，还是说信息过载反而会带来困扰？这些问题都凸显出要达成一个普遍认可的信息定义并非易事。

关于信息的第二个难点在于，一旦我们严格地定义它，如果不借助数学就很难解释它的测量方法。你可能会惊讶地发现，即使是科学家们也会对新的数学公式感到头疼。正因如此，无论是专家还是普通读者，都倾向于回避用详实而精确的方式来普及这个概念。这种现象非常普遍，以至于当史蒂芬·霍金（Stephen Hawking）在写作他的畅销书《时间简史》时，他的编辑就曾经半开玩笑地说，书中每多一个数学公式，销量就会减少一半。

尽管存在这些挑战，我们还是发展出了一个被广泛接受的信息定义，它既客观又具有一致性，而且应用范围广泛。通过去除非本质的细节，我们可以用简短的篇幅来阐述信息的核心含义。

不出所料，我们现代信息概念的根源可以追溯到古希腊。古希腊人在定义信息时提出了一个基础性的观点：一个事件所包含的信息量，在某种程度上仅取决于这个事件发生的实际概率（probability）。例如，亚里士多德等哲学家认为，一个事件越令人惊讶，它所携带的信息量就越大。按照这个逻辑，在英格兰的秋天遇到一个阳光明媚的晴天会是一个非常令人惊讶的事件，而随时遇到毛毛细雨则不会让任何人感到意外。这是因为在英格兰的任何时候下雨的概率都很高。由此我们可以得出结论：越是不太可能发生的事件（即发生概率很小的事件），就越能引起我们的惊讶，因此也就携带着越多的信息。

根据这个逻辑，我们可以得出结论：信息量必须与事件的概率成反比（inversely proportional），也就是说，发生概率越小的事件携带的信息量就越大。这样，信息就可以简化为仅与概率相关的概念，而概率本身具有客观性，不依赖于人类的理解或其他因素。（举个例子，虽然你可能不喜欢英格兰经常下雨这个事实，但这并不会改变下雨的客观概率。）

信息还有另一个重要特性，这个特性与其客观性一起，构成了现代信息度量的基础。让我们来看两个相继发生但相互独立（independent）的事件中的信息量。比如说，我今晚出门的概率是 70%，同时我接到手机来电的概率是 60%（无论我在家还是外出都可能接到电话）。那么，我既出门又在外面接到电话的概率是多少呢？因为这两个事件都必须发生，所以总概率等于两个事件概率的乘积。具体计算就是：70% × 60% = 42%（即 0.7 乘以 0.6）。

那么，这两个独立事件所包含的信息量是如何计算的呢？如果一个事件已经让你感到意外，而后又独立发生了另一个事件，你对整体事件的认知意外程度会增加，这种增加仅取决于新事件的概率。因此，对于两个独立事件，它们的总信息量应该等于各自信息量的总和。这个概念虽然初看有点抽象，但很快你就会理解。更有趣的是，数学家们发现只有一个函数能够满足这个要求，这个函数就是对数（logarithm，简称 log）。

对数是由苏格兰数学家 John Napier 发明的，它在简化长数字乘法计算方面发挥了重要作用。18 世纪著名的法国数学家 Pierre-Simon de Laplace 曾经这样评价对数：「通过减少计算工作量，对数让天文学家的工作效率提高了一倍。」在那个年代，天文学家需要手工计算行星和其他天体的运行轨道，这往往需要大量的计算过程并记录在纸上。当然，如今有了计算器和计算机，乘法运算变得简单多了，这反而让对数看起来不那么重要，甚至让人望而生畏。

所以总结来说，信息的现代定义是：一个事件的信息量与其发生概率的倒数的对数成正比（proportional to the logarithm of its inverse probability）：

这个定义之所以如此强大，是因为我们只需要满足两个条件就可以讨论信息：第一，存在可观察的事件（即有事情发生）；第二，能够计算这些事件发生的概率。这些要求非常基础，我们可以在周围的任何现象中找到它们。例如在生物学领域，一个事件可能是环境刺激导致的基因变异（genetic modification）。在经济学领域，可能是股票价格的波动。在量子物理学（quantum physics）中，可能是激光器开启时发出的光。无论是什么类型的事件，我们都可以用信息理论来分析它。这就是为什么我可以说，信息是自然界中所有过程的基础。

既然我们已经理解了这个相对简单的信息定义，让我们来看看信息理论最早应用于解决现实问题的例子之一。这个故事要从美国工程师 Claude Shannon 说起，地点是在 1940 年代新泽西州的贝尔实验室（Bell Labs）。

在 Shannon 到来之前，贝尔实验室就已经是一个令人敬仰的科研重地。这个在 20 世纪中期达到鼎盛时期的研究机构，不仅获得了众多奖项（包括 6 项诺贝尔奖），还在科学和工程领域做出了重大贡献，开发了一系列革命性的技术。这些成就包括：射电天文学（radio astronomy)、晶体管（transistor)、激光（laser)、UNIX 操作系统和 C 编程语言。

不出所料，作为以杰出发明家 Alexander Graham Bell 命名的研究机构，贝尔实验室最引以为豪的成就之一就是在电话通信领域的贡献。Shannon 就在这个领域工作，主要研究如何提高通信安全性。例如，当「Alice」给「Bob」打电话时（在信息安全领域，通常用 Alice 和 Bob 作为通信双方的代称），她需要依靠像 Shannon 这样的专家来确保没有未经授权的人能够截获或窃听通话内容。当时这个研究特别重要，因为美国正要参与第二次世界大战，通信安全变得至关重要。经过几个月的研究，Shannon 成功地提出了确保通信安全的理论基础，可以有效防止未经授权的窃听。（有趣的是，他的密码学（cryptography）理论为现代信息安全奠定了基础 —— 每当你使用 ATM 取款或在网上购物时，都在受益于他的这项研究成果。）

从这项研究开始，Shannon 产生了一个更深层次的兴趣：人们究竟能在多大程度上通过物理系统（如电话网络）进行有效通信？他思考是否可能通过一根电话线同时传输多个通话，而不仅仅是一个。这个研究当然不仅仅出于学术兴趣。作为世界最大公司之一的员工，任何能够提高现有基础设施使用效率的发现都将有助于职业发展。通过深入分析这个问题，Shannon 最终提出了我们前面讨论过的信息的严格定义（即信息量与事件概率的对数成反比）。

Shannon 在 1948 年发表了一篇具有里程碑意义的论文，总结了他的研究成果。这篇论文不仅开创了现代信息理论（information theory）的新领域，还彻底改变了通信技术的发展方向。他建立的理论被工程师、数学家、计算机科学家和物理学家称为香农信息理论（Shannon's information theory）。

Shannon 设想了两个使用电话线通话的用户，Alice 和 Bob。他意识到，要分析 Alice 和 Bob 之间交换的信息，就必须采用完全客观的方法。对 Shannon 来说，Alice 告诉 Bob「我爱你」还是「我恨你」并不重要，因为从技术角度看，这两条消息的长度完全相同，对贝尔实验室来说产生的收益也是一样的。正如我们之前讨论过的，人类的情感不是消息的客观特征，因此 Shannon 没有考虑这个因素；同样，具体使用什么语言也不重要。无论 Alice 和 Bob 使用英语、西班牙语还是斯瓦希里语交流，对贝尔实验室的业务都没有影响。换句话说，信息量不应该取决于我们如何表达它，而应该有一个更基础的表示方法。Shannon 发现，他所寻找的这种基础表示方法早在一个世纪前就被英国小学教师 George Boole 开发出来了。

1854 年，Boole 在研究思维规律的过程中发表了一部开创性著作，他将人类的所有思维过程简化为对 0 和 1 的操作。这本书的开篇写道：「本论文旨在研究推理过程中大脑运作的基本法则；用符号演算的语言来表达这些法则，并在此基础上建立逻辑科学及其方法论。」他证明了所有的逻辑运算都可以仅用 0 和 1 两个数字来完成。这种只能是 0 或 1 的数字被称为二进制数字（binary digit），简称为「比特」（bit）。Shannon 正是基于比特的概念发展出了他的信息理论。

值得一提的是，古希腊之所以未能发展出完整的信息理论，其中一个重要原因是他们没有「零」这个概念。在古希腊，人们从未想过要用一个数字来表示「无」这个概念。实际上，「零」是由印度人在公元前发明的。这个概念在中世纪时期经由印度传到波斯和阿拉伯，最后传入欧洲。当欧洲人掌握了「零」这个概念，并将其与从古希腊继承的数学知识相结合后，他们创造出了一个比罗马数字更为灵活的计数系统。这个新的计数系统对科学和数学的发展起到了关键作用，推动了文艺复兴的到来，并最终影响到今天。「零」的历史如此引人入胜，完全可以写成一部专著来详细讲述。

让我们回到 Shannon 关于优化通信系统的研究。借助 Boole 的二进制系统（binary system），我们可以将 Alice 的消息「我爱你」编码为「1」，将「我恨你」编码为「0」。要完成这个编码系统，我们需要知道的是 Alice 发送「0」和「1」各自的概率。用这个比喻来说，就是需要知道她爱或不爱 Bob 的概率。

假设 Bob 认为有 90% 的概率 Alice 会发送「0」（表示她不喜欢他）。在这种情况下，如果他通过通信系统收到了「1」，这个消息会让他非常惊讶，因为这个事件的概率很低（只有 10%），因此它携带了更多的信息量。（当然，在实际的通信系统中，Alice 和 Bob 并不是直接用 0 和 1 交谈。他们说的是普通的语言，比如「我爱你」或「我恨你」，而是通信设备（communication device）在传输过程中将这些消息编码（encode）成二进制数字，然后在接收端再解码（decode）成原始消息。）

这个编码框架可以轻松扩展到处理更复杂的消息，比如「让我们在伦敦特拉法加广场的纳尔逊纪念柱前见面」。这样的消息可以被编码成一串二进制数字，例如：00110010101000。显然，我们希望每条消息使用尽可能少的二进制位，这样可以提高通信线路的使用效率（即在同样的信道中传输更多的信息）。Shannon 提出的基本原则是：出现概率较低的消息应该使用较长的编码，而出现概率较高的消息则使用较短的编码。这个原则的依据很简单：我们经常需要传输的信息应该保持简短，否则就会造成通信带宽的浪费。现在这个道理看起来很简单明了，对吧？

如果我们把语言本身看作一个通信系统，会发现它已经通过长期发展优化到了一个相对高效的状态。例如，我们最常用的词（如英语中的「the」、「of」、「and」、「to」）都很短，这是因为它们出现的频率很高。相反，那些不常用的词往往比较长，因为它们出现的概率较低。通过分析最常用词和短语所需的字母数量，我们可以比较不同语言（如英语、德语、法语或斯瓦希里语）的信息传递效率。有趣的是，George Zipf 在 1949 年独立研究语言时发现了类似的规律，这就是著名的齐普夫定律（Zipf's law)：任何词的出现频率与其在频率排名的倒数成正比。也就是说，最常用词的出现频率大约是第二常用词的两倍，而第二常用词的频率又是第四常用词的两倍，以此类推。

Shannon 通过类似的推理得出结论：要最大化通信信道（communication channel）的容量（也就是提高贝尔实验室的效益），最优的编码方案应该让消息长度与「事件发生概率的倒数的对数」成正比。这样，我们前面讨论的信息度量方式恰好可以用来量化任何通信系统的最优传输容量。

在此之后，许多学科领域的研究者们都在直接或间接地拓展 Shannon 的信息理论。我也在 1990 年代末期通过博士论文加入了这个研究队伍，研究如何将 Shannon 的信息理论应用到量子力学（quantum mechanics）领域。我的研究证明，Shannon 信息理论的基本原理在量子世界中仍然适用，而且能帮助我们更好地理解最新的物理学模型。关于这方面的详细内容，我会在本书的第二部分进行介绍。

当我完成博士论文时，我的朋友和同事们送了我一份特殊的礼物：一张装裱好的 Shannon 照片，照片背面有他们所有人的签名。他们深知 Shannon 的研究对我的影响，认为这份礼物再合适不过了 —— 毕竟我研究 Shannon 的工作的时间比与他们相处的时间都要长。照片中的 Shannon 看起来是一位严谨而卓越的科学家，为了推动科学进步和追求知识而孜孜不倦。这确实是一份富有心意的礼物。

我想用一个有趣的故事来结束本章。Shannon 最初并没有把他发现的这个量称为「信息」，而是称之为「熵」（entropy）。事实上，我们前面一直称作 Shannon 信息的概念，在工程师、数学家、计算机科学家和物理学家的圈子里通常被称为 Shannon 熵（Shannon entropy）。

这个命名有一个有趣的背景。当 Shannon 推导出他的「概率倒数的对数」公式后，他向当时著名的匈牙利裔美国数学家 John von Neumann 请教应该如何命名这个新发现的量。von Neumann 建议他使用「熵」这个词。有一个广为流传的趣闻说，von Neumann 之所以这样建议，是因为「没人真正理解熵是什么」，这样 Shannon 在任何科学讨论中都会占据优势！考虑到 von Neumann 以机智过人著称，这个说法听起来似乎很有道理。但实际原因是，Shannon 发现的这个数学量在物理学中早就以「熵」的形式存在了。这个物理学概念是由德国物理学家 Rudolf Clausius 在大约一个世纪前首次提出的。

物理学中的熵看似与通信和信道容量毫无关系，但它们具有相同的数学形式绝非巧合。这个联系不仅是理解热力学第二定律（Second Law of thermodynamics）的关键，还能帮助我们深入理解经济和社会现象。

总的来说，Shannon 在解决通信容量优化问题和建立信息理论的过程中，借鉴了许多前人的智慧（用牛顿的话说，站在巨人的肩膀上）。这些「巨人」包括古希腊的思想家们、George Boole、John Napier 和 John von Neumann。在科学发展史上，从来没有真正的独创者 —— 每一项重大发现都建立在前人的基础之上。但这并不影响 Shannon 成就的伟大。他以非凡的智慧、敏锐的洞察力和坚定的决心，将不同的思想理论融会贯通，最终实现了 20 世纪最重要的科学突破之一。

### Key points

The concept of information is fundamental. It can be given an objective meaning.

In the information age many of our problems are related to directly optimizing information as opposed to work and heat.

The basic unit of information is the bit, a digit whose value is either zero or one.

Information is a measure of how surprising something is. Unlikely, low probability events contain a high degree of information. Likewise, high probability events contain very little information.

If two parties want to communicate efficiently then their messages to one another should be encoded according to Shannon's prescription: unlikely messages should be encoded with many zeros and ones; frequent messages should be given a shorter code.

关键要点：

信息是一个基础性的概念，它具有客观的度量标准。

在信息时代，我们面临的许多挑战都直接关系到信息的优化，而不是传统工业时代的功和热。

信息的基本单位是比特（bit），即只能取值为 0 或 1 的二进制数字。

信息量与事件的意外程度相关：低概率事件包含更多信息，而高概率事件包含较少信息。

在有效的通信系统中，消息的编码应遵循 Shannon 的原理：低概率消息使用较长的二进制编码，高频率消息使用较短的编码。